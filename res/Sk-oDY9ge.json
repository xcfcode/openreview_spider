{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488567108380, "tcdate": 1478297497299, "number": 505, "id": "Sk-oDY9ge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Sk-oDY9ge", "signatures": ["~Adriana_Romero1"], "readers": ["everyone"], "content": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396630775, "tcdate": 1486396630775, "number": 1, "id": "rk1kTGUdx", "invitation": "ICLR.cc/2017/conference/-/paper505/acceptance", "forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers agreed that the paper proposed a solid and original contribution that was evaluated well. The authors would be encouraged to improve the presentation and provide a more precise mathematical presentation, as discussed by reviewer 1.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396632770, "id": "ICLR.cc/2017/conference/-/paper505/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396632770}}}, {"tddate": null, "tmdate": 1483998767063, "tcdate": 1483724639670, "number": 11, "id": "r1uvw8prg", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "signatures": ["~Adriana_Romero1"], "readers": ["everyone"], "writers": ["~Adriana_Romero1"], "content": {"title": "Revisions", "comment": "We hope the revisions made to the paper properly address the concerns raised by the reviewers and other researchers, and improve the quality of the manuscript. \n\nSummary of revisions:\n\n* Embedding learnt end-to-end from raw data: Added paragraph (Section 2.2, page 5) to explain what the embedding learnt end-to-end is and how it is used within the Diet Networks framework.\n\n* Figure 3:  Scaled both confusion matrices so that maximum for each class is 1.\n\n* Table 1:  Added extended PCA results. \n\n* Table 1: Updated number of free parameters column to only account for the number of free parameters in the fat layers of the model, with the aim to make the presentation clearer.\n\n* Made code publicly available and added footnote with the corresponding url (page 6).\n\n* Section 4.2. (Results), paragraph 6: introduced extended PCA results (more PCs and MLP + classifier).\n\n* Appendix B: Added the instructions to download the dataset.\n\n* Appendix C: Added an analysis on PCA (number of components used) and performance.\n\n* Fixed typos.\n\n* Table 1: Added comment about feature embeddings dimensionality.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1483998715173, "tcdate": 1483998715173, "number": 14, "id": "B1XbUFWLe", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "B1vEvj2mg", "signatures": ["~Adriana_Romero1"], "readers": ["everyone"], "writers": ["~Adriana_Romero1"], "content": {"title": "Answers to AnonReviewer3", "comment": "We thank the reviewer for the nice comments and hope we have properly addressed their questions. We added a comment regarding the embedding dimensionality in Table 1's caption."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1483728879374, "tcdate": 1483728879374, "number": 13, "id": "ryPeuPpBx", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "rkTSpEyNl", "signatures": ["~Pierre_Luc_Carrier1"], "readers": ["everyone"], "writers": ["~Pierre_Luc_Carrier1"], "content": {"title": "RE: Table 1", "comment": "Yes, it is due to the 5-fold cross-validation.\n\nThe reason the model has 290K parameters and not at least 345K is that we do not use every subject in the dataset to learn the feature embeddings. We only use the training examples. As detailed at the beginning of section 4.2, we perform our experiments with 5-fold cross-validation. Since, at any given time, one fold is used for testing, one fold is used for validation and three folds are used for training, this means we have (3/5)*3450 = 2070 training examples which are used to learn the embeddings.\n\nAlso, we have updated Table 1 to, hopefully, make it clearer. It now shows only the number of free parameters needed for the fat layers of the models instead of the total number of parameters in the whole models (since these other parameters are present in all considered architectures). We hope this makes it easier to understand how many free parameters each approach contributes to the model."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1483728187878, "tcdate": 1483728187878, "number": 12, "id": "SkNBrwpre", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "rJG0hH6Xx", "signatures": ["~Pierre_Luc_Carrier1"], "readers": ["everyone"], "writers": ["~Pierre_Luc_Carrier1"], "content": {"title": "Dropout and weight decay", "comment": "2- In the experiments where we used dropout, a probability of p=0.5 was used. In the case of weight decay, some experiments used a rate of 0 (no weight decay) and other used a rate of 1e-4.\n\nIf you are interested in knowing more about the selected hyper-parameter values, the code is now fully public (a footnote has been added to the paper to provide the location of the code)\n\nThank you for your questions and comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1483722868009, "tcdate": 1483721126205, "number": 10, "id": "rkAitBpSe", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "Bye2wHbEl", "signatures": ["~Adriana_Romero1"], "readers": ["everyone"], "writers": ["~Adriana_Romero1"], "content": {"title": "RE: Answers to AnonReviewer2", "comment": "We thank the reviewer for the nice comments/suggestions that are helping improve the paper and hope we have properly addressed their previous questions. As suggested, we have updated Table 1 with PCA performance for different numbers of PCs and different MLP classifier architectures. We have also added an appendix section to analyze the influence of using different numbers of pcs. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1483718516083, "tcdate": 1483718516083, "number": 9, "id": "rkhOJHTrl", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "rJ-yTHZVe", "signatures": ["~Adriana_Romero1"], "readers": ["everyone"], "writers": ["~Adriana_Romero1"], "content": {"title": "RE: Answers to AnonReviewer1", "comment": "We thank the reviewer for the nice comments and the useful observations that are helping improve the paper.\n\n\n1) The basic idea of the paper is interesting and the applied deep learning methodology appears reasonable.  The experimental evaluation is rather \nweak as it only covers a single data set and a very limited number of cross validation folds. Given the significant variation in the performances of all the methods, it seems the differences between the better-performing methods are probably not statistically significant. More comprehensive empirical validation could clearly strengthen the paper.\n\nA1: Among the better performing architectures, the goal was to reduce the number of parameters of our model. Hence, we show that we are able to drastically reduce the number of free parameters in the model without hurting the performance. Moreover, to strengthen the argument on superiority of deep learning methods over classical approaches used in the genomics domain (e.g. PCA-based), we performed a binomial test to compare the results achieved by classical approaches to the Diet Networks ones. When comparing the best results achieved by PCA (with 200 pcs) to the best Diet Networks model, the results are statistically significant (for alpha=0.05). We obtain a p-value of 2.4e-6, highlighting the potential of deep learning approaches to handle genomic data. \n\n\n2) The writing is generally good both in terms of the biology and ML, but more mathematical rigour would make it easier to understand precisely\nwhat was done. The different architectures are explained on an intuitive level and might benefit from a clear mathematical definition. I was ultimately left unsure of what the \"raw end2end\" model is - given so few parameters it cannot work on raw 300k dimensional input but I could not figure out what kind of embedding was used.\n\nA2: In raw end2end, the auxiliary network takes as input one SNP (2070 features, corresponding to training samples) and learns a feature embedding of dimensionality 100 (Fig. 1 (b) Emb.), followed by a second feature representation of dimensionality 100 (Fig. 1 (b) MLP, in this case one single hidden layer). The second feature representation is used as We. So, the auxiliary network has 2070*100 + 100 + 100*100+100 free parameters instead of the 315345*100 free parameters that We in Fig. 1 (a) would have.\n\nWe thank the reviewer for pointing out this issue.  We have included the following paragraph in the Feature Embedding section with the goal of improving the clarity of the presentation w.r.t. the raw end2end embedding:\n\n\u201cEmbedding learnt end-to-end from raw data: In this case, we consider the feature embedding to be another MLP, whose input corresponds to the values that a SNP takes for each of the training samples and, whose parameters are learnt jointly with the rest of the network. Note that the layer(s) corresponding to the feature embedding are shared among auxiliary networks. For experiments reported in Section 4, we used a single hidden layer as embedding.\u201d\n\nMoreover, we updated Table 1 such that it only includes the number of free parameters in the fat layers of the model, since the other free parameters are present in all architectures. We hope this will simplify the computation of the number of free parameters.\n\n\n3) The results in Fig. 3 might be clearer if scaled so that maximum for each class is 1 to avoid confounding from different numbers of subjects in different classes. In the text, please use the standard italics math font for all symbols such as N, N_d, ...\n\nA3: As suggested, we have updated both Fig. 3 and the notation in the most recent version of the paper.\n\n\n4) I think releasing the code as promised would be a must.\n\nA4: The code is now publicly available and the link to the repository has been included in the paper (see footnote on page 6).\n\nWe hope we properly addressed the reviewer's concerns and improved our paper with their comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1481922787350, "tcdate": 1481922624736, "number": 8, "id": "Hy_HdCbNl", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "rJG0hH6Xx", "signatures": ["~Pierre_Luc_Carrier1"], "readers": ["everyone"], "writers": ["~Pierre_Luc_Carrier1"], "content": {"title": "Performance of the PCA", "comment": "5- Follow up on the previous one, why do you think does the new neural net architecture outperform a PCA-based linear classifier method by 1-3%? Shouldn't we expect a larger margin?\n\nThe ethnicity is often a strong signal in genetic data, this is why the PCA does so well on it. The paper \"Genes mirror geography within Europe\" (http://www.nature.com/nature/journal/v456/n7218/full/nature07331.html), published in Nature, has a figure that illustrates this well. The authors performed a PCA on SNP data from 1387 European individuals and kept only the 2 principal components. Then they plotted the individuals, colored by country of origin, in this new space and showed that the resulting plot (Figure 1a) bears a certain resemblance to a map of Europe.\n\nBecause the PCA does this well, it is difficult to significantly improve the results. Nonetheless, it should also be possible to widen the gap between our method and the PCA by performing a more exhaustive hyper-parameter search.\n\nAddendum : the article is not fully accessible publicly but a copy of the figure I mentioned appears to be available here : http://www.nature.com/nature/journal/v456/n7218/images/nature07331-f1.2.jpg"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1481905666423, "tcdate": 1481905666423, "number": 7, "id": "S19W89ZVg", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "rJG0hH6Xx", "signatures": ["~Akram_Erraqabi1"], "readers": ["everyone"], "writers": ["~Akram_Erraqabi1"], "content": {"title": "PCA extra results ", "comment": "4- Do you mind sharing the results of the linear model with more than 100 PCs?\n\nHere is a figure that gives PCA results when considering 100, 200, 400, 800 and 1000 PCs.\n\nhttps://drive.google.com/open?id=0BzU2n_t7X56EaUFpQ3FNd3diT2c\n\nAs you can see, the best validation error comes with PC200. The test error in this case is 9.33 +/- 1.24.\n\nThanks for your questions and comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1481886937048, "tcdate": 1481886937048, "number": 3, "id": "rJ-yTHZVe", "invitation": "ICLR.cc/2017/conference/-/paper505/official/review", "forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "signatures": ["ICLR.cc/2017/conference/paper505/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper505/AnonReviewer1"], "content": {"title": "Interesting but somewhat early work on deep learning with very high dimensional genomic data", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents an application of deep learning to genomic SNP data\nwith a comparison of possible approaches for dealing with the very\nhigh data dimensionality. The approach looks very interesting but the\nexperiments are too limited to draw firm conclusions about the\nstrengths of different approaches. The presentation would benefit from\nmore precise math.\n\n\nQuality:\n\nThe basic idea of the paper is interesting and the applied deep\nlearning methodology appears reasonable. The experimental evaluation\nis rather weak as it only covers a single data set and a very limited\nnumber of cross validation folds. Given the significant variation in\nthe performances of all the methods, it seems the differences between\nthe better-performing methods are probably not statistically\nsignificant. More comprehensive empirical validation could clearly\nstrengthen the paper.\n\n\nClarity:\n\nThe writing is generally good both in terms of the biology and ML, but\nmore mathematical rigour would make it easier to understand precisely\nwhat was done. The different architectures are explained on an\nintuitive level and might benefit from a clear mathematical\ndefinition. I was ultimately left unsure of what the \"raw end2end\"\nmodel is - given so few parameters it cannot work on raw 300k\ndimensional input but I could not figure out what kind of embedding\nwas used.\n\nThe results in Fig. 3 might be clearer if scaled so that maximum for\neach class is 1 to avoid confounding from different numbers of\nsubjects in different classes. In the text, please use the standard\nitalics math font for all symbols such as N, N_d, ...\n\n\nOriginality:\n\nThe application and the approach appear quite novel.\n\n\nSignificance:\n\nThere is clearly strong interest for deep learning in the genomics\narea and the paper seeks to address some of the major bottlenecks\nhere. It is too early to tell whether the specific techniques proposed\nin the paper will be the ultimate solution, but at the very least the\npaper provides interesting new ideas for others to work on.\n\n\nOther comments:\n\nI think releasing the code as promised would be a must.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512561897, "id": "ICLR.cc/2017/conference/-/paper505/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper505/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper505/AnonReviewer3", "ICLR.cc/2017/conference/paper505/AnonReviewer2", "ICLR.cc/2017/conference/paper505/AnonReviewer1"], "reply": {"forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512561897}}}, {"tddate": null, "tmdate": 1481885608517, "tcdate": 1481885608517, "number": 2, "id": "Bye2wHbEl", "invitation": "ICLR.cc/2017/conference/-/paper505/official/review", "forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "signatures": ["ICLR.cc/2017/conference/paper505/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper505/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The paper addresses the important problem (d>>n) in deep learning. \nThe proposed approach, based on lower-dimensional feature embeddings, is reasonable and makes applying deep learning methods to data with large d possible.\nThe paper is well written and the results show improvements over reasonable baselines.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512561897, "id": "ICLR.cc/2017/conference/-/paper505/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper505/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper505/AnonReviewer3", "ICLR.cc/2017/conference/paper505/AnonReviewer2", "ICLR.cc/2017/conference/paper505/AnonReviewer1"], "reply": {"forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512561897}}}, {"tddate": null, "tmdate": 1481829459599, "tcdate": 1481829380494, "number": 6, "id": "SJ3Zhvx4e", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "rJG0hH6Xx", "signatures": ["~Julie_Hussin1"], "readers": ["everyone"], "writers": ["~Julie_Hussin1"], "content": {"title": "1000 Genomes Data", "comment": "Many thanks for your comments and questions.\n\nHow do you handle missing genotypes? When I download the same 1000G Affymetrix file and preprocess it using plink2 options \"--maf 0.05 --geno 0 --not-chr X Y MT --indep-pairwise 500 5 0.5\", I end up with 120,036 SNPs, rather than 315,345. I appreciate it if you could share more details on preprocessing.\n\n- We\u2019ve now included a description of the pre-processing steps with plink2 in the manuscript and we've made our pre-processed dataset available to readers. We did not use the --geno 0 option in our pre-processing of the data, which explains the discrepancies between the number of SNPs you report and the number we\u2019ve analyzed. In our experiment, all missing genotypes were considered to be homozygote reference, but we plan to explore missing data handling further in the future.\n\nAre you planning to apply the method on full 1000G Phase3 dataset which consists of 84 million variants to make n<<p problem more challenging?\n\n- We are indeed planning to apply the method to larger genetic datasets, although for this ethnicity prediction task, including rare variation may not add very much. However, since other traits would benefit from inclusion of rare variants (including detection of finer-scale population structure), we will use larger datasets with rare variants in the future.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1481755210227, "tcdate": 1481624778507, "number": 3, "id": "rJG0hH6Xx", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "signatures": ["~G\u00f6kcen_Eraslan1"], "readers": ["everyone"], "writers": ["~G\u00f6kcen_Eraslan1"], "content": {"title": "Few questions", "comment": "Thanks for this very nice paper. I have a few technical questions:\n\n1- How do you handle missing genotypes? When I download the same 1000G Affymetrix file and preprocess it using plink2 options \"--maf 0.05 --geno 0 --not-chr X Y MT --indep-pairwise 500 5 0.5\", I end up with 120,036 SNPs, rather than 315,345. I appreciate it if you could share more details on preprocessing.\n\n2- What are the final dropout and weight decay rates that have been used?\n\n3- Are you planning to apply the method on full 1000G Phase3 dataset which consists of 84 million variants to make n<<p problem more challenging?\n\n4- Do you mind sharing the results of the linear model with more than 100 PCs?\n\n5- Follow up on the previous one, why do you think does the new neural net architecture outperform a PCA-based linear classifier method by 1-3%? Shouldn't we expect a larger margin?\n\n6- On page 7, last paragraph has a small typo. \"100 genomes dataset\" should be corrected as \"1000 Genomes dataset\".\n\nBest regards.\n\nedit: added numbers to questions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1481752004218, "tcdate": 1481751876869, "number": 5, "id": "rkTSpEyNl", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "Hy3zufkNx", "signatures": ["~G\u00f6kcen_Eraslan1"], "readers": ["everyone"], "writers": ["~G\u00f6kcen_Eraslan1"], "content": {"title": "Table 1", "comment": "If \"raw end2end\" model also has embedding of size 100, then shouldn't the number of its free parameters be at least 3450x100=345K? Why is it 290K in Table1? Because of 5-fold CV?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1481742355965, "tcdate": 1481742355959, "number": 4, "id": "Hy3zufkNx", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "B1vEvj2mg", "signatures": ["~Pierre_Luc_Carrier1"], "readers": ["everyone"], "writers": ["~Pierre_Luc_Carrier1"], "content": {"title": "RE: Nice paper; very clear presentation.", "comment": "Thank you for your review.\n\n1) The 'Per class histograms' model, as briefly described in section 2.2, uses feature embeddings of size 78 because every feature has 3 possible values and the data contains 26 classes (26 * 3 = 78). For the other methods, the results reported in table 1 use feature embeddings of size 100.\n\n2) Yes, exactly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1481582678203, "tcdate": 1481582382615, "number": 1, "id": "B1vEvj2mg", "invitation": "ICLR.cc/2017/conference/-/paper505/official/review", "forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "signatures": ["ICLR.cc/2017/conference/paper505/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper505/AnonReviewer3"], "content": {"title": "Nice paper; very clear presentation.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The problem addressed here is practically important (supervised learning with n<<d), and as far as I know, the approach is novel. I thought their proposed solution was innovative, and I enjoyed the paper. The presentation is clear and it has nice experiments. \n\nComments/questions:\n1) What is the dimensionality of the feature embeddings? \n2) SNPtoVec still requires training a very fat autoencoder network on X --- I suppose this doesn't contribute to the size of the final run-time model and overfitting is avoided because the parameters are fit in an unsupervised manner?\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512561897, "id": "ICLR.cc/2017/conference/-/paper505/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper505/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper505/AnonReviewer3", "ICLR.cc/2017/conference/paper505/AnonReviewer2", "ICLR.cc/2017/conference/paper505/AnonReviewer1"], "reply": {"forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512561897}}}, {"tddate": null, "tmdate": 1481569350891, "tcdate": 1481569350885, "number": 2, "id": "By1UNu27l", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "H1eOB77me", "signatures": ["~Akram_Erraqabi1"], "readers": ["everyone"], "writers": ["~Akram_Erraqabi1"], "content": {"title": "Thanks for your question", "comment": "As mentionned in the paper, \"adding capacity didn't improve the performance\". To be more specific, we give here some extra results for the PCA + MLP classifier. We report these results for PCA100 that had the best performance among the PCA-based experiments.\n\nPCA100 + MLP(50)\nmean : 12.67\nstd : 0.67\n\nPCA100+ MLP(100) :\nmean : 12.18\nstd : 1.75\n\nPCA100 + MLP(100,100)\nmean : 11.95\nstd : 2.29\n\nThe mean and the standard deviation are computed over the same folds as the PCA-only experiments.\n\nWe thank you for the suggestion and will add as suggested these results in the comparison.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1480959335743, "tcdate": 1480959335736, "number": 2, "id": "H1eOB77me", "invitation": "ICLR.cc/2017/conference/-/paper505/pre-review/question", "forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "signatures": ["ICLR.cc/2017/conference/paper505/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper505/AnonReviewer2"], "content": {"title": "Comparison to PCA + MLP", "question": "It would be informative if the authors could include the performance of the model consisting of PCA preprocessor + MLP classifier in the comparison, especially as PCA is closely related to auto-encoding / reconstruction."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959336593, "id": "ICLR.cc/2017/conference/-/paper505/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper505/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper505/AnonReviewer1", "ICLR.cc/2017/conference/paper505/AnonReviewer2"], "reply": {"forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959336593}}}, {"tddate": null, "tmdate": 1480698589385, "tcdate": 1480698589379, "number": 1, "id": "rkHkimJXl", "invitation": "ICLR.cc/2017/conference/-/paper505/public/comment", "forum": "Sk-oDY9ge", "replyto": "ByuFvRAfe", "signatures": ["~Adriana_Romero1"], "readers": ["everyone"], "writers": ["~Adriana_Romero1"], "content": {"title": "RE: Clarification on the experimental procedure", "comment": "Thanks for your question. \n\nThe training folds are used to learn the parameters of the model. The validation fold is used for hyperparameter tuning (setting which learning rate, weights decay, gamma, architecture work best) and for early stopping. During training, accuracy on the validation fold is monitored and after 100 epochs of no improvement, the training is stopped. The test fold is used to report results (when the final model is fully trained and we want to assess the performance of the trained model). Note that the test fold contains new fresh data that the model has never seen before."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287548437, "id": "ICLR.cc/2017/conference/-/paper505/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Sk-oDY9ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper505/reviewers", "ICLR.cc/2017/conference/paper505/areachairs"], "cdate": 1485287548437}}}, {"tddate": null, "tmdate": 1480677247574, "tcdate": 1480677247570, "number": 1, "id": "ByuFvRAfe", "invitation": "ICLR.cc/2017/conference/-/paper505/pre-review/question", "forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "signatures": ["ICLR.cc/2017/conference/paper505/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper505/AnonReviewer1"], "content": {"title": "Clarification on the experimental procedure", "question": "Thanks for the very interesting paper!\n\nI would like to ask for a clarification of the experimental procedure described in Sec. 4.2. You mention splitting the data to 5 folds: 3 for training, 1 for testing and 1 for validation. Can you please explain how these are used, especially what is the difference between testing and validation folds?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diet Networks: Thin Parameters for Fat Genomics", "abstract": "Learning tasks such as those involving genomic data often poses a serious challenge: the number of input features can be orders of magnitude larger than the number of training examples, making it difficult to avoid overfitting, even when using the known regularization techniques. We focus here on tasks in which the input is a description of the genetic variation specific to a patient, the single nucleotide polymorphisms (SNPs), yielding millions of ternary inputs. Improving the ability of deep learning to handle such datasets could have an important impact in medical research, more specifically in precision medicine, where high-dimensional data regarding a particular patient is used to make predictions of interest. Even though the amount of data for such tasks is increasing, this mismatch between the number of examples and the number of inputs remains a concern. Naive implementations of classifier neural networks involve a huge number of free parameters in their first layer (number of input features times number of hidden units): each input feature is associated with as many parameters as there are hidden units. We propose a novel neural network parametrization which considerably reduces the number of free parameters. It is based on the idea that we can first learn or provide a distributed representation for each input feature (e.g. for each position in the genome where variations are observed in data), and then learn (with another neural network called the parameter prediction network) how to map a feature's distributed representation (based on the feature's identity not its value) to the vector of parameters specific to that feature in the classifier neural network (the weights which link the value of the feature to each of the hidden units). This approach views the problem of producing the parameters associated with each feature as a multi-task learning problem. We show experimentally on a population stratification task of interest to medical studies that the proposed approach can significantly reduce both the number of parameters and the error rate of the classifier.", "pdf": "/pdf/ebb3a58d659cc976a5880934adb8841fd47a9f3b.pdf", "TL;DR": "Drastically reducing the number of parameters, when the number of input features is orders of magnitude larger than the number of training examples, such as in genomics.", "paperhash": "romero|diet_networks_thin_parameters_for_fat_genomics", "conflicts": ["umontreal.ca", "cvc.uab.es", "ub.edu", "well.ox.ca.uk", "icm.rc.org", "mhi-rc.org"], "keywords": ["Deep learning", "Unsupervised Learning", "Supervised Learning", "Applications"], "authors": ["Adriana Romero", "Pierre Luc Carrier", "Akram Erraqabi", "Tristan Sylvain", "Alex Auvolat", "Etienne Dejoie", "Marc-Andr\u00e9 Legault", "Marie-Pierre Dub\u00e9", "Julie G. Hussin", "Yoshua Bengio"], "authorids": ["adriana.romero.soriano@umontreal.ca", "pierre-luc.carrier@umontreal.ca", "akram.er-raqabi@umontreal.ca", "Tristan.sylvain@umontreal.ca", "alexis211@gmail.com", "etiennedejoie@gmail.com", "marc-andre.legault.1@umontreal.ca", "julieh@well.ox.ac.uk", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959336593, "id": "ICLR.cc/2017/conference/-/paper505/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper505/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper505/AnonReviewer1", "ICLR.cc/2017/conference/paper505/AnonReviewer2"], "reply": {"forum": "Sk-oDY9ge", "replyto": "Sk-oDY9ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper505/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959336593}}}], "count": 21}