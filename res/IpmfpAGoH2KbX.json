{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392852360000, "tcdate": 1392852360000, "number": 5, "id": "rGZJRE7IJwrK3", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "IpmfpAGoH2KbX", "replyto": "IpmfpAGoH2KbX", "signatures": ["Charles Martin"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "It is noted that the connection between RG and multi-scale modeling has been pointed out by Candes in \r\n\r\nE. J. Cand\u00e8s, P. Charlton and H. Helgason. Detecting highly oscillatory signals by chirplet path pursuit. Appl. Comput. Harmon. Anal. 24 14-40.\r\n\r\nwhere it was noted that the multi-scale basis suggested in this convex optimization approach is equivalent to the Wilson basis from his original work on RG theory in the 1970s"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Deep learning and the renormalization group", "abstract": "Renormalization group methods, which analyze the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. The aim of this paper is to compare and contrast the ideas behind the renormalization group (RG) on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.", "pdf": "https://arxiv.org/abs/1301.3124", "paperhash": "b\u00e9ny|deep_learning_and_the_renormalization_group", "authors": ["C\u00e9dric B\u00e9ny"], "authorids": ["cedric.beny@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1365121080000, "tcdate": 1365121080000, "number": 4, "id": "7Kq-KFuY-y7S_", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "IpmfpAGoH2KbX", "replyto": "IpmfpAGoH2KbX", "signatures": ["Yann LeCun"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "It seems to me like there could be an interesting connection between approximate inference in graphical models and the renormalization methods. \r\n\r\nThere is in fact a long history of interactions between condensed matter physics and graphical models. For example, it is well known that the loopy belief propagation algorithm for inference minimizes the Bethe free energy (an approximation of the free energy in which only pairwise interactions are taken into account and high-order interactions are ignored). More generally, variational methods inspired by statistical physics have been a very popular topic in graphical model inference.\r\n\r\nThe renormalization methods could be relevant to deep architectures in the sense that the grouping of random variable resulting from a change of scale could be be made analogous with the pooling and subsampling operations often used in deep models. \r\n\r\nIt's an interesting idea, but it will probably take more work (and more tutorial expositions of RG) to catch the attention of this community."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Deep learning and the renormalization group", "abstract": "Renormalization group methods, which analyze the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. The aim of this paper is to compare and contrast the ideas behind the renormalization group (RG) on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.", "pdf": "https://arxiv.org/abs/1301.3124", "paperhash": "b\u00e9ny|deep_learning_and_the_renormalization_group", "authors": ["C\u00e9dric B\u00e9ny"], "authorids": ["cedric.beny@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363477320000, "tcdate": 1363477320000, "number": 1, "id": "tb0cgaJXQfgX6", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "IpmfpAGoH2KbX", "replyto": "IpmfpAGoH2KbX", "signatures": ["Aaron Courville"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Reviewer 441c,\r\n\r\nHave you taken a look at the new version of the paper? Does it go some way to addressing your concerns?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Deep learning and the renormalization group", "abstract": "Renormalization group methods, which analyze the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. The aim of this paper is to compare and contrast the ideas behind the renormalization group (RG) on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.", "pdf": "https://arxiv.org/abs/1301.3124", "paperhash": "b\u00e9ny|deep_learning_and_the_renormalization_group", "authors": ["C\u00e9dric B\u00e9ny"], "authorids": ["cedric.beny@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363212060000, "tcdate": 1363212060000, "number": 1, "id": "4Uh8Uuvz86SFd", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "IpmfpAGoH2KbX", "replyto": "7to37S6Q3_7Qe", "signatures": ["C\u00e9dric B\u00e9ny"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I have submitted a replacement to the arXiv on March 13, which should be available the same day at 8pm EST/EDT as version 4.\r\n\r\nIn order to address the first issue, I rewrote section 2 to make it less confusing, specifically by not trying to be overly general. I also rewrote the caption of figure 1 to make it a nearly self-contained explanation of what the model is for a specific one-dimensional example. The content of section 2 essentially explains what features must be kept for any generalization, and section 3 clarifies why these features are important. \r\n\r\nConcerning the second issue, I agree that this work is preliminary, and implementation is the next step."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Deep learning and the renormalization group", "abstract": "Renormalization group methods, which analyze the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. The aim of this paper is to compare and contrast the ideas behind the renormalization group (RG) on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.", "pdf": "https://arxiv.org/abs/1301.3124", "paperhash": "b\u00e9ny|deep_learning_and_the_renormalization_group", "authors": ["C\u00e9dric B\u00e9ny"], "authorids": ["cedric.beny@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362321600000, "tcdate": 1362321600000, "number": 2, "id": "7to37S6Q3_7Qe", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "IpmfpAGoH2KbX", "replyto": "IpmfpAGoH2KbX", "signatures": ["anonymous reviewer 441c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep learning and the renormalization group", "review": "The model tries to relate renormalization group and deep learning, specifically hierarchical Bayesian network. The primary problems are that 1) the paper is only descriptive - it does not explain models clearly and precisely, and 2) it has no numerical experiments showing that it works.\r\n\r\nWhat it needs is something like: \r\n1) Define the DMRG (or whatever verion of RG you need) and Define the machine learning model. Do these with explicit formulas so reader can know what exactly they are. Things like 'Instead, we only allow for maps \u03c0j which are local in two important ways: firstly, each input vertex can only causally influence the values associated with the m output vertices that it represents plus all kth degree neighbors of these, where k would typically be small' are very hard to follow.\r\n\r\n2) Show the mapping between the two models. \r\n\r\n3) Show what it does on real data and that it does something interesting and/or useful. (Real data e.g. sound signals, images, text,...)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Deep learning and the renormalization group", "abstract": "Renormalization group methods, which analyze the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. The aim of this paper is to compare and contrast the ideas behind the renormalization group (RG) on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.", "pdf": "https://arxiv.org/abs/1301.3124", "paperhash": "b\u00e9ny|deep_learning_and_the_renormalization_group", "authors": ["C\u00e9dric B\u00e9ny"], "authorids": ["cedric.beny@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362219360000, "tcdate": 1362219360000, "number": 3, "id": "Qj1vSox-vpQ-U", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "IpmfpAGoH2KbX", "replyto": "IpmfpAGoH2KbX", "signatures": ["anonymous reviewer acf4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep learning and the renormalization group", "review": "This paper discusses deep learning from the perspective of renormalization groups in theoretical physics.  Both concepts are naturally related; however, this relation has not been formalized adequately thus far and advancing this is a novelty of the paper.  The paper contains a non-technical and insightful exposition of concepts and discusses a learning algorithm for stochastic networks based on the `multiscale entanglement   renormalization ansatz' (MERA). This contribution will potentially evoke the interest of many readers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Deep learning and the renormalization group", "abstract": "Renormalization group methods, which analyze the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. The aim of this paper is to compare and contrast the ideas behind the renormalization group (RG) on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.", "pdf": "https://arxiv.org/abs/1301.3124", "paperhash": "b\u00e9ny|deep_learning_and_the_renormalization_group", "authors": ["C\u00e9dric B\u00e9ny"], "authorids": ["cedric.beny@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358262900000, "tcdate": 1358262900000, "number": 45, "id": "IpmfpAGoH2KbX", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "IpmfpAGoH2KbX", "signatures": ["cedric.beny@gmail.com"], "readers": ["everyone"], "content": {"decision": "reject", "title": "Deep learning and the renormalization group", "abstract": "Renormalization group methods, which analyze the way in which the effective behavior of a system depends on the scale at which it is observed, are key to modern condensed-matter theory and particle physics. The aim of this paper is to compare and contrast the ideas behind the renormalization group (RG) on the one hand and deep machine learning on the other, where depth and scale play a similar role. In order to illustrate this connection, we review a recent numerical method based on the RG---the multiscale entanglement renormalization ansatz (MERA)---and show how it can be converted into a learning algorithm based on a generative hierarchical Bayesian network model. Under the assumption---common in physics---that the distribution to be learned is fully characterized by local correlations, this algorithm involves only explicit evaluation of probabilities, hence doing away with sampling.", "pdf": "https://arxiv.org/abs/1301.3124", "paperhash": "b\u00e9ny|deep_learning_and_the_renormalization_group", "authors": ["C\u00e9dric B\u00e9ny"], "authorids": ["cedric.beny@gmail.com"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 7}