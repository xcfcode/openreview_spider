{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392622080000, "tcdate": 1392622080000, "number": 1, "id": "2oaEotjZYbogy", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "qqUMpzcNswxen", "replyto": "kkIykb7My3s0n", "signatures": ["Yunchao Gong"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for the comments.\r\n\r\n'Comment 1 about tag dictionary size'\r\n\r\nThe NUS-WIDE dataset is the largest publicly available annotated multilabel dataset we have access to. We agree with the reviewer that the power of Wsabie is not fully explored for this dictionary size, however, our goal is to show that the weighted ranking formulation can effectively improve multilabel annotation accuracy. The reason we use WARP is because it is easy to implement, and potentially scales well to large dictionary size.\r\n\r\n\r\n'Comment 2 about reuse ImageNet model'\r\n\r\nAs mentioned in our response to reviewer 2, we have tried to initialize the model with ImageNet pretrained model, and have further obtained around 2% improvement for all methods. However, our goal is to evaluate which loss is the best for multilabel prediction problems, so we directly trained the model from scratch to provide the cleanest experimental setting."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Ranking for Multilabel Image Annotation", "decision": "submitted, no decision", "abstract": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, the recent deep convolutional feature shows potentials to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with an approximate top-$k$ ranking objective function, as such objectives naturally fit the multilabel tagging problem. Our experiments on the publicly available NUS-WIDE dataset outperforms the conventional visual features by about $10%$, obtaining the best reported performance in the literature.", "pdf": "https://arxiv.org/abs/1312.4894", "paperhash": "ioffe|deep_convolutional_ranking_for_multilabel_image_annotation", "keywords": [], "conflicts": [], "authors": ["Sergey Ioffe", "Alexander Toshev", "Yangqing Jia", "Thomas Leung", "Yunchao Gong"], "authorids": ["leungt@google.com", "toshev@google.com", "jiayq84@gmail.com", "sioffe@google.com", "yunchao@cs.unc.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392621780000, "tcdate": 1392621780000, "number": 1, "id": "ddxgh_GILedj-", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "qqUMpzcNswxen", "replyto": "H1MIH9BzIJ1qm", "signatures": ["Yunchao Gong"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "'Are there some labels more important than others, or shouldn\u2019t we employ taxonomic distances' \r\n\r\nThe standard evaluation protocol described in [25] is used in our work, and we have evaluated different methods by 5 different protocols (which is more comprehensive than [25]). The overall precision and overall recall emphasis on frequent tags, and per-class recall and per-class precision emphasis on infrequent tags. So we believe the evaluation is thorough. The point raised by the reviewer is definitely interesting, however we believe it is orthogonal to this paper.\r\n\r\n\r\n'How make model to decide on number of output labels ?'\r\n\r\nWe follow the standard practice in most previous works [25,14,26] to fix the number of output labels for each image to 3 or 5.\r\n\r\n\r\n 'It would be nice to have experiments comparing it to the network pretrained on imagenet.'\r\n\r\nWe have tested it before and found using pretrained weights can further improve the performance for around 2% for all methods. However, our goal is to perform a clear comparison between different loss functions for multilabel annotation, and want to use the simplest experimental setting, so we did not includ the pretrained results."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Ranking for Multilabel Image Annotation", "decision": "submitted, no decision", "abstract": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, the recent deep convolutional feature shows potentials to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with an approximate top-$k$ ranking objective function, as such objectives naturally fit the multilabel tagging problem. Our experiments on the publicly available NUS-WIDE dataset outperforms the conventional visual features by about $10%$, obtaining the best reported performance in the literature.", "pdf": "https://arxiv.org/abs/1312.4894", "paperhash": "ioffe|deep_convolutional_ranking_for_multilabel_image_annotation", "keywords": [], "conflicts": [], "authors": ["Sergey Ioffe", "Alexander Toshev", "Yangqing Jia", "Thomas Leung", "Yunchao Gong"], "authorids": ["leungt@google.com", "toshev@google.com", "jiayq84@gmail.com", "sioffe@google.com", "yunchao@cs.unc.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392621300000, "tcdate": 1392621300000, "number": 1, "id": "BHP9Bpdr8SHE-", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "qqUMpzcNswxen", "replyto": "U_9J_msdzlEuM", "signatures": ["Yunchao Gong"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you for the comments!\r\n\r\nSince most previous work on this dataset use a smaller subset of this whole dataset (such as NUS-light), or use their own training/testing split, directly comparing the numbers seem to be hard. However, we included a baseline recognition system [11] which is published in IJCV 2013. This baseline is based on a combination of 9 different visual features, and can be considered to be a quite strong baseline."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Ranking for Multilabel Image Annotation", "decision": "submitted, no decision", "abstract": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, the recent deep convolutional feature shows potentials to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with an approximate top-$k$ ranking objective function, as such objectives naturally fit the multilabel tagging problem. Our experiments on the publicly available NUS-WIDE dataset outperforms the conventional visual features by about $10%$, obtaining the best reported performance in the literature.", "pdf": "https://arxiv.org/abs/1312.4894", "paperhash": "ioffe|deep_convolutional_ranking_for_multilabel_image_annotation", "keywords": [], "conflicts": [], "authors": ["Sergey Ioffe", "Alexander Toshev", "Yangqing Jia", "Thomas Leung", "Yunchao Gong"], "authorids": ["leungt@google.com", "toshev@google.com", "jiayq84@gmail.com", "sioffe@google.com", "yunchao@cs.unc.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392110100000, "tcdate": 1392110100000, "number": 4, "id": "kkIykb7My3s0n", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "qqUMpzcNswxen", "replyto": "qqUMpzcNswxen", "signatures": ["anonymous reviewer 2486"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Convolutional Ranking for Multilabel Image Annotation", "review": "*Summary*\r\nThis paper proposes to use convolutional networks for image annotation. Great care is given to the selection of an appropriate loss function as well as the comparison with reasonable baselines over the NUS/Flickr dataset. The paper reads well, gives enough context and references to related work. It\r\nreports improvement with respect to the state of the art. In my opinion, this is a good paper, with the only drawback that the evaluation is conducted over a single dataset, with a vocabulary of only 80\r\n tags, which is small compared to realistic application.\r\n\r\n *Detailed review*\r\n I would like to clarify only two points regarding (i) small tag vocabulary in NUS, (ii) relationship with imageNET classification. \r\n\r\n (i) the vocabulary of NUS/Flickr is only 80 different tags. This is very  small compare to web annotation or even personal photo gallery annotation. In particular, the fact that your network has 80 outputs make the evaluation of the output score of every tag for every forward/backward step very\r\n inexpensive (compared to evaluating the rest of the network). This is very different from the initial conditions in which the WARP loss was introduced. Loss functions which does not rely on sampling can perfectly be used and might be better. I notably think at  T. Joachims, A Support Vector Method\r\n for Multivariate Performance Measures, Proceedings of the International Conference on Machine Learning (ICML), 2005 or Ranking with ordered weighted pairwise classification from Usunier et al 2009. More fundamentally, I feel that focussing on 80 tags hides most of the interesting challenges in real tasks: reasonable 10k vocabularies implies greater perplexity and therefore require greater performance for the CNN. They also suggest giving greater importance on tag coocurences and language modeling to understand unlikely predictions like ocean and lake tag in the same image from your example.\r\n\r\n (ii) I appreciate that you highlight the difference between annotation and classification, and that you want a model trained from scratch for fair comparisons (Section 2.1). However, the CNN trained over ImageNET of [20] or subsequent work has spurred hopes for a universal vision machine. If large\r\n CNNs trained over 1k and 20k imagenet were available to you, it might be interesting to evaluate how a NUS model initialized from those would perform. This would be an additional result which would not replace the network trained from scratch but rather analyze the reusability of the\r\n imageNET network and give perspective on the importance of the imageNET breakthrough.\r\n\r\n *Comments along the text*\r\n Page 2. 'parametric model might not be sufficient to capture the complex distribution of the data' this sentence should be removed. Parametric model can model complex distribution for non linear problems. Use a different wording to introduce that nearest neighbor approaches are competitive.\r\n Page 3. 'staircase weight-decay' I am not familiar with this name, which is rather explicit though. You might want to sprincke references over neural net specific terms to allow other ML and core vision people to read your paper. E.g. references after momentum, asynchronous SGD, staircase weight\r\n decay might help. \r\n Page 3 'posterior probability of an image x_i and class j' the wording is wrong, it should read posterior of class j given image x_i.\r\n Page 5 'weight kNN' should read 'weighted kNN'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Ranking for Multilabel Image Annotation", "decision": "submitted, no decision", "abstract": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, the recent deep convolutional feature shows potentials to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with an approximate top-$k$ ranking objective function, as such objectives naturally fit the multilabel tagging problem. Our experiments on the publicly available NUS-WIDE dataset outperforms the conventional visual features by about $10%$, obtaining the best reported performance in the literature.", "pdf": "https://arxiv.org/abs/1312.4894", "paperhash": "ioffe|deep_convolutional_ranking_for_multilabel_image_annotation", "keywords": [], "conflicts": [], "authors": ["Sergey Ioffe", "Alexander Toshev", "Yangqing Jia", "Thomas Leung", "Yunchao Gong"], "authorids": ["leungt@google.com", "toshev@google.com", "jiayq84@gmail.com", "sioffe@google.com", "yunchao@cs.unc.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391404320000, "tcdate": 1391404320000, "number": 3, "id": "H1MIH9BzIJ1qm", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "qqUMpzcNswxen", "replyto": "qqUMpzcNswxen", "signatures": ["anonymous reviewer 3761"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "- Are there some labels more important than others, or shouldn\u2019t we employ taxonomic distances ? \r\n- How make model to decide on number of output labels ? \r\n- It would be nice to have experiments comparing it to the network pretrained on imagenet."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Ranking for Multilabel Image Annotation", "decision": "submitted, no decision", "abstract": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, the recent deep convolutional feature shows potentials to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with an approximate top-$k$ ranking objective function, as such objectives naturally fit the multilabel tagging problem. Our experiments on the publicly available NUS-WIDE dataset outperforms the conventional visual features by about $10%$, obtaining the best reported performance in the literature.", "pdf": "https://arxiv.org/abs/1312.4894", "paperhash": "ioffe|deep_convolutional_ranking_for_multilabel_image_annotation", "keywords": [], "conflicts": [], "authors": ["Sergey Ioffe", "Alexander Toshev", "Yangqing Jia", "Thomas Leung", "Yunchao Gong"], "authorids": ["leungt@google.com", "toshev@google.com", "jiayq84@gmail.com", "sioffe@google.com", "yunchao@cs.unc.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391404260000, "tcdate": 1391404260000, "number": 2, "id": "nqJonBRKZmikW", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "qqUMpzcNswxen", "replyto": "qqUMpzcNswxen", "signatures": ["anonymous reviewer 3761"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Convolutional Ranking for Multilabel Image Annotation", "review": "* A brief summary of the paper's contributions, in the context of prior work.\r\nPaper considers several loss functions for multiclass label annotation.\r\n\r\n* An assessment of novelty and quality. \r\nThey have done good job, and ran experiments on the proper, large scale, however work is not very novel (or creative).\r\n\r\n* A list of pros and cons (reasons to accept/reject).\r\npros:\r\n- Gives reasonable advice, which loss function use for multi class image annotation.\r\n\r\ncons:"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Ranking for Multilabel Image Annotation", "decision": "submitted, no decision", "abstract": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, the recent deep convolutional feature shows potentials to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with an approximate top-$k$ ranking objective function, as such objectives naturally fit the multilabel tagging problem. Our experiments on the publicly available NUS-WIDE dataset outperforms the conventional visual features by about $10%$, obtaining the best reported performance in the literature.", "pdf": "https://arxiv.org/abs/1312.4894", "paperhash": "ioffe|deep_convolutional_ranking_for_multilabel_image_annotation", "keywords": [], "conflicts": [], "authors": ["Sergey Ioffe", "Alexander Toshev", "Yangqing Jia", "Thomas Leung", "Yunchao Gong"], "authorids": ["leungt@google.com", "toshev@google.com", "jiayq84@gmail.com", "sioffe@google.com", "yunchao@cs.unc.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390308720000, "tcdate": 1390308720000, "number": 1, "id": "U_9J_msdzlEuM", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "qqUMpzcNswxen", "replyto": "qqUMpzcNswxen", "signatures": ["anonymous reviewer 0cae"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Deep Convolutional Ranking for Multilabel Image Annotation", "review": "This paper proposes to use deep convolutional neural network (DCNN)combined with ranking training criteria to attack the multi-label image annotation problem. DCNN is now widely used in image classification (annotation) problems. Applying it to multi-label image annotation problem is a natural extension of prior arts. The combination of DCNN with the ranking training criteria to solve the multi-label annotation problem, however, is new and is the main contribution of the paper.\r\n\r\nThe authors evaluated the proposed approach on the NUS-WIDE dataset, which is considered the largest multi-label image dataset available. They compared the proposed approach with baselines that use manually designed image features and showed that the proposed approach outperforms the baseline by 10%. They claim that this is mostly due to the features learned from DCNN. They also compared several different ranking criteria and demonstrated that the weighted Approximate Ranking (WARP) criterion performs the best.\r\n\r\nWhile their results are not surprising given the recent success of DCNN on image classification tasks, this paper does show a novel usage of the DCNN on the multi-label image annotation problem. \r\n\r\nIf the paper is to be improved, I would suggest to include published results on the same task as part of the baselines. This allows readers to understand better the position of the proposed approach."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Ranking for Multilabel Image Annotation", "decision": "submitted, no decision", "abstract": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, the recent deep convolutional feature shows potentials to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with an approximate top-$k$ ranking objective function, as such objectives naturally fit the multilabel tagging problem. Our experiments on the publicly available NUS-WIDE dataset outperforms the conventional visual features by about $10%$, obtaining the best reported performance in the literature.", "pdf": "https://arxiv.org/abs/1312.4894", "paperhash": "ioffe|deep_convolutional_ranking_for_multilabel_image_annotation", "keywords": [], "conflicts": [], "authors": ["Sergey Ioffe", "Alexander Toshev", "Yangqing Jia", "Thomas Leung", "Yunchao Gong"], "authorids": ["leungt@google.com", "toshev@google.com", "jiayq84@gmail.com", "sioffe@google.com", "yunchao@cs.unc.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387347720000, "tcdate": 1387347720000, "number": 69, "id": "qqUMpzcNswxen", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "qqUMpzcNswxen", "signatures": ["leungt@google.com"], "readers": ["everyone"], "content": {"title": "Deep Convolutional Ranking for Multilabel Image Annotation", "decision": "submitted, no decision", "abstract": "Multilabel image annotation is one of the most important challenges in computer vision with many real-world applications. While existing work usually use conventional visual features for multilabel annotation, the recent deep convolutional feature shows potentials to significantly boost performance. In this work, we propose to leverage the advantage of such features and analyze key components that lead to better performances. Specifically, we show that a significant performance gain could be obtained by combining convolutional architectures with an approximate top-$k$ ranking objective function, as such objectives naturally fit the multilabel tagging problem. Our experiments on the publicly available NUS-WIDE dataset outperforms the conventional visual features by about $10%$, obtaining the best reported performance in the literature.", "pdf": "https://arxiv.org/abs/1312.4894", "paperhash": "ioffe|deep_convolutional_ranking_for_multilabel_image_annotation", "keywords": [], "conflicts": [], "authors": ["Sergey Ioffe", "Alexander Toshev", "Yangqing Jia", "Thomas Leung", "Yunchao Gong"], "authorids": ["leungt@google.com", "toshev@google.com", "jiayq84@gmail.com", "sioffe@google.com", "yunchao@cs.unc.edu"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 8}