{"notes": [{"id": "BJgQ4lSFPH", "original": "HygDQ8ltvH", "number": 2242, "cdate": 1569439787028, "ddate": null, "tcdate": 1569439787028, "tmdate": 1588995964651, "tddate": null, "forum": "BJgQ4lSFPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": ["Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si"], "authorids": ["hebian.ww@alibaba-inc.com", "b.bi@alibaba-inc.com", "ym119608@alibaba-inc.com", "wuchen.wc@alibaba-inc.com", "jiangnan.xjn@alibaba-inc.com", "zuyi.bzy@alibaba-inc.com", "liwei.peng@alibaba-inc.com", "luo.si@alibaba-inc.com"], "keywords": [], "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.\n\nThe StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "pdf": "/pdf/cdf67a70e320f5ae662c3125fbd60815b3449035.pdf", "paperhash": "wang|structbert_incorporating_language_structures_into_pretraining_for_deep_language_understanding", "_bibtex": "@inproceedings{\nWang2020StructBERT:,\ntitle={StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},\nauthor={Wei Wang and Bin Bi and Ming Yan and Chen Wu and Jiangnan Xia and Zuyi Bao and Liwei Peng and Luo Si},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgQ4lSFPH}\n}", "original_pdf": "/attachment/5c636b602c138c3b16b505bc5c06d9b812373538.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "4-Ry-UlJY", "original": null, "number": 1, "cdate": 1576798744114, "ddate": null, "tcdate": 1576798744114, "tmdate": 1576800892031, "tddate": null, "forum": "BJgQ4lSFPH", "replyto": "BJgQ4lSFPH", "invitation": "ICLR.cc/2020/Conference/Paper2242/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes a pair of complementary word- and sentence-level pretraining objectives for BERT-style models, and shows that they are empirically effective, especially when used with an already-pretrained RoBERTa model.\n\nWork of this kind has been extremely impactful in NLP, and so I'm somewhat biased toward acceptance: If this isn't published, it seems likely that other groups will go to the trouble to replicate roughly these experiments. However, I think the paper is borderline. Reviewers were impressed by the results, but not convinced that the ablations and analyses were sufficient to motivate the proposed methods, suggesting that some variants of the proposed methods could likely be substantially better. In addition, I agree strongly with R3 that framing this work around 'language structure' is disingenuous, and actively misleads readers about the contribution to the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": ["Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si"], "authorids": ["hebian.ww@alibaba-inc.com", "b.bi@alibaba-inc.com", "ym119608@alibaba-inc.com", "wuchen.wc@alibaba-inc.com", "jiangnan.xjn@alibaba-inc.com", "zuyi.bzy@alibaba-inc.com", "liwei.peng@alibaba-inc.com", "luo.si@alibaba-inc.com"], "keywords": [], "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.\n\nThe StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "pdf": "/pdf/cdf67a70e320f5ae662c3125fbd60815b3449035.pdf", "paperhash": "wang|structbert_incorporating_language_structures_into_pretraining_for_deep_language_understanding", "_bibtex": "@inproceedings{\nWang2020StructBERT:,\ntitle={StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},\nauthor={Wei Wang and Bin Bi and Ming Yan and Chen Wu and Jiangnan Xia and Zuyi Bao and Liwei Peng and Luo Si},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgQ4lSFPH}\n}", "original_pdf": "/attachment/5c636b602c138c3b16b505bc5c06d9b812373538.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJgQ4lSFPH", "replyto": "BJgQ4lSFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707820, "tmdate": 1576800256095, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2242/-/Decision"}}}, {"id": "HJgNRaZ0tr", "original": null, "number": 1, "cdate": 1571851723988, "ddate": null, "tcdate": 1571851723988, "tmdate": 1575426707002, "tddate": null, "forum": "BJgQ4lSFPH", "replyto": "BJgQ4lSFPH", "invitation": "ICLR.cc/2020/Conference/Paper2242/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper proposes to use additional structures within and between sentences for pre-training BERT. The basic idea is to shuffle either some n-grams within sentences or the sentences in texts, then train the model to predict the correct orders. Experiments in this work show that, with this additional training objective, the proposed pre-trained model, StructBERT, obtains good performance on the tasks including natural language understanding and question answering.\n\nOverall, I think the experiments and results in this work are not sufficient enough to support the claim:\n\n- It is necessary to show the performance of BERT only trained with the proposed word and sentence objectives. Otherwise, it is not clear how much benefit the model can get from them and the work is basically incremental. \n- Some justification is needed about why choosing trigrams and why 5% is a good number of sampling trigrams from texts\n\nBesides, there are some recent work on analyzing why BERT encodes any linguistic properties of texts, for example \n\n- Goldberg. Assessing BERT's syntactic abilities. 2019\n- Tenny et al. BERT Rediscovers the Classical NLP Pipeline. ACL 2019\n- Tenny et al. What do you learn from context? ICLR 2019\n\nAll of them show positive results on BERT can capture some syntactic information from text automatically. Which makes me wonder why the simple additional training objective proposed in this work can still lead to performance improvement. Is there an explanation? \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2242/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2242/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": ["Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si"], "authorids": ["hebian.ww@alibaba-inc.com", "b.bi@alibaba-inc.com", "ym119608@alibaba-inc.com", "wuchen.wc@alibaba-inc.com", "jiangnan.xjn@alibaba-inc.com", "zuyi.bzy@alibaba-inc.com", "liwei.peng@alibaba-inc.com", "luo.si@alibaba-inc.com"], "keywords": [], "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.\n\nThe StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "pdf": "/pdf/cdf67a70e320f5ae662c3125fbd60815b3449035.pdf", "paperhash": "wang|structbert_incorporating_language_structures_into_pretraining_for_deep_language_understanding", "_bibtex": "@inproceedings{\nWang2020StructBERT:,\ntitle={StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},\nauthor={Wei Wang and Bin Bi and Ming Yan and Chen Wu and Jiangnan Xia and Zuyi Bao and Liwei Peng and Luo Si},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgQ4lSFPH}\n}", "original_pdf": "/attachment/5c636b602c138c3b16b505bc5c06d9b812373538.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgQ4lSFPH", "replyto": "BJgQ4lSFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575660490818, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2242/Reviewers"], "noninvitees": [], "tcdate": 1570237725656, "tmdate": 1575660490832, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2242/-/Official_Review"}}}, {"id": "rJxVMwHhsH", "original": null, "number": 3, "cdate": 1573832460092, "ddate": null, "tcdate": 1573832460092, "tmdate": 1573836447206, "tddate": null, "forum": "BJgQ4lSFPH", "replyto": "HJgNRaZ0tr", "invitation": "ICLR.cc/2020/Conference/Paper2242/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you so much for going through the paper carefully and providing positive and useful feedback to our work! \nPlease see our responses below:\n\nC1. Thanks for the suggestion. In StructBERT, the new sentence objective is designed to replace the Next Sentence Prediction (NSP) objective in original BERT, while the new word objective is a supplement to the masked LM objective. We will pretrain only with the new sentence and word objectives (without masked LM) to study how well it performs compared with BERT.\n\nC2. Following BERT's methodology, we experimented with different configurations of shuffled N-grams  and sampling rates, and found out the best setting of trigrams and 5% sampling rate. Analysis of the experiments will be detailed in the final version to justify our configuration choices.\n\nC3. Although original BERT can capture some syntactic information from text, we believe that our new structural objectives can inject more capability into the language model: 1) The new word objective forces the model to correct locally shuffled trigrams, and thus enhances its capability in modeling local syntax. Besides, the new word objective used for word ordering can be effective in controlling local fluency, which is also indicated in [1].  2) The new sentence objective, on the other hand, enables the model to capture discourse-level coherence properties between sentences (e.g., strengthening, contrast and causality). Similar findings are also reported in [2, 3].\n\n[1] Discriminative Syntax-Based Word Ordering for Text Generation, Zhang and Clark 2015\n[2] Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning, Jernite et al. 2017\n[3] ALBERT: A Lite BERT for Self-supervised Learning of Language Representations, Lan et al. 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper2242/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": ["Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si"], "authorids": ["hebian.ww@alibaba-inc.com", "b.bi@alibaba-inc.com", "ym119608@alibaba-inc.com", "wuchen.wc@alibaba-inc.com", "jiangnan.xjn@alibaba-inc.com", "zuyi.bzy@alibaba-inc.com", "liwei.peng@alibaba-inc.com", "luo.si@alibaba-inc.com"], "keywords": [], "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.\n\nThe StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "pdf": "/pdf/cdf67a70e320f5ae662c3125fbd60815b3449035.pdf", "paperhash": "wang|structbert_incorporating_language_structures_into_pretraining_for_deep_language_understanding", "_bibtex": "@inproceedings{\nWang2020StructBERT:,\ntitle={StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},\nauthor={Wei Wang and Bin Bi and Ming Yan and Chen Wu and Jiangnan Xia and Zuyi Bao and Liwei Peng and Luo Si},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgQ4lSFPH}\n}", "original_pdf": "/attachment/5c636b602c138c3b16b505bc5c06d9b812373538.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgQ4lSFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference/Paper2242/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2242/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2242/Reviewers", "ICLR.cc/2020/Conference/Paper2242/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2242/Authors|ICLR.cc/2020/Conference/Paper2242/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144250, "tmdate": 1576860552264, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference/Paper2242/Reviewers", "ICLR.cc/2020/Conference/Paper2242/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2242/-/Official_Comment"}}}, {"id": "S1gP2REhjH", "original": null, "number": 1, "cdate": 1573830318949, "ddate": null, "tcdate": 1573830318949, "tmdate": 1573831860204, "tddate": null, "forum": "BJgQ4lSFPH", "replyto": "H1lYNb_e5S", "invitation": "ICLR.cc/2020/Conference/Paper2242/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you so much for going through the paper carefully and providing positive and useful feedback to our work! \nPlease see our responses below:\n\nC1. Thanks for your valuable suggestion. In our paper, structure refers to the word and sentence ordering inherent in natural language. Word trigrams are unscrambled to uncover the word ordering structure. We understand that the references to Elman 1990 are not very effective in clarification, and will remove them as suggested.\n\nC2. Thanks for the comment. We will add the missing citations accordingly.\n\nC3. The pretraining objective in XLNet belongs to the autoregressive (AR) language modeling, where they use all permutations of the factorization order to approximate the AR objective. By contrast, our new word objective is still an autoencoding (AE) one, which is designed to model local language structures by incorporating word  ordering into pretraining. Moreover, our word objective also differs from XLNet's in that ours permutates word order while XLNet's objective permutates factorization order. Specifically, the order of words in XLNet does not change given their fixed positional embeddings. In contrast, StructBERT shuffles words by changing their positions in text. We will include the elaboration in our final version.\n\nC4. We did try bigram and four-gram shuffling orders, but did not observe further improvement over trigram shuffling. We speculate that shuffling less words (bigrams) cannot take full advantage of the word ordering structure, while shuffling more words (4+-grams) can introduce more noise and harm the robustness of the model.\n\nC5. The Binary Ordering of Sentences in [2]  models the ordering of two consecutive sentences. Despite the similarity, our new objective differs in two ways: 1) It is defined on textual segments rather than natural sentences. 2) It is 3-way classification of segments while the objective in [2] determines binary ordering of sentences. We will add this work and its difference from ours in our final version.\n\nC6. It has been fixed."}, "signatures": ["ICLR.cc/2020/Conference/Paper2242/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": ["Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si"], "authorids": ["hebian.ww@alibaba-inc.com", "b.bi@alibaba-inc.com", "ym119608@alibaba-inc.com", "wuchen.wc@alibaba-inc.com", "jiangnan.xjn@alibaba-inc.com", "zuyi.bzy@alibaba-inc.com", "liwei.peng@alibaba-inc.com", "luo.si@alibaba-inc.com"], "keywords": [], "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.\n\nThe StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "pdf": "/pdf/cdf67a70e320f5ae662c3125fbd60815b3449035.pdf", "paperhash": "wang|structbert_incorporating_language_structures_into_pretraining_for_deep_language_understanding", "_bibtex": "@inproceedings{\nWang2020StructBERT:,\ntitle={StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},\nauthor={Wei Wang and Bin Bi and Ming Yan and Chen Wu and Jiangnan Xia and Zuyi Bao and Liwei Peng and Luo Si},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgQ4lSFPH}\n}", "original_pdf": "/attachment/5c636b602c138c3b16b505bc5c06d9b812373538.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgQ4lSFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference/Paper2242/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2242/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2242/Reviewers", "ICLR.cc/2020/Conference/Paper2242/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2242/Authors|ICLR.cc/2020/Conference/Paper2242/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144250, "tmdate": 1576860552264, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference/Paper2242/Reviewers", "ICLR.cc/2020/Conference/Paper2242/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2242/-/Official_Comment"}}}, {"id": "H1eXLxr2oB", "original": null, "number": 2, "cdate": 1573830731211, "ddate": null, "tcdate": 1573830731211, "tmdate": 1573830731211, "tddate": null, "forum": "BJgQ4lSFPH", "replyto": "rJer0FEJqB", "invitation": "ICLR.cc/2020/Conference/Paper2242/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you so much for going through the paper carefully and providing such a positive feedback. Please see below our response to your comments:\n\nC1. The intuition of word ordering task is from the task of Grammatical error correction, while the intuition of sentence ordering task is inspired from the discourse-level coherence property and causal relationship between the natural  sentences. Yes, we have also tested some other tasks: 1) mask only entities or nouns, 2) increase the mask rate, 3) predict the next sentence, nonadjacent sentence in the same document, and random sentence from another document. But we did not observe more improvement.\n\nC2. We agree with the reviewer about this and it has been fixed accordingly.\n\nC3. We have been in touch with SQuAD's administrator to evaluate our submitted model and update its score on the leaderboard. This process involves much manual effort from both us and the administrator. We have not got the updated score from the administrator yet. We will update our results on SQuAD upon receipt."}, "signatures": ["ICLR.cc/2020/Conference/Paper2242/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": ["Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si"], "authorids": ["hebian.ww@alibaba-inc.com", "b.bi@alibaba-inc.com", "ym119608@alibaba-inc.com", "wuchen.wc@alibaba-inc.com", "jiangnan.xjn@alibaba-inc.com", "zuyi.bzy@alibaba-inc.com", "liwei.peng@alibaba-inc.com", "luo.si@alibaba-inc.com"], "keywords": [], "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.\n\nThe StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "pdf": "/pdf/cdf67a70e320f5ae662c3125fbd60815b3449035.pdf", "paperhash": "wang|structbert_incorporating_language_structures_into_pretraining_for_deep_language_understanding", "_bibtex": "@inproceedings{\nWang2020StructBERT:,\ntitle={StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},\nauthor={Wei Wang and Bin Bi and Ming Yan and Chen Wu and Jiangnan Xia and Zuyi Bao and Liwei Peng and Luo Si},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgQ4lSFPH}\n}", "original_pdf": "/attachment/5c636b602c138c3b16b505bc5c06d9b812373538.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJgQ4lSFPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference/Paper2242/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2242/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2242/Reviewers", "ICLR.cc/2020/Conference/Paper2242/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2242/Authors|ICLR.cc/2020/Conference/Paper2242/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144250, "tmdate": 1576860552264, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2242/Authors", "ICLR.cc/2020/Conference/Paper2242/Reviewers", "ICLR.cc/2020/Conference/Paper2242/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2242/-/Official_Comment"}}}, {"id": "rJer0FEJqB", "original": null, "number": 2, "cdate": 1571928525076, "ddate": null, "tcdate": 1571928525076, "tmdate": 1572972364440, "tddate": null, "forum": "BJgQ4lSFPH", "replyto": "BJgQ4lSFPH", "invitation": "ICLR.cc/2020/Conference/Paper2242/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a new pre-trained language model based on BERT, called StructBERT. The key contributions are the two new pre-train objectives, (1) word structural objective, where the goal is to reconstruct the right order of intentionally shuffled word tokens, and (2) sentence structural objective, a three-class sentence-pair prediction, either the 2nd sentence precedes the 1st, the 2nd succeeds the 1st, or the 2nd is randomly selected. Unlike the original NSP (next sentence prediction) task, which is simple but tends out to be not so helpful in many downstream tasks, both proposed pre-train objectives seem to be rather useful in benchmarks tested in the paper, including GLUE, SNLI, and SQuAD. \n\nThe paper is well written and understandable for anyone who has a basic background about BERT or pre-train. The experimental results are impressive. Some of my questions / suggestions:\n\n- The two auxiliary tasks are evidently helpful. I wonder what intuition/theory leads to the selection of these two tasks? If the authors have test multiple other tasks that were not as helpful, it is also interesting to know them. \n\n- The wording of the text should be revised to reflect the up-to-date leaderboard results. Personally, I don't think the leaderboard results are that critical, but just want to make sure the writing is accurate at the time of publishing.\n\n- Please also update the results from SQuAD 1.1 CodaLab. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2242/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2242/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": ["Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si"], "authorids": ["hebian.ww@alibaba-inc.com", "b.bi@alibaba-inc.com", "ym119608@alibaba-inc.com", "wuchen.wc@alibaba-inc.com", "jiangnan.xjn@alibaba-inc.com", "zuyi.bzy@alibaba-inc.com", "liwei.peng@alibaba-inc.com", "luo.si@alibaba-inc.com"], "keywords": [], "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.\n\nThe StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "pdf": "/pdf/cdf67a70e320f5ae662c3125fbd60815b3449035.pdf", "paperhash": "wang|structbert_incorporating_language_structures_into_pretraining_for_deep_language_understanding", "_bibtex": "@inproceedings{\nWang2020StructBERT:,\ntitle={StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},\nauthor={Wei Wang and Bin Bi and Ming Yan and Chen Wu and Jiangnan Xia and Zuyi Bao and Liwei Peng and Luo Si},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgQ4lSFPH}\n}", "original_pdf": "/attachment/5c636b602c138c3b16b505bc5c06d9b812373538.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgQ4lSFPH", "replyto": "BJgQ4lSFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575660490818, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2242/Reviewers"], "noninvitees": [], "tcdate": 1570237725656, "tmdate": 1575660490832, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2242/-/Official_Review"}}}, {"id": "H1lYNb_e5S", "original": null, "number": 3, "cdate": 1572008241041, "ddate": null, "tcdate": 1572008241041, "tmdate": 1572972364397, "tddate": null, "forum": "BJgQ4lSFPH", "replyto": "BJgQ4lSFPH", "invitation": "ICLR.cc/2020/Conference/Paper2242/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces two new tasks for large scale language model pretraining: trigram word unscrambling and contextual sentence ordering. Using these tasks to pretrain on top of masked language modelling shows improvements when the resulting model is finetuned on downstream tasks. The proposed tasks are simple to implement, and particularly the sentence ordering task is an improvement over the original BERT next sentence task, which is widely regarded as too simple to drive learning good representations. For this reason, I recommend acceptance of this paper.\n\nSome minor quibbles:\n1) Structure in language usually means syntactic structure. How does unscrambling word trigrams help uncover syntactic structure? The references to Elman 1990 also don't serve to clarify anything, I suggest that they are removed.\n2) Some prior work on word ordering (e.g. [1] and older papers cited therein) is missing.\n3) The permutation objective seems very similar to the XLNet objective. Could the authors elaborate more on this in the paper?\n4) Did the authors try with other n-gram shuffling orders?\n5) The sentence ordering task has been used previously (e.g. [2]).\n6) Table 1 overhangs the right margin.\n\nReferences:\n[1] Discriminative Syntax-Based Word Ordering for Text Generation, Zhang and Clark 2015\n[2] Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning, Jernite et al. 2017"}, "signatures": ["ICLR.cc/2020/Conference/Paper2242/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2242/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding", "authors": ["Wei Wang", "Bin Bi", "Ming Yan", "Chen Wu", "Jiangnan Xia", "Zuyi Bao", "Liwei Peng", "Luo Si"], "authorids": ["hebian.ww@alibaba-inc.com", "b.bi@alibaba-inc.com", "ym119608@alibaba-inc.com", "wuchen.wc@alibaba-inc.com", "jiangnan.xjn@alibaba-inc.com", "zuyi.bzy@alibaba-inc.com", "liwei.peng@alibaba-inc.com", "luo.si@alibaba-inc.com"], "keywords": [], "abstract": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman, we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.\n\nThe StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models at the time of model submission), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.", "pdf": "/pdf/cdf67a70e320f5ae662c3125fbd60815b3449035.pdf", "paperhash": "wang|structbert_incorporating_language_structures_into_pretraining_for_deep_language_understanding", "_bibtex": "@inproceedings{\nWang2020StructBERT:,\ntitle={StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},\nauthor={Wei Wang and Bin Bi and Ming Yan and Chen Wu and Jiangnan Xia and Zuyi Bao and Liwei Peng and Luo Si},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgQ4lSFPH}\n}", "original_pdf": "/attachment/5c636b602c138c3b16b505bc5c06d9b812373538.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgQ4lSFPH", "replyto": "BJgQ4lSFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2242/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575660490818, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2242/Reviewers"], "noninvitees": [], "tcdate": 1570237725656, "tmdate": 1575660490832, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2242/-/Official_Review"}}}], "count": 8}