{"notes": [{"id": "H1e5GJBtDr", "original": "B1g1fQh_vr", "number": 1589, "cdate": 1569439505687, "ddate": null, "tcdate": 1569439505687, "tmdate": 1577168255175, "tddate": null, "forum": "H1e5GJBtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QJEcfi_vQn", "original": null, "number": 1, "cdate": 1576798727307, "ddate": null, "tcdate": 1576798727307, "tmdate": 1576800909202, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a self-attention-based autoregressive model called Axial Transformers for images and other data organized as high dimensional tensors. The Axial Attention is applied within each axis of the data to accelerate the processing.\n\nMost of the authors claim that main idea behind Axial Attention is widely applicable, which can be used in many core vision tasks, such as detection and classification. However, the revision fails to provide more application for Axial attention.\n\nOverall, the idea behind this paper is interesting but more convincing experimental results are needed.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704859, "tmdate": 1576800252522, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Decision"}}}, {"id": "HkgnTM9hir", "original": null, "number": 6, "cdate": 1573851843621, "ddate": null, "tcdate": 1573851843621, "tmdate": 1573851843621, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment", "content": {"title": "Response to all reviewers", "comment": "Dear reviewers, thank you for your comments. We have uploaded a revised version of our paper incorporating your feedback. Specifically:\n\n- We are now more explicit about the scope of our paper and its intended contribution. Our work is about autoregressive modeling for images and other data organized as multidimensional tensors -- it falls in the same line of work and scope as papers such as Pixel Recurrent Neural Networks (van den Oord et al 2016), Image Transformer (Parmar et al 2018), Subscale Pixel Networks (Menick and Kalchbrenner 2019), and many others.\n\n- We have included improved results on video modeling (1.29 bits/dim on BAIR robot pushing).\n\n- We have included an ablation study using a baseline architecture for our image model. Specifically, we replace the inner decoder with an LSTM. We find that the results on ImageNet 32x32 and 64x64 are slightly worse by 0.01 and 0.02 bits/dim, respectively, and also that training time is slower than our original model. See Section 4.1 in the revised paper for the full discussion.\n\n- We have included discussion on relationship with other attention proposals in the computer vision literature, such as CCNet. Our contribution and emphasis is on uses of masked axial attention and how to combine it in a way that leads to a valid autoregressive image model (the dependencies between outputs and inputs must obey the raster scan order so that it defines a valid probabilistic model), whereas other works do not employ masking and are not focused on defining an autoregressive model.\n\n- We have also increased our emphasis of the semi-parallel sampling aspect of our model, that is unique among autoregressive image and video models.\n\nAll in all we believe the paper, proposed methods, and open source code will be very useful to the generative image modeling community, and we ask you to consider this when making your final decision. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1e5GJBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1589/Authors|ICLR.cc/2020/Conference/Paper1589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153778, "tmdate": 1576860545257, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment"}}}, {"id": "Bkp7J9Mor", "original": null, "number": 4, "cdate": 1573195556517, "ddate": null, "tcdate": 1573195556517, "tmdate": 1573253620626, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper proposes a novel approach to deal with the computational problems of self-attention without introducing independence assumptions. The proposed approach is simple, easy to understand, and easy to implement.\n\nHowever, evaluation for this paper is severely lacking. As it is, there is not enough information provided to adequately assess the proposed method's strengths in practice. The following should be added:\n\nEvaluation on a variety of different tasks, such as image segmentation, temporally consistent object detection, object tracking, etc. Why are the evaluations limited to generative modeling? To prove the generality of the method (as claimed), it needs to be applied to various tasks.\nRuntime (in inference) comparisons for each of the datasets and for each of the baselines. Additionally, a theoretical analysis for runtime in terms of the size of the input should be given (the column in Table 1 should have runtimes for each method clearly specified, and this should be done for each dataset and baseline)\nAblation study. What is the baseline architecture used without axial attention? There is only comparison to previous work which may have used a different architecture.\n\nIf these concerns are thoroughly addressed, I would be happy to increase my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575628661289, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Reviewers"], "noninvitees": [], "tcdate": 1570237735178, "tmdate": 1575628661303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Review"}}}, {"id": "S1l61fO7jS", "original": null, "number": 5, "cdate": 1573253605235, "ddate": null, "tcdate": 1573253605235, "tmdate": 1573253605235, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "SyeFnbO7jB", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment", "content": {"title": "based on this discussion -- changing rating to reject ", "comment": "see title. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1e5GJBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1589/Authors|ICLR.cc/2020/Conference/Paper1589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153778, "tmdate": 1576860545257, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment"}}}, {"id": "SyeFnbO7jB", "original": null, "number": 4, "cdate": 1573253552842, "ddate": null, "tcdate": 1573253552842, "tmdate": 1573253552842, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "H1ezBTaziH", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment", "content": {"title": "response", "comment": "Thank you for your response.\n\nImage modeling attention-based architectures is a very narrow scope indeed. If this is truly the scope, I vote to reject the paper. My concerns are as follows:\n\nImage modeling is a very broad task. It is currently unclear whether attention-based architectures will be superior to other methods. What is the reason to limit to only this specific subset of image modeling? It seems arbitrary. If image modeling is the true task, a full list of prior work on image modeling should be included and compared with.\nTransformers are used in many applications beyond image modeling as well. It seems as if the proposed attention mechanism could deliver significant gains in these areas. Is there a reason to focus only on generative image modeling versus other popular CV or NLP tasks as mentioned before? This would be a powerful paper if gains were shown on a wide variety of tasks, with minimal modification to the underlying method (as claimed). As it stands, the scope is too narrow"}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1e5GJBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1589/Authors|ICLR.cc/2020/Conference/Paper1589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153778, "tmdate": 1576860545257, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment"}}}, {"id": "r1lycyCMsH", "original": null, "number": 3, "cdate": 1573212039352, "ddate": null, "tcdate": 1573212039352, "tmdate": 1573212039352, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "ryeuXv839B", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment", "content": {"title": "Intended scope of paper falls exclusively within generative modelling (here too)", "comment": "Thanks for your remarks. Some of your major points (2 and to some extent 1) concern the scope of the notions that we introduce, specifically axial attention. Please note that the intended scope is only axial attention within multidimensional transformers, that is within generative modelling of multidimensional data such as images and videos. We are aiming at making this very clear in the paper. Please see our related remarks to the other reviewers."}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1e5GJBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1589/Authors|ICLR.cc/2020/Conference/Paper1589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153778, "tmdate": 1576860545257, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment"}}}, {"id": "H1ezBTaziH", "original": null, "number": 1, "cdate": 1573211450273, "ddate": null, "tcdate": 1573211450273, "tmdate": 1573211693162, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "Bkp7J9Mor", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment", "content": {"title": "Intended scope of paper falls exclusively within generative modelling", "comment": "Thank you for your comments. We would like to point outright that the intended scope and focus of the paper is exclusively generative models of images with an extension to videos. Some aspects of the paper make this clear:\n- The title centrally includes \"multidimensional transformers\" that are only generative models indeed with an encoder part and a decoder part (like the original transformer for language).\n- Our main contribution is the Axial Transformer architecture itself, i.e. how to easily apply (masked) axial attention to multi-dimensional transformers by using a number of additional features: reordering of RGB channels, shifting operation for the rows, shallow and hence faster strict autoregressive decoder, no need for custom kernels.\n- The thorough and exclusive comparison with previous image modelling attention-based architectures.\n\nHowever, we also realize now that some sentences in the paper may hint at axial attention as a stand-alone operation to be used beyond generative modelling. Showing this is beyond the scope of our paper and we are working to make this clear and rephrase the relevant passages and subsections."}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1e5GJBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1589/Authors|ICLR.cc/2020/Conference/Paper1589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153778, "tmdate": 1576860545257, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment"}}}, {"id": "SkgThTazjS", "original": null, "number": 2, "cdate": 1573211572598, "ddate": null, "tcdate": 1573211572598, "tmdate": 1573211572598, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "BylznUt3qr", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment", "content": {"title": "Intended scope of paper falls exclusively within generative modelling", "comment": "Thank you for remarks. Since one of your major objections is at its core the same objection as that by reviewer #1, please see comment above. We want to treat our paper as a new architecture for image (and video) generation and we are making this clear in the text."}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1e5GJBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1589/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1589/Authors|ICLR.cc/2020/Conference/Paper1589/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153778, "tmdate": 1576860545257, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Authors", "ICLR.cc/2020/Conference/Paper1589/Reviewers", "ICLR.cc/2020/Conference/Paper1589/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Comment"}}}, {"id": "S1lPeozCYH", "original": null, "number": 1, "cdate": 1571855086829, "ddate": null, "tcdate": 1571855086829, "tmdate": 1572972448972, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes axial attention as an alternative of self-attention for data arranged as large multidimensional tensors, which costs too much computational resource since the complexity of traditional self-attention is quadratic in order to capture long-range dependencies for full receptive fields. The axial attention is applied within each axis of the data separately while keeping information along other axes independent. Therefore, for a d-dimensional tensor with N = S^d, axial attention saves O(N^{(d\u22121)/d}) computation over standard attention. The proposed axial attention can be used within standard Transformer layers in a straightforward manner to produce Axial Transformer layers, without changing the basic building blocks of traditional Transformer architecture.  The authors did experiments on two standard datasets for generative image and video models: down-sampled ImageNet and BAIR Robot Pushing, and they claim that their proposed method matches or outperforms the state-of-the-art on ImageNet-32 and ImageNet-64 image benchmarks and sets a significant new state-of-the-art on the BAIR Robot Pushing video benchmark. \n\nReasons to accept:\n\n1.\tSimple, easy-to-implement yet effective approach to adapt self-attention to large multidimensional data, which can save considerable computation for efficiency, while still have competitive performance.\n2.\tClear writing, with sufficient but not redundant introduction of background knowledge and explanation of both the advantages and drawbacks of existing models (too large computational complexity on high-dimensional data).\n\nSuggestions for improvement:\n\n1.\tIt would be better if the authors can provide more analysis or case study to show the reason why Axial attention (Axial Transformer) can reach good performance even if it omits considerable operations compared to traditional Transformers, or to show why the attention operations within axis are important instead of attention operations between axis. \n2.\tDefinition of \u201caxis\u201d should be more clear in section 3 (there could be some ambiguities of \u201caxis\u201d).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575628661289, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Reviewers"], "noninvitees": [], "tcdate": 1570237735178, "tmdate": 1575628661303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Review"}}}, {"id": "ryeuXv839B", "original": null, "number": 2, "cdate": 1572788000102, "ddate": null, "tcdate": 1572788000102, "tmdate": 1572972448929, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "It is known that the standard self-attention method is computationally expensive and cost a significantly large amount of storage when the number of points to be attended is large.\n\nThis paper attempts to solve this problem and proposed the Axial Attention method. It is claimed to be able to save an O(N^(d-1)/d) factor of resources over standard self-attention.\n\nThe proposed method looks novel to me, but some of the related works are missing and the experiment session is insufficient. \n\n1)  The author should at least include the following works which also aim to reduce the cost of self-attention. Since the author did not mention these works which also focus on solving the same problem, It is hard for me to judge if the proposed method is better than existing works.\n[a] CCNet: Criss-Cross Attention for Semantic Segmentation\n[b] A^2-Nets: Double Attention Networks\n\n2) self-attention has shown its effectiveness on a broad range of computer vision tasks, including image generation, detection, segmentation, and classification. I do not get why the proposed method is only benchmarked for generative models. Is it because the proposed method cannot be adopted on other popular CV tasks, such as detection, segmentation, and classification? Extra experiments should be included if the proposed method is not only designed for generative models.\n\n3) The ablation study is missing. The author directly compared its own method with other existing methods that are implemented and trained with different hyperparameters. It is hard to know which indeed benefits the accuracy gain and how significant is the proposed method. \n\n4) In table 2 and 3, I do not see a clear advantage of the proposed method over the SOTA methods."}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575628661289, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Reviewers"], "noninvitees": [], "tcdate": 1570237735178, "tmdate": 1575628661303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Review"}}}, {"id": "BylznUt3qr", "original": null, "number": 3, "cdate": 1572800169595, "ddate": null, "tcdate": 1572800169595, "tmdate": 1572972448885, "tddate": null, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "invitation": "ICLR.cc/2020/Conference/Paper1589/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #5", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper claims to propose a new approach to solve the computational problems of self-attention. However, the paper mainly focuses on adapting Transformer for image generation, which has far less applications. The whole paper needs to be rewritten to make their target and contribution clearer.\n\n1. The authors overclaim that they provide a new approach for accelerating self-attention. However, they only adapted Transformer for image generation. In fact, Transformer does not equal to self-attention. Currently, two directional self-attention like Bert has much wider applications compared with Transformer like sequential self-attention. \n\n2. For a paper claim to improve self-attention, they should show its effectiveness on a broad range of tasks, with comprehensive experimental evaluation. However, authors mainly reported the image generation on several datasets. \n\nOverall, the authors need to rewrite the paper. They should either show more applications with the proposed self-attention approach or treat it as a new approach for image generation."}, "signatures": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1589/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jonathanho@google.com", "nalk@google.com", "diwe@google.com", "salimans@google.com"], "title": "Axial Attention in Multidimensional Transformers", "authors": ["Jonathan Ho", "Nal Kalchbrenner", "Dirk Weissenborn", "Tim Salimans"], "pdf": "/pdf/2c2be8a5f23b2fdb3df67a1720ccdc4932aaa206.pdf", "TL;DR": "Easy-to-implement and effective multidimensional Transformer with faster sampling", "abstract": "Self-attention effectively captures large receptive fields with high information bandwidth, but its computational resource requirements grow quadratically with the number of points over which attention is performed. For data arranged as large multidimensional tensors, such as images and videos, the quadratic growth makes self-attention prohibitively expensive. These tensors often have thousands of positions that one wishes to capture and proposed attentional alternatives either limit the resulting receptive field or require custom subroutines. We propose Axial Attention, a simple generalization of self-attention that naturally aligns with the multiple dimensions of the tensors in both the encoding and the decoding settings. The Axial Transformer uses axial self-attention layers and a shift operation to efficiently build large and full receptive fields.  Notably the proposed structure of the layers allows for the vast majority of the context to be computed in parallel during decoding without introducing any independence assumptions. This semi-parallel structure goes a long way to making decoding from even a very large Axial Transformer broadly applicable. We demonstrate state-of-the-art results for the Axial Transformer on the ImageNet-32 and ImageNet-64 image benchmarks as well as on the BAIR Robotic Pushing video benchmark. We open source the implementation of Axial Transformers.", "keywords": ["self-attention", "transformer", "images", "videos"], "paperhash": "ho|axial_attention_in_multidimensional_transformers", "original_pdf": "/attachment/8c348015fd491b5557fdf3cdf774432a58ccdbf7.pdf", "_bibtex": "@misc{\nho2020axial,\ntitle={Axial Attention in Multidimensional Transformers},\nauthor={Jonathan Ho and Nal Kalchbrenner and Dirk Weissenborn and Tim Salimans},\nyear={2020},\nurl={https://openreview.net/forum?id=H1e5GJBtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1e5GJBtDr", "replyto": "H1e5GJBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1589/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575628661289, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1589/Reviewers"], "noninvitees": [], "tcdate": 1570237735178, "tmdate": 1575628661303, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1589/-/Official_Review"}}}], "count": 12}