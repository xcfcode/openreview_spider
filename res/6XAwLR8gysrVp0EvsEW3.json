{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458228335154, "tcdate": 1458228335154, "id": "5QzqAxN1vIZgXpo7i3N6", "invitation": "ICLR.cc/2016/workshop/-/paper/113/comment", "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "zvwWvn38QSM8kw3ZingL", "signatures": ["~Mudassar_Abbas1"], "readers": ["everyone"], "writers": ["~Mudassar_Abbas1"], "content": {"title": "Author response", "comment": "Please see above for responses from the authors and for a link to a revised version of the manuscript."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others", "abstract": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods.", "pdf": "/pdf/6XAwLR8gysrVp0EvsEW3.pdf", "paperhash": "abbas|closetoclean_regularization_relates_virtual_adversarial_training_ladder_networks_and_others", "conflicts": ["aalto.fi", "zenrobotics.com", "thecuriousaicompany.com", "ed.ac.uk", "umontreal.ca"], "authors": ["Mudassar Abbas", "Jyri Kivinen", "Tapani Raiko"], "authorids": ["mudassar.abbas@aalto.fi", "jyri.kivinen@aalto.fi", "tapani.raiko@aalto.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455818379485, "ddate": null, "super": null, "final": null, "tcdate": 1455818379485, "id": "ICLR.cc/2016/workshop/-/paper/113/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "6XAwLR8gysrVp0EvsEW3", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/113/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458228183346, "tcdate": 1458228183346, "id": "XL9jYPxZ0cXB8D1RUG75", "invitation": "ICLR.cc/2016/workshop/-/paper/113/comment", "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "MwVknv8vzcqxwkg1t7Wr", "signatures": ["~Mudassar_Abbas1"], "readers": ["everyone"], "writers": ["~Mudassar_Abbas1"], "content": {"title": "Author response", "comment": "Please see above for responses from the authors and for a link to a revised version of the manuscript."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others", "abstract": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods.", "pdf": "/pdf/6XAwLR8gysrVp0EvsEW3.pdf", "paperhash": "abbas|closetoclean_regularization_relates_virtual_adversarial_training_ladder_networks_and_others", "conflicts": ["aalto.fi", "zenrobotics.com", "thecuriousaicompany.com", "ed.ac.uk", "umontreal.ca"], "authors": ["Mudassar Abbas", "Jyri Kivinen", "Tapani Raiko"], "authorids": ["mudassar.abbas@aalto.fi", "jyri.kivinen@aalto.fi", "tapani.raiko@aalto.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455818379485, "ddate": null, "super": null, "final": null, "tcdate": 1455818379485, "id": "ICLR.cc/2016/workshop/-/paper/113/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "6XAwLR8gysrVp0EvsEW3", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/113/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458227765557, "tcdate": 1458227765557, "id": "91Ex3O98OHkRlNvXUV0M", "invitation": "ICLR.cc/2016/workshop/-/paper/113/comment", "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "P7VnoJr49SKvjNORtJ6l", "signatures": ["~Mudassar_Abbas1"], "readers": ["everyone"], "writers": ["~Mudassar_Abbas1"], "content": {"title": "Author response", "comment": "Please see above for responses from the authors and for a link to a revised version of the manuscript."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others", "abstract": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods.", "pdf": "/pdf/6XAwLR8gysrVp0EvsEW3.pdf", "paperhash": "abbas|closetoclean_regularization_relates_virtual_adversarial_training_ladder_networks_and_others", "conflicts": ["aalto.fi", "zenrobotics.com", "thecuriousaicompany.com", "ed.ac.uk", "umontreal.ca"], "authors": ["Mudassar Abbas", "Jyri Kivinen", "Tapani Raiko"], "authorids": ["mudassar.abbas@aalto.fi", "jyri.kivinen@aalto.fi", "tapani.raiko@aalto.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455818379485, "ddate": null, "super": null, "final": null, "tcdate": 1455818379485, "id": "ICLR.cc/2016/workshop/-/paper/113/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "6XAwLR8gysrVp0EvsEW3", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/113/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458227122208, "tcdate": 1458227122208, "id": "2xwnGxw0ZupKBZvXtQX7", "invitation": "ICLR.cc/2016/workshop/-/paper/113/comment", "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "6XAwLR8gysrVp0EvsEW3", "signatures": ["~Mudassar_Abbas1"], "readers": ["everyone"], "writers": ["~Mudassar_Abbas1"], "content": {"title": "Comments from the authors on the reviews, updated manuscript", "comment": "We thank the reviewers for their comments and constructive criticism. We have the overall impression that the paper and the work has been appreciated by the reviewers, each of the reviews stating that the paper either presents interesting novel results (R11,R10) or advances work in an useful area of research (R12), with overall positive rating. \n\nWe have made a revised version of the paper in which we have tried to take into account the comments in the reviews suggesting updates, as well as possible. We find the reviews have enabled us to critically improve the document. An updated version of the manuscript is available from:\nhttps://drive.google.com/file/d/0B8h1xkyYY_msakE0bWtBNk9qOTg/view?usp=sharing\n\nBelow are our responses and actions to specific comments and criticisms in the reviews.\n\nReview 12:\n--------------\n\nWe thank the reviewer for pointing us the Bachman et al., 2014 article, a relevant article we had missed but fortunately not affecting our main contributions. We have taken a number of steps to take the issue into account in the revised version of the paper, including the following:\n* We have removed the claim of proposing a new regularization method\n* We now state studying as opposed to proposing the particular regularization method and state that it can be seen as an instance of the \nPseudo-Ensemble Agreement regularization method proposed in Bachman et al., 2014.\n\nWe thank the reviewer for sharing helpful experiences on regularization and also comments in terms of future work directions.\n\nReview 11:\n--------------\n\nWe thank the reviewer for pointing out a connection of the studied regularization framework to existing methods in the literature. We would appreciate the reviewer for providing us with any references on regularizers similar to the studied one, in the field of ''graph-based semi-supervised learning\" mentioned, so as to improve the paper on discussing relevant related work. We had also missed a paper pointed out in review 1, and made changes to our manuscript taking that into account; see comments above for details related to these, especially noticing that the main contributions of the paper have not been affected. \n\nWe are thankful for the reviewer on sharing opinions on effective regularization methods. On enforcing constraints on the entire space of the output function: it is of expected interest to us to only require the function to be smooth in areas densely populated with data and the function can be non-smooth elsewhere.\n\nReview 10:\n--------------\n\nWe thank the reviewer for suggestions on improving the document, and we have tried to take these into account in the revised paper as follows:\nRemark 1): the epsilon is now introduced after the definition at the end of the second paragraph in section 2.\nRemark 2): The VAT-section (now Sec. 3.4) has been modified for clarity in terms of the minimization/maximization issue pointed out; all methods are assuming minimization, and we have introduced R_VAT and used it in describing the results, for clarity.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others", "abstract": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods.", "pdf": "/pdf/6XAwLR8gysrVp0EvsEW3.pdf", "paperhash": "abbas|closetoclean_regularization_relates_virtual_adversarial_training_ladder_networks_and_others", "conflicts": ["aalto.fi", "zenrobotics.com", "thecuriousaicompany.com", "ed.ac.uk", "umontreal.ca"], "authors": ["Mudassar Abbas", "Jyri Kivinen", "Tapani Raiko"], "authorids": ["mudassar.abbas@aalto.fi", "jyri.kivinen@aalto.fi", "tapani.raiko@aalto.fi"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455818379485, "ddate": null, "super": null, "final": null, "tcdate": 1455818379485, "id": "ICLR.cc/2016/workshop/-/paper/113/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "6XAwLR8gysrVp0EvsEW3", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/113/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457635836912, "tcdate": 1457635836912, "id": "P7VnoJr49SKvjNORtJ6l", "invitation": "ICLR.cc/2016/workshop/-/paper/113/review/12", "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "6XAwLR8gysrVp0EvsEW3", "signatures": ["ICLR.cc/2016/workshop/paper/113/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/113/reviewer/12"], "content": {"title": "Useful area of research, method not particularly novel", "rating": "6: Marginally above acceptance threshold", "review": "The regularizer examined in this paper was previously presented in a more general form in \"Learning with Pseudo-Ensembles\" by Bachman et al. (NIPS 2014). See Eq. 2 on page 3 of: http://papers.nips.cc/paper/5487-learning-with-pseudo-ensembles.pdf. Bachman et al. consider controlling variation in any observable property of a model's behaviour w.r.t. some distribution over perturbations of the model and/or its inputs.\n\nIn my experience, penalizing first-order variation in euclidean space is often only marginally useful. It suffers from a strong bias towards shrinking all network outputs towards a constant value. It may be better to penalize stochastic approximations of higher-order derivatives.\n\nIn the semi-supervised classification setting, penalizing variation in distribution space using KL divergence works well. The KL divergence becomes robust to relatively large changes in the raw (i.e. pre softmax) output once the predictions become confident, so it suffers less from the tendency to shrink all outputs towards a constant value. Penalizing worst-case KL divergence over a ball of fixed radius was tried in the VAT paper, and penalizing expected-case KL divergence was tried in the paper by Bachman et al. Penalizing worst-case behaviour seems to be a more effective regularizer, but comes with a higher computational cost.\n\nAs you develop this work further, you should consider looking at the literature on \"stochastic progamming\" (https://en.wikipedia.org/wiki/Stochastic_programming) and \"robust optimization\" (https://en.wikipedia.org/wiki/Robust_optimization). Stochastic programming is about solving an optimization problem in expectation w.r.t. some distribution over misspecification of the problem, and robust optimization is about solving an optimization problem in the worst case w.r.t. some bounds over misspecification of the problem. A lot of mathematically rigorous work has been done in these fields, which may help with your formal analyses.\n\n******************\nReview summary\n******************\n1. The regularizer described in this paper is not particularly novel.\n\n2. Developing a stronger formal understanding of connections between different practical instantiations of the underlying concept (i.e. robustness to model/input perturbation) is worthwhile. This paper provides a decent step in that direction.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others", "abstract": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods.", "pdf": "/pdf/6XAwLR8gysrVp0EvsEW3.pdf", "paperhash": "abbas|closetoclean_regularization_relates_virtual_adversarial_training_ladder_networks_and_others", "conflicts": ["aalto.fi", "zenrobotics.com", "thecuriousaicompany.com", "ed.ac.uk", "umontreal.ca"], "authors": ["Mudassar Abbas", "Jyri Kivinen", "Tapani Raiko"], "authorids": ["mudassar.abbas@aalto.fi", "jyri.kivinen@aalto.fi", "tapani.raiko@aalto.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579948434, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579948434, "id": "ICLR.cc/2016/workshop/-/paper/113/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "6XAwLR8gysrVp0EvsEW3", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/113/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457625848919, "tcdate": 1457625848919, "id": "MwVknv8vzcqxwkg1t7Wr", "invitation": "ICLR.cc/2016/workshop/-/paper/113/review/11", "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "6XAwLR8gysrVp0EvsEW3", "signatures": ["ICLR.cc/2016/workshop/paper/113/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/113/reviewer/11"], "content": {"title": "An interesting view on commonly used regularizers", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a regularization based on the distance of predictions for similar inputs. This kind of regularization has been widely used in the 2000s for graph-based semi-supervised learning but may also be applied to fully supervised learning.\n\nThe connection with the penalty on the Jacobian is known but the implications of that regularization when using adversarial examples is interesting and new to me.\n\nMy main concern is about regularizations which explicitly enforce constraints on the output of the function. There are cases where we want the output of a function to change quickly as a function of the input and I believe enforcing the same smoothness across the entire space will likely be too strong or too weak.\n\nDue to this concern, I'm slightly leaning towards rejection but, as this is pure personal preference, I am fine with the paper being accepted.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others", "abstract": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods.", "pdf": "/pdf/6XAwLR8gysrVp0EvsEW3.pdf", "paperhash": "abbas|closetoclean_regularization_relates_virtual_adversarial_training_ladder_networks_and_others", "conflicts": ["aalto.fi", "zenrobotics.com", "thecuriousaicompany.com", "ed.ac.uk", "umontreal.ca"], "authors": ["Mudassar Abbas", "Jyri Kivinen", "Tapani Raiko"], "authorids": ["mudassar.abbas@aalto.fi", "jyri.kivinen@aalto.fi", "tapani.raiko@aalto.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579948711, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579948711, "id": "ICLR.cc/2016/workshop/-/paper/113/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "6XAwLR8gysrVp0EvsEW3", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/113/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457380408455, "tcdate": 1457380408455, "id": "zvwWvn38QSM8kw3ZingL", "invitation": "ICLR.cc/2016/workshop/-/paper/113/review/10", "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "6XAwLR8gysrVp0EvsEW3", "signatures": ["ICLR.cc/2016/workshop/paper/113/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/113/reviewer/10"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper studies the connection between different types of regularization for neural network training. The authors found that these regularizations can be expressed as different functionals of the singular values of the Jacobian matrix. The result is interesting and could be presented as a workshop paper. There are a few minor remarks to be taken into account.\n\n1.\tIn the paragraph after Eq. (1), the notation epsilon is mentioned before its actual usage in the first paragraph on the 2nd page. It could be defined after that.\n2.\tIt is not clear why VAT is maximizing the maximum eigenvalue of J^TJ (minimizing the negative value of the maximum eigenvalue of J^TJ ) while the others are performing the minimization. Clarification on this would be helpful.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others", "abstract": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods.", "pdf": "/pdf/6XAwLR8gysrVp0EvsEW3.pdf", "paperhash": "abbas|closetoclean_regularization_relates_virtual_adversarial_training_ladder_networks_and_others", "conflicts": ["aalto.fi", "zenrobotics.com", "thecuriousaicompany.com", "ed.ac.uk", "umontreal.ca"], "authors": ["Mudassar Abbas", "Jyri Kivinen", "Tapani Raiko"], "authorids": ["mudassar.abbas@aalto.fi", "jyri.kivinen@aalto.fi", "tapani.raiko@aalto.fi"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579948771, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579948771, "id": "ICLR.cc/2016/workshop/-/paper/113/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "6XAwLR8gysrVp0EvsEW3", "replyto": "6XAwLR8gysrVp0EvsEW3", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/113/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455712020422, "tcdate": 1455712020422, "id": "6XAwLR8gysrVp0EvsEW3", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "6XAwLR8gysrVp0EvsEW3", "signatures": ["~Mudassar_Abbas1"], "readers": ["everyone"], "writers": ["~Mudassar_Abbas1"], "content": {"CMT_id": "", "title": "Close-to-clean regularization relates virtual adversarial training, ladder networks and others", "abstract": "We propose a regularization framework where we feed an original clean data point and a nearby point through a mapping, which is then penalized by the Euclidian distance between the corresponding outputs. The nearby point may be chosen randomly or adversarially. We relate this framework to many existing regularization methods: It is a stochastic estimate of penalizing the Frobenius norm of the Jacobian of the mapping as in Poggio & Girosi (1990), it generalizes noise regularization (Sietsma & Dow, 1991), and it is a simplification of the canonical regularization term by the ladder networks in Rasmus et al. (2015). We also study the connection to virtual adversarial training (VAT) (Miyato et al., 2016) and show how VAT can be interpreted as penalizing the largest eigenvalue of a Fisher information matrix. Our main contribution is discovering connections between the proposed and existing regularization methods.", "pdf": "/pdf/6XAwLR8gysrVp0EvsEW3.pdf", "paperhash": "abbas|closetoclean_regularization_relates_virtual_adversarial_training_ladder_networks_and_others", "conflicts": ["aalto.fi", "zenrobotics.com", "thecuriousaicompany.com", "ed.ac.uk", "umontreal.ca"], "authors": ["Mudassar Abbas", "Jyri Kivinen", "Tapani Raiko"], "authorids": ["mudassar.abbas@aalto.fi", "jyri.kivinen@aalto.fi", "tapani.raiko@aalto.fi"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 8}