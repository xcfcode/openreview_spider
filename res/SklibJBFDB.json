{"notes": [{"id": "SklibJBFDB", "original": "Bye4gTouPH", "number": 1555, "cdate": 1569439490854, "ddate": null, "tcdate": 1569439490854, "tmdate": 1577168263164, "tddate": null, "forum": "SklibJBFDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Evaluating Semantic Representations of Source Code", "authors": ["Yaza Wainakh", "Moiz Rauf", "Michael Pradel"], "authorids": ["yaza.wainakh@gmail.com", "moiz.rauf@iste.uni-stuttgart.de", "michael@binaervarianz.de"], "keywords": ["embeddings", "representation", "source code", "identifiers"], "TL;DR": "A benchmark to evaluate neural embeddings of identifiers in source code.", "abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n", "code": "https://my.pcloud.com/publink/show?code=kZR2LzkZAViNngFuJ3ykFqNzbQF1TJRedJs7", "pdf": "/pdf/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "paperhash": "wainakh|evaluating_semantic_representations_of_source_code", "original_pdf": "/attachment/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "_bibtex": "@misc{\nwainakh2020evaluating,\ntitle={Evaluating Semantic Representations of Source Code},\nauthor={Yaza Wainakh and Moiz Rauf and Michael Pradel},\nyear={2020},\nurl={https://openreview.net/forum?id=SklibJBFDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "AZc4_CBOt", "original": null, "number": 1, "cdate": 1576798726315, "ddate": null, "tcdate": 1576798726315, "tmdate": 1576800910161, "tddate": null, "forum": "SklibJBFDB", "replyto": "SklibJBFDB", "invitation": "ICLR.cc/2020/Conference/Paper1555/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents a dataset to evaluate the quality of embeddings learnt for source code. The dataset consists of three different subtasks: relatedness, similarity, and contextual similarity. The main contribution of the paper is the construction of these datasets which should be useful to the community. However, there are valid concerns raised about the size of the datasets (which is pretty small) and the baselines used to evaluate the embeddings -- there should be a baselines using a contextual embeddings model like BERT which could have been fine-tuned on the source code data. If these comments are addressed, the paper can be a good contribution in an NLP conference. As of now, I recommend a Rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Semantic Representations of Source Code", "authors": ["Yaza Wainakh", "Moiz Rauf", "Michael Pradel"], "authorids": ["yaza.wainakh@gmail.com", "moiz.rauf@iste.uni-stuttgart.de", "michael@binaervarianz.de"], "keywords": ["embeddings", "representation", "source code", "identifiers"], "TL;DR": "A benchmark to evaluate neural embeddings of identifiers in source code.", "abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n", "code": "https://my.pcloud.com/publink/show?code=kZR2LzkZAViNngFuJ3ykFqNzbQF1TJRedJs7", "pdf": "/pdf/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "paperhash": "wainakh|evaluating_semantic_representations_of_source_code", "original_pdf": "/attachment/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "_bibtex": "@misc{\nwainakh2020evaluating,\ntitle={Evaluating Semantic Representations of Source Code},\nauthor={Yaza Wainakh and Moiz Rauf and Michael Pradel},\nyear={2020},\nurl={https://openreview.net/forum?id=SklibJBFDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SklibJBFDB", "replyto": "SklibJBFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726652, "tmdate": 1576800278834, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1555/-/Decision"}}}, {"id": "SyljNtrNoB", "original": null, "number": 3, "cdate": 1573308722775, "ddate": null, "tcdate": 1573308722775, "tmdate": 1573308722775, "tddate": null, "forum": "SklibJBFDB", "replyto": "HJeSIqF6KS", "invitation": "ICLR.cc/2020/Conference/Paper1555/-/Official_Comment", "content": {"title": "Response to Official Blind Review #2", "comment": "Thanks a lot for your insightful review! We are happy to see that the motivation for our work and the contributions of our paper have been made clear."}, "signatures": ["ICLR.cc/2020/Conference/Paper1555/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Semantic Representations of Source Code", "authors": ["Yaza Wainakh", "Moiz Rauf", "Michael Pradel"], "authorids": ["yaza.wainakh@gmail.com", "moiz.rauf@iste.uni-stuttgart.de", "michael@binaervarianz.de"], "keywords": ["embeddings", "representation", "source code", "identifiers"], "TL;DR": "A benchmark to evaluate neural embeddings of identifiers in source code.", "abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n", "code": "https://my.pcloud.com/publink/show?code=kZR2LzkZAViNngFuJ3ykFqNzbQF1TJRedJs7", "pdf": "/pdf/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "paperhash": "wainakh|evaluating_semantic_representations_of_source_code", "original_pdf": "/attachment/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "_bibtex": "@misc{\nwainakh2020evaluating,\ntitle={Evaluating Semantic Representations of Source Code},\nauthor={Yaza Wainakh and Moiz Rauf and Michael Pradel},\nyear={2020},\nurl={https://openreview.net/forum?id=SklibJBFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklibJBFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference/Paper1555/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1555/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1555/Reviewers", "ICLR.cc/2020/Conference/Paper1555/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1555/Authors|ICLR.cc/2020/Conference/Paper1555/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154287, "tmdate": 1576860541931, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference/Paper1555/Reviewers", "ICLR.cc/2020/Conference/Paper1555/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1555/-/Official_Comment"}}}, {"id": "BJxZMKBEiS", "original": null, "number": 2, "cdate": 1573308681320, "ddate": null, "tcdate": 1573308681320, "tmdate": 1573308681320, "tddate": null, "forum": "SklibJBFDB", "replyto": "Skxrit96Kr", "invitation": "ICLR.cc/2020/Conference/Paper1555/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1", "comment": "Thanks for your review. Please let us address your three concerns:\n\n1) Importance of identifier embeddings:\nThe first four paragraphs of the paper try to answer this question. In short: There are various code-related tasks that recent work has started to address through learning-based techniques, including bug detection, predicting names of methods, predicting types, finding similar code, and automatically fixing bugs. All these techniques rely on a representation of code, which typically is built from representations of identifiers. Simply reusing pre-trained word embeddings from NLP is insufficient, because the vocabulary of source code differs significantly from natural language. \n\nWe\u2019d love to receive more specific feedback or suggestions for improving the description of our motivation. \n\n2) Requirements for humans who contributed the labels:\nOur survey targeted software developers, but did not require knowledge of a specific programming language. To filter uninformed and incorrect ratings, we performed multiple data cleaning steps (Section 2.2). For setMinutes and setSeconds, the reason for a non-zero similarity score presumably is that both functions expect a unit of time as their argument and then store it. The fact that one cannot substitute the other is nicely illustrated by the very low contextual similarity score of 0.06.\n\n3) Evaluating other models/embeddings, e.g., GPT2 (TabNine):\nUnfortunately, the model (or an embedding derived from it) of TabNine isn\u2019t publicly available (see https://github.com/zxqfl/TabNine). However, if available, then this and any future embeddings can be easily evaluated using or benchmark. If you have concrete pointers to publicly available identifier embeddings, then we'll very happy to include them."}, "signatures": ["ICLR.cc/2020/Conference/Paper1555/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Semantic Representations of Source Code", "authors": ["Yaza Wainakh", "Moiz Rauf", "Michael Pradel"], "authorids": ["yaza.wainakh@gmail.com", "moiz.rauf@iste.uni-stuttgart.de", "michael@binaervarianz.de"], "keywords": ["embeddings", "representation", "source code", "identifiers"], "TL;DR": "A benchmark to evaluate neural embeddings of identifiers in source code.", "abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n", "code": "https://my.pcloud.com/publink/show?code=kZR2LzkZAViNngFuJ3ykFqNzbQF1TJRedJs7", "pdf": "/pdf/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "paperhash": "wainakh|evaluating_semantic_representations_of_source_code", "original_pdf": "/attachment/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "_bibtex": "@misc{\nwainakh2020evaluating,\ntitle={Evaluating Semantic Representations of Source Code},\nauthor={Yaza Wainakh and Moiz Rauf and Michael Pradel},\nyear={2020},\nurl={https://openreview.net/forum?id=SklibJBFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklibJBFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference/Paper1555/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1555/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1555/Reviewers", "ICLR.cc/2020/Conference/Paper1555/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1555/Authors|ICLR.cc/2020/Conference/Paper1555/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154287, "tmdate": 1576860541931, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference/Paper1555/Reviewers", "ICLR.cc/2020/Conference/Paper1555/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1555/-/Official_Comment"}}}, {"id": "B1xgSdrVoH", "original": null, "number": 1, "cdate": 1573308472231, "ddate": null, "tcdate": 1573308472231, "tmdate": 1573308472231, "tddate": null, "forum": "SklibJBFDB", "replyto": "rygm5dnr9S", "invitation": "ICLR.cc/2020/Conference/Paper1555/-/Official_Comment", "content": {"title": "Response to Official Blind Review #3", "comment": "Thanks for your review. Here are answers to your two concerns.\n\n1) Size of dataset:\nThe size of the dataset is similar to popular datasets in NLP (Rubenstein & Goodenough RG: 65 pairs, Miller & Charles MC: 30 pairs, Simlex: 999 pairs, WordSim: 353 pairs, MEN: 3000 pairs, but they used a different rating strategy, which made it possible to collect ratings for such a large number of pairs). Since the dataset is gathered from human ratings, obtaining ratings for many more pairs is difficult. Our contribution is not about the size, but about the quality of a benchmark created by human ratings.\n\nComparison the number of pairs and total identifiers in a corpus is misleading. Large code corpora may have hundreds of thousands of unique identifiers, i.e., using this argument, any number of pairs is \u201csmall\u201d. The reason why we sample pairs of identifiers from a large corpus is to cover different domains and different degrees of similarity/relatedness.\n\n2) Importance of contribution:\nSimilar efforts in NLP have served as a catalyst for improved embeddings techniques. Data collection and cleaning is at the heart of creating such benchmarks. As also pointed out by Reviewer 2, having a benchmark is important for the community, and we do not see why an important contribution should be described in a technical report only. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1555/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Semantic Representations of Source Code", "authors": ["Yaza Wainakh", "Moiz Rauf", "Michael Pradel"], "authorids": ["yaza.wainakh@gmail.com", "moiz.rauf@iste.uni-stuttgart.de", "michael@binaervarianz.de"], "keywords": ["embeddings", "representation", "source code", "identifiers"], "TL;DR": "A benchmark to evaluate neural embeddings of identifiers in source code.", "abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n", "code": "https://my.pcloud.com/publink/show?code=kZR2LzkZAViNngFuJ3ykFqNzbQF1TJRedJs7", "pdf": "/pdf/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "paperhash": "wainakh|evaluating_semantic_representations_of_source_code", "original_pdf": "/attachment/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "_bibtex": "@misc{\nwainakh2020evaluating,\ntitle={Evaluating Semantic Representations of Source Code},\nauthor={Yaza Wainakh and Moiz Rauf and Michael Pradel},\nyear={2020},\nurl={https://openreview.net/forum?id=SklibJBFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklibJBFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference/Paper1555/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1555/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1555/Reviewers", "ICLR.cc/2020/Conference/Paper1555/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1555/Authors|ICLR.cc/2020/Conference/Paper1555/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154287, "tmdate": 1576860541931, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1555/Authors", "ICLR.cc/2020/Conference/Paper1555/Reviewers", "ICLR.cc/2020/Conference/Paper1555/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1555/-/Official_Comment"}}}, {"id": "HJeSIqF6KS", "original": null, "number": 1, "cdate": 1571818060529, "ddate": null, "tcdate": 1571818060529, "tmdate": 1572972453196, "tddate": null, "forum": "SklibJBFDB", "replyto": "SklibJBFDB", "invitation": "ICLR.cc/2020/Conference/Paper1555/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper introduces a new dataset that includes manually labelled identifier name pairs. The labels determine how much the corresponding embeddings are useful for determining their meaning in context of code, a setting that has sufficient differences from a natural language.\n\nA significant part of the paper is devoted on data cleaning and relating the computed metrics to similar efforts in natural language processing. While there is not much novelty in this part of the paper, it is doing a good job at addressing many possible questions on the validity of their dataset. Another important aspect that is covered by the work is different kinds of similarities of identifier names - similarity corresponding to having the same or similar type in a precise type system or similarity corresponding to being synonyms. Having several of these dimensions would make the results applicable for a wide range of applications of identifier name embeddings.\n\nWhile not introducing new concepts, this paper is important for the community, because it has the potential to change the way embedding computation is done for \u201cBig Code\u201d problems. Right now, most papers either introduce their own embeddings, or use non-optimal ones like Code2Vec.\n\nThe paper also has a surprising finding that even techniques designed for code are in some cases not as good as the FastText embeddings. This is an interesting result, because few other works include this kind of embedding in their experiments. Furthermore, the paper deep dives into the strong and weak sides of several solutions and shows that there is a large opportunity to improve on the existing results.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1555/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1555/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Semantic Representations of Source Code", "authors": ["Yaza Wainakh", "Moiz Rauf", "Michael Pradel"], "authorids": ["yaza.wainakh@gmail.com", "moiz.rauf@iste.uni-stuttgart.de", "michael@binaervarianz.de"], "keywords": ["embeddings", "representation", "source code", "identifiers"], "TL;DR": "A benchmark to evaluate neural embeddings of identifiers in source code.", "abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n", "code": "https://my.pcloud.com/publink/show?code=kZR2LzkZAViNngFuJ3ykFqNzbQF1TJRedJs7", "pdf": "/pdf/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "paperhash": "wainakh|evaluating_semantic_representations_of_source_code", "original_pdf": "/attachment/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "_bibtex": "@misc{\nwainakh2020evaluating,\ntitle={Evaluating Semantic Representations of Source Code},\nauthor={Yaza Wainakh and Moiz Rauf and Michael Pradel},\nyear={2020},\nurl={https://openreview.net/forum?id=SklibJBFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklibJBFDB", "replyto": "SklibJBFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575211766732, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1555/Reviewers"], "noninvitees": [], "tcdate": 1570237735674, "tmdate": 1575211766744, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1555/-/Official_Review"}}}, {"id": "Skxrit96Kr", "original": null, "number": 2, "cdate": 1571821980720, "ddate": null, "tcdate": 1571821980720, "tmdate": 1572972453154, "tddate": null, "forum": "SklibJBFDB", "replyto": "SklibJBFDB", "invitation": "ICLR.cc/2020/Conference/Paper1555/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposed a benchmark dataset for evaluating different embedding methods for identifiers (like variables, functions) in programs. The groudtruth evaluation (similar/related) are labeled via Amazon MTurk. Experiments are carried out on several word embedding methods, and it seems these methods didn\u2019t get good enough correlation with human scores. \n\nOverall I appreciate the effort paid by the authors and human labors. However I have sevarl concerns:\n\n1) why the identifier embedding is important? As pre-trained word embeddings are useful for many NLP downstream tasks, what is the scenario of identifier embedding usage?\n\n2) To collect the human labels, are there any requirements? e.g., experiences in javascript. Especially, I\u2019m curious why in Table 3, setMinutes and setSeconds get score of 0.22 (which is too high).\n\n3) It would make more sense to compare with state of the art language pretraining methods, like bert, xlnet, etc. People have trained the language model with GPT2 (TabNine) that works well with code. So to make the work more convincing, I would suggest to include these.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1555/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1555/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Semantic Representations of Source Code", "authors": ["Yaza Wainakh", "Moiz Rauf", "Michael Pradel"], "authorids": ["yaza.wainakh@gmail.com", "moiz.rauf@iste.uni-stuttgart.de", "michael@binaervarianz.de"], "keywords": ["embeddings", "representation", "source code", "identifiers"], "TL;DR": "A benchmark to evaluate neural embeddings of identifiers in source code.", "abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n", "code": "https://my.pcloud.com/publink/show?code=kZR2LzkZAViNngFuJ3ykFqNzbQF1TJRedJs7", "pdf": "/pdf/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "paperhash": "wainakh|evaluating_semantic_representations_of_source_code", "original_pdf": "/attachment/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "_bibtex": "@misc{\nwainakh2020evaluating,\ntitle={Evaluating Semantic Representations of Source Code},\nauthor={Yaza Wainakh and Moiz Rauf and Michael Pradel},\nyear={2020},\nurl={https://openreview.net/forum?id=SklibJBFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklibJBFDB", "replyto": "SklibJBFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575211766732, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1555/Reviewers"], "noninvitees": [], "tcdate": 1570237735674, "tmdate": 1575211766744, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1555/-/Official_Review"}}}, {"id": "rygm5dnr9S", "original": null, "number": 3, "cdate": 1572354186648, "ddate": null, "tcdate": 1572354186648, "tmdate": 1572972453111, "tddate": null, "forum": "SklibJBFDB", "replyto": "SklibJBFDB", "invitation": "ICLR.cc/2020/Conference/Paper1555/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper presented a crowdsourced dataset for evaluating the semantic relatedness, similarity, and contextual similarity of source code identifiers. Motivated by similar tasks in evaluating the quality of word embeddings in the natural language, authors collected the smalls-scale evaluation datasets (less than three hundreds of code identifier pairs) and further evaluated the performance of several current embeddings techniques for codes or computing the similarity scores of code identifiers. Although I appreciated the efforts to build such evaluation dataset, I think the contributions are limited in terms of scientific contribution. \n\nPros:\n\n1) They collected a new small-scale evaluation dataset for evaluating the quality of semantic representations of various existing embedding techniques for code identifiers. \n\n2) They performed evaluations on these embeddings techniques for code and provided a few interesting findings based on these tasks. \n\nCons:\n\n1) The proposed datasets are very small. The total number of code pairs are less than 300 pairs out of total code identifiers 17,000, which is a very small set of the total pairs (17000 x 17000). Therefore, it is hard to fully evaluate the embeddings quality of various methods with high confidence. \n\n2) The whole paper is mainly about the data collection as well as a few of evaluations of several existing code embedding techniques. The scientific contributions are quite limited. It would be nice to put these efforts to have a competition of code embedding techniques and this paper could be served as a technical report on this direction. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1555/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1555/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Semantic Representations of Source Code", "authors": ["Yaza Wainakh", "Moiz Rauf", "Michael Pradel"], "authorids": ["yaza.wainakh@gmail.com", "moiz.rauf@iste.uni-stuttgart.de", "michael@binaervarianz.de"], "keywords": ["embeddings", "representation", "source code", "identifiers"], "TL;DR": "A benchmark to evaluate neural embeddings of identifiers in source code.", "abstract": "Learned representations of source code enable various software developer tools, e.g., to detect bugs or to predict program properties. At the core of code representations often are word embeddings of identifier names in source code, because identifiers account for the majority of source code vocabulary and convey important semantic information. Unfortunately, there currently is no generally accepted way of evaluating the quality of word embeddings of identifiers, and current evaluations are biased toward specific downstream tasks. This paper presents IdBench, the first benchmark for evaluating to what extent word embeddings of identifiers represent semantic relatedness and similarity. The benchmark is based on thousands of ratings gathered by surveying 500 software developers. We use IdBench to evaluate state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions, as these are often used in current developer tools. Our results show that the effectiveness of embeddings varies significantly across different embedding techniques and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing embedding provides a satisfactory representation of semantic similarities, e.g., because embeddings consider identifiers with opposing meanings as similar, which may lead to fatal mistakes in downstream developer tools. IdBench provides a gold standard to guide the development of novel embeddings that address the current limitations.\n", "code": "https://my.pcloud.com/publink/show?code=kZR2LzkZAViNngFuJ3ykFqNzbQF1TJRedJs7", "pdf": "/pdf/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "paperhash": "wainakh|evaluating_semantic_representations_of_source_code", "original_pdf": "/attachment/c62c376c1ea19bc9ccceecf4e38814d9f3baebe7.pdf", "_bibtex": "@misc{\nwainakh2020evaluating,\ntitle={Evaluating Semantic Representations of Source Code},\nauthor={Yaza Wainakh and Moiz Rauf and Michael Pradel},\nyear={2020},\nurl={https://openreview.net/forum?id=SklibJBFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklibJBFDB", "replyto": "SklibJBFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575211766732, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1555/Reviewers"], "noninvitees": [], "tcdate": 1570237735674, "tmdate": 1575211766744, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1555/-/Official_Review"}}}], "count": 8}