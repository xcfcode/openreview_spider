{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396356866, "tcdate": 1486396356866, "number": 1, "id": "HkaasGIOx", "invitation": "ICLR.cc/2017/conference/-/paper101/acceptance", "forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The authors agree with the reviewers that this manuscript is not yet ready."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396357455, "id": "ICLR.cc/2017/conference/-/paper101/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396357455}}}, {"tddate": null, "tmdate": 1482856279346, "tcdate": 1482856118781, "number": 3, "id": "SJJpUfgBe", "invitation": "ICLR.cc/2017/conference/-/paper101/public/comment", "forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "signatures": ["~Leslie_N_Smith1"], "readers": ["everyone"], "writers": ["~Leslie_N_Smith1"], "content": {"title": "Regarding the Reviewers' evaluation", "comment": "We wish to thank the esteemed Reviewers for their time and this valuable feedback to our paper.  We believe that the Reviewers are correct in their evaluations. Hence, our paper will require a significant rewrite and will not be ready for the ICLR conference paper deadline"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287728108, "id": "ICLR.cc/2017/conference/-/paper101/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJQNqLFgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper101/reviewers", "ICLR.cc/2017/conference/paper101/areachairs"], "cdate": 1485287728108}}}, {"tddate": null, "tmdate": 1481919627756, "tcdate": 1481919627756, "number": 3, "id": "SJEc2TZVl", "invitation": "ICLR.cc/2017/conference/-/paper101/official/review", "forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "signatures": ["ICLR.cc/2017/conference/paper101/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper101/AnonReviewer2"], "content": {"title": "", "rating": "3: Clear rejection", "review": "The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.\n\nI'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an \"introduction to training CNNs\" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).\n\nThe paper states that \"it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer\", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (\"the nature of design patterns is that they only apply some of the time\") does not excuse making such sweeping claims. This should probably be removed.\n\n\"We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively\" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with \"we feel\", as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.\n\nThe connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.\n\nOverall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512696833, "id": "ICLR.cc/2017/conference/-/paper101/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper101/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper101/AnonReviewer1", "ICLR.cc/2017/conference/paper101/AnonReviewer3", "ICLR.cc/2017/conference/paper101/AnonReviewer2"], "reply": {"forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512696833}}}, {"tddate": null, "tmdate": 1481909734528, "tcdate": 1481909734528, "number": 2, "id": "ByRJLo-Vg", "invitation": "ICLR.cc/2017/conference/-/paper101/official/review", "forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "signatures": ["ICLR.cc/2017/conference/paper101/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper101/AnonReviewer3"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.\n\nThe authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.\n\nOverall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, \"Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality\" is presented as one of 14 core design principles without any further justification. Similarly \"Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference\" is presented in the middle of a paragraph with no supporting references or further explanation.\n\nThe experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.\n\n\nPreliminary rating:\nIt is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512696833, "id": "ICLR.cc/2017/conference/-/paper101/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper101/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper101/AnonReviewer1", "ICLR.cc/2017/conference/paper101/AnonReviewer3", "ICLR.cc/2017/conference/paper101/AnonReviewer2"], "reply": {"forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512696833}}}, {"tddate": null, "tmdate": 1481907550083, "tcdate": 1481907550083, "number": 1, "id": "S1IwT5bNl", "invitation": "ICLR.cc/2017/conference/-/paper101/official/review", "forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "signatures": ["ICLR.cc/2017/conference/paper101/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper101/AnonReviewer1"], "content": {"title": "interesting direction - but not really solid", "rating": "4: Ok but not good enough - rejection", "review": "The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. \n\nThere are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the \"community service\" aspect of helping someone who starts figure out the \"coordinate system\" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.\n\nHowever I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. \n\nFirstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...) and I do not see how it stands on the same grounds as something as general as \"3 Strive for simplicity\".\n\nSimilarly some of the patterns are as vague as \"Increase symmetry\" and are backed up by statements such as \"we noted a special degree of elegance in the FractalNet\". I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. \n\nSome other patterns are phrased with weird names \"7 Cover the problem space\" - which I guess stands for dataset augmentation; or \"6 over-train\" which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding \"overtrain\"), which then has no connection to the description of \"over-train\" provided by the authors (\"training a network on a harder problem to improve generalization\"). If \"harder problem\" means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing \"regularization\" with something that sounds like \"overfitting\" (i.e. the exact opposite).\n\nFurthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out \n-how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. \n-whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0) \n-and, most importantly, how these design patterns would be deployed in practice to think of a new network. \n\nTo be more concrete, the authors mention that they propose the \"freeze-drop-path\" variant from \"symmetry considerations\" to \"drop-path\". \nIs this an application of the \"increase symmetry\" pattern? How would \"freeze-drop-path\" be more symmetric that \"drop-path\"?\n Can this be expressed concretely, or is it some intuitive guess? If the second, it is not really part of applying a pattern, in my understanding. If the first, this is missing. \n\n\nWhat I would have appreciated more (and would like to see in a revised version) would have been a table of \"design patterns\" on one axis, \"Deep network\" on another, and a breakdown of which network applies which design pattern. \n\nA big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful. \n\n\n  \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512696833, "id": "ICLR.cc/2017/conference/-/paper101/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper101/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper101/AnonReviewer1", "ICLR.cc/2017/conference/paper101/AnonReviewer3", "ICLR.cc/2017/conference/paper101/AnonReviewer2"], "reply": {"forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512696833}}}, {"tddate": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1481658762048, "tcdate": 1478220330758, "number": 101, "id": "SJQNqLFgl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJQNqLFgl", "signatures": ["~Leslie_N_Smith1"], "readers": ["everyone"], "content": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481225148937, "tcdate": 1480936707835, "number": 2, "id": "S12Z6pzQl", "invitation": "ICLR.cc/2017/conference/-/paper101/public/comment", "forum": "SJQNqLFgl", "replyto": "Bky5Rc1Qe", "signatures": ["~Leslie_N_Smith1"], "readers": ["everyone"], "writers": ["~Leslie_N_Smith1"], "content": {"title": "Reply to experimental results", "comment": "We will amend Table 1 in the paper to also report the standard deviation.\n\nLet me add that when I re-ran the experiments in Table 1, I obtained the same results.  This turns out to be due to the caffe solver.prototxt file having specified the random_seed: 831486.  I am currently running these experiments without this random_seed in order to obtain a standard deviation.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287728108, "id": "ICLR.cc/2017/conference/-/paper101/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJQNqLFgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper101/reviewers", "ICLR.cc/2017/conference/paper101/areachairs"], "cdate": 1485287728108}}}, {"tddate": null, "tmdate": 1480868918628, "tcdate": 1480868918621, "number": 1, "id": "H1JrETbmg", "invitation": "ICLR.cc/2017/conference/-/paper101/public/comment", "forum": "SJQNqLFgl", "replyto": "rJNEjfk7x", "signatures": ["~Leslie_N_Smith1"], "readers": ["everyone"], "writers": ["~Leslie_N_Smith1"], "content": {"title": "Reply to a few questions", "comment": "Thank you for these excellent questions.  These provide a great starting point for a discussion.  We hold no illusions that the design patterns we deliniate are the correct ones but rather hope this paper starts a conversation in the community as to what invisible principles underlie the vast number of architectures in the literature.\n\nThe nature of Design Patterns is that they only apply some of the time.  Which pattern applies depends on the problem (e.g., data, task, ...).  You are correct that a pyramidal shape is sometimes not desirable (eg., semantic segmentation, super-resolution, ...).  \n\nOur attempt to define Design Pattern 6 arose from considering regularization methods such as dropout, DropConnect, stochastic/drop path, Swapout, and methods like adding noise to the gradient for improving generalization.  These regularization methods can be viewed as using a noisy architecture, which is similar to adding noise to the gradients. Though regularization improves performance on the base problem, it effectively trains the network on a more difficult problem. This is not superficially obvious and we hope a useful insight.\n\nRegarding maxout, we noticed in the literature several joining mechanisms, such as summation, mean, concatenation, and sometimes maxout. In the section on joining mechanisms, we also address maxout as one of the options.  As discussed in the Experiments section, we did not find it advantageous to use maxout compared to summation.\n\nThe idea behind freeze-path came from drop-path and consideration of statistical learning.  It followed from a discussion of stagewise boosting (Algorithm 10.2 in the book \"The Elements of Statistical Learning\").  It says there \"Boosting is a way of fitting an additive expansion in a set of elementary \"basis\" functions.  Here the basis functions are the individual classifiers\".   Freeze-path allows branches of a network to be these basis functions of an additive expansion.  Also, this idea of parts of the network being fixed is not foreign.  Very recently the paper \"Wider or Deeper: Revisiting the ResNet Model for Visual Recognition\" at https://arxiv.org/abs/1611.10080 talks as though part of the ResNet is fixed (see their Figure 1).  By freezing the weights of certain paths, we hope to prevent overfitting, though a more explicit justification could certainly be added to the paper.\n\nWe hope these comments help clarify our paper and lead to further discussion.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287728108, "id": "ICLR.cc/2017/conference/-/paper101/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJQNqLFgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper101/reviewers", "ICLR.cc/2017/conference/paper101/areachairs"], "cdate": 1485287728108}}}, {"tddate": null, "tmdate": 1480728198883, "tcdate": 1480728198879, "number": 2, "id": "Bky5Rc1Qe", "invitation": "ICLR.cc/2017/conference/-/paper101/pre-review/question", "forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "signatures": ["ICLR.cc/2017/conference/paper101/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper101/AnonReviewer3"], "content": {"title": "Experimental Results", "question": "The accuracies are presented as the mean of test accuracy of the networks every 500 iterations for the final 3000 of training. This seems non-standard, why report results in this way? Could you also provide the standard deviation of these results? \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959462267, "id": "ICLR.cc/2017/conference/-/paper101/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper101/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper101/AnonReviewer2", "ICLR.cc/2017/conference/paper101/AnonReviewer3"], "reply": {"forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959462267}}}, {"tddate": null, "tmdate": 1480694572283, "tcdate": 1480694572278, "number": 1, "id": "rJNEjfk7x", "invitation": "ICLR.cc/2017/conference/-/paper101/pre-review/question", "forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "signatures": ["ICLR.cc/2017/conference/paper101/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper101/AnonReviewer2"], "content": {"title": "a few questions", "question": "Design pattern 5: while I'm aware that the scope of the paper is image classification only, even within this domain there are tasks where pyramidal shape is not necessarily desirable, e.g. semantic segmentation (which is a pixelwise classification problem). Could the authors explain whether they intended to include such tasks, and otherwise could they more clearly define which types of image classification tasks are considered and which aren't?\n\nDesign pattern 6: only one sentence of the paper discusses this, but it's not clear to me what is meant by \"training a network on a harder problem than necessary to improve generalisation performance\". Does this encompass any form of regularisation? Because I would argue that some forms of regularisation actually make the training problem easier (i.e. improve convergence as well as have a regularising effect). Could the authors clarify this?\n\nCompared to the others, design pattern 14 seems oddly specific: most people treat maxout as yet another nonlinearity. Could the authors comment on why they treat maxout more as an essential design element rather than a choice of nonlinearity?\n\nFreeze-path: why is this expected to be a useful technique? As long as a given branch is still used in the forward pass, not updating its weights has no effect whatsoever on the updates in the rest of the network. If anything it seems like it would lead to underfitting. I think this warrants a better justification."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Convolutional Neural Network Design Patterns", "abstract": "Recent research in the deep learning field has produced a plethora of new architectures.  At the same time, a growing number of groups are applying deep learning to new applications.  Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet).  Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.  In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files are available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work.\n", "pdf": "/pdf/44adf7f72a63655056dcf064e0b7c72a36db3c58.pdf", "TL;DR": "We take a high-level view of the network architectures as the basis for discovering universal principles of the design of convolutional neural network architecture.. ", "paperhash": "smith|deep_convolutional_neural_network_design_patterns", "keywords": [], "conflicts": ["nrl.navy.mil", "umbc.edu"], "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin@umbc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959462267, "id": "ICLR.cc/2017/conference/-/paper101/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper101/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper101/AnonReviewer2", "ICLR.cc/2017/conference/paper101/AnonReviewer3"], "reply": {"forum": "SJQNqLFgl", "replyto": "SJQNqLFgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper101/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959462267}}}], "count": 10}