{"notes": [{"id": "SkeUG30cFQ", "original": "r1giUzRqY7", "number": 1265, "cdate": 1538087949594, "ddate": null, "tcdate": 1538087949594, "tmdate": 1545355409807, "tddate": null, "forum": "SkeUG30cFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "The Expressive Power of Deep Neural Networks with Circulant Matrices", "abstract": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results \nand the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\nbounded width and small depth can approximate a deep ReLU network in which the dense matrices are\nof low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "keywords": ["deep learning", "circulant matrices", "universal approximation"], "authorids": ["alexandre.araujo@dauphine.eu", "benjamin.negrevergne@dauphine.fr", "yann.chevaleyre@lamsade.dauphine.fr", "jamal.atif@lamsade.dauphine.fr"], "authors": ["Alexandre Araujo", "Benjamin Negrevergne", "Yann Chevaleyre", "Jamal Atif"], "TL;DR": "We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.", "pdf": "/pdf/6640b3c1a3ecb4248b3fbb31ce90ec578e2ab4b0.pdf", "paperhash": "araujo|the_expressive_power_of_deep_neural_networks_with_circulant_matrices", "_bibtex": "@misc{\naraujo2019the,\ntitle={The Expressive Power of Deep Neural Networks with Circulant Matrices},\nauthor={Alexandre Araujo and Benjamin Negrevergne and Yann Chevaleyre and Jamal Atif},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeUG30cFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Sygd0qmJeN", "original": null, "number": 1, "cdate": 1544661711552, "ddate": null, "tcdate": 1544661711552, "tmdate": 1545354504322, "tddate": null, "forum": "SkeUG30cFQ", "replyto": "SkeUG30cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1265/Meta_Review", "content": {"metareview": "The paper conveys interesting study but the reviewers expressed concerns regarding the difference of this work compared to existing approaches and pointed a room for more thorough empirical evaluation.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Metareview"}, "signatures": ["ICLR.cc/2019/Conference/Paper1265/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1265/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Deep Neural Networks with Circulant Matrices", "abstract": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results \nand the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\nbounded width and small depth can approximate a deep ReLU network in which the dense matrices are\nof low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "keywords": ["deep learning", "circulant matrices", "universal approximation"], "authorids": ["alexandre.araujo@dauphine.eu", "benjamin.negrevergne@dauphine.fr", "yann.chevaleyre@lamsade.dauphine.fr", "jamal.atif@lamsade.dauphine.fr"], "authors": ["Alexandre Araujo", "Benjamin Negrevergne", "Yann Chevaleyre", "Jamal Atif"], "TL;DR": "We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.", "pdf": "/pdf/6640b3c1a3ecb4248b3fbb31ce90ec578e2ab4b0.pdf", "paperhash": "araujo|the_expressive_power_of_deep_neural_networks_with_circulant_matrices", "_bibtex": "@misc{\naraujo2019the,\ntitle={The Expressive Power of Deep Neural Networks with Circulant Matrices},\nauthor={Alexandre Araujo and Benjamin Negrevergne and Yann Chevaleyre and Jamal Atif},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeUG30cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1265/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352771381, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeUG30cFQ", "replyto": "SkeUG30cFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1265/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1265/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1265/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352771381}}}, {"id": "B1gyINsFhX", "original": null, "number": 1, "cdate": 1541153862854, "ddate": null, "tcdate": 1541153862854, "tmdate": 1542882842625, "tddate": null, "forum": "SkeUG30cFQ", "replyto": "SkeUG30cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1265/Official_Review", "content": {"title": "neither original nor thorough enough [title no longer appropriate after rebuttals]", "review": "The paper proposes using structured matrices, specifically circulant and diagonal matrices, to speed up computation and reduce memory requirements in NNs. The idea has been previously explored by a number of papers, as described in the introduction and related work.  The main contribution of the paper is to do some theoretical analysis, which is interesting but of uncertain impact.\n\nThe experiments compare performance against DeepBagOf`Fframes (DBOF) and MixturesOfExperts (MOE). However, there are other algorithms that are both more competitive and more closely related. I would like to see head-to-head comparisons with tensor-based algorithms such as Novikov et al: https://papers.nips.cc/paper/5787-tensorizing-neural-networks, which achieves huge compression ratios (~200 000x), and other linear-algebra based approaches. \n\nAFTER READING REBUTTAL\nI've increased my score because the authors point out previous work comparing their decomposition and tensortrains (although note the comparisons in Moczulski are on different networks and thus hard to interpret) and make a reasonable case that their work contributes to improve understanding of why circulant networks are effective. \n\nI strongly agree with authors when they state: \"We also believe that this paper brings results with a larger scope than the specific problem of designing compact neural networks. Circulant matrices deserve a particular attention in deep learning because of their strong ties with convolutions: a circulant matrix operator is equivalent to the convolution operator with circular paddings\".  I would broaden the topic to structured linear algebra more generally. I hope to someday see a comprehensive investigation of the topic.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1265/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The Expressive Power of Deep Neural Networks with Circulant Matrices", "abstract": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results \nand the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\nbounded width and small depth can approximate a deep ReLU network in which the dense matrices are\nof low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "keywords": ["deep learning", "circulant matrices", "universal approximation"], "authorids": ["alexandre.araujo@dauphine.eu", "benjamin.negrevergne@dauphine.fr", "yann.chevaleyre@lamsade.dauphine.fr", "jamal.atif@lamsade.dauphine.fr"], "authors": ["Alexandre Araujo", "Benjamin Negrevergne", "Yann Chevaleyre", "Jamal Atif"], "TL;DR": "We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.", "pdf": "/pdf/6640b3c1a3ecb4248b3fbb31ce90ec578e2ab4b0.pdf", "paperhash": "araujo|the_expressive_power_of_deep_neural_networks_with_circulant_matrices", "_bibtex": "@misc{\naraujo2019the,\ntitle={The Expressive Power of Deep Neural Networks with Circulant Matrices},\nauthor={Alexandre Araujo and Benjamin Negrevergne and Yann Chevaleyre and Jamal Atif},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeUG30cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1265/Official_Review", "cdate": 1542234192145, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeUG30cFQ", "replyto": "SkeUG30cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1265/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335984185, "tmdate": 1552335984185, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1265/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hygqlw3G07", "original": null, "number": 9, "cdate": 1542797042181, "ddate": null, "tcdate": 1542797042181, "tmdate": 1542797097310, "tddate": null, "forum": "SkeUG30cFQ", "replyto": "B1xQrvG_pX", "invitation": "ICLR.cc/2019/Conference/-/Paper1265/Official_Comment", "content": {"title": " justification for using diagonal-circulant networks as compact representations. ", "comment": "We would like to answer your comment \"I do not see how the result can be seen as a justification for using diagonal-circulant networks as compact representations\" in more precise terms.\n\nThere are plenty of matrix decomposition methods aiming at compact representation. For e.g., Givens-matrix decompositions, DCT-diagonal products, product of Toeplitz matrices, Diagonal-Hadamard products, low rank decomposition, etc..\n\nUp to know, there was no theoretical argument supporting the choice of one of these compact representations.\n\nWe argue that the most basic requirement for a compact representation is that it must allow to compactly represent low-rank matrices. Products of Circulant-Diagonal matrices do satisfy this requirement. It is unknown for the others decompositions, and we conjecture that it does not hold for Toeplitz.\n\nMoreover, we argue that circulant-diagonal products are MORE EXPRESSIVE than low-rank decomposition, because:\n- Any rank-k matrix (represented by 2nk parameters) can be represented by a circulant-diagonal products involving O(nk) parameters\n- The converse is not true: there exists full-rank circulant matrices, so these circulant matrices (represented by n parameters) are rank-n matrices requiring n^2 parameters\n\n "}, "signatures": ["ICLR.cc/2019/Conference/Paper1265/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1265/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Deep Neural Networks with Circulant Matrices", "abstract": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results \nand the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\nbounded width and small depth can approximate a deep ReLU network in which the dense matrices are\nof low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "keywords": ["deep learning", "circulant matrices", "universal approximation"], "authorids": ["alexandre.araujo@dauphine.eu", "benjamin.negrevergne@dauphine.fr", "yann.chevaleyre@lamsade.dauphine.fr", "jamal.atif@lamsade.dauphine.fr"], "authors": ["Alexandre Araujo", "Benjamin Negrevergne", "Yann Chevaleyre", "Jamal Atif"], "TL;DR": "We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.", "pdf": "/pdf/6640b3c1a3ecb4248b3fbb31ce90ec578e2ab4b0.pdf", "paperhash": "araujo|the_expressive_power_of_deep_neural_networks_with_circulant_matrices", "_bibtex": "@misc{\naraujo2019the,\ntitle={The Expressive Power of Deep Neural Networks with Circulant Matrices},\nauthor={Alexandre Araujo and Benjamin Negrevergne and Yann Chevaleyre and Jamal Atif},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeUG30cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1265/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626763, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeUG30cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference/Paper1265/Reviewers", "ICLR.cc/2019/Conference/Paper1265/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1265/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1265/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1265/Authors|ICLR.cc/2019/Conference/Paper1265/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1265/Reviewers", "ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference/Paper1265/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626763}}}, {"id": "Bkeezu4n6X", "original": null, "number": 6, "cdate": 1542371336158, "ddate": null, "tcdate": 1542371336158, "tmdate": 1542371336158, "tddate": null, "forum": "SkeUG30cFQ", "replyto": "B1xQrvG_pX", "invitation": "ICLR.cc/2019/Conference/-/Paper1265/Official_Comment", "content": {"title": "Expressivity of circulant matrices", "comment": "Thanks for the review, this is an interesting comment and will address it in the next revision of the paper.\n\nThe circulant diagonal decomposition is in fact more general than the low rank matrix factorization in the sense that any low rank matrix can be represented using a circulant matrix, but converse is not necessarily true.\nIt is true that the circulant diagonal generally requires more parameters, however it only requires linearly more parameters. This explain the circulant diagonal decomposition generally performs better in practice as demonstrated in [1], (table 1).\n\nAnother important difference  is the computational complexity of the matrix-vector multiplication. With low rank decomposition the matrix vector multiplication O(nk^2) wheras with a circulant diagonal decomposition it is O(k n log(n))\n\n\n[1] Moczulski, Marcin, et al. \"ACDC: A structured efficient linear layer.\" ICLR (2016). "}, "signatures": ["ICLR.cc/2019/Conference/Paper1265/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1265/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Deep Neural Networks with Circulant Matrices", "abstract": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results \nand the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\nbounded width and small depth can approximate a deep ReLU network in which the dense matrices are\nof low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "keywords": ["deep learning", "circulant matrices", "universal approximation"], "authorids": ["alexandre.araujo@dauphine.eu", "benjamin.negrevergne@dauphine.fr", "yann.chevaleyre@lamsade.dauphine.fr", "jamal.atif@lamsade.dauphine.fr"], "authors": ["Alexandre Araujo", "Benjamin Negrevergne", "Yann Chevaleyre", "Jamal Atif"], "TL;DR": "We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.", "pdf": "/pdf/6640b3c1a3ecb4248b3fbb31ce90ec578e2ab4b0.pdf", "paperhash": "araujo|the_expressive_power_of_deep_neural_networks_with_circulant_matrices", "_bibtex": "@misc{\naraujo2019the,\ntitle={The Expressive Power of Deep Neural Networks with Circulant Matrices},\nauthor={Alexandre Araujo and Benjamin Negrevergne and Yann Chevaleyre and Jamal Atif},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeUG30cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1265/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626763, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeUG30cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference/Paper1265/Reviewers", "ICLR.cc/2019/Conference/Paper1265/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1265/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1265/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1265/Authors|ICLR.cc/2019/Conference/Paper1265/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1265/Reviewers", "ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference/Paper1265/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626763}}}, {"id": "B1xQrvG_pX", "original": null, "number": 3, "cdate": 1542100794561, "ddate": null, "tcdate": 1542100794561, "tmdate": 1542100794561, "tddate": null, "forum": "SkeUG30cFQ", "replyto": "SkeUG30cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1265/Official_Review", "content": {"title": "Claims not sufficiently justified", "review": "The experiments in the paper are similar to those explored in previous work! The main contribution claimed in the paper is the theoretical formulation for compact design of neural networks using circulant matrices instead of fully connected matrices. \n\nI do not think the claim is sufficiently justified by the theoretical results provided. \n\nEarlier result already shows how any matrix fully connected matrices can be approximated by 2n-1 circulant matrices. As the authors themselves point out, this theoretical result does not necessarily imply reduction in number of parameters since the for a depth l network, the equivalent diagonal-circulant-ReLU network will now require (2n-1)l depth, or 2n(2n-1)l parameters. \n\nThe main results (Proposition 3, 4) show that if the fully connected networks of depth l network are parameterized by (approximately) rank k matrices, then the resultant depth of diagonal-circulant network required to approximate the original network is (4k+1)l, which results in a total of 8n(4k+1)l parameters. Similar to the case of full rank fully connected networks (proposition 2), this result does not necessarily indicate a compression of number of parameters either. In particular, if fully connected networks are indeed rank k, then we only need nkl parameters parameters to represent the matrix, which is lower than the number of parameters required by the diagonal-circulant network. \n\nSo I do not see how the result can be seen as a justification for using diagonal-circulant networks as compact representations. \n\nWriting:\nTheorem 1: The statement about approximability with B_1B_2\u2026B_{2n-1} is independent of p and S. \nProposition 3: The expression for depth should be \\sum_{i=1}^l (4k_i+1)  \u2014 sum should go from i=1 to l and there should be no multiplicative factor l \n\nOther non-critical comments: Multiplication by circulant matrices amounts to circular convolution with full dimensional kernel. In this sense, replacing a fully connected layers by circulant matrices is similar to replacing it with convolutional layers.  May be this connection can be explicitly stated in the paper.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1265/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Deep Neural Networks with Circulant Matrices", "abstract": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results \nand the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\nbounded width and small depth can approximate a deep ReLU network in which the dense matrices are\nof low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "keywords": ["deep learning", "circulant matrices", "universal approximation"], "authorids": ["alexandre.araujo@dauphine.eu", "benjamin.negrevergne@dauphine.fr", "yann.chevaleyre@lamsade.dauphine.fr", "jamal.atif@lamsade.dauphine.fr"], "authors": ["Alexandre Araujo", "Benjamin Negrevergne", "Yann Chevaleyre", "Jamal Atif"], "TL;DR": "We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.", "pdf": "/pdf/6640b3c1a3ecb4248b3fbb31ce90ec578e2ab4b0.pdf", "paperhash": "araujo|the_expressive_power_of_deep_neural_networks_with_circulant_matrices", "_bibtex": "@misc{\naraujo2019the,\ntitle={The Expressive Power of Deep Neural Networks with Circulant Matrices},\nauthor={Alexandre Araujo and Benjamin Negrevergne and Yann Chevaleyre and Jamal Atif},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeUG30cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1265/Official_Review", "cdate": 1542234192145, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeUG30cFQ", "replyto": "SkeUG30cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1265/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335984185, "tmdate": 1552335984185, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1265/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1leyKeua7", "original": null, "number": 4, "cdate": 1542093016123, "ddate": null, "tcdate": 1542093016123, "tmdate": 1542095219936, "tddate": null, "forum": "SkeUG30cFQ", "replyto": "B1gyINsFhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1265/Official_Comment", "content": {"title": "About the impact of our contribution", "comment": "\nWe would like to thank both reviewers for a valuable feedback and we apologize for the typos and grammatical errors. We have double checked the current version so hopefully there won't be anymore in the next revision. \n\nYou (Reviewer 2) have expressed some doubts about the significance of our theoretical contributions. We would like to clarify and emphasize some points hereafter.\n\nFirstly a comparison between the decomposition we use and other decompositions such as TensorTrain has already been published in ICLR [2]. As one can see in Table 1 and Figure 4 from [2], the circulant matrix decomposition compares favorably to TensorTrain. For completeness, we will add a comparison between TensorTrain and our decomposition on the YouTube dataset (in Table 2) but this should not change the essence of our contribution.\n\nSecondly, our main contribution is of importance: we provide a similar approximation result as [3] did for classical networks, but for circulant networks. Indeed we provide a bound on the approximation error of a circulant neural network with bounded width and height. We believe that this result can be of interest to anyone trying to build compact networks using circulant matrices. \n\nMore importantly, without the result we provide in our paper,  the good results reported in [2,1] were difficult to explain with the existing theory: [9] states that any linear operator can be decomposed into a product of at least n diagonal and circulant factors (where n can be as big as 1024), but in practice good results have been observed in [1,2] with as few as 1 factor. So in a sense, the situation is analogous to the one of  neural networks based on fully connected layers *before* the first (celebrated) results on approximation with bounded nets [3,4].\n\nWe also believe that this paper brings results with a larger scope than the specific problem of designing compact neural networks. Circulant martices deserve a particular attention in deep learning because of their strong ties with convolutions: a circulant matrix operator is equivalent to the convolution operator with circular paddings (as shown in [5]). This fact makes any contribution to the area of circulant matrices particularly relevant to the field of deep learning with impacts beyond the problem of designing compact models. \nFor instance, it is currently not known whether convolutional neural networks are universal approximators. Our work proves that a particular type of convolutional neural nets are universal approximators. We believe that this is a strong first result that paves the way to more general results about error bounds  in general CNNs. \n\nFinally, regarding the architecture, we choose the Deep Bag-of-Frames (DBoF) and Mixtures of Experts (MoE) architectures since they are  state of the art in the computer vision area, as discussed in [6, 7, 8]. \n\n \n[1] Cheng, Yu, et al. \"An exploration of parameter redundancy in deep networks with circulant projections.\" Proceedings of the IEEE International Conference on Computer Vision. 2015.\n\n[2] Moczulski, Marcin, et al. \"ACDC: A structured efficient linear layer.\" ICLR (2016).\n\n[3] Barron, A. R. (1993). \"Universal approximation bounds for superpositions of a sigmoidal function. IEEE Transactions on Information theory, 39(3), 930-945.\n\n[4] Hanin, Boris. \"Universal function approximation by deep neural nets with bounded width and relu activations.\" arXiv preprint arXiv:1708.02691 (2017).\n\n[5] Xiao et al. \"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks.\" ICML 2018\n\n[6] Abu-El-Haija et al. \"YouTube-8M: A Large-Scale Video Classification Benchmark\", arXiv preprint arXiv:1609.08675\n\n[7] Miech et al. \"Learnable pooling with Context Gating for video classification\", Proc. of the CVPR Workshop on YouTube-8M Large-Scale Video Understanding (2017)\n\n[8] Paul Natsev, \"Context-Gated DBoF Models for YouTube-8M\", https://static.googleusercontent.com/media/research.google.com/fr//youtube8m/workshop2018/natsev.pdf\n\n[9]  Huhtanen, M., & Per\u00e4m\u00e4ki, A. (2015). Factoring matrices into the product of circulant and diagonal matrices. Journal of Fourier Analysis and Applications, 21(5), 1018-1033.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1265/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1265/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Deep Neural Networks with Circulant Matrices", "abstract": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results \nand the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\nbounded width and small depth can approximate a deep ReLU network in which the dense matrices are\nof low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "keywords": ["deep learning", "circulant matrices", "universal approximation"], "authorids": ["alexandre.araujo@dauphine.eu", "benjamin.negrevergne@dauphine.fr", "yann.chevaleyre@lamsade.dauphine.fr", "jamal.atif@lamsade.dauphine.fr"], "authors": ["Alexandre Araujo", "Benjamin Negrevergne", "Yann Chevaleyre", "Jamal Atif"], "TL;DR": "We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.", "pdf": "/pdf/6640b3c1a3ecb4248b3fbb31ce90ec578e2ab4b0.pdf", "paperhash": "araujo|the_expressive_power_of_deep_neural_networks_with_circulant_matrices", "_bibtex": "@misc{\naraujo2019the,\ntitle={The Expressive Power of Deep Neural Networks with Circulant Matrices},\nauthor={Alexandre Araujo and Benjamin Negrevergne and Yann Chevaleyre and Jamal Atif},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeUG30cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1265/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626763, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeUG30cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference/Paper1265/Reviewers", "ICLR.cc/2019/Conference/Paper1265/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1265/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1265/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1265/Authors|ICLR.cc/2019/Conference/Paper1265/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1265/Reviewers", "ICLR.cc/2019/Conference/Paper1265/Authors", "ICLR.cc/2019/Conference/Paper1265/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626763}}}, {"id": "r1l27pBshX", "original": null, "number": 2, "cdate": 1541262627632, "ddate": null, "tcdate": 1541262627632, "tmdate": 1541532990103, "tddate": null, "forum": "SkeUG30cFQ", "replyto": "SkeUG30cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1265/Official_Review", "content": {"title": "An important contribution.", "review": "In this paper, the authors prove that bounded width diagonal-circulant ReLU networks (I will call them DC-ReLU henceforth) are universal approximators (this was shown previously without the bounded width condition). They also show that bounded width and small depth DC-ReLUs can approximate deep ReLU nets with row rank parameters matrices. This explains the observed success of such networks. The authors also provide experiments to demonstrate the compression one can achieve without sacrificing accuracy.\n\nPros: The authors provide strong approximation results that explain the observed success of DC-ReLUs.\n\nCons: Too many grammatical errors (mainly improper pluralization of verbs and punctuation errors), typos, stylistic inconsistencies seriously affect the readability of the paper. The authors should pay more attention to these.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1265/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Expressive Power of Deep Neural Networks with Circulant Matrices", "abstract": "Recent results from linear algebra stating that any matrix can be decomposed into products of diagonal and circulant matrices has lead to the design of compact deep neural network architectures that perform well in practice. In this paper, we bridge the gap between these good empirical results \nand the theoretical approximation capabilities of Deep diagonal-circulant ReLU networks. More precisely, we first demonstrate  that a Deep diagonal-circulant ReLU networks of\nbounded width and small depth can approximate a deep ReLU network in which the dense matrices are\nof low rank. Based on this result, we provide new bounds on the expressive power and universal approximativeness of this type of networks. We support our experimental results with thorough experiments on a large, real world video classification problem.", "keywords": ["deep learning", "circulant matrices", "universal approximation"], "authorids": ["alexandre.araujo@dauphine.eu", "benjamin.negrevergne@dauphine.fr", "yann.chevaleyre@lamsade.dauphine.fr", "jamal.atif@lamsade.dauphine.fr"], "authors": ["Alexandre Araujo", "Benjamin Negrevergne", "Yann Chevaleyre", "Jamal Atif"], "TL;DR": "We provide a theoretical study of the properties of Deep circulant-diagonal ReLU Networks and demonstrate that they are bounded width universal approximators.", "pdf": "/pdf/6640b3c1a3ecb4248b3fbb31ce90ec578e2ab4b0.pdf", "paperhash": "araujo|the_expressive_power_of_deep_neural_networks_with_circulant_matrices", "_bibtex": "@misc{\naraujo2019the,\ntitle={The Expressive Power of Deep Neural Networks with Circulant Matrices},\nauthor={Alexandre Araujo and Benjamin Negrevergne and Yann Chevaleyre and Jamal Atif},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeUG30cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1265/Official_Review", "cdate": 1542234192145, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeUG30cFQ", "replyto": "SkeUG30cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1265/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335984185, "tmdate": 1552335984185, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1265/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}