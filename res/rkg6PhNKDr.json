{"notes": [{"id": "rkg6PhNKDr", "original": "S1gDAasnBS", "number": 21, "cdate": 1569438820926, "ddate": null, "tcdate": 1569438820926, "tmdate": 1577168265915, "tddate": null, "forum": "rkg6PhNKDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?", "authors": ["Fawaz Sammani", "Mahmoud Elsayed", "Abdelsalam Hamdi"], "authorids": ["fawaz.sammani@aol.com", "elsayedmahmoud@aol.com", "abdelsalam.h.a.a@gmail.com"], "keywords": ["weights update", "weights importance", "weight freezing"], "TL;DR": "An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.", "abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "pdf": "/pdf/815e43289650dde28f32a3def294fe96622c38a8.pdf", "paperhash": "sammani|how_important_are_network_weights_to_what_extent_do_they_need_an_update", "original_pdf": "/attachment/815e43289650dde28f32a3def294fe96622c38a8.pdf", "_bibtex": "@misc{\nsammani2020how,\ntitle={{\\{}HOW{\\}} {\\{}IMPORTANT{\\}} {\\{}ARE{\\}} {\\{}NETWORK{\\}} {\\{}WEIGHTS{\\}}? {\\{}TO{\\}} {\\{}WHAT{\\}} {\\{}EXTENT{\\}} {\\{}DO{\\}} {\\{}THEY{\\}} {\\{}NEED{\\}} {\\{}AN{\\}} {\\{}UPDATE{\\}}?},\nauthor={Fawaz Sammani and Mahmoud Elsayed and Abdelsalam Hamdi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6PhNKDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "gGqW8uewU", "original": null, "number": 1, "cdate": 1576798685211, "ddate": null, "tcdate": 1576798685211, "tmdate": 1576800949700, "tddate": null, "forum": "rkg6PhNKDr", "replyto": "rkg6PhNKDr", "invitation": "ICLR.cc/2020/Conference/Paper21/-/Decision", "content": {"decision": "Reject", "comment": "The authors demonstrate that starting from the 3rd epoch, freezing a large fraction of the weights (based on gradient information), but not entire layers, results in slight drops in performance.\n\nGiven existing literature, the reviewers did not find this surprising, even though freezing only some of a layers weights has not been explicitly analyzed before. Although this is an interesting observation, the authors did not explain why this finding is important and it is unclear what the impact of such a finding will be. The authors are encouraged to expand on the implications of their finding and theoretical basis for it. Furthermore, reviewers raised concerns about the extensiveness of the empirical evaluation.\n\nThis paper falls below the bar for ICLR, so I recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?", "authors": ["Fawaz Sammani", "Mahmoud Elsayed", "Abdelsalam Hamdi"], "authorids": ["fawaz.sammani@aol.com", "elsayedmahmoud@aol.com", "abdelsalam.h.a.a@gmail.com"], "keywords": ["weights update", "weights importance", "weight freezing"], "TL;DR": "An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.", "abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "pdf": "/pdf/815e43289650dde28f32a3def294fe96622c38a8.pdf", "paperhash": "sammani|how_important_are_network_weights_to_what_extent_do_they_need_an_update", "original_pdf": "/attachment/815e43289650dde28f32a3def294fe96622c38a8.pdf", "_bibtex": "@misc{\nsammani2020how,\ntitle={{\\{}HOW{\\}} {\\{}IMPORTANT{\\}} {\\{}ARE{\\}} {\\{}NETWORK{\\}} {\\{}WEIGHTS{\\}}? {\\{}TO{\\}} {\\{}WHAT{\\}} {\\{}EXTENT{\\}} {\\{}DO{\\}} {\\{}THEY{\\}} {\\{}NEED{\\}} {\\{}AN{\\}} {\\{}UPDATE{\\}}?},\nauthor={Fawaz Sammani and Mahmoud Elsayed and Abdelsalam Hamdi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6PhNKDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkg6PhNKDr", "replyto": "rkg6PhNKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730677, "tmdate": 1576800283518, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper21/-/Decision"}}}, {"id": "HkxqGH95Yr", "original": null, "number": 1, "cdate": 1571624210126, "ddate": null, "tcdate": 1571624210126, "tmdate": 1574258824788, "tddate": null, "forum": "rkg6PhNKDr", "replyto": "rkg6PhNKDr", "invitation": "ICLR.cc/2020/Conference/Paper21/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "\nIn this paper, the authors performed an empirical study on the importance of neural network weights and to which extent they need to be updated. Some observations are obtained such as from the third epoch on, a large proportion of weights do not need to be updated and the performance of the network is not significantly affected.\n\nOverall speaking, the qualitative result in the paper has already been discovered in many previous work, although the quantitative results seem to be new. However, there is large room to improve regarding the experimental design and the comprehensiveness of the experiments. Just name a few as follows:\n\n1)\tFor different models and different tasks, the quantitative results are different. There is no deep discussion on the intrinsic reason for this, and what is the most important factor that influences the redundancy of weight updates. The authors came to the conclusion that from the third epoch on, no need to update most of the weights. \u201c3\u201d seems to be a magic number to me. Why is it? No solid experiments were done regarding this, and no convincing analysis was made.\n\n2)\tThe datasets used in the experiments are not diverse enough and are not of large scale. For example, the CIFA-10 and MNIST datasets are relatively of small scale. What if the datasets are much larger like ImageNet. In such more complicated case, will the weight updates still be unnecessary? Will the ratio and the epoch number change? What is the underlying factor determining these? For another example, there are many NLP datasets for language understanding and machine translation, which are of large scale. Why choosing an image captioning dataset (which I do not agree to be real-life experiments when compared with language understanding and machine translation)? Can the observations generalizable to more complicated tasks and datasets?\n\n3)\tThe models studied in the paper are also a little simple, especially for the text task. Why just using a single-layer LSTM? Why not popularly used Transformer? \n\nAs a summary, for an empirical study to be convincing, the tasks, datasets, scales, model structures, detailed settings, and discussions are the critical aspects. However, as explained above, this paper has not done a good job on these aspects. Significantly more work needs to be done in order to make it an impactful work. \n\n*I read the author rebuttal, but would like to keep my rating unchanged.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper21/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper21/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?", "authors": ["Fawaz Sammani", "Mahmoud Elsayed", "Abdelsalam Hamdi"], "authorids": ["fawaz.sammani@aol.com", "elsayedmahmoud@aol.com", "abdelsalam.h.a.a@gmail.com"], "keywords": ["weights update", "weights importance", "weight freezing"], "TL;DR": "An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.", "abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "pdf": "/pdf/815e43289650dde28f32a3def294fe96622c38a8.pdf", "paperhash": "sammani|how_important_are_network_weights_to_what_extent_do_they_need_an_update", "original_pdf": "/attachment/815e43289650dde28f32a3def294fe96622c38a8.pdf", "_bibtex": "@misc{\nsammani2020how,\ntitle={{\\{}HOW{\\}} {\\{}IMPORTANT{\\}} {\\{}ARE{\\}} {\\{}NETWORK{\\}} {\\{}WEIGHTS{\\}}? {\\{}TO{\\}} {\\{}WHAT{\\}} {\\{}EXTENT{\\}} {\\{}DO{\\}} {\\{}THEY{\\}} {\\{}NEED{\\}} {\\{}AN{\\}} {\\{}UPDATE{\\}}?},\nauthor={Fawaz Sammani and Mahmoud Elsayed and Abdelsalam Hamdi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6PhNKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg6PhNKDr", "replyto": "rkg6PhNKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper21/Reviewers"], "noninvitees": [], "tcdate": 1570237758283, "tmdate": 1574723094201, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper21/-/Official_Review"}}}, {"id": "ryeXJWZBiB", "original": null, "number": 3, "cdate": 1573355739174, "ddate": null, "tcdate": 1573355739174, "tmdate": 1573355739174, "tddate": null, "forum": "rkg6PhNKDr", "replyto": "HkxqGH95Yr", "invitation": "ICLR.cc/2020/Conference/Paper21/-/Official_Comment", "content": {"title": "response", "comment": "Thanks for your feedback. \nFor point number 1, we will try to further discuss and analyze this in a convincing manner. \n\nFor point 2, we have chosen Image Captioning as it combines both vision (CNN operating on large-sized images) and language (RNNs for language modeling), and we believe that it reflects image understanding and language modeling at the same time, which is the reason why we chose it. As for using transformers, they themselves are very sensitive to train, and in our study we only focus on delivering our concern, without focusing on using \"the best model\". \n\nFor point 3, For your comment on the model simplicity, we believe that the used models though they are simple (such as one layer LSTM) they are sufficient enough to proof the concept as this is mainly a conceptual paper to theoretically prove that it is possible to freeze certain insignificant weights in a neural network. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper21/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?", "authors": ["Fawaz Sammani", "Mahmoud Elsayed", "Abdelsalam Hamdi"], "authorids": ["fawaz.sammani@aol.com", "elsayedmahmoud@aol.com", "abdelsalam.h.a.a@gmail.com"], "keywords": ["weights update", "weights importance", "weight freezing"], "TL;DR": "An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.", "abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "pdf": "/pdf/815e43289650dde28f32a3def294fe96622c38a8.pdf", "paperhash": "sammani|how_important_are_network_weights_to_what_extent_do_they_need_an_update", "original_pdf": "/attachment/815e43289650dde28f32a3def294fe96622c38a8.pdf", "_bibtex": "@misc{\nsammani2020how,\ntitle={{\\{}HOW{\\}} {\\{}IMPORTANT{\\}} {\\{}ARE{\\}} {\\{}NETWORK{\\}} {\\{}WEIGHTS{\\}}? {\\{}TO{\\}} {\\{}WHAT{\\}} {\\{}EXTENT{\\}} {\\{}DO{\\}} {\\{}THEY{\\}} {\\{}NEED{\\}} {\\{}AN{\\}} {\\{}UPDATE{\\}}?},\nauthor={Fawaz Sammani and Mahmoud Elsayed and Abdelsalam Hamdi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6PhNKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg6PhNKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference/Paper21/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper21/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper21/Reviewers", "ICLR.cc/2020/Conference/Paper21/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper21/Authors|ICLR.cc/2020/Conference/Paper21/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177579, "tmdate": 1576860540733, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference/Paper21/Reviewers", "ICLR.cc/2020/Conference/Paper21/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper21/-/Official_Comment"}}}, {"id": "H1xb9heHjB", "original": null, "number": 2, "cdate": 1573354633135, "ddate": null, "tcdate": 1573354633135, "tmdate": 1573354752770, "tddate": null, "forum": "rkg6PhNKDr", "replyto": "HylH3mj2Kr", "invitation": "ICLR.cc/2020/Conference/Paper21/-/Official_Comment", "content": {"title": "response", "comment": "Thank you very much for your very useful feedback. This is mainly a conceptual paper to theoretically prove that it is possible to freeze certain insignificant weights in the neural network. By right if the number of the updates needed to be performed on the parameters is much lesser, the backward pass time is shorter, which will eventually speed up the whole process. However, freezing individual weights within a layer is not possible by any current deep learning framework (only freezing a complete layer will all its weights is possible), and developing the code from scratch to perform as efficiently as any framework would will take a considerable amount of time. Nevertheless, the purposes of this paper is to prove the concept theoretically with sufficient empirical evidences. "}, "signatures": ["ICLR.cc/2020/Conference/Paper21/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?", "authors": ["Fawaz Sammani", "Mahmoud Elsayed", "Abdelsalam Hamdi"], "authorids": ["fawaz.sammani@aol.com", "elsayedmahmoud@aol.com", "abdelsalam.h.a.a@gmail.com"], "keywords": ["weights update", "weights importance", "weight freezing"], "TL;DR": "An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.", "abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "pdf": "/pdf/815e43289650dde28f32a3def294fe96622c38a8.pdf", "paperhash": "sammani|how_important_are_network_weights_to_what_extent_do_they_need_an_update", "original_pdf": "/attachment/815e43289650dde28f32a3def294fe96622c38a8.pdf", "_bibtex": "@misc{\nsammani2020how,\ntitle={{\\{}HOW{\\}} {\\{}IMPORTANT{\\}} {\\{}ARE{\\}} {\\{}NETWORK{\\}} {\\{}WEIGHTS{\\}}? {\\{}TO{\\}} {\\{}WHAT{\\}} {\\{}EXTENT{\\}} {\\{}DO{\\}} {\\{}THEY{\\}} {\\{}NEED{\\}} {\\{}AN{\\}} {\\{}UPDATE{\\}}?},\nauthor={Fawaz Sammani and Mahmoud Elsayed and Abdelsalam Hamdi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6PhNKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg6PhNKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference/Paper21/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper21/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper21/Reviewers", "ICLR.cc/2020/Conference/Paper21/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper21/Authors|ICLR.cc/2020/Conference/Paper21/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177579, "tmdate": 1576860540733, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference/Paper21/Reviewers", "ICLR.cc/2020/Conference/Paper21/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper21/-/Official_Comment"}}}, {"id": "BJgInolBsS", "original": null, "number": 1, "cdate": 1573354414231, "ddate": null, "tcdate": 1573354414231, "tmdate": 1573354414231, "tddate": null, "forum": "rkg6PhNKDr", "replyto": "SkgTDt7pYS", "invitation": "ICLR.cc/2020/Conference/Paper21/-/Official_Comment", "content": {"title": "response", "comment": "Thank you for your feedback. To best of our knowledge, the previous studies have observed the process of freezing complete layers in a neural network unlike our studies that investigates the importance of the individual gradient parameters even in different layers without the need to freeze the complete layer and here lies the uniqueness of our study. "}, "signatures": ["ICLR.cc/2020/Conference/Paper21/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?", "authors": ["Fawaz Sammani", "Mahmoud Elsayed", "Abdelsalam Hamdi"], "authorids": ["fawaz.sammani@aol.com", "elsayedmahmoud@aol.com", "abdelsalam.h.a.a@gmail.com"], "keywords": ["weights update", "weights importance", "weight freezing"], "TL;DR": "An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.", "abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "pdf": "/pdf/815e43289650dde28f32a3def294fe96622c38a8.pdf", "paperhash": "sammani|how_important_are_network_weights_to_what_extent_do_they_need_an_update", "original_pdf": "/attachment/815e43289650dde28f32a3def294fe96622c38a8.pdf", "_bibtex": "@misc{\nsammani2020how,\ntitle={{\\{}HOW{\\}} {\\{}IMPORTANT{\\}} {\\{}ARE{\\}} {\\{}NETWORK{\\}} {\\{}WEIGHTS{\\}}? {\\{}TO{\\}} {\\{}WHAT{\\}} {\\{}EXTENT{\\}} {\\{}DO{\\}} {\\{}THEY{\\}} {\\{}NEED{\\}} {\\{}AN{\\}} {\\{}UPDATE{\\}}?},\nauthor={Fawaz Sammani and Mahmoud Elsayed and Abdelsalam Hamdi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6PhNKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg6PhNKDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference/Paper21/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper21/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper21/Reviewers", "ICLR.cc/2020/Conference/Paper21/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper21/Authors|ICLR.cc/2020/Conference/Paper21/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177579, "tmdate": 1576860540733, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper21/Authors", "ICLR.cc/2020/Conference/Paper21/Reviewers", "ICLR.cc/2020/Conference/Paper21/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper21/-/Official_Comment"}}}, {"id": "HylH3mj2Kr", "original": null, "number": 2, "cdate": 1571759021185, "ddate": null, "tcdate": 1571759021185, "tmdate": 1572972648622, "tddate": null, "forum": "rkg6PhNKDr", "replyto": "rkg6PhNKDr", "invitation": "ICLR.cc/2020/Conference/Paper21/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents the empirical observation that one can freeze (stop updating) a significant fraction of neural network parameters after only training for a short amount of time, without hurting final performance too much. The technical contribution made by this paper is an algorithm for determining which weights to freeze, called partial backpropagation, and an empirical validation of the algorithm on various models for image recognition.\n\nThe observation that weights can be frozen is somewhat interesting, although similar findings have been reported before. \nIt's not clear the proposed algorithm is useful. The authors mention that fully parameterized models are expensive to run, but they don't demonstrate any speed-ups using their approach. Such speed-up would also not be expected since the forward pass of the algorithm cannot get faster by freezing weights, and the impact on the backward pass is limited. I'd be willing to raise my rating if the authors can convince me of the usefulness of their algorithm."}, "signatures": ["ICLR.cc/2020/Conference/Paper21/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper21/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?", "authors": ["Fawaz Sammani", "Mahmoud Elsayed", "Abdelsalam Hamdi"], "authorids": ["fawaz.sammani@aol.com", "elsayedmahmoud@aol.com", "abdelsalam.h.a.a@gmail.com"], "keywords": ["weights update", "weights importance", "weight freezing"], "TL;DR": "An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.", "abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "pdf": "/pdf/815e43289650dde28f32a3def294fe96622c38a8.pdf", "paperhash": "sammani|how_important_are_network_weights_to_what_extent_do_they_need_an_update", "original_pdf": "/attachment/815e43289650dde28f32a3def294fe96622c38a8.pdf", "_bibtex": "@misc{\nsammani2020how,\ntitle={{\\{}HOW{\\}} {\\{}IMPORTANT{\\}} {\\{}ARE{\\}} {\\{}NETWORK{\\}} {\\{}WEIGHTS{\\}}? {\\{}TO{\\}} {\\{}WHAT{\\}} {\\{}EXTENT{\\}} {\\{}DO{\\}} {\\{}THEY{\\}} {\\{}NEED{\\}} {\\{}AN{\\}} {\\{}UPDATE{\\}}?},\nauthor={Fawaz Sammani and Mahmoud Elsayed and Abdelsalam Hamdi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6PhNKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg6PhNKDr", "replyto": "rkg6PhNKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper21/Reviewers"], "noninvitees": [], "tcdate": 1570237758283, "tmdate": 1574723094201, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper21/-/Official_Review"}}}, {"id": "SkgTDt7pYS", "original": null, "number": 3, "cdate": 1571793252923, "ddate": null, "tcdate": 1571793252923, "tmdate": 1572972648578, "tddate": null, "forum": "rkg6PhNKDr", "replyto": "rkg6PhNKDr", "invitation": "ICLR.cc/2020/Conference/Paper21/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper studies the importance of a neural networks weights and to which extend do they need to be updated. Particularly, the authors show that freezing weights which have small gradient in the very beginning of the training only results in a very slight drop in the final accuracy.\n\nThis paper should be rejected because (1) the paper only provides some empirical results on freezing network network weights, I don't think there are much insights and useful information; (2) To my knowledge, the phenomenon that only a few parameters are important has been observed before by many papers.\n\nGiven that, I vote for a rejection."}, "signatures": ["ICLR.cc/2020/Conference/Paper21/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper21/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "HOW IMPORTANT ARE NETWORK WEIGHTS? TO WHAT EXTENT DO THEY NEED AN UPDATE?", "authors": ["Fawaz Sammani", "Mahmoud Elsayed", "Abdelsalam Hamdi"], "authorids": ["fawaz.sammani@aol.com", "elsayedmahmoud@aol.com", "abdelsalam.h.a.a@gmail.com"], "keywords": ["weights update", "weights importance", "weight freezing"], "TL;DR": "An experimental paper that proves the amount of redundant weights that can be freezed from the third epoch only, with only a very slight drop in accuracy.", "abstract": "In the context of optimization, a gradient of a neural network indicates the amount a specific weight should change with respect to the loss. Therefore, small gradients indicate a good value of the weight that requires no change and can be kept frozen during training. This paper provides an experimental study on the importance of a neural network weights, and to which extent do they need to be updated. We wish to show that starting from the third epoch, freezing weights which have no informative gradient and are less likely to be changed during training, results in a very slight drop in the overall accuracy (and in sometimes better). We experiment on the MNIST, CIFAR10 and Flickr8k datasets using several architectures (VGG19,\nResNet-110 and DenseNet-121). On CIFAR10, we show that freezing 80% of the VGG19 network parameters from the third epoch onwards results in 0.24% drop in accuracy, while freezing 50% of Resnet-110 parameters results in 0.9% drop in accuracy and finally freezing 70% of Densnet-121 parameters results in 0.57% drop in accuracy. Furthermore, to experiemnt with real-life applications, we train an image captioning model with attention mechanism on the Flickr8k dataset using LSTM networks, freezing 60% of the parameters from the third epoch onwards, resulting in a better BLEU-4 score than the fully trained model. Our source code can be found in the appendix.", "pdf": "/pdf/815e43289650dde28f32a3def294fe96622c38a8.pdf", "paperhash": "sammani|how_important_are_network_weights_to_what_extent_do_they_need_an_update", "original_pdf": "/attachment/815e43289650dde28f32a3def294fe96622c38a8.pdf", "_bibtex": "@misc{\nsammani2020how,\ntitle={{\\{}HOW{\\}} {\\{}IMPORTANT{\\}} {\\{}ARE{\\}} {\\{}NETWORK{\\}} {\\{}WEIGHTS{\\}}? {\\{}TO{\\}} {\\{}WHAT{\\}} {\\{}EXTENT{\\}} {\\{}DO{\\}} {\\{}THEY{\\}} {\\{}NEED{\\}} {\\{}AN{\\}} {\\{}UPDATE{\\}}?},\nauthor={Fawaz Sammani and Mahmoud Elsayed and Abdelsalam Hamdi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg6PhNKDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg6PhNKDr", "replyto": "rkg6PhNKDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper21/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper21/Reviewers"], "noninvitees": [], "tcdate": 1570237758283, "tmdate": 1574723094201, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper21/-/Official_Review"}}}], "count": 8}