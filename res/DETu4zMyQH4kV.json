{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392710280000, "tcdate": 1392710280000, "number": 6, "id": "MJ29KaZP96KsB", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DETu4zMyQH4kV", "replyto": "DETu4zMyQH4kV", "signatures": ["Anna Choromanska"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We would like to thank all the reviewers for their comments. The paper after revisions was posted on arxiv today and will be soon publicly visible. Below we enclose answers to reviewers comments:\r\n\r\nReviewer 1: \r\n\r\nWe thank the reviewer for the comments. We have highlighted representation learning as an important application in the introduction, and furthermore pointed out that our majorization methods have been applied to this area in previous work in the context of batch learning. Since the main contribution of this paper is to propose a semi-stochastic extension, the results are directly applicable to representation learning. Specific extensions for deep learning are noted as future work in the conclusions. \r\n\r\nReviewer 2:\r\n\r\nWe thank the reviewer for the comments.\r\n1) Hessian-free optimization (also known as truncated Newton methods) solve Newton systems (and damped/inexact variants) inexactly to obtain descent directions. While classic methods use full gradients (and so are not competitive in the stochastic setting, w.r.t passes through the data), recent work also uses subsampling for gradient and Hessian approximations. Depending on the accuracy to which second order terms are approximated, these methods may require more or less storage than the method we propose. \r\n\r\nWe emphasize that our main contribution in this paper is to show majorization methods can be used in a semi-stochastic setting to compete with state-of-the-art fully stochastic methods, like SGD, ASGD or SAG, with respect to passes through the data.\r\n\r\n2) We presented the results on six datasets, three of them are sparse and the remaining ones are dense. The number of examples in these datasets are between 12K and 580K. Among these datasets we have some with 4932 dimensions and 46236 dimensions. These are not small datasets, and in fact commonly used to test new algorithms in the community (see e.g. Le Roux, Schmidt, Bach 2012). \r\n\r\nWith regard to Hessian-free optimization, we leave it to future work to compare majorization-based schemes with truncated Newton-based schemes. As far as we know, Hessian-free optimization methods have not been compared with state-of-the-art stochastic methods with respect to passes through the data. Also, in the batch setting, majorization methods have recently performed favorably in comparison to full Newton methods and quasi-Newton methods.\r\n\r\n3) Theorem 1 part 1 is correct - it neither relies on  E[Sigma_S^{-1}] = Sigma^{-1}, nor claims the average direction is unbiased. The point is that the average direction (s^k in the paper), although biased, is still gradient related, which we show in part 2. Note that in the definition of s^k = E[(Sigma_S^k + eta I)^{-1}] g^k,  the expectation is taken after the inverse. \r\n\r\nUsing the framework of Bertsekas and Tsitsiklis, it is enough to have the average direction be gradient related 'enough', which is shown in (7). \r\n\r\n4) Majorization methods have been shown to be competitive with second order optimization methods in prior work, in the batch setting. The main contribution of this paper is to make these methods competitive in the stochastic setting. While the number of samples used to approximate the gradient increases across iterations until the entire training data is eventually used (we control this rate of increase as explained in the experimental section), the number of samples used to obtain the second order bound-based approximation remains capped at a small number, as also explained in the experimental section. Growing the mini-batch is essential to achieve linear convergence rate as captured in Theorem 2; otherwise the rate of convergence would be sublinear, which is the typical convergence rate of state-of-the-art fully stochastic methods, like e.g. SGD.  \r\n\r\nIt is not clear what is meant by 'if other methods grow their minibatch'. It is always possible that other methods can be improved, either using some ideas in this paper or other ideas. Our main contribution is to show that majorization-based methods are competitive with state-of-the-art in the stochastic setting. \r\n\r\nReviewer 3:\r\n\r\nWe thank the reviewer for the comments.\r\n1) The summary of the paper provided by the reviewer is a nice; however `quadratic bound' is a better title, since the quadratic approximation is based on the bound for the partition function, rather than the Hessian or a standard Hessian approximation. \r\n\r\n2) We leave comparisons with Byrd et al. to future work as their setting differs from ours. In particular Byrd et. al. do not compare to state-of-the-art stochastic methods in their 2012 paper; their experiments show that dynamically growing the batch size is faster than a full batch method of the same type, and more accurate than a Newton-type method that uses a fixed sample size; they also compare with OWL. In their 2011 paper, they consider a batch-gradient, sampled Hessian approach. \r\n\r\n3) We refer the reviewer to Jebara & Choromanska 2012, where the bound method was shown to outperform BFGS and Newton methods in the batch setting (for convex problems), intuitively due to adapting not only locally, like most methods including gradient methods and Newton-like methods, but also globally, to the underlying optimization problem. The main motivation is that when a global tight bound is used, optimization methods based on majorization are competitive with standard optimization methods for problems involving the partition function. \r\n\r\n4) We used the phrase 'maximum likelihood inference' in the context of \r\nmaximum likelihood estimation. Thanks for pointing out this issue, the text has now been corrected. \r\n\r\n5) Different batching strategies indeed have a long history. We found that the theoretical arguments in Friedlander & Schmidt (2011) strongly motivate a batch growing scheme, which we call semi stochastic, and which they also call 'hybrid' (on the other hand, by stochastic we mean having fixed size mini-batch). They show that the convergence rate is a function of the conditioning of the problem and the degree of error in the gradient approximation; therefore, as long as the latter is dominated by the former, we can recover a linear rate in the stochastic setting. \r\n\r\n6) We indeed used word 'iterations' to refer to passes through the data in the phrase pointed by the reviewer. Thanks for pointing out this issue - the text has now been corrected. \r\n\r\n7) We have corrected the introduction according to reviewers suggestions. We have added the comment that we focus on partition functions, which are of central interests in many learning problems, like training CRFs or log-linear models. We also indicated early in the introduction that we will be extending the work of Jebara & Choromanska (2012) to the semi stochastic setting. \r\n\r\n8) The Bound Computation subroutine is directly taken from the previous work of Jebara & Choromanska (2012) (see Algorithm 1). The order indeed matters and slightly affects S, but not z or r.  Jebara & Choromanska (2012) investigated various ordering schemes and noted no significant difference in performance (see page 2).\r\n\r\n9) The reviewer is correct,n was indeed undefined. n is now defined when Omega is introduced. \r\n\r\n10) Omega is indeed the set of values that y can take.  We thank the reviewer, this issue has been fixed. \r\n\r\n11) We agree with the reviewer - current results show that curvature information can be incorporated in a way that guarantees convergence to stationarity under weak assumptions (Theorem 1) and the recovery of a linear rate provided an aggressive batch growing scheme for logistic regression (Theorem 2); in both cases the use curvature approximations from samples is not proven to help. We are also interested in finding a stronger result of the type that the reviewer is suggesting, perhaps under suitable assumptions on the problem class, but we leave this to future work. \r\n\r\n12) Note that additional requirements are required to ensure a uniform lower bound on the smallest eigenvalue in the absence of regularization. When eta is present, it provides such a lower bound; so we have removed the corollary and simply noted this. \r\n\r\n13) The problem with missing rho was corrected. We thank the reviewer for pointing this out.\r\n\r\n14) We agree with the point made by the reviewer - one recovers the linear rate by essentially controlling the error from sampling to be bounded by the geometric terms from the deterministic setting. The reviewer's point is well taken - the theorem does not show that inverse of the curvature matrix is actually helping, as the empirical results suggest. \r\n\r\n15) The fact that inexact solutions to subproblems can be interpreted as regularization is often used in the inverse problems community, and explained in some detail in Vogel's book. This is in fact the reference we wanted to cite, and we are very grateful that the reviewer caught this error! We have also added the reference that the reviewer suggested. \r\n\r\n16) We thank the reviewer for the remark about Page 7, section 12.1, Lemma 4 - this has been noted prior to the presentation of the lemma."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Semistochastic Quadratic Bound Methods", "decision": "submitted, no decision", "abstract": "Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets.", "pdf": "https://arxiv.org/abs/1309.1369", "paperhash": "aravkin|semistochastic_quadratic_bound_methods", "keywords": [], "conflicts": [], "authors": ["Aleksandr Y. Aravkin", "Anna Choromanska", "Tony Jebara", "Dimitri Kanevsky"], "authorids": ["sasha.aravkin@gmail.com", "aec2163@columbia.edu", "jebara@cs.columbia.edu", "dimitri.kanevsky@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392137580000, "tcdate": 1392137580000, "number": 5, "id": "f6UJcohNWS-vg", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DETu4zMyQH4kV", "replyto": "DETu4zMyQH4kV", "signatures": ["anonymous reviewer 7b7e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Semistochastic Quadratic Bound Methods", "review": "This paper looks at performing a stochastic truncated Newton method to general linear models (GLMs), utilizing a bound to the partition function from Jebra & Choromanska (2012).  Some basic theory is given, along with some experiments with logistic regression.\r\n\r\nStochastic or semi-stochastic truncated Newton methods such as Hessian-free optimization, and the work of Byrd et al. have already been applied to learning neural networks whose objective functions correspond to the negative LL of neural networks prediction under cross entropy error, which is like taking a GLM replacing theta in the definition expression with g(theta), where g is the neural network function.  In the special case that g=I this correspond exactly to logistic regression. \r\n\r\nOne thing I'm very confused about is what the bounding scheme of Jebra & Choromanska (2012) that is applied in this paper actually does.  Since it involves summing over all possible states, it can't be more efficient than just computing the partition function, and its various derivatives and second-directives directly.  Why use it then?  The Hessian of the negative LL of a general linear model will already be PSD, so that can't be the reason. \r\n\r\n\r\nDetailed comments:\r\n\r\nAbs:  What do you mean by 'maximum likelihood inference'?  Do you mean estimation?  Learning?\r\n\r\nPage 1: There are versions of those batch methods that can use minibatches and they seem to work pretty well, despite lack of strong theoretical results.  I guess though that this would fall into the category of what you are calling 'semi-stochastic'?  Actually, having read further it appears that you would only call these stochastic if the size of the minibatch grows during optimization.\r\n\r\nPage 1: When you say that stochastic methods converge in less iterations that batch ones, this makes no sense.  Perhaps you meant to say passes over the training set, not iterations.\r\n\r\nPage 2: The abstract made prominent mention of partition functions.  Yet, the introduction doesn't make any mention of them, and seems to be describing a new optimization method for standard tractable objective functions.\r\n\r\nYour intro should mention that you will be focusing on generalized linear models (what you are calling generalized linear model) and extending the work of Jebra & Choromanska (2012) to the stochastic case.  This becomes clear only once the author has read well passed the intro.\r\n\r\n\r\n\r\nPage 3: I think you should have some discussion of this Bound Computation subroutine.  It looks quite mysterious.  Does the order at which the loop goes over the different y's matter?  I can't see why it wouldn't, and that seems problematic.\r\n\r\n\r\n\r\nPage 3:  You don't define n.  Is this the size of Omega?\r\n\r\nPage 3:  'dataset Omega'?   I thought Omega was the set of values that y can take.\r\n\r\nPage 4:  The statement and proof of Theorem 1 seems similar to one of the theorems from the Byrd et al (2011) paper you cite.  And like that result, it is extremely weak.  Basically all it says is that as long as the curvature matrices are not so badly behaved that the size of their inverses grow arbitrarily, multiplying the gradient by their inverses won't prevent gradient descent from converging, provided the learning rate becomes small enough to combat the finite amount of blowing up that there is.  It says nothing about why you might actually *want* to multiply by the inverse of this matrix.  But I guess in this general setting there is nothing stronger that can be shown, since in general, multiplying the the inverse curvature matrix computing on only a subset of the data may sometimes do a lot more harm than good.\r\n\r\nPage 6:  I don't see why the first part of Cor 1 should be true.  In particular, it isn't enough to for the matrix to be positive definite everywhere.  It has to be positive definite with a lower bound on the smallest eigenvalue that works for all x (i.e. a uniform bound).\r\n\r\nPage 6:  Theorem 2 should start 'there exists mu, L>0, and rho>0'.  You are missing the rho.\r\n\r\nPage 6:  This theorem doesn't seem to show that multiplying by the inverse curvature matrix is actually helping.  One could just as easily prove a similar bound for standard SGD.  Like this bound, it certainly wouldn't give linear convergence for SGD (an absurdity!), due to presence of the Ck term in the bound, which will eventually dominate in stochastic optimization.\r\n\r\n\r\nPage 6:  Could you elaborate more on the point 'further regularizes the sub-problems'?   There is a detailed discussion of this kind effect in 'Training Deep and Recurrent Neural Networks with Hessian-Free Optimization', section 8.7.  What in particular does [38] say about this?\r\n\r\nPage 7:  In section 12.1 from the above mentioned article, a similar result to Lemma 4 is proved.  It is shown that the quadratic associated with the CG optimization is bounded, which implies the range result (since if the vector is not in the range, the optimization must be unbounded).  They look at the Gauss-Newton matrix, but for general linear models, the Hessian has the same structure, or the matrix from the bound from Jebra & Choromanska (2012) has the same basic structure as this matrix."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Semistochastic Quadratic Bound Methods", "decision": "submitted, no decision", "abstract": "Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets.", "pdf": "https://arxiv.org/abs/1309.1369", "paperhash": "aravkin|semistochastic_quadratic_bound_methods", "keywords": [], "conflicts": [], "authors": ["Aleksandr Y. Aravkin", "Anna Choromanska", "Tony Jebara", "Dimitri Kanevsky"], "authorids": ["sasha.aravkin@gmail.com", "aec2163@columbia.edu", "jebara@cs.columbia.edu", "dimitri.kanevsky@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391906880000, "tcdate": 1391906880000, "number": 4, "id": "Jxjf_jmDfngE0", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DETu4zMyQH4kV", "replyto": "DETu4zMyQH4kV", "signatures": ["anonymous reviewer a474"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Semistochastic Quadratic Bound Methods", "review": "The paper introduces a certain second-order method that is based on quadratic upper-bounds to convex functions and\r\non slowly increasing the size of the batch.  A few results show that the method is well-behaved and has reasonable\r\nconvergence rates on logistic regression. \r\n\r\nThis work is very similar to Hessian-free optimization, because it also uses CG to invert low-rank approximations \r\nto the curvature matrix, and it has comparable cost but greater memory complexity due to its need to store many\r\nparameter vectors.   Likewise, it builds up on previous work that finds quadratic upper bounds to convex functions,\r\nbut a quadratic upper bound seems restrictive, and perhaps a quadratic approximation would be more appropriate.\r\n\r\n\r\nPros:  Method is somewhat novel.\r\n\r\n\r\nCons:  \r\n - Experiments very small and unrealistic (sometimes tens of dimensions), and there is no comparison with Hessian-free optimization\r\n - Theorem 1 part 1 is wrong:  the method is biased, because while E[Sigma_S] = Sigma,  E[Sigma_S^{-1}] != Sigma^{-1}.  In general,\r\n second order methods that use modest numbers of samples for the curvature matrix are necessarily biased.\r\n - The paper has two ideas:  a certain second order method, and a simple scheme for growing the minibatch.  But which of these is essential?  Would we get similar results if we didn't grow the minibtach?  How large would the minibatch end up being?   What if the other methods grow their minibatch as well, do\r\n they become competitive?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Semistochastic Quadratic Bound Methods", "decision": "submitted, no decision", "abstract": "Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets.", "pdf": "https://arxiv.org/abs/1309.1369", "paperhash": "aravkin|semistochastic_quadratic_bound_methods", "keywords": [], "conflicts": [], "authors": ["Aleksandr Y. Aravkin", "Anna Choromanska", "Tony Jebara", "Dimitri Kanevsky"], "authorids": ["sasha.aravkin@gmail.com", "aec2163@columbia.edu", "jebara@cs.columbia.edu", "dimitri.kanevsky@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391723820000, "tcdate": 1391723820000, "number": 3, "id": "GXPfmkNE8um1e", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DETu4zMyQH4kV", "replyto": "DETu4zMyQH4kV", "signatures": ["anonymous reviewer 7e18"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Semistochastic Quadratic Bound Methods", "review": "The paper describes a second order stochastic optimization method where the gradient is computed on mini-batches of increasing size and the curvature is estimated using a bound computed on a possibly separate mini-batch of possibly constant size. This is clearly a state-of-the-art method. The authors derive a rather complete ensemble of theoretical guarantees, including the guarantee of converging with a nice linear rate.  This is a strong paper about stochastic optimization.  In the specific context of ICLR, I regret that the authors did not explain why this technique is useful to learn representations. The basic setup is that of maximum likelihood training of an exponential family model.  In practice, there are many reasons to believe that such a technique would work on mixture models or models that induce representations (although the theory might not be as simple.)  I believe that this paper should be accepted provided that the author pay at least some lip service to 'representation learning'."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Semistochastic Quadratic Bound Methods", "decision": "submitted, no decision", "abstract": "Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets.", "pdf": "https://arxiv.org/abs/1309.1369", "paperhash": "aravkin|semistochastic_quadratic_bound_methods", "keywords": [], "conflicts": [], "authors": ["Aleksandr Y. Aravkin", "Anna Choromanska", "Tony Jebara", "Dimitri Kanevsky"], "authorids": ["sasha.aravkin@gmail.com", "aec2163@columbia.edu", "jebara@cs.columbia.edu", "dimitri.kanevsky@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390940460000, "tcdate": 1390940460000, "number": 1, "id": "dd1X31kbEJ3zP", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DETu4zMyQH4kV", "replyto": "DETu4zMyQH4kV", "signatures": ["Anna Choromanska"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear readers and reviewers, we have just updated the paper on arxiv. In particular, we simplified Inequality (9) and Proof 6, clarified the statement of Theorem 2, and fixed accidental typos."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Semistochastic Quadratic Bound Methods", "decision": "submitted, no decision", "abstract": "Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets.", "pdf": "https://arxiv.org/abs/1309.1369", "paperhash": "aravkin|semistochastic_quadratic_bound_methods", "keywords": [], "conflicts": [], "authors": ["Aleksandr Y. Aravkin", "Anna Choromanska", "Tony Jebara", "Dimitri Kanevsky"], "authorids": ["sasha.aravkin@gmail.com", "aec2163@columbia.edu", "jebara@cs.columbia.edu", "dimitri.kanevsky@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390940460000, "tcdate": 1390940460000, "number": 2, "id": "WpCbeNsJXqeuj", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "DETu4zMyQH4kV", "replyto": "DETu4zMyQH4kV", "signatures": ["Anna Choromanska"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear readers and reviewers, we have just updated the paper on arxiv. In particular, we simplified Inequality (9) and Proof 6, clarified the statement of Theorem 2, and fixed accidental typos."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Semistochastic Quadratic Bound Methods", "decision": "submitted, no decision", "abstract": "Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets.", "pdf": "https://arxiv.org/abs/1309.1369", "paperhash": "aravkin|semistochastic_quadratic_bound_methods", "keywords": [], "conflicts": [], "authors": ["Aleksandr Y. Aravkin", "Anna Choromanska", "Tony Jebara", "Dimitri Kanevsky"], "authorids": ["sasha.aravkin@gmail.com", "aec2163@columbia.edu", "jebara@cs.columbia.edu", "dimitri.kanevsky@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387882140000, "tcdate": 1387882140000, "number": 51, "id": "DETu4zMyQH4kV", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "DETu4zMyQH4kV", "signatures": ["sasha.aravkin@gmail.com"], "readers": ["everyone"], "content": {"title": "Semistochastic Quadratic Bound Methods", "decision": "submitted, no decision", "abstract": "Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets.", "pdf": "https://arxiv.org/abs/1309.1369", "paperhash": "aravkin|semistochastic_quadratic_bound_methods", "keywords": [], "conflicts": [], "authors": ["Aleksandr Y. Aravkin", "Anna Choromanska", "Tony Jebara", "Dimitri Kanevsky"], "authorids": ["sasha.aravkin@gmail.com", "aec2163@columbia.edu", "jebara@cs.columbia.edu", "dimitri.kanevsky@gmail.com"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}