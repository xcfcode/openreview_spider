{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487861709806, "tcdate": 1478378166590, "number": 593, "id": "SkkTMpjex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkkTMpjex", "signatures": ["~Jimmy_Ba1"], "readers": ["everyone"], "content": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396695774, "tcdate": 1486396695774, "number": 1, "id": "rklQpGUde", "invitation": "ICLR.cc/2017/conference/-/paper593/acceptance", "forum": "SkkTMpjex", "replyto": "SkkTMpjex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "All reviewers agreed that this is an interesting contribution, but all agreed that the testing environment was somewhat small-scale and that significant difficulties could arise is scaling it up. However, the overall sentiment was still sufficiently positive.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396696266, "id": "ICLR.cc/2017/conference/-/paper593/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkkTMpjex", "replyto": "SkkTMpjex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396696266}}}, {"tddate": null, "tmdate": 1484704577300, "tcdate": 1484704577300, "number": 9, "id": "r1FHjSn8x", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "rkgMSRKrx", "signatures": ["~Jimmy_Ba1"], "readers": ["everyone"], "writers": ["~Jimmy_Ba1"], "content": {"title": "The additional communication cost of Distributed K-FAC is modest; The new revision addresses all the typo and references", "comment": "Thank the reviewer for the valuable comments and detailed suggestions on how to improve the paper. \n\nWe used academic scale resources for our experiments (see our comment below), but it would indeed be interesting to see how our method scales to much more computational resources. \n\nIn the minibatch size experiment, we focused on training curves because the focus of our paper was on optimization rather than generalization; however, we qualitatively observed that the validation curves display a very similar trend as the training curves for most of the optimization. Models trained with Distributed K-FAC do overfit more than the BN baselines at the end, but this is likely due to the extra noise that BN is adding to the update (similar to drop-out), which is an unintended side-effect of that algorithm.   One could considering adding other types of noise-based regularizers (e.g. some flavor of drop-out) to make up for this difference, if it were important.\n\nThe reviewer also asked about the potential communication bottleneck that could rise from scaling the distributed K-FAC algorithm. Scaling up the algorithm would primarily involve adding more gradient workers. Our scheme for computing gradients over large mini-batches is simply the same as the scheme used in standard synchronous SGD framework, and other researchers have already studied how to scale this up to hundreds of workers. We believe any additional communication costs specific to K-FAC will be modest.\n\nIn general, second-order statistics, i.e. the Kronecker factors, are computed using additional stats workers asynchronously and are independent from the gradient workers. The main communication bottleneck of distributed K-FAC is to communicate the Kronecker factors and to update all of the factors within a Fisher block at the same time on the parameter server. In a large CNN, the communication cost of transferring Kronecker factors amounts to O(K^4 C^2), where K is the kernel width and C is the number of channels, comparing to transferring gradient and parameters that is O(K^2 C^2). We think the communication bottleneck of transferring Kronecker factors, which is about K^2 more costly than transferring gradient, can be amortized by transferring and refresh the Kronecker factors occasionally (which was shown to work well in our experiments). For AlexNet, the Kronecker factors are refreshed once every 200 parameter updates which still gives a substantial 2x speed up comparing to our \u201cimproved Batch Norm\u201d baseline.\n\nWe have created a new revision which addresses the typos found by the reviewer, adds the references they requested, and addresses other issues such as figure spacing and the problem with the \u201cs = WA\u201d equation (this revealed a more far-reaching problem with our notation which we have now rectified - thanks for catching this!).  We are glad the reviewer appreciates the main ideas behind our paper and our experiments.  In the next revision of paper, we plan to incorporate the above discussion about communication costs, etc."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}, {"tddate": null, "tmdate": 1484313978430, "tcdate": 1484313978430, "number": 8, "id": "SJMYSLILg", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "S1rvHnTQe", "signatures": ["~James_Martens1"], "readers": ["everyone"], "writers": ["~James_Martens1"], "content": {"title": "Response to review from AnonReviewer2", "comment": "Thank you for your review.\n\nIn response to your concerns I can tell you we are currently working on an industrial-scale implementation of distributed K-FAC within TensorFlow.  This should hopefully be available within a month or two, and will be significantly more powerful and easier to use than the prototype implementation analyzed in our experiments. \n\nWe remain hopeful that distributed K-FAC will scale well to a truly distributed setting with multiple networked machines.  This seems likely since the number of iterations used by the method is much lower than the number of iterations used by Sync SGD, and gradients/parameters only need to be communicated once per iteration.\n\nWe have corrected the typos and other minor errors noted."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}, {"tddate": null, "tmdate": 1483494663712, "tcdate": 1483494663712, "number": 3, "id": "rkgMSRKrx", "invitation": "ICLR.cc/2017/conference/-/paper593/official/review", "forum": "SkkTMpjex", "replyto": "SkkTMpjex", "signatures": ["ICLR.cc/2017/conference/paper593/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper593/AnonReviewer4"], "content": {"title": "Review - Distributed K-FAC", "rating": "7: Good paper, accept", "review": "In this paper, the authors present a partially asynchronous variant of the K-FAC method. The authors adapt/modify the K-FAC method in order to make it computationally tractable for optimizing deep neural networks. The method distributes the computation of the gradients and the other quantities required by the K-FAC method (2nd order statistics and Fisher Block inversion). The gradients are computed in synchronous manner by the \u2018gradient workers\u2019 and the quantities required by the K-FAC method are computed asynchronously by the \u2018stats workers\u2019 and \u2018additional workers\u2019. The method can be viewed as an augmented distributed Synchronous SGD method with additional computational nodes that update the approximate Fisher matrix and computes its inverse. The authors illustrate the performance of the method on the CIFAR-10 and ImageNet datasets using several models and compare with synchronous SGD.\n\nThe main contributions of the paper are:\n1) Distributed variant of K-FAC that is efficient for optimizing deep neural networks. The authors mitigate the computational bottlenecks of the method (second order statistic computation and Fisher Block inverses) by asynchronous updating.\n2) The authors propose a \u201cdoubly-factored\u201d Kronecker approximation for layers whose inputs are too large to be handled by the standard Kronecker-factored approximation. They also present (Appendix A) a cheaper Kronecker factored approximation for convolutional layers.\n3) Empirically illustrate the performance of the method, and show:\n- Asynchronous Fisher Block inversions do not adversely affect the performance of the method (CIFAR-10)\n- K-FAC is faster than Synchronous SGD (with and without BN, and with momentum) (ImageNet)\n- Doubly-factored K-FAC method does not deteriorate the performance of the method (ImageNet and ResNet)\n- Favorable scaling properties of K-FAC with mini-batch size\n\nPros:\n- Paper presents interesting ideas on how to make computationally demanding aspects of K-FAC tractable. \n- Experiments are well thought out and highlight the key advantages of the method over Synchronous SGD (with and without BN).\n\nCons: \n- \u201c\u2026it should be possible to scale our implementation to a larger distributed system with hundreds of workers.\u201d The authors mention that this should be possible, but fail to mention the potential issues with respect to communication, load balancing and node (worker) failure. That being said, as a proof-of-concept, the method seems to perform well and this is a good starting point.\n- Mini-batch size scaling experiments: the authors do not provide validation curves, which may be interesting for such an experiment. Keskar et. al. 2016 (On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima) provide empirical evidence that large-batch methods do not generalize as well as small batch methods. As a result, even if the method has favorable scaling properties (in terms of mini-batch sizes), this may not be effective.\n\nThe paper is clearly written and easy to read, and the authors do a good job of communicating the motivation and main ideas of the method. There are a few minor typos and grammatical errors. \n\nTypos:\n- \u201cupdates that accounts for\u201d \u2014 \u201cupdates that account for\u201d\n- \u201cKronecker product of their inverse\u201d \u2014 \u201cKronecker product of their inverses\u201d\n- \u201cwhere P is distribution over\u201d \u2014 \u201cwhere P is the distribution over\u201d\n- \u201cback-propagated loss derivativesas\u201d \u2014 \u201cback-propagated loss derivatives as\u201d\n- \u201cinverse of the Fisher\u201d \u2014 \u201cinverse of the Fisher Information matrix\u201d\n- \u201cwhich amounts of several matrix\u201d \u2014 \u201cwhich amounts to several matrix\u201d\n- \u201cThe diagram illustrate the distributed\u201d \u2014 \u201cThe diagram illustrates the distributed\u201d\n- \u201cGradient workers computes\u201d \u2014 \u201cGradient workers compute\u201d \n- \u201cStat workers computes\u201d \u2014 \u201cStat workers compute\u201d \n- \u201coccasionally and uses stale values\u201d \u2014 \u201coccasionally and using stale values\u201d \n- \u201cThe factors of rank-1 approximations\u201d \u2014 \u201cThe factors of the rank-1 approximations\u201d\n- \u201cbe the first singular value and its left and right singular vectors\u201d \u2014 \u201cbe the first singular value and the left and right singular vectors \u2026 , respectively.\u201d\n- \u201c\\Psi is captures\u201d \u2014 \u201c\\Psi captures\u201d\n- \u201cmultiplying the inverses of the each smaller matrices\u201d \u2014 \u201cmultiplying the inverses of each of the smaller matrices\u201d\n- \u201cwhich is a nested applications of the reshape\u201d \u2014 \u201cwhich is a nested application of the reshape\u201d\n- \u201cprovides a computational feasible alternative\u201d \u2014 \u201cprovides a computationally feasible alternative\u201d\n- \u201caccording the geometric mean\u201d \u2014 \u201caccording to the geometric mean\u201d\n- \u201canalogous to shrink\u201d \u2014 \u201canalogous to shrinking\u201d\n- \u201capplied to existing model-specification code\u201d \u2014 \u201capplied to the existing model-specification code\u201d\n- \u201c: that the alternative parametrization\u201d \u2014 \u201c: the alternative parameterization\u201d\n\nMinor Issues:\n- In paragraph 2 (Introduction) the authors mention several methods that approximate the curvature matrix. However, several methods that have been developed are not mentioned. For example:\n1) (AdaGrad) Adaptive Subgradient Methods for Online Learning and Stochastic Optimization (http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n2) Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization (https://arxiv.org/abs/1607.01231)\n3) adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs (http://link.springer.com/chapter/10.1007/978-3-319-46128-1_1)\n4) A Self-Correcting Variable-Metric Algorithm for Stochastic Optimization (http://jmlr.org/proceedings/papers/v48/curtis16.html)\n5) L-SR1: A Second Order Optimization Method for Deep Learning (https://openreview.net/pdf?id=By1snw5gl)\n- Page 2, equation s = WA, is there a dimension issue in this expression?\n- x-axis for top plots in Figures 3,4,5,7 (Updates x XXX) appear to be a headings for the lower plots.\n- \u201cJames Martens. Deep Learning via Hessian-Free Optimization\u201d appears twice in References section.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483494664288, "id": "ICLR.cc/2017/conference/-/paper593/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper593/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper593/AnonReviewer2", "ICLR.cc/2017/conference/paper593/AnonReviewer1", "ICLR.cc/2017/conference/paper593/AnonReviewer4"], "reply": {"forum": "SkkTMpjex", "replyto": "SkkTMpjex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper593/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper593/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483494664288}}}, {"tddate": null, "tmdate": 1482954351200, "tcdate": 1482954351200, "number": 7, "id": "rJwOI5bre", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "H1gDgMH4e", "signatures": ["~Roger_Baker_Grosse1"], "readers": ["everyone"], "writers": ["~Roger_Baker_Grosse1"], "content": {"title": "We used academic scale computing resources, but we still addressed the key issues", "comment": "Thank you for your review. The reason we were limited to 8 GPUs (rather than 100, like the Google Brain paper you reference) is that our experiments were run on academic-scale computing resources. \n\nWe note, however, that our 8 GPUs experimental setup is comparable to the training environment used in the recent publications of the state-of-the-art ImageNet models (He et al. 2015) . We feel like 8 GPUs was a sufficient scale to address the key research questions surrounding the large-scale second-order optimization presented in our paper. We obtained substantial speedups on the four most widely used object recognition networks relative to a widely used and well-engineered baseline. All the evidence so far indicates that more highly parallel (i.e. larger mini-batch) settings are strictly more favorable to K-FAC relative to SGD, so the fact that the algorithm performs well even at this modest scale is a strong signal. \n\nFurthermore, our method is based on the synchronous SGD framework, which has already been applied at a massive scale, so there\u2019s no reason to believe we would suddenly encounter major scalability issues. In fact, because the inverse Fisher blocks are refreshed asynchronously, which can be implemented as a background thread on the parameter server, we can expect distributed K-FAC effectively has the same communication pattern and cost as synchronous SGD. Note that improving the efficiency of massive scale synchronous SGD is beyond the scope of our paper.\n\nThere is already some concern that machine learning conferences are becoming a party for the rich, as evidenced by the question that rose to #2 in the Deep Learning Symposium poll: https://www.facebook.com/events/636544206518160/permalink/636550569850857/\n\nIn our experiments, we ran a variety of architectures on ImageNet, and for each one, carefully tuned the baselines using a grid search over hyperparameters. This already makes it a very expensive set of experiments by academic standards. As a community, we need to be careful to avoid winding up in a system where you need the resources of a major company lab in order to get papers accepted to ICLR. If you have concerns about the scalability of particular aspects of the algorithm, perhaps you can suggest a way to address them while staying within a reasonable budget?\n\nIt will indeed be interesting to see how the algorithm scales up to very large-scale parallelism, which is something we intend to pursue now that the method has been validated at academic scale. (This is a good example of why a system of open scientific publication is useful to the tech industry -- without the method having been validated at academic scale, there probably would be nothing to justify trying it at industrial scale.)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}, {"tddate": null, "tmdate": 1482239871243, "tcdate": 1482133592237, "number": 2, "id": "H1gDgMH4e", "invitation": "ICLR.cc/2017/conference/-/paper593/official/review", "forum": "SkkTMpjex", "replyto": "SkkTMpjex", "signatures": ["ICLR.cc/2017/conference/paper593/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper593/AnonReviewer1"], "content": {"title": "Official Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes an asynchronous distributed K-FAC method for efficient optimization of \ndeep networks. The authors introduce interesting ideas that many computationally demanding \nparts of the original K-FAC algorithm can be efficiently implemented in distributed fashion. The\ngradients and the second-order statistics are computed by distributed workers separately and \naggregated at the parameter server along with the inversion of the approximate Fisher matrix \ncomputed by a separate CPU machine. The experiments are performed in CIFAR-10 and ImageNet\nclassification problems using models such as AlexNet, ResNet, and GoogleReNet.\n\nThe paper includes many interesting ideas and techniques to derive an asynchronous distributed \nversion from the original K-FAC. And the experiments also show good results on a few \ninteresting cases. However, I think the empirical results are not thorough and convincing \nenough yet. Particularly, experiments on various and large number of GPU workers (in the same machine, \nor across multiple workers) are desired. For example, as pointed by the authors in the answer of a comment,\nChen et.al. (Revisiting Distributed Synchronous SGD, 2015) used 100 workers to test their distributed deep \nlearning algorithm. Even considering that the authors have a limitation in computing resource under the \nacademic research setting, the maximum number of 4 or 8 GPUs seems too limited as the only test case of \ndemonstrating the efficiency of a distributed learning algorithm.  ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483494664288, "id": "ICLR.cc/2017/conference/-/paper593/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper593/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper593/AnonReviewer2", "ICLR.cc/2017/conference/paper593/AnonReviewer1", "ICLR.cc/2017/conference/paper593/AnonReviewer4"], "reply": {"forum": "SkkTMpjex", "replyto": "SkkTMpjex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper593/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper593/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483494664288}}}, {"tddate": null, "tmdate": 1481656478102, "tcdate": 1481656478094, "number": 6, "id": "SkLiu6TXl", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "Hk7ZRtaXe", "signatures": ["~Jimmy_Ba1"], "readers": ["everyone"], "writers": ["~Jimmy_Ba1"], "content": {"title": "a double subscript typo...", "comment": "Yes it should indeed be \\pi_{{D_{s}} in Eq.(3) and the discussion above the equation. We will fix this typeset error in our next revision. Thanks for the comment!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}, {"tddate": null, "tmdate": 1481380240358, "tcdate": 1481380240349, "number": 5, "id": "Bkd5b9F7x", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "S1ETb6PQg", "signatures": ["~Jimmy_Ba1"], "readers": ["everyone"], "writers": ["~Jimmy_Ba1"], "content": {"title": "We use the most common synchronous SGD optimizer as our baseline ", "comment": "Thanks for the comments. Our baseline is a synchronous SGD optimizer that uses the averaged gradient from each of the gradient workers. The parameter server adjusts the parameter once it has received all the gradients from each worker and blocking mechanism is used for synchronization among the gradient workers. It is the standard optimizer used by ResNet (He et.al. 2015). More recently, Chen et.al. (Revisiting Distributed Synchronous SGD, 2015) have shown the synchronous SGD outperforms the Downpour style asynchronous SGD. We think using synchronous SGD in our experiments is a fair and strong baseline. Our distributed K-FAC algorithm can be viewed as a straightforward add-on module to complement any distributed SGD system and should provide substantial improvement over the original SGD system, but such evaluating is beyond the scope of this paper. Moreover, distributed K-FAC, like any second-order methods, can benefit from a low variance gradient and has a good affinity to synchronous SGD method, where averaging the gradient workers is a straightforward low noise gradient estimation.  Because we are using synchronous SGD method, using only 4 GPUs is still a good simulation for the truly distributed setup."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}, {"tddate": null, "tmdate": 1481377134674, "tcdate": 1481377134668, "number": 4, "id": "H1P_HFF7g", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "rJSvMR_Ql", "signatures": ["~Jimmy_Ba1"], "readers": ["everyone"], "writers": ["~Jimmy_Ba1"], "content": {"title": "use revision history for the next revision", "comment": "Thanks for the pointer. We will use the revision history feature for our next update to the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}, {"tddate": null, "tmdate": 1481327846938, "tcdate": 1481327846932, "number": 3, "id": "B1klSTuXg", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "SylCNdOQl", "signatures": ["~James_Martens1"], "readers": ["everyone"], "writers": ["~James_Martens1"], "content": {"title": "We updated the PDF hosted on Github.", "comment": "We updated the PDF hosted on Github.  Is there a revision history feature on OpenReview.net that we should be using?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}, {"tddate": null, "tmdate": 1481261513904, "tcdate": 1481261499978, "number": 2, "id": "S1ETb6PQg", "invitation": "ICLR.cc/2017/conference/-/paper593/pre-review/question", "forum": "SkkTMpjex", "replyto": "SkkTMpjex", "signatures": ["ICLR.cc/2017/conference/paper593/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper593/AnonReviewer1"], "content": {"title": "Questions", "question": "Could you elaborate a bit more on how the SGD is implemented on 4 GPUs? And, why not compare to other distributed optimization methods such as Downpour SGD and Elastic Averaging SGD?\n\nIsn't the 4 GPU too small scale to evaluate a distributed algorithm?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481261500532, "id": "ICLR.cc/2017/conference/-/paper593/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper593/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper593/AnonReviewer2", "ICLR.cc/2017/conference/paper593/AnonReviewer1"], "reply": {"forum": "SkkTMpjex", "replyto": "SkkTMpjex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper593/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper593/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481261500532}}}, {"tddate": null, "tmdate": 1480356491654, "tcdate": 1480356491651, "number": 2, "id": "HJV5ze9Mg", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "SkkTMpjex", "signatures": ["~James_Martens1"], "readers": ["everyone"], "writers": ["~James_Martens1"], "content": {"title": "Paper is updated", "comment": "We revised the paper, improving the writing quality and clarity of the Experiments section in particular."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}, {"tddate": null, "tmdate": 1480354515651, "tcdate": 1480354515647, "number": 1, "id": "S1h05yqfe", "invitation": "ICLR.cc/2017/conference/-/paper593/public/comment", "forum": "SkkTMpjex", "replyto": "rJjrCRKfg", "signatures": ["~Jimmy_Ba1"], "readers": ["everyone"], "writers": ["~Jimmy_Ba1"], "content": {"title": "Fixed figure arrangement in the revised paper. Training error is typically higher than validation error during the majority of the ImageNet training time.", "comment": "Thanks for pointing out the figure arrangement issue. We have fixed that in the latest version. The figure 3 has also been updated so that the legend does not cover the line plots.\n\nIt is generally true that validation error is higher than training error. Despite having 1.2 million images in the ImageNet training set, a data pre-processing pipeline is almost always used for training ImageNet that includes image jittering and aspect distortion. The pre-processing creates an augmented training set that is more difficult than the undistorted validation set. Therefore, the validation error is often lower than training error during the first 90% of the training time. This observation is consistent with the previously published results (He et al., 2015, \"Deep Residual Learning for Image Recognition\").\n\nThanks again for the comment."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Distributed Second-Order Optimization using Kronecker-Factored Approximations", "abstract": "As more computational resources become available, machine learning researchers train ever larger neural networks on millions of data points using stochastic gradient descent (SGD). Although SGD scales well in terms of both the size of dataset and the number of parameters of the model, it has rapidly diminishing returns as parallel computing resources increase.  Second-order optimization methods have an affinity for well-estimated gradients and large mini-batches, and can therefore benefit much more from parallel computation in principle.   Unfortunately,  they often employ severe approximations to the curvature matrix in order to scale to large models with millions of parameters, limiting their effectiveness in practice versus well-tuned SGD with momentum.  The recently proposed K-FAC method(Martens and Grosse, 2015) uses a stronger and more sophisticated curvature approximation, and has been shown to make much more per-iteration progress than SGD, while only introducing a modest overhead.  In this paper, we develop a version of K-FAC that distributes the computation of gradients and additional quantities required by K-FAC across multiple machines, thereby taking advantage of method\u2019s superior scaling to large mini-batches and mitigating its additional overheads. We provide a Tensorflow implementation of our approach which is easy to use and can be applied to many existing codebases without modification.  Additionally, we develop several algorithmic enhancements to K-FAC which can improve its computational performance for very large models. Finally, we show that our distributed K-FAC method speeds up training of various state-of-the-art ImageNet classification models by a factor of two compared to Batch Normalization(Ioffe and Szegedy, 2015).", "pdf": "https://jimmylba.github.io/papers/nsync.pdf", "TL;DR": "Fixed typos pointed out by AnonReviewer1 and AnonReviewer4 and added the experiments in Fig. 6 showing the poor scaling of batch normalized SGD using a batch size of 2048 on googlenet. ", "paperhash": "ba|distributed_secondorder_optimization_using_kroneckerfactored_approximations", "keywords": ["Deep learning", "Optimization"], "conflicts": ["cs.toronto.edu", "google.com"], "authors": ["Jimmy Ba", "Roger Grosse", "James Martens"], "authorids": ["jimmy@psi.toronto.edu", "rgrosse@cs.toronto.edu", "jmartens@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287507951, "id": "ICLR.cc/2017/conference/-/paper593/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkkTMpjex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper593/reviewers", "ICLR.cc/2017/conference/paper593/areachairs"], "cdate": 1485287507951}}}], "count": 14}