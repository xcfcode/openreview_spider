{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392750540000, "tcdate": 1392750540000, "number": 4, "id": "7OBiO3szQ4OS5", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YvgSX22hONWpI", "replyto": "YvgSX22hONWpI", "signatures": ["Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for the feedback and suggested improvements, which we will incorporate in the revised paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multimodal Transitions for Generative Stochastic Networks", "decision": "submitted, no decision", "abstract": "Generative Stochastic Networks (GSNs) have been recently introduced as an alternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a Markov chain whose stationary distribution is an estimator of the data generating distribution. The result of training is therefore a machine that generates samples through this Markov chain. However, the previously introduced GSN consistency theorems suggest that in order to capture a wide class of distributions, the transition operator in general should be multimodal, something that has not been done before this paper. We introduce for the first time multimodal transition distributions for GSNs, in particular using models in the NADE family (Neural Autoregressive Density Estimator) as output distributions of the transition operator. A NADE model is related to an RBM (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. The parameters of the NADE are obtained as a learned function of the previous state of the learned Markov chain. Experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal GSNs.", "pdf": "https://arxiv.org/abs/1312.5578", "paperhash": "ozair|multimodal_transitions_for_generative_stochastic_networks", "keywords": [], "conflicts": [], "authors": ["Sherjil Ozair", "Li Yao", "Yoshua Bengio"], "authorids": ["sherjilozair@gmail.com", "yaoli.email@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392750240000, "tcdate": 1392750240000, "number": 1, "id": "USe3SVdvf8Pp1", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "YvgSX22hONWpI", "replyto": "4ErBEqulds9aZ", "signatures": ["Yoshua Bengio"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We thank the review for the thoughtful comments.\r\n\r\nWe believe that we have found a way to speed-up sampling of GSN-NADE (especially deep NADE) in order to incorporate the walkback algorithm in the training process.  The idea is to avoid the lengthy loop over all the pixels by only resampling subsets at a time (according to a GSN framework!).\r\n\r\nWe have started experiments with this approach.  We will add these results to the final version of the paper.\r\n\r\nThe main contribution however remains that GSNs with a multimodal output distribution can be trained and that they can avoid *modeling* difficulties that can otherwise hurt GSNs with unimodal/factorized output distributions (such as those used in previous papers).\r\n\r\nWe will also add results on other datasets."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multimodal Transitions for Generative Stochastic Networks", "decision": "submitted, no decision", "abstract": "Generative Stochastic Networks (GSNs) have been recently introduced as an alternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a Markov chain whose stationary distribution is an estimator of the data generating distribution. The result of training is therefore a machine that generates samples through this Markov chain. However, the previously introduced GSN consistency theorems suggest that in order to capture a wide class of distributions, the transition operator in general should be multimodal, something that has not been done before this paper. We introduce for the first time multimodal transition distributions for GSNs, in particular using models in the NADE family (Neural Autoregressive Density Estimator) as output distributions of the transition operator. A NADE model is related to an RBM (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. The parameters of the NADE are obtained as a learned function of the previous state of the learned Markov chain. Experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal GSNs.", "pdf": "https://arxiv.org/abs/1312.5578", "paperhash": "ozair|multimodal_transitions_for_generative_stochastic_networks", "keywords": [], "conflicts": [], "authors": ["Sherjil Ozair", "Li Yao", "Yoshua Bengio"], "authorids": ["sherjilozair@gmail.com", "yaoli.email@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391887320000, "tcdate": 1391887320000, "number": 3, "id": "IbETu001doIwY", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YvgSX22hONWpI", "replyto": "YvgSX22hONWpI", "signatures": ["anonymous reviewer 6682"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Multimodal Transitions for Generative Stochastic Networks", "review": "Generative Stochastic Networks have recently been introduced as a new generative model formalism. Instead of directly parameterizing the data distribution, GSNs learn the transition distribution for a Markov chain which generates samples from the learned distribution. Previous results on GSNs, namely using the denoising criterion, have used a transition distribution that is factorized or unimodal. This makes it easier to learn the GSN, but limits the expressivity of the model. This paper proposes a type of GSN which uses conditional NADE to represent multi-modal output distributions for the transition operator. Results show it performing better than the uni-modal approach on MNIST and a toy 2d dataset.\r\n\r\nThe paper is novel. The GSN is a relatively recent formalism, with essentially all work coming from the same research group. This is the first work to propose a multi-modal transition operator. It's also an interesting use of NADE and bears some similarity to the recent use of NADE as an output distribution for recurrent neural networks. It is an interesting paper: well motivated and well written.\r\n\r\nPros:\r\n* In my opinion, GSNs are a fascinating framework and this paper addresses one of the limitations of previous work\r\n* The paper is clear and gives a great deal of background; in summarizing the work to-date on GSNs, it's complimentary to other published work\r\n\r\nCons:\r\n* Experiments are limited to a toy dataset (2d - a few hundred (?) examples) and MNIST; the former is ok for visualization and the later is used to compare methods based on an approximation to test log likelihood proposed by the same group\r\n* The comparisons aren't very extensive - essentially the model without NADE and the baseline model using 'walkback' training\r\n* Although the background and review of GSNs are welcome, more than half of the paper is devoted to background. I feel that the paper makes a good contribution, but not as substantial as other ICLR papers. It could certainly be strengthened by exploring other types of output distribution models beyond NADE or experiments beyond MNIST.\r\n\r\nComments\r\n--------\r\n\r\nIn the experiments section, I suggest giving a pointer that discussion re: walkback not being applied to GSN-NADE will be addressed later (i.e. in next section).\r\n\r\nSection 4 refers to a Figure 2, but I believe it should be Figure 1.\r\n\r\nSection 6 'fatorial' -> 'factorial'\r\n\r\nEven though the CSL method is referenced, you could give a short description of it in the experiments section.\r\n\r\nIs R-NADE being used for the 2d dataset and regular NADE being used for MNIST? I didn't see this explicitly stated.\r\n\r\nFirst sentence on page 7 reads 'On the contrary, GSN-NADE with the walkback training alleviates this issue.' This is confusing, GSN-NADE didn't use walkback training. Should it say 'GSN-NADE without the walkback training?'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multimodal Transitions for Generative Stochastic Networks", "decision": "submitted, no decision", "abstract": "Generative Stochastic Networks (GSNs) have been recently introduced as an alternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a Markov chain whose stationary distribution is an estimator of the data generating distribution. The result of training is therefore a machine that generates samples through this Markov chain. However, the previously introduced GSN consistency theorems suggest that in order to capture a wide class of distributions, the transition operator in general should be multimodal, something that has not been done before this paper. We introduce for the first time multimodal transition distributions for GSNs, in particular using models in the NADE family (Neural Autoregressive Density Estimator) as output distributions of the transition operator. A NADE model is related to an RBM (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. The parameters of the NADE are obtained as a learned function of the previous state of the learned Markov chain. Experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal GSNs.", "pdf": "https://arxiv.org/abs/1312.5578", "paperhash": "ozair|multimodal_transitions_for_generative_stochastic_networks", "keywords": [], "conflicts": [], "authors": ["Sherjil Ozair", "Li Yao", "Yoshua Bengio"], "authorids": ["sherjilozair@gmail.com", "yaoli.email@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391570400000, "tcdate": 1391570400000, "number": 2, "id": "hcwAfe0-aRZg7", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YvgSX22hONWpI", "replyto": "YvgSX22hONWpI", "signatures": ["anonymous reviewer 8572"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Multimodal Transitions for Generative Stochastic Networks", "review": "The authors present work that tackles learning data distributions indirectly, by learning a neural-network based transition operator (a Generative Stochastic Neural Network, GSN), whose stationary distribution is the desired distribution. Specifically, they introduce a GSN where the transition distribution is multimodal. The proposed model is evaluated in two experiments (spiral distribution and generating MNIST) and shown to produce samples that better capture the data distribution.\r\n\r\nI'm not enough of an expert to give an in-depth analysis of the proposed method, however, I review the paper in general terms.\r\n\r\n\r\nPros: addressing sampling issues is worthwhile, and I would like to learn more about the proposed approach.\r\nCons: the initial part of the paper is way too wordy. At the same time, details and explanations regarding the model and experimental procedures are missing. There's few actual results; this should have been written as a short workshop paper.\r\n\r\n\r\nThe introduction/motivation section is very long and has some content that is not directly relevant to the paper, or detail that is unnecessary. The authors write over a page to motivate unsupervised learning in general, and to describe well-known facts (Boltzmann machines are difficult to handle; the existence of many, separated modes makes MCMC difficult). This strikes me as too general for the paper and could have been one or two short paragraphs instead. Similarly, what is the benefit of spelling out in full Theorem 1, from an earlier reference? Etc. \r\n\r\nThe writing should be made more succinct and be structured better. In Section 1, the general introduction of the topic and overview of the paper should be separated. Wording could be improved (e.g., '[...] MAP approximations could all fall on their face.'; or 'In a sense, this is something that we are directly testing in this paper.'--in a sense?). \r\n\r\nIn Section 5, I take it that the NADE factorization is now over pixels rather than frames? The notation should make this clearer (x_{} is used for both). RNADE was not defined (should be with ref [12]). A figure would have been helpful for understanding the GSN-NADE architecture (and the other models).\r\n\r\nResults are not presented until Section 6, more than halfway through the paper. Here, the methods used are not properly explained and not enough details are given. \r\n\r\nIn the first experiment, what are the model parameters and training procedure? For the second experiment, the authors write 'The training of model uses the same procedure as in [13]', with no further explanation. Similarly: 'To evaluate the quality of trained GSNs as generative models, we adopt the Conservative Sampling-based Log-likelihood (CSL) estimator proposed in [9]' and 'The GSN-1-w is [...] trained with the walkback procedure proposed in Bengio et al. [11].' No explanation is given for any of this. The references are all very recent (2013), so it's not like the general reader can be expected to be familiar with these methods. Again, model parameters are missing ('For the details of models being compared, please refer to [9].').\r\n\r\n\r\nIn conclusion, if the authors were to expand the experimental section and better explain the methods used, and cut out unnecessary content, this could make for a workshop track paper. But I don't see enough content for a conference paper.\r\n\r\nFurther notes:\r\n\r\nPage 3: 'Monte-Carlo Markov Chain (MCMC) methods' -> Markov chain Monte Carlo (MCMC) methods\r\nPage 4: 'Figure 2' should be Figure 1?\r\n\r\nIt looks like the paper was updated after the review period had started (24 Jan.). It would be good in such cases to notify reviewers by posting a comment here, to make sure they are reviewing the latest version."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multimodal Transitions for Generative Stochastic Networks", "decision": "submitted, no decision", "abstract": "Generative Stochastic Networks (GSNs) have been recently introduced as an alternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a Markov chain whose stationary distribution is an estimator of the data generating distribution. The result of training is therefore a machine that generates samples through this Markov chain. However, the previously introduced GSN consistency theorems suggest that in order to capture a wide class of distributions, the transition operator in general should be multimodal, something that has not been done before this paper. We introduce for the first time multimodal transition distributions for GSNs, in particular using models in the NADE family (Neural Autoregressive Density Estimator) as output distributions of the transition operator. A NADE model is related to an RBM (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. The parameters of the NADE are obtained as a learned function of the previous state of the learned Markov chain. Experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal GSNs.", "pdf": "https://arxiv.org/abs/1312.5578", "paperhash": "ozair|multimodal_transitions_for_generative_stochastic_networks", "keywords": [], "conflicts": [], "authors": ["Sherjil Ozair", "Li Yao", "Yoshua Bengio"], "authorids": ["sherjilozair@gmail.com", "yaoli.email@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391480160000, "tcdate": 1391480160000, "number": 1, "id": "4ErBEqulds9aZ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YvgSX22hONWpI", "replyto": "YvgSX22hONWpI", "signatures": ["anonymous reviewer 3e78"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Multimodal Transitions for Generative Stochastic Networks", "review": "Recently, (Bengio et al. 2013b,2013c) proposed a novel way to estimate a data distribution indirectly, by training the transition operator T(x_{t}|x_{t-1}) of a Markov chain instead of trying to estimate the data distribution directly. These Generative Stochastic Networks (GSN) have several theoretical benefits, mainly avoiding the costly process of sampling in a highly multi-modal distribution (either when trying to sample from the model to estimate the so called 'negative gradient update', or when trying to perform MAP inference) as is traditionally required when training highly multi-modal models.\r\n\r\nThis paper proposes using a Neural Auto-Regressive Density Estimator (NADE) to learn a *multimodal* transitition operator, thus hopefully improving results compared to the above works in which the transition operator is unimodal.\r\n\r\nThe authors make a very good review of recent advances related to GSNs and argue that using a multi-modal transition operator could allow for better mixing of the markov chain and deal more gracefully with huge numbers of modes.\r\n\r\nTwo sets of experiments are performed, the first trying to estimate a 'spiral' distribution in 2D (a 2D slice of the traditional Swiss roll seen in manifold learning studies), the second on the Mnist dataset of handwritten digits. These experiments compare GSNs using the proposed multi-modal NADE transition operator with GSNs based on uni-modal transition operators which were used in previous works.\r\n\r\nThe results show that GSN-NADE fare much better at representing the spiral dataset, but fall a little short of expectations on the Mnist dataset where the walkback trick proposed in (Bengio et al. 2013c) allows a GSN with a uni-modal transition operator to fare better than the proposed GSN-NADE. In other words, the results support the hypothesis that a multi-modal NADE transition operator can be helpful, but using the walkback trick to deal with spurious modes appears to be even more helpful. As a possible future work, the authors propose to try and find a way to combine both benefits since walkback is not trivially applicable to the proposed GSN-NADE.\r\n\r\nThis study presents a new approach to a problem clearly identified in an earlier study, gives a very good summary of the state-of-the-art, clearly identifies key issues to overcome, and honestly presents its own limitations. Although the results fall a little short of expectations, they do indeed point to the fact that a multi-modal transition operator can be helpful in GSNs.\r\nRegrettably the experiments do not try to assess the quality of the representations obtained with GSN-NADE in a classification setting and on more complex datasets than Mnist."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multimodal Transitions for Generative Stochastic Networks", "decision": "submitted, no decision", "abstract": "Generative Stochastic Networks (GSNs) have been recently introduced as an alternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a Markov chain whose stationary distribution is an estimator of the data generating distribution. The result of training is therefore a machine that generates samples through this Markov chain. However, the previously introduced GSN consistency theorems suggest that in order to capture a wide class of distributions, the transition operator in general should be multimodal, something that has not been done before this paper. We introduce for the first time multimodal transition distributions for GSNs, in particular using models in the NADE family (Neural Autoregressive Density Estimator) as output distributions of the transition operator. A NADE model is related to an RBM (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. The parameters of the NADE are obtained as a learned function of the previous state of the learned Markov chain. Experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal GSNs.", "pdf": "https://arxiv.org/abs/1312.5578", "paperhash": "ozair|multimodal_transitions_for_generative_stochastic_networks", "keywords": [], "conflicts": [], "authors": ["Sherjil Ozair", "Li Yao", "Yoshua Bengio"], "authorids": ["sherjilozair@gmail.com", "yaoli.email@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387594920000, "tcdate": 1387594920000, "number": 22, "id": "YvgSX22hONWpI", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "YvgSX22hONWpI", "signatures": ["sherjilozair@gmail.com"], "readers": ["everyone"], "content": {"title": "Multimodal Transitions for Generative Stochastic Networks", "decision": "submitted, no decision", "abstract": "Generative Stochastic Networks (GSNs) have been recently introduced as an alternative to traditional probabilistic modeling: instead of parametrizing the data distribution directly, one parametrizes a transition operator for a Markov chain whose stationary distribution is an estimator of the data generating distribution. The result of training is therefore a machine that generates samples through this Markov chain. However, the previously introduced GSN consistency theorems suggest that in order to capture a wide class of distributions, the transition operator in general should be multimodal, something that has not been done before this paper. We introduce for the first time multimodal transition distributions for GSNs, in particular using models in the NADE family (Neural Autoregressive Density Estimator) as output distributions of the transition operator. A NADE model is related to an RBM (and can thus model multimodal distributions) but its likelihood (and likelihood gradient) can be computed easily. The parameters of the NADE are obtained as a learned function of the previous state of the learned Markov chain. Experiments clearly illustrate the advantage of such multimodal transition distributions over unimodal GSNs.", "pdf": "https://arxiv.org/abs/1312.5578", "paperhash": "ozair|multimodal_transitions_for_generative_stochastic_networks", "keywords": [], "conflicts": [], "authors": ["Sherjil Ozair", "Li Yao", "Yoshua Bengio"], "authorids": ["sherjilozair@gmail.com", "yaoli.email@gmail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 6}