{"notes": [{"id": "LiX3ECzDPHZ", "original": "hYyY3ldwrNF", "number": 1373, "cdate": 1601308153242, "ddate": null, "tcdate": 1601308153242, "tmdate": 1616003634840, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BosdvbZAMr", "original": null, "number": 1, "cdate": 1610040387320, "ddate": null, "tcdate": 1610040387320, "tmdate": 1610473981122, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "LiX3ECzDPHZ", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper describes a cool application of online learning from bandit feedback -- creating personalized, adaptive typing interfaces for users with sensorimotor impairments. The problem is well-motivated -- the interface can observe users' gaze (e.g. via a webcam image), predict a character as an action, and bandit feedback can be collected by observing whether users use the backspace key after the interface's action. Prior work showed that gaze-to-text can be less burdensome than typing, but this can quickly become untrue the more mistakes the interface makes. So, the goal is to personalize the interaction policy so that it makes fewer mistakes than the default interaction policy trained using a fixed dataset of expert demonstrations.\n\nThe high-point of the paper is the empirical user study with 12-60 participants -- the study convincingly demonstrates that indeed a simple bandit algorithm can improve over the default interface; moreover, users exhibit intriguing co-adaptation patterns with the adaptive interfaces. These findings may prove to be an interesting point for future studies in user co-adaptation.\n\nThe low-point of the paper is its algorithmic development. There is a vast literature on bandit/RL algorithms, and incorporating human feedback into their operation (the paper rightly cites TAMER, COACH, etc.) but it is very unclear why any one of these algorithms could not be used for the paper's application. COACH (human feedback gives an explicit view of the action's advantage -- which in the contextual bandit setting exactly matches the paper's assumptions) seems particularly appropriate. Although the algorithm proposed in the paper is simple, how applicable is it in any other context? how does it compare to COACH/etc.? when should we prefer this algorithm over others? Furthermore, given that X2T trains a reward model from observed user-behavior, a natural baseline would use an epsilon-greedy strategy (fraction of the time, pick actions greedily according to the reward model) -- this might isolate the benefit of the approximately Boltzmann exploration being conducted on top of the reward estimates in Eqn 2. Finally, since X2T trains a reward model per user it could be particularly informative to visualize what the models have learned to illustrate qualitatively how X2T is personalizing across its user base.\n\nThe paper could have a much bigger impact if the authors can figure out some creative way to enable the broader research community to work on this problem domain. A testbed or environment (like RecSim for content recommendation https://github.com/google-research/recsim) with configurable but realistic reward models could allow researchers to test several bandit algorithms, MDP vs CB formulations, other ways to interpret user feedback etc. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "LiX3ECzDPHZ", "replyto": "LiX3ECzDPHZ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040387306, "tmdate": 1610473981102, "id": "ICLR.cc/2021/Conference/Paper1373/-/Decision"}}}, {"id": "tWaibGQXvrf", "original": null, "number": 25, "cdate": 1606274480150, "ddate": null, "tcdate": 1606274480150, "tmdate": 1606274480150, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "NdiYT9nkw5Q", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Response to author comments", "comment": "Thank you for your comments. Just to ensure we are on the same page - I do believe applications are (an integral) part of ICLR.\n\nThat said, despite the authors response, I do retain my reservations about the scope of contributions of this paper (which of course deals with an interesting paradigm), and, as I have made clear, I do believe this paper requires more work to be accepted for publication. I will leave it to the AC to make the final call on this paper. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "s5Qf2ALrECl", "original": null, "number": 2, "cdate": 1603872655344, "ddate": null, "tcdate": 1603872655344, "tmdate": 1606258903509, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "LiX3ECzDPHZ", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "Summary: \nThis work presents a method for online learning of an assistive typing user interface (XT2) with implicit user feedback. User inputs for such an assistive typing interface are assumed to be in the form of eye gaze or handwritten characters. However, the implicit human feedback is assumed to be backspaces typed on a keyboard. Backspaces are used to delete words predicted by the assistive typing interface based on the user\u2019s input. The online learning of such an interface to improve its assistive performance and adapt to the user over time is framed as a contextual bandit problem. A reward prediction network is trained to predict the use of backspaces (implicit feedback) by the user. This reward prediction network combined with the default interface policy using Baye\u2019s theorem is used to update the policy of the typing interface. The experimental results with two user studies reveal that the presented method performs better than a non-adaptive default interface, stimulates user co-adaptation to the interface, and offline learning accelerates online learning.\n\n########################\n\n\nPros:\n- The paper is well-written and easy to follow.\n- Strong experimental results to support the idea of the work.\n- Offline pretraining of the reward model reduces the burden on online user interactions needed for the interface to be improved.\n- The presented technique is applicable to any form of user inputs and the authors test their approach with two different forms of inputs (eye gaze and handwritten characters).\n- Use of backspaces to train an interface with RL is a novel idea.\n\n########################\n\nCons:\n- The implicit feedback is assumed to be perfect and available via a keyboard. How realistic is this assumption? What happens if the feedback is noisy? Is the availability of an independent backspace action being true to the nature of the interface being truly assistive?\n- It was unclear to me while reading the paper if the implicit feedback (backspaces) are provided via a keyboard or are also predicted with the user\u2019s input (such as with gaze tracking). I assumed its the former with subtle hints in the paper and Fig 1. It would be helpful to make this explicit for the reader in Section 2.1 (as part of assumption 1).\n- This approach does not model the temporal effects of learning. Practically, modeling the problem as an MDP would be more realistic versus contextual bandits.\n\n########################\n\nReason for score: \nThis simple approach is presented with clarity and supported with well-reported experiments (including several ablative analyses). Some minor issues in the writing could be improved but overall the idea is well-presented and well-evaluated.\n\n########################\n\nQuestions during rebuttal:\n- Sec 4.1: Is the user study conducted with the interface type variable being varied within-subjects (i.e. each user uses both default and X2T)? From the experimental results, it seems this is the case but it is not explicitly stated. If this is so, is the order in which users attempt to use the two interfaces (default and X2T) counterbalanced?\n- Please address other questions raised as part of the Cons section and other feedback.\n\n########################\n\nSome typos and other feedback:\n- Section 2,  paragraph 1, sentence 2: \u201c\u2026 relies on an assistive typing interface to infer the user\u2019s intended action from available inputs \u2026\u201d -> \u201c\u2026 relies on an assistive typing interface, where the user\u2019s intended action is inferred from available inputs \u2026\u201d \n- Algorithm 1: Consider defining what p_user(x) is. Are you making any assumptions on such a model of human user inputs (such as random sampling as suggested in Sec 4.3)? \n- Consider citing a recent work on learning from imperfect implicit user feedback such as facial expressions):\u2028Cui, Y., Zhang, Q., Allievi, A., Stone, P., Niekum, S., & Knox, W. B. (2020). The EMPATHIC Framework for Task Learning from Implicit Human Feedback. Conference on Robot Learning (CoRL), 2020.\n- Even though compared to prior work, this work does not assume access to ground truth action labels from the user provided to the interface, it does assume access to ground truth backspace actions. It would be beneficial to emphasize this in paragraph 1 of Section 3.\n- It only becomes clear to me by Section 3 what is meant by \u201chandwriting as an input\u201d and how it can be an assistive input modality. A reference to Appendix Figure 5 early on the introduction, along with highlighting this can be an easier mode of user input versus typing on a keyboard, would be helpful. \n- Section 4, paragraph 1: References to the subsection numbers can be made when stating the evaluation questions. For example: Q1 (Sec 4.1): Does X2T improve with use and learn to outperform a non-adaptive interface? The questions are well-framed and very clear though!\n- Section 42: One of the insights presented for the presented method is that the XT2 interface can learn to automatically overcome calibration issues with the gaze tracker, thus the interface adapts to the mis-calibrations over time without the need for recalibration, even though external conditions would require a recalibration for better eye gaze prediction. This should be highlighted in the introduction as well.\n- Section 4.3: Isn\u2019t p_LM(u) conditioned on the preceding characters of the text seen so far? Would p_LM(u|t) be a better representation?\n- Section 4.4, last paragraph: Consider reversing the order of the first two results presented (they are in opposite order to the questions posed in the previous paragraph).\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LiX3ECzDPHZ", "replyto": "LiX3ECzDPHZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120163, "tmdate": 1606915768812, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1373/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Review"}}}, {"id": "9M1lLnrOHJr", "original": null, "number": 20, "cdate": 1606256808381, "ddate": null, "tcdate": 1606256808381, "tmdate": 1606256808381, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "hMhgyyRpk1v", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Response to follow up", "comment": "Thank you for your comments and suggestions. We have moderated the claims made in the 2nd paragraph of Section 2 to clarify that more complex domains may require more sophisticated exploration strategies, such as the gap-dependent exploration method mentioned by R2. We have also updated the future work in Section 5 to include testing X2T with complementary methods for training personalized language models, such as the Gmail Smart Compose algorithm mentioned by R2."}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "hMhgyyRpk1v", "original": null, "number": 18, "cdate": 1606252098628, "ddate": null, "tcdate": 1606252098628, "tmdate": 1606252098628, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "4cqMbIFpFC3", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Follow Up", "comment": "Thanks to the authors for their response; I'm satisfied with the answers and the updates to the manuscript.\n\nI've read the threads with the other reviewers, and I'll keep my score as-is at a 7. With regard to R2's concerns in general, I'd recommend the authors moderate the claims in the paper in order to make clear that their solution is particular to the studied domains only and that other approaches in the literature are probably more appropriate for more complex domains."}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "NdiYT9nkw5Q", "original": null, "number": 16, "cdate": 1606102200123, "ddate": null, "tcdate": 1606102200123, "tmdate": 1606102200123, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "4wIZsnnrjtX", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Follow up", "comment": "Thank you for your response. \n\nWe appreciate that Smart Compose is an example of a system that predicts the next word given previous words, but **one of the reasons it scales to action spaces containing thousands of words is that it is solving a different problem and makes different assumptions than the assistive typing domain we study**: Smart Compose (1) trains a model that operates on discrete token sequence inputs, rather than noisy inputs like eye gaze or handwriting; and (2) trains a model with supervised learning on ground-truth action labels, rather than bandit feedback. Precise user inputs and action labels enable Smart Compose to scale, but they are not available in typical assistive typing domains in which users can only provide noisy inputs and action feedback. In these settings, choosing an action space with thousands of words is infeasible, which is why state-of-the-art systems, e.g., for typing via brain-computer interfaces [1], often use character-level actions. In such prior work [1], it is extremely difficult for the user to encode their desired word (out of thousands of possible words) in their input, but much easier for them to encode their desired character (out of 31 possible characters). **The size of the action space is limited not just by the ability of our learning algorithm, but also by the ability of the user to provide precise inputs.**\n\nWe would like to highlight that papers that focus on applications are within the scope of ICLR, and that while our proposed algorithm is not necessarily novel, our experiments show novel results (see \u201cClarifications\u201d above).\n\nWe agree that backspace feedback is explicit, not implicit, and have updated the paper to not use \u201cimplicit\u201d when describing user feedback. \n\nWe agree that positive feedback indicates that the interface took the correct action, but the key challenge in the bandit setting involves learning from trial and error, which includes learning from negative feedback. During learning, our method does not have access to ground-truth action labels, which means it must explore different actions and learn from both negative and positive feedback. Hence, action feedback is a weaker learning signal than action labels.\n\n[1] https://www.biorxiv.org/content/10.1101/2020.07.01.183384v1\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "4wIZsnnrjtX", "original": null, "number": 15, "cdate": 1606097758671, "ddate": null, "tcdate": 1606097758671, "tmdate": 1606097758671, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "FH5o112jN30", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Response to author comments", "comment": "Thank you for your comments. \n\nWith regards to the smart compose paper, I pointed to an example system that can scale up to the next word prediction. How does it connect with this paper? Well, consider similar settings as ones explored by this paper, where, we replace character prediction with next word prediction. While the authors suggest that this doesn't fit in within current state of the art of assistive typing systems -- my point is this can be furthered by relying on potentially more advanced approaches that have appeared in the literature on contextual bandits and related areas. Otherwise, as it stands, clearly, as the authors point out, the techniques used in the paper aren't novel, their application to the specific problem potentially is - and I will leave it to the AC to judge the significance of this contribution. My broader point also is that these techniques do not appear to be promising for problems going beyond a stylized instantiation of the assistive typing problem where we predict the next character (i.e. when very deal with very low cardinality action space) conditioned on context.\n\nWith regards to the binary feedback: I'd like to point out that the feedback mechanism is clearly supervised - backspaces implies negatives, and non-backspaces implies correct completions. This isn't implicit feedback by any means. Yes, we do observe feedback only for the actions picked by the interface, but the feedback essentially reveals the right/wrong answer.\n\nI will retain my current score for the submission and thank the authors for their perspective."}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "FH5o112jN30", "original": null, "number": 14, "cdate": 1606095287572, "ddate": null, "tcdate": 1606095287572, "tmdate": 1606095977911, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "nzkzE89JcqH", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Clarifications", "comment": "Thank you for your response. There seem to be two concerns in your post: (1) that practical interfaces have to operate at the word level, and (2) that our proposed algorithm is not novel. \n\nChoice of action space\n-------------------------------\nWhile the Gmail Smart Compose method does indeed operate at the word level, it is solving a different problem than assistive typing: Smart Compose seeks to predict the user\u2019s next words given their previous words, while in the assistive typing domain the aim is to translate the user\u2019s input (e.g., eye gaze from a webcam, or handwriting from a stylus) into their desired word or character. Smart Compose requires access to ground-truth action labels (similar to prior work discussed in the 1st paragraph of Section 3 of our paper) in the form of a large dataset of user-composed emails, whereas our method learns online from binary feedback signals that are easier for motor-impaired users to provide than ground-truth actions. Furthermore, user input in assistive typing tends to be noisy (e.g., due to variations in webcam images), which makes predicting word-level actions more difficult than in the Smart Compose setting. Hence, while defining the action space to be a large set of words is appropriate for Smart Compose, it is not necessarily the best choice of action space for assistive typing systems. For example, the state of the art in assistive typing via brain-computer interfaces predicts individual characters, not words [1]. \n\nNovelty\n---------\nWhile our proposed method is based on prior work on contextual bandits, our main contribution is demonstrating its successful application to assistive typing. Furthermore, we contribute new experimental insights in the assistive typing domain on the importance of offline pretraining of the reward model, using a prior policy to guide exploration, and using online learning to overcome calibration problems (e.g., with the gaze tracker) that would otherwise cause performance to degrade over time. In addition to validating these components of our method, our experiments illustrate surprising and useful results, such as user co-adaptation to the system.\n\n[1] https://www.biorxiv.org/content/10.1101/2020.07.01.183384v1\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "nzkzE89JcqH", "original": null, "number": 13, "cdate": 1606090123663, "ddate": null, "tcdate": 1606090123663, "tmdate": 1606090232056, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "b53QNbOtRSw", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Comments on the author response", "comment": "Thank you once again for the detailed responses.\n\nMy point about large action spaces is not just towards scaling up the interface to the level of characters of English language. The real challenge is in scaling up the interface to the word level; this really does require using both a language model and (potentially) a non-trivial exploration strategy. And, in reality, we do have well-performing systems in practice, for e.g.,  look at the Gmail Smart Compose: Real-Time Assisted Writing paper by Chen et al.  (https://arxiv.org/pdf/1906.00080.pdf). This paper deals with a system that presents non-trivial challenges.  Considering noisy feedback at the scale of the system in the Gmail smart compose paper would be highly non-trivial; even with realizable rewards, when working with lesser data/badly performing model, one might require to use a sophisticated exploration strategy, for e.g. see a recent paper of Xu et al. (https://arxiv.org/pdf/2003.12699.pdf) to do gap dependent exploration.\n\nWhile I do understand the general premise of this problem setting/motivation is interesting and useful, the scale of problems considered by this paper in my view makes the solution very simple - in fact, I am not sure which of these techniques are novel excepting, perhaps their application to this problem. I am happy to understand the perspective of the authors if they believe I have missed out on anything with regards to evaluating the novelty of the algorithms that their paper presents."}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "i2OwEUk4Xyj", "original": null, "number": 11, "cdate": 1606088604303, "ddate": null, "tcdate": 1606088604303, "tmdate": 1606088604303, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "UaliB7vuNvC", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Please Respond to Author Rebuttal", "comment": "**The discussion period ends in two days**, and we would like to have enough time to run any additional experiments that you might suggest. We would like to know if our rebuttal (see below, \"Response to R2\u201d) adequately addressed the concerns in your original review. We would also appreciate any additional feedback on the revised paper. Are there any other aspects of the paper that you think could be improved?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "gyXX0OCeJ1B", "original": null, "number": 9, "cdate": 1605897289747, "ddate": null, "tcdate": 1605897289747, "tmdate": 1605897289747, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "lCTKH_sllDO", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Updates in response to R3's suggestions", "comment": "Thank you again for the constructive feedback.\n\n - Yes, we used the eye gaze user study data in Figure 5. We have updated the caption to clarify this.\n - The noise is added to the rewards before training the reward model. We have added this information to the caption.\n - The action prediction accuracy on the y-axis of Figure 5 is averaged over all 250 timesteps of each user experiment, similar to the \"cumulative accuracy\" in Figure 2b.\n - We have adjusted the language in the paper to clarify that the backspace feedback is not \u201cimplicit\u201d, and added this limitation to the 2nd paragraph of the discussion in Section 5.\n - Thank you for the positive feedback about our experiments!\n - We have updated the paper to address all other suggested edits, including changing the acronym to X2T.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "lCTKH_sllDO", "original": null, "number": 8, "cdate": 1605863642897, "ddate": null, "tcdate": 1605863642897, "tmdate": 1605863853158, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "ScirrH2dF3H", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Follow up by R3", "comment": "Thanks for your response and updated experiment to show how noise in the reward signal impact's XT2's performance. I have some follow-up questions and comments:\n- Can the authors clarify the domain on which Fig 5 in the appendix was evaluated? I am assuming this is the gaze-based interface, based on the reference to Section 4.2 in the caption of Figure 5. However, it would be helpful for the user if this was made explicit.\n- Fig 5: Is the noise in the reward signal added prior to training the reward prediction model or in the output of the predicted rewards?\n- Fig 5: How many user interactions were used for Fig 5? Was this kept fixed for this experiment? If so, how was that value chosen?\n- Similar to concerns raised by other reviewers, the assumption that providing implicit feedback via a keyboard is not exactly how an assistive interface would work. This feedback should not be called \"implicit\" and the discussion/introduction section must clearly address how implicit feedback can increase the complexity of the problem being attempted in this work. The authors should not run the risk of overselling the paper with more than what it can offer and demonstrate.\n- That said, I do like the thoroughness of the experiments, evaluations with 2 human subject studies, and clarity of the paper. \n- I agree with R1 that X2T would be a better acronym for the title.\n- Please also address any typing errors and suggested recommendations by all reviewers for the main text of the paper. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "H7tWHf3Tgmq", "original": null, "number": 7, "cdate": 1605831921966, "ddate": null, "tcdate": 1605831921966, "tmdate": 1605831921966, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "UaliB7vuNvC", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Request for Feedback", "comment": "Thank you again for the thoughtful review. We would like to know if our rebuttal (see below, \"Response to R2\u201d) adequately addressed your concerns. We would also appreciate any additional feedback on the revised paper. Are there any other aspects of the paper that you think could be improved?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "4cqMbIFpFC3", "original": null, "number": 6, "cdate": 1605389559226, "ddate": null, "tcdate": 1605389559226, "tmdate": 1605389559226, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "h9t_ZCvCd0Z", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Response to R1", "comment": "Thank you for the thoughtful and constructive feedback. \n\n(Q1a) R1 is correct that we compute accuracy for each individual timestep, so that we can measure how accuracy changes over the course of the experiment. It is unlikely that the task became easier for XT2 over time, since the default interface performed worse over time, which suggests that the task may have actually been getting more difficult due to distributional shift in the user\u2019s inputs.\n\n(Q1b) R1 and R3 both raise this important concern about the assumption that backspaces can be generated independently of XT2. This assumption restricts XT2 to settings where the user can communicate a binary signal through a single-button press or a sip-and-puff interface, or in settings where a separate binary classifier has already been trained to generate feedback signals from facial expressions or brain activity. We will add this discussion to Section 5.\n\n(Q2) R1 is correct that, in practice, users may choose not to backspace even when the previous action was incorrect, leading to noisy rewards. To address this concern raised by R1 and the other reviewers, we have added a new experiment (Figure 5 in the appendix) that shows XT2 outperforms the default interface even when the reward signal is noisy, although XT2\u2019s accuracy does decrease as the reward misabeling rate increases.\n\n(Q3) In this setting, training a single model on all users\u2019 data would probably perform worse than training a separate model on each individual user\u2019s data, since each user\u2019s handwriting style is different. Assuming the users\u2019 input styles are similar, we could potentially use meta-learning to leverage data from previous users to accelerate learning from a new user.\n\n(W2) We apologize for the miscommunication. Our reward model does not take the default interface\u2019s 2D gaze position estimates as input, but rather the original webcam image as input. Hence, XT2 can outperform the default interface, even when the default interface\u2019s 2D gaze position estimates are inaccurate (the green \u2018x\u2019s in the scatterplot). The red circles indicate that while XT2 tends to outperform the default interface, there are still a few instances where XT2 predicts the incorrect action when the default interface would have predicted the correct action. We will clarify this in the caption.\n\n(MC1) XT2 was intended abbreviate x-to-text in the same way that TD3 abbreviates Twin Delayed Deep Deterministic policy gradient [1], but we actually like R1\u2019s suggestion to use X2T instead, and will consider changing the acronym in the final paper.\n\n[1] https://arxiv.org/abs/1802.09477"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "ScirrH2dF3H", "original": null, "number": 5, "cdate": 1605389522751, "ddate": null, "tcdate": 1605389522751, "tmdate": 1605389522751, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "s5Qf2ALrECl", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Response to R3", "comment": "Thank you for the thoughtful and constructive feedback. \n\nAddressing cons:\n - We have added a new experiment (Figure 5 in the appendix) that shows XT2 outperforms the default interface even when the reward signal is noisy, although XT2\u2019s accuracy does decrease as the reward misabeling rate increases. R3 raises an important concern about the assumption that backspaces can be generated independently of XT2. This assumption restricts XT2 to settings where the user can communicate a binary signal through a button press or a sip-and-puff interface, or in settings where a separate binary classifier has already been trained to generate feedback signals from facial expressions or brain activity. We will add this discussion to Section 5.\n - We have updated assumption (1) in Section 2.1 as per R3\u2019s suggestion.\n - We agree with R3 that the contextual bandit assumption is limiting, and that formulating assistance as an MDP is more general. In this project, we chose to trade off generality for sample efficiency, since learning a single-step reward model requires less data than learning a sequential policy. We will clarify this decision in Section 5.\n\nAddressing questions:\n - We used a within-subjects allocation and counterbalanced the order of the two conditions. Appendix A.1.1 includes additional details about the experiment design. We will move these details into the main paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "b53QNbOtRSw", "original": null, "number": 4, "cdate": 1605389477264, "ddate": null, "tcdate": 1605389477264, "tmdate": 1605389477264, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "UaliB7vuNvC", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment", "content": {"title": "Response to R2", "comment": "Thank you for the thoughtful and constructive feedback. Based on the review, the main reservations about our work appear to be (1) that XT2 may be sensitive to noisy reward signals, and (2) that XT2 may not be able to effectively explore a large action space. We address (1) by adding a new experiment with the eye gaze data (Figure 5 in the appendix), and address (2) by clarifying our handwriting experiments (Sections 4.3 and 4.4). In light of these revisions and clarifications, we believe that most of these issues should be addressed. Please let us know if any further concerns remain, or if you would like to see any further modifications.\n\nSensitivity to noisy rewards\n--------------------------------------\nTo address concerns from R2 and the other reviewers about XT2\u2019s sensitivity to noisy rewards, we have added a new experiment (Figure 5 in the appendix). We ran an experiment with the eye gaze data in which we flipped each reward r to 1-r with probability p, where p is the \u201creward mislabeling rate\u201d. We found that XT2 outperformed the default interface for a wide range of mislabeling rates p, and only performed worse than the default interface when the rewards were completely random (i.e., the mislabeling rate is p=0.5). In practice, users' backspaces tend to follow the assumptions in Section 2.1 -- i.e., users tend to backspace incorrect actions, and not backspace correct actions -- leading to the low empirical mislabeling rate of p=0.014 (as mentioned at the end of Section 4.1).\n\nExploring large action spaces\n-----------------------------------------\nR2 is correct that exploring a large action space with bandit feedback alone is challenging. To study this problem, we conducted the handwriting experiments in Sections 4.3 and 4.4 with a 26-dimensional action space of characters. To guide exploration and improve initial performance, we use a language model, as described in the 2nd paragraph of Section 4.3. The ablation studies in Section 4.4 show that incorporating the language model into our prior policy, as well as training our reward model on online user feedback, both substantially improve XT2\u2019s accuracy. This result (see Figure 4 for details) suggests that XT2 can be applied to practical problems with large action spaces. We have added implementation details for the language model to Appendix A.1.2 in the updated paper.\n\nImplicit vs. explicit feedback\n---------------------------------------\nWe will adjust the language in the paper to clarify that backspaces are explicit feedback, rather than implicit feedback. R2 is correct that XT2 trains a reward regressor, and that bandit feedback is only given for actions taken by the interface."}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "LiX3ECzDPHZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1373/Authors|ICLR.cc/2021/Conference/Paper1373/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860446, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Comment"}}}, {"id": "h9t_ZCvCd0Z", "original": null, "number": 1, "cdate": 1603770123726, "ddate": null, "tcdate": 1603770123726, "tmdate": 1605024461505, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "LiX3ECzDPHZ", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Review", "content": {"title": "An attractive and practical algorithm for improving user interfaces", "review": "SUMMARY:\nThe authors propose a simple algorithm for using online learning from implicit human feedback to improve systems that operate in the contextual bandit setting. The main idea is to capture the presence/absence of corrective actions and use this information to infer a reward signal that the system can use to make decisions later. The proposed method is instantiated for text-entry tasks as a system called XT2, and the authors perform extensive empirical evaluation that seems to show that the system is very successful.\n\n\nSTRENGTHS:\n\t(S1) The authors present a simple-yet-seemingly-powerful algorithm for building adaptive interfaces from implicit user feedback.\n\t(S2) The proposed algorithm can easily leverage existing interfaces and therefore provides an interesting path to improving all kinds of existing systems.\n\t(S2) The authors instantiate the algorithm in a compelling use-case scenario -- non-traditional text entry -- and demonstrate seemingly compelling results empirically.\n\n\nWEAKNESSES:\n\t(W1) It's unclear as to whether or not the evaluation in the major plots (\"[Number of User Interactions] vs. [Action Prediction Accuracy]\") is meaningful. In particular, it's not clear what data was used to compute the reported prediction accuracy. It would seem that this is computed over the prediction *for that time step*, but then more detail is needed on the datasets in order to determine whether or not the task somehow got easier for XT2 over time instead of it actually learning.\n\t(W2) Some of the experimental results are confusing. For example, what is the explanation for the position of the red circles in Figure 2c? There seems to be very little intuitive reasoning behind where red vs. green appears -- has XT2 really learned such a complicated and sensitive decision surface? If yes, why should we \"trust\" this decision surface? In analyzing 2c closely, it seems like a nearest-neighbor classifier would have done just as well.\n\t(W3) While I greatly appreciate the authors making clear their assumptions about backspaces in Section 2.1, the authors did not justify why their first assumption (\"the user can perform a backspace action independently of our interface\") is valid. It would seem to me that, in situations where XT2 is necessary, that such an assumption may not actually be valid.\n\t(W4) In Section 2.2, the authors state that they \"assign a positive reward to actions that were not backspaced,\" but this choice is also not justified. It seems to me that there could be cases in which the user simply accepts an incorrect action from the system without providing the feedback.\n\n\nRECOMMENDATION STATEMENT:\nWhile I have listed a number of weaknesses above, overall I really like the paper. The algorithm is simple and can build upon existing interfaces as opposed to simply replacing them. Moreover, the algorithm is built to leverage data that is easy to collect, and therefore it seems feasible that similar systems could be deployed for a number of applications. That said, the current paper is lacking in the description of the experimental results and justifications for some of the assumptions.\n\n\nQUESTIONS FOR AUTHORS:\n\t(Q1) How was accuracy computed for the \"[Number of User Interactions] vs. [Action Prediction Accuracy]\" figures?  How did the authors ensure that the task didn't somehow get easier regardless of system learning?\n\t(Q1) What justification do the authors have for their first assumption about backspaces (i.e., that they can be performed independently -- and, presumably, reliably -- of the XT2 interface).\n\t(Q2) What justification do the authors have for their implicit assumption that no backspace means positive reward?\n\t(Q3) While Table 1 is convincing that XT2 adapts to individual users, it also brings up the question as to how a model trained on _all_ user data would do. Do the authors have the ability to comment on this?\n\n\nMINOR COMMENTS:\n\t(MC1) I find it odd that the system is named \"x-to-text\" but abbreviated \"XT2.\" Should it not be \"X2T\" since one would replace the \"to\" with \"2\" (\"x 2 text\" -> \"X2T\")? I'm not taking any issue with the current name if this is how the authors meant it, rather just wondering if this is a typo.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LiX3ECzDPHZ", "replyto": "LiX3ECzDPHZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120163, "tmdate": 1606915768812, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1373/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Review"}}}, {"id": "UaliB7vuNvC", "original": null, "number": 3, "cdate": 1603982643372, "ddate": null, "tcdate": 1603982643372, "tmdate": 1605024461373, "tddate": null, "forum": "LiX3ECzDPHZ", "replyto": "LiX3ECzDPHZ", "invitation": "ICLR.cc/2021/Conference/Paper1373/-/Official_Review", "content": {"title": "Interesting problem motivation.", "review": "This paper presents Machine Learning based approaches that map inputs to desired actions with applications to assistive interfaces. The paper leverages user feedback to improve the predictive model to perform desired actions. The paper conducts user studies that gauge the impact of the proposed interface in contrast to a non-adaptive baseline interface. The problem is well motivated and the paper is well written.\n\nSome questions/comments:\n\n[1] What the paper refers to as \u201cimplicit\u201d feedback, i.e., in the context of the paper, the backspace command is actually explicit. It is clear that the assumptions enforced by the paper indicate that a backspace means a strongly negative signal, and anything that has not been backspaced to be the right action. This can, for e.g., be used for training an appropriate reward regressor. This feedback however is incomplete, because, we tend to observe the feedback only for the actions presented by the interface.\n\n[2] This paper attempts to solve a problem that is highly non-trivial when the cardinality of actions grows large, for e.g., the English language vocabulary. It is unclear how one can utilize a standard yes/no feedback using a backspace key to effectively learn a highly accurate policy that presents us with the next action (for example, a word) given the context. The paper however doesn\u2019t present a discussion surrounding these challenges, which for me is a major shortcoming of this paper.\n\n[3] Naturally, some of the assumptions considered with regards to the use of backspaces is pretty strong. The feedback model assumed by the paper is essentially noiseless in that the user always does the right thing with regards to presenting feedback on good/bad actions. The paper doesn\u2019t validate how errors in the use of the backspace key manifest themselves in the performance of the policy learning step.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1373/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1373/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback", "authorids": ["jenseng@berkeley.edu", "~Siddharth_Reddy1", "~Glen_Berseth1", "~Anca_Dragan1", "~Sergey_Levine1"], "authors": ["Jensen Gao", "Siddharth Reddy", "Glen Berseth", "Anca Dragan", "Sergey Levine"], "keywords": ["reinforcement learning", "human-computer interaction"], "abstract": "We aim to help users communicate their intent to machines using flexible, adaptive interfaces that translate arbitrary user input into desired actions. In this work, we focus on assistive typing applications in which a user cannot operate a keyboard, but can instead supply other inputs, such as webcam images that capture eye gaze. Standard methods train a model on a fixed dataset of user inputs, then deploy a static interface that does not learn from its mistakes; in part, because extracting an error signal from user behavior can be challenging. We investigate a simple idea that would enable such interfaces to improve over time, with minimal additional effort from the user: online learning from user feedback on the accuracy of the interface's actions. In the typing domain, we leverage backspaces as feedback that the interface did not perform the desired action. We propose an algorithm called x-to-text (X2T) that trains a predictive model of this feedback signal, and uses this model to fine-tune any existing, default interface for translating user input into actions that select words or characters. We evaluate X2T through a small-scale online user study with 12 participants who type sentences by gazing at their desired words, and a large-scale observational study on handwriting samples from 60 users. The results show that X2T learns to outperform a non-adaptive default interface, stimulates user co-adaptation to the interface, personalizes the interface to individual users, and can leverage offline data collected from the default interface to improve its initial performance and accelerate online learning.", "one-sentence_summary": "We use online learning from user feedback to train an adaptive interface for typing words using inputs like eye gaze.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gao|x2t_training_an_xtotext_typing_interface_with_online_learning_from_user_feedback", "pdf": "/pdf/24a7932cce8e9dcdd1c659e377a108df6332b6a2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngao2021xt,\ntitle={X2T: Training an X-to-Text Typing Interface with Online Learning from User Feedback},\nauthor={Jensen Gao and Siddharth Reddy and Glen Berseth and Anca Dragan and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=LiX3ECzDPHZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "LiX3ECzDPHZ", "replyto": "LiX3ECzDPHZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1373/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538120163, "tmdate": 1606915768812, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1373/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1373/-/Official_Review"}}}], "count": 19}