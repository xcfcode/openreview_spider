{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487859674084, "tcdate": 1478286605035, "number": 329, "id": "r1rz6U5lg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1rz6U5lg", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396519901, "tcdate": 1486396519901, "number": 1, "id": "BJxd3G8Ol", "invitation": "ICLR.cc/2017/conference/-/paper329/acceptance", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper applies REINFORCE to learn MCMC proposals within the existing STOKE scheme for super-optimization. It's a neat paper, with interesting results.\n \n It's not clear whether interesting representations are learned, and the algorithms are not really new. However, it's a neat piece or work, that some ICLR reviewers found interesting, and could inspire more representation learning work in this area.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396521851, "id": "ICLR.cc/2017/conference/-/paper329/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396521851}}}, {"tddate": null, "tmdate": 1484658825289, "tcdate": 1484658825289, "number": 9, "id": "ryW5dcjIx", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "writers": ["~Rudy_R_Bunel1"], "content": {"title": "Uploaded new version", "comment": "We thank the reviewers for their helpful feedback. We uploaded a new version of\nthe paper. The elements that have changed are:\n\nClarity of experiments:\n- The complexity of each model in term of number of parameters is given in Table 1 and the hyperparameters / architecture were added as Appendix A.\n- Addition of a better representation of the consequences of a learned bias (Figure 3). We compare Stoke (uniform distribution)  running for (200,400) iterations with the learned optimizer running for (100, 200) iterations (at training time, the horizon is set at 200 iterations). Even with four times less iterations, a better distribution of the score is achieved.\n- The numbers for the impact on throughput of the learned proposal distribution have been added (Table 3). This measures not just the time for the proposal (which we gave as answer to the question of AnonReviewer2) but the whole time of an MCMC iterations which represent more accurately the throughput of the algorithm.\n\nExplanations:\n- We added the citation that AnonReviewer3 suggested (Section 2 - Related Works).\n- We added our argument that superoptimization provides a good benchmark to estimate representation of programs to the introduction (Section 1 - Introduction, 1st paragraph).\n- Clarified throughout the paper that the \"Uniform\" model is Stoke (Section 3.2.2-Parametrization and Section 4.1-Models, Table 1).\n- Fixed some typos"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}, {"tddate": null, "tmdate": 1482242637990, "tcdate": 1482242637990, "number": 8, "id": "BkIL5hINe", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "Bke9X3UEl", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "writers": ["~Rudy_R_Bunel1"], "content": {"title": "Infinite data question", "comment": "While we would technically have infinite data using synthesized programs, these don't correspond to \"proper\" training data as they lack the internal coherence that actual programs exhibit. Indeed, as they are obtained from a completely random walk, they exhibit almost no structure and would therefore be of poor quality to learn a structured representation.\n\nThis experiment on synthetic data was intended to show that the proposed method was not entirely limited to a restricted class of bitwise manipulations of a few registers. We will make this goal more clear in the paper's revision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}, {"tddate": null, "tmdate": 1482240904239, "tcdate": 1482240904239, "number": 3, "id": "Bke9X3UEl", "invitation": "ICLR.cc/2017/conference/-/paper329/official/comment", "forum": "r1rz6U5lg", "replyto": "H1xUG2LNg", "signatures": ["ICLR.cc/2017/conference/paper329/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper329/AnonReviewer2"], "content": {"title": "Comment", "comment": "\"However, this option requires the availability of a large amount of data\"\n\nYour data is being generated on-the-fly, right? You technically have infinite data?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618345, "id": "ICLR.cc/2017/conference/-/paper329/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper329/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper329/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618345}}}, {"tddate": null, "tmdate": 1482240694879, "tcdate": 1482240694879, "number": 7, "id": "HkyTz3IVg", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "Sk_Z7V7Nx", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "writers": ["~Rudy_R_Bunel1"], "content": {"title": "Reply", "comment": "We thank the reviewer for the comments. The main critique is the relevance of the work in the context of representation learning, which we clarify below.\n\nWhile superoptimization may not directly be one of the main interests of the ICLR audience, it does provide a natural benchmark for evaluating representations of programs. Representations need to be evaluated on tasks and Superoptimization as a task requires the representation to decouple  the semantics of the program (its specification / desired behaviour) from its superfluous properties (the exact implementation). \n\nWe hope that our work will spur interest in the ML community on the problem of learning programs representation.  While we used the BOW descriptor as our base representation, we intend to investigate more generic tree and sequence architectures in the future. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}, {"tddate": null, "tmdate": 1482240583632, "tcdate": 1482240583632, "number": 6, "id": "H1xUG2LNg", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "H1MN4b7El", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "writers": ["~Rudy_R_Bunel1"], "content": {"title": "Answers", "comment": "Thank you for your comments. We provide our responses below.\n\nComment: Only the features->proposal part is learned?\n\nResponse: Indeed, only the mapping from the features to the proposal distribution is learned. The \u2018Uniform\u2019 model is indeed Stoke as it was described in the work of Schkufza et al. We will make this more clear in the paper.\n\nComment: Learning the features?\n\nResponse: Our multi-layered network learns a representation on top of the Bag Of Words (BOW) descriptor. As another reviewer suggests, we did consider directly using the program tree as input to a tree structured neural network. However, this option requires the availability of a large amount of data to learn a generic feature representation, which is unfortunately not available in our current setting. While we used the BOW descriptor as our base representation, we intend to investigate more generic tree and sequence architectures in the future.\n\nComment: Related work by Salimans et al.\n\nResponse: Thank you for the reference. It is indeed relevant and presents another interpretation of the \u201ctraces\u201d of MCMC algorithms. The key difference of our work lies in the objective: approximating the posterior vs. finding the mode more efficiently, for which we optimize directly by specifying it as our reward function. We will add a discussion in the related work section. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}, {"tddate": null, "tmdate": 1482011656434, "tcdate": 1482011392244, "number": 3, "id": "Sk_Z7V7Nx", "invitation": "ICLR.cc/2017/conference/-/paper329/official/review", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["ICLR.cc/2017/conference/paper329/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper329/AnonReviewer2"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This work builds on top of STOKE (Schkufza et al., 2013), which is a superoptimization engine for program binaries. It works by starting with an existing program, and proposing modifications to it according to a proposal distribution. Proposals are accepted according to the Metropolis-Hastings criteria. The acceptance criteria takes into account the correctness of the program, and performance of the new program. Thus, the MCMC process is likely to converge to correct programs with high performance. Typically, the proposal distribution is fixed. The contribution of this work is to learn the proposal distribution as a function of the features of the program (bag of words of all the opcodes in the program). The experiments compare with the baselines of uniform proposal distribution, and a baseline where one just learns the weights of the proposal distribution but without conditioning on the features of the program. The evaluation shows that the proposed method has slightly better performance than the compared baselines.\n\nThe significance of this work at ICLR seems to be quite low., both because this is not a progress in learning representations, but a straightforward application of neural networks and REINFORCE to yet another task which has non-differentiable components. The task itself (superoptimization) is not of significant interest to ICLR readers/attendees. A conference like AAAI/UAI seem a better fit for this work.\n\nThe proposed method is seemingly novel. Typical MCMC-based synthesis methods are lacking due to their being no learning components in them. However, to make this work compelling, the authors should consider demonstrating the proposed method in other synthesis tasks, or even more generally, other tasks where MH-MCMC is used, and a learnt proposal distribution can be beneficial. Superoptimization alone (esp with small improvements over baselines) is not compelling enough.\n\nIt is also not clear if there is any significant representation learning is going on. Since a BoW feature is used to represent the programs, the neural network cannot possibly learn anything more than just correlations between presence of opcodes and good moves. Such a model cannot possibly understand the program semantics in any way. It would have been a more interesting contribution if the authors had used a model (such as Tree-LSTM) which attempts to learn the semantics the program. The quite naive method of learning makes this paper not a favorable candidate for acceptance.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512621900, "id": "ICLR.cc/2017/conference/-/paper329/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper329/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper329/AnonReviewer4", "ICLR.cc/2017/conference/paper329/AnonReviewer3", "ICLR.cc/2017/conference/paper329/AnonReviewer2"], "reply": {"forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512621900}}}, {"tddate": null, "tmdate": 1481999401594, "tcdate": 1481999401594, "number": 2, "id": "H1MN4b7El", "invitation": "ICLR.cc/2017/conference/-/paper329/official/review", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["ICLR.cc/2017/conference/paper329/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper329/AnonReviewer3"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This is an interesting and pleasant paper on superoptimization, that extends the  problem approached by the stochastic search STOKE to a learned stochastic search, where the STOKE proposals are the output of a neural network which takes some program embedding as an input. The authors then use REINFORCE to learn an MCMC scheme with the objective of minimizing the final program cost.\n\nThe writing is clear and results highlight the efficacy of the method.\n\ncomments / questions:\n- Am I correct in understanding that of the entire stochastic computation graph, only the features->proposal part is learned. The rest is still effectively the stoke MCMC scheme? Does that imply that the 'uniform' model is effectively Stoke and is your baseline (this should probably be made explicit )\n\n- Did the authors consider learning the features instead of using out of the box features (could be difficult given the relatively small amount of data - the feature extractor might not generalize).\n\n- In a different context, 'Markov Chain Monte Carlo and Variational Inference:Bridging the Gap' by Salimans et al. suggests considering a MCMC scheme as a stochastic computation graph and optimizing using a variational (i.e. RL) criterion. The problem is different, it uses HMC instead of MCMC, but it might be worth citing as a similar approach to 'meta-optimized' MCMC algorithms.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512621900, "id": "ICLR.cc/2017/conference/-/paper329/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper329/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper329/AnonReviewer4", "ICLR.cc/2017/conference/paper329/AnonReviewer3", "ICLR.cc/2017/conference/paper329/AnonReviewer2"], "reply": {"forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512621900}}}, {"tddate": null, "tmdate": 1481975822947, "tcdate": 1481975822947, "number": 5, "id": "ByvfdifEg", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "ByNBkuZXx", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "writers": ["~Rudy_R_Bunel1"], "content": {"title": "Speed of proposal", "comment": "Thank you for your questions.\n\nThe impact on runtime is negligible. The proposal distribution is\nsampled once at the beginning of the optimisation and remains the same throughout\none run. The only difference comes from the fact that Vanilla Stoke samples from\na uniform distribution while we sample from a categorical distribution.\n\nRunning a quick experiment on my desktop, using uniform sampling, we can reach a throughput of 200 000 proposal per second. Sampling from the learned distribution, we can reach 50 000 proposals per seconds. \nHowever, the proposal time is negligible in comparison to the evaluation of the cost function so the throughput is similar. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}, {"tddate": null, "tmdate": 1481952430680, "tcdate": 1481952430680, "number": 4, "id": "SJw33HfVe", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "HkWEVNk4g", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "writers": ["~Rudy_R_Bunel1"], "content": {"title": "Superoptimization is a good benchmark to evaluate representations", "comment": "We believe this paper has important implications for representation learning. Apart from the reviewer\u2019s remark on representing (generated) programs, we believe super-optimization provides a natural benchmark for evaluating representations of programs. This is particularly true in light of recent resurgence of interest  in machine learning on Program Induction/Synthesis and its connections to Hierarchical RL. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}, {"tddate": null, "tmdate": 1481952179061, "tcdate": 1481952179061, "number": 3, "id": "H1onjrfNg", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "SJV4pdjMg", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "writers": ["~Rudy_R_Bunel1"], "content": {"title": "Used Model", "comment": "Thank you for your questions.\n\nThe model that was used is the following (in our case the number of opcodes is 3874) :\nBag of Words over the instructions -> Linear Layer (nb of opcodes -> 100) + ReLU -> Linear Layer (100 -> 300) + ReLU -> Linear Layer (300 -> 300) -> Embedding\nEmbedding -> Linear Layer (300 -> nb of type of move) + Softmax -> Probability over the types of move\nEmbedding -> Linear Layer (300 -> nb of opcodes) + Softmax -> Probability over the opcodes\nI will add an appendix with those details to the paper.\n\nWe didn't perform any exhaustive fine tuning on the model's architecture. The only parameter that needed to be selected was the learning rate to ensure that\nthe training loss was going down.\n\nWe haven\u2019t explored the use of other models but any other differentiable architecture can be used in place of the MLP. A vanilla logistic regression baseline would have a lot of parameters ( nb of opcodes * (nb of opcodes + nb of type of move) ~= 15e6. In contrast, our current model is at 1.7e6 parameters). We will conduct experiments with a linear model and will add the results to the paper. This will show whether or not the depth of the model is actually helping in this context.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}, {"tddate": null, "tmdate": 1481931196492, "tcdate": 1481931196492, "number": 2, "id": "rkN6YxG4e", "invitation": "ICLR.cc/2017/conference/-/paper329/official/comment", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["ICLR.cc/2017/conference/paper329/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper329/AnonReviewer2"], "content": {"title": "Pinging authors to answer the posted questions.", "comment": "Answers to the questions posted by reviewers would help for a more high-quality review. Thanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618345, "id": "ICLR.cc/2017/conference/-/paper329/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper329/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper329/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618345}}}, {"tddate": null, "tmdate": 1481749544858, "tcdate": 1481749544851, "number": 1, "id": "HkWEVNk4g", "invitation": "ICLR.cc/2017/conference/-/paper329/official/review", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["ICLR.cc/2017/conference/paper329/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper329/AnonReviewer4"], "content": {"title": "Interesting ideas, not sure it belongs @ ICLR", "rating": "7: Good paper, accept", "review": "Two things I really liked about this paper:\n1. The whole idea of having a data-dependent proposal distribution for MCMC. I wasn't familiar with this, although it apparently was previously published. I went back: the (Zhu, 2000) paper was unreadable. The (Jampani, 2014) paper on informed sampling was good. So, perhaps this isn't a good reason for accepting to ICLR.\n\n2. The results are quite impressive. The rough rule-of-thumb is that optimization can help you speed up code by 10%. The standard MCMC results presented on the paper on randomly-generated programs roughly matches this (15%). The fact that the proposed algorithm get ~33% speedup is quite surprising, and worth publishing.\n\nThe argument against accepting this paper is that it doesn't match the goals of ICLR. I don't go to ICLR to hear about generic machine learning papers (we have NIPS and ICML for that). Instead, I go to learn about how to automatically represent data and models. Now, maybe this paper talks about how to represent (generated) programs, so it tangentially lives under the umbrella of ICLR. But it will compete against more relevant papers in the conference -- it may just be a poster. Sending this to a programming language conference may have more eventual impact.\n\nNonetheless, I give this paper an \"accept\", because I learned something valuable and the results are very good. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512621900, "id": "ICLR.cc/2017/conference/-/paper329/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper329/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper329/AnonReviewer4", "ICLR.cc/2017/conference/paper329/AnonReviewer3", "ICLR.cc/2017/conference/paper329/AnonReviewer2"], "reply": {"forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512621900}}}, {"tddate": null, "tmdate": 1480847163680, "tcdate": 1480847163675, "number": 2, "id": "ByNBkuZXx", "invitation": "ICLR.cc/2017/conference/-/paper329/pre-review/question", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["ICLR.cc/2017/conference/paper329/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper329/AnonReviewer2"], "content": {"title": "Computational considerations", "question": "Typically, in MCMC-based synthesis, it's important to be able to make a large number of proposals/evaluations. If you're using an MLP to propose changes to a program, does this not impact the number of evaluations per second metric? I would be interesting in knowing how does the proposed method compare with the baselines in terms of the wall clock time to completion. This dimension has consequences wrt the scalability of the approach to bigger synthesis/superoptimization tasks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959336571, "id": "ICLR.cc/2017/conference/-/paper329/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper329/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper329/AnonReviewer4", "ICLR.cc/2017/conference/paper329/AnonReviewer2"], "reply": {"forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959336571}}}, {"tddate": null, "tmdate": 1480457516178, "tcdate": 1480457516174, "number": 1, "id": "SJV4pdjMg", "invitation": "ICLR.cc/2017/conference/-/paper329/pre-review/question", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["ICLR.cc/2017/conference/paper329/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper329/AnonReviewer4"], "content": {"title": "Model selection? parameters?", "question": "The paper mentions that an MLP was used with three hidden layers, but does not specify any other hyperparameters. How many hidden units? Was the MLP tuned on a separate validation set, or on the final test results  shown in the paper?\n\nWere any other models tried? Seems like logistic regression or random forests could do very well on this task, also."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959336571, "id": "ICLR.cc/2017/conference/-/paper329/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper329/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper329/AnonReviewer4", "ICLR.cc/2017/conference/paper329/AnonReviewer2"], "reply": {"forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper329/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959336571}}}, {"tddate": null, "tmdate": 1478770359103, "tcdate": 1478770359098, "number": 2, "id": "HJkpC3-Zx", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "BymKJwRlg", "signatures": ["~Rudy_R_Bunel1"], "readers": ["everyone"], "writers": ["~Rudy_R_Bunel1"], "content": {"title": "Format Corrected", "comment": "Hello,\nThe format has been corrected. Apologies for the issue, a style for some of the figures got wrongly applied to everything."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}, {"tddate": null, "tmdate": 1478554313761, "tcdate": 1478549371241, "number": 1, "id": "BymKJwRlg", "invitation": "ICLR.cc/2017/conference/-/paper329/public/comment", "forum": "r1rz6U5lg", "replyto": "r1rz6U5lg", "signatures": ["~Tara_N_Sainath1"], "readers": ["everyone"], "writers": ["~Tara_N_Sainath1"], "content": {"title": "ICLR Paper Format", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning to superoptimize programs", "abstract": "  Code super-optimization is the task of transforming any given program to a more efficient version while preserving its input-output behaviour. In some sense, it is similar to the paraphrase problem from natural language processing where the intention is to change the syntax of an utterance without changing its semantics. Code-optimization has been the subject of years of research that has resulted in the development of rule-based transformation strategies that are used by compilers. More recently, however, a class of stochastic search based methods have been shown to outperform these strategies. This approach involves repeated sampling of modifications to the program from a proposal distribution, which are accepted or rejected based on whether they preserve correctness, and the improvement they achieve. These methods, however, neither learn from past behaviour nor do they try to leverage the semantics of the program under consideration. Motivated by this observation, we present a novel learning based approach for code super-optimization. Intuitively, our method works by learning the proposal distribution using unbiased estimators of the gradient of the expected improvement. Experiments on benchmarks comprising of automatically generated as well as existing (``Hacker's Delight'') programs show that the proposed method is able to significantly outperform state of the art approaches for code super-optimization.\n", "pdf": "/pdf/a999b868a17de58898d8cf1e0956a50b348593d8.pdf", "paperhash": "bunel|learning_to_superoptimize_programs", "keywords": [], "conflicts": ["microsoft.com", "ox.ac.uk", "ecp.fr", "utc.fr"], "authors": ["Rudy Bunel", "Alban Desmaison", "M. Pawan Kumar", "Philip H.S. Torr", "Pushmeet Kohli"], "authorids": ["rudy@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "pawan@robots.ox.ac.uk", "philip.torr@eng.ox.ac.uk", "pkohli@microsoft.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618470, "id": "ICLR.cc/2017/conference/-/paper329/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1rz6U5lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper329/reviewers", "ICLR.cc/2017/conference/paper329/areachairs"], "cdate": 1485287618470}}}], "count": 18}