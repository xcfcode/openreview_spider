{"notes": [{"id": "HJxMYANtPH", "original": "SkgISMuOPr", "number": 1238, "cdate": 1569439353532, "ddate": null, "tcdate": 1569439353532, "tmdate": 1583912021425, "tddate": null, "forum": "HJxMYANtPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "-TjmqiTGTz", "original": null, "number": 1, "cdate": 1576798718299, "ddate": null, "tcdate": 1576798718299, "tmdate": 1576800918254, "tddate": null, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper presents a new phenomenon referred to as the \"local elasticity of neural networks\". The main argument is that the SGD update for nonlinear network at a local input x does not change the predictions at a different input x' (see Fig. 2). This is then connected to similarity using nearest-neighbor and kernel methods. An algorithm is also presented.\n\nThe reviewers find the paper intriguing and believe that this could be interesting for the community. After the rebuttal period, one of the reviewers increased their score.\n\nI do agree with the view of the reviewers, although I found that the paper's presentation can be improved. For example, Fig. 1 is not clear at all, and the related work section basically talks about many existing works but does not discuss why they are related to this work and how this work add value to this existing works. I found Fig. 2 very clear and informative. I hope that the authors could further improve the presentation. This should help in improving the impact of the paper.\n\nWith the reviewers score, I recommend to accept this paper, and encourage the authors to improve the presentation of the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717606, "tmdate": 1576800267945, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Decision"}}}, {"id": "SylBdSQ6tH", "original": null, "number": 1, "cdate": 1571792237050, "ddate": null, "tcdate": 1571792237050, "tmdate": 1573687548148, "tddate": null, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper contributes to the understanding of neural networks and provides a new clustering technique:\n  1) The paper introduces the interesting notion of local elasticity which considers the relative variation in output values for two different inputs before and after an SGD-style update;\n  2) The derived similarity metric between input samples (obtained by as SGD unfolds) can be used for clustering and is amenable to a kernelized formulation;\n  3) Empirical measurements on visual classification tasks show that the new similarity metric can offer a better clustering performance than PCA + K-means.\n\nI found the paper very interesting but the writing appeared somewhat unclear at times. I believe that some rewriting is needed for the authors to argue that the newly introduced elasticity metric provides a significantly new understanding of neural networks. In particular, I did not find the argumentation around explaining generalization to be very convincing or clear.\n\nThe kernelized formulation of the elasticity metric seems compelling and I found that turning the insights developed by the theoretical section of the paper into an actionable algorithm for clustering was a nice contribution.\n\nUnfortunately, the empirical results did not really convince me that the resulting clustering algorithm really improves on the SOTA in clustering as only relatively weak baselines were considered.\n\nI believe that considering more solid baselines for the clustering experiments would help.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575416629538, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Reviewers"], "noninvitees": [], "tcdate": 1570237740293, "tmdate": 1575416629549, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Review"}}}, {"id": "BkxUgbM5oS", "original": null, "number": 6, "cdate": 1573687534193, "ddate": null, "tcdate": 1573687534193, "tmdate": 1573687534193, "tddate": null, "forum": "HJxMYANtPH", "replyto": "B1g_nrRWiS", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment", "content": {"title": "Changing rating to weak accept", "comment": "The authors have addressed some of my comments.\n\nI do believe that the submission is now stronger and change my rating to weak accept."}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxMYANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1238/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1238/Authors|ICLR.cc/2020/Conference/Paper1238/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159083, "tmdate": 1576860543068, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment"}}}, {"id": "rygKVEAKoB", "original": null, "number": 5, "cdate": 1573671984851, "ddate": null, "tcdate": 1573671984851, "tmdate": 1573671984851, "tddate": null, "forum": "HJxMYANtPH", "replyto": "ryxdtTj1qS", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment", "content": {"title": "Thanks for suggesting the investigation of local elasticity with mini-batch SGD", "comment": "In Appendix A.2 of the revision, we added a simulation study showing that the local elasticity phenomenon persists with mini-batch SGD. In short, a test image has a large change in its prediction after an SGD update if it is similar to at least one of the images in the mini-batch used for computing the gradient."}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxMYANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1238/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1238/Authors|ICLR.cc/2020/Conference/Paper1238/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159083, "tmdate": 1576860543068, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment"}}}, {"id": "r1xqNmAKsH", "original": null, "number": 4, "cdate": 1573671729746, "ddate": null, "tcdate": 1573671729746, "tmdate": 1573671729746, "tddate": null, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment", "content": {"title": "One more simulation study added", "comment": "The revision included an additional simulation showing the presence of local elasticity in the case of mini-batch SGD. Details are given in Appendix A.2."}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxMYANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1238/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1238/Authors|ICLR.cc/2020/Conference/Paper1238/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159083, "tmdate": 1576860543068, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment"}}}, {"id": "B1g_nrRWiS", "original": null, "number": 3, "cdate": 1573148080276, "ddate": null, "tcdate": 1573148080276, "tmdate": 1573148080276, "tddate": null, "forum": "HJxMYANtPH", "replyto": "SylBdSQ6tH", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment", "content": {"title": "Official Comment", "comment": "Thanks for your insightful comments. \n\nThe goal of this paper is to confirm the local elasticity which we had hypothesized for years. We basically took three steps to fulfill the goal. First, we used synthetic examples to confirm this phenomenon in a concrete fashion. Next, we made a connection to the theory of neural tangent kernels to corroborate our hypothesis from a theoretical perspective. Last, we introduced a clustering algorithm that leverages local elasticity. If local elasticity were not true, the performance of the algorithm would basically be the same as randomly tossing a coin. Fortunately, the algorithm performs reasonably well and thus local elasticity must hold.\n\nTherefore, the clustering algorithm was \u201conly\u201d to confirm the local elasticity phenomenon. In this paper we don\u2019t recommend the use of this algorithm in practice and we even didn\u2019t manage to further improve the efficiency of the algorithm. In fact, it would be quite straightforward to improve the algorithm by replacing the k-means with some more sophisticated algorithms, as mentioned in Section 4.2.\n\nBelow please find the response in a point-by-point manner.\n\n\u201cI found the paper very interesting but the writing appeared somewhat unclear at times. \u2026. In particular, I did not find the argumentation around explaining generalization to be very convincing or clear.\u201d\n\nResponse: Thanks for your interest! We\u2019ve significantly improved the writing of the paper and added more experiments, as shown in the revised version. In particular, the revision confirmed local elasticity of ResNet-152 on ImageNet. The implication of local elasticity on generalization was interpreted through stability, by making use of the intimate connection between the two established in the seminal paper by Bousquet and Elisseeff. In the revision, we elaborated on stability in a more detailed way.\n\n\u201cUnfortunately, the empirical results did not really convince me that the resulting clustering algorithm really improves on the SOTA in clustering as only relatively weak baselines were considered. I believe that considering more solid baselines for the clustering experiments would help.\u201d\n\nResponse: We agree that our clustering algorithm at the present form does not improve on SOTA. However, as pointed out earlier, this is not the aim of the paper. As declared in the introduction, the clustering algorithm is designed to corroborate our hypothesis that neural networks (with non-linear activation) are locally elastic. Because the geodesic distance in real data is generally unknown, we cannot directly verify our hypothesis. We instead use the clustering algorithm to indirectly corroborate our hypothesis. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxMYANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1238/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1238/Authors|ICLR.cc/2020/Conference/Paper1238/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159083, "tmdate": 1576860543068, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment"}}}, {"id": "rygYDrRbjH", "original": null, "number": 2, "cdate": 1573148000753, "ddate": null, "tcdate": 1573148000753, "tmdate": 1573148000753, "tddate": null, "forum": "HJxMYANtPH", "replyto": "BklTr756KH", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment", "content": {"title": "Official Comment", "comment": "Thanks for your insightful comments. In particular, your remark on the connection between local elasticity and the local memorization ability is very interesting and deserves future investigations.\n\nBelow please find the response in a point-by-point manner.\n\n\u201cIt feels like the experimental results in the present paper is a rather indirect evidence for the local elasticity \u2026 In the present form the experiments perhaps at most says that the similarity score makes sense, not yet that a fully quantitative characterization of the local elasticity.\u201d\n\nResponse: The paper first provided direct evidence of local elasticity using synthetic data in Figure 1, in which we can get the exact geodesic distance between feature vectors. In the updated version, we further provide direct evidence using examples from real data in Appendix A.2.  Using pre-trained ResNet-152 on ImageNet, specifically, we found that the change of the predictions (KL divergence of 0.03) on the tiger cat (Figure 6(b)) is more drastic than that (KL divergence of 0.002) on the warplane (Figure 6(c)) after a SGD update on the tabby cat (Figure 6(a)), although the Euclidean distance (511) between the warplane and the tabby cat is smaller than that (854) between the tiger cat and the tabby cat.\n \nWe acknowledge that the evidence provided by the clustering algorithm is indirect. However, it is quite difficult to directly confirm local elasticity on real data since the geodesic distance is generally unknown. In fact, it took us a very long time to devise the clustering algorithm for indirect evidence. Moreover, if local elasticity were not true, it would be very challenging to explain the effectiveness of the clustering algorithm. As such, we believe the indirect evidence is quite solid. As for different architectures, the paper actually already compared three architectures, FNN, CNN, and ResNet, in Table 3. We found that CNN shows very pronounced local elasticity. We can explore more architectures as needed during the rebuttal period. \n\n\u201cI\u2019m also a little bit concerned about the fairness of the clustering experiment, Is there a way of modifying the K-means and PCA K-means so that they can also use this auxiliary dataset while still giving a sensible algorithm for the primary 2-class clustering task?\u201d\n\nResponse: Thanks for making this point. In fact, it is entirely nontrivial to make use of the auxiliary dataset for K-means and PCA K-means, if provided. Our goal is not to design an effective clustering algorithm for practical use. Instead, it is to corroborate our hypothesis that neural networks (with non-linear activation) are locally elastic. So the comparison is to only show that the clustering algorithm effectively leverages local elasticity. In the revision, we\u2019ve made this point clear.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxMYANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1238/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1238/Authors|ICLR.cc/2020/Conference/Paper1238/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159083, "tmdate": 1576860543068, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment"}}}, {"id": "Hygy1rCWjH", "original": null, "number": 1, "cdate": 1573147862662, "ddate": null, "tcdate": 1573147862662, "tmdate": 1573147862662, "tddate": null, "forum": "HJxMYANtPH", "replyto": "ryxdtTj1qS", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment", "content": {"title": "Official Comment", "comment": "Thanks for your insightful comments. Below please find the response in a point-by-point manner.\n\n\u201cThe paper is well presented (with a small typo in the definition of S_ker(x,x)).\u201d\n\nResponse: Thanks for pointing out these. We\u2019ve fixed these typos in our updated version.\n\n\u201cIn the experiments, it will be interesting to further investigate how the local elastic property changes with large batch size in that large batch size may encourage more diversity of the examples in a batch.\u201d \n\nResponse: It\u2019s a very interesting direction for future work. In the case of using mini-batch SGD, the prediction change of a point is presumably a weighted effect from all points in the mini-batch. It requires quite an amount of effort to investigate this extension, and we are working on it now and will report any findings once available. Having said this, the aim of this paper is to identify the local elasticity phenomenon and we believe our setting, albeit simple, is sufficient for the confirmation of this phenomenon.\n\n\u201cFurthermore, it will be even more interesting to explore how these similarities can improve the performance of a simple k-nearest neighbor classifier.\u201d\n\nResponse: Thanks for pointing out this good direction for the application of our local elasticity! K-NN is an effective algorithm if we have a good distance measure and sufficient data points. This paper mainly focuses on corroborating our hypothesis that neural networks (with non-linear activation) are locally elastic. For any test point, it is possible to leverage the local elasticity phenomenon to evaluate the \u201cdistance\u201d between this point and any other point. With the distance information in place, we can apply K-NN to predict the label of the test point. In turn, we believe that this use further illustrates the power and profound implication of local elasticity."}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxMYANtPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1238/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1238/Authors|ICLR.cc/2020/Conference/Paper1238/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504159083, "tmdate": 1576860543068, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Authors", "ICLR.cc/2020/Conference/Paper1238/Reviewers", "ICLR.cc/2020/Conference/Paper1238/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Comment"}}}, {"id": "BklTr756KH", "original": null, "number": 2, "cdate": 1571820356581, "ddate": null, "tcdate": 1571820356581, "tmdate": 1572972494744, "tddate": null, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n[Summary]\nThis paper proposes and studies the \u201clocal elasticity\u201d, a quantitative measure for the ability of neural networks to only locally change its prediction (around x) after a stochastic gradient step at x. The paper verifies experimentally that nonlinear neural nets are locally elastic through showing that an elasticity-motivated similarity score can perform clustering well.\n\n[Pros]\nThe notion of local elasticity is interesting and has the potential of opening up lots of further directions. The way I understand it is to relate to memorization (as the authors have indeed discussed) --- I think \u201clocal elasticity\u201d can be viewed as some sort of \u201clocal memorization ability\u201d, in that the NN is able to change its prediction only in a small neighborhood of x---without affecting predictions at other remote x\u2019s---after one SGD step on x. Conceptually this is something not covered by the existing narratives in deep learning theory, yet the phenomenon itself is quite convincing and could provide a new perspective into lots of things.\n\n[Cons]\nIt feels like the experimental results in the present paper is a rather indirect evidence for the local elasticity -- that the similarity score coming from the elasticity works well for a downstream clustering task. Could there be some more direct evidence about the local elasticity? How would the elasticity compare on different architectures? In the present form the experiments perhaps at most says that the similarity score makes sense, not yet that a fully quantitative characterization of the local elasticity.\n\nI\u2019m also a little bit concerned about the fairness of the clustering experiment, in that the elasticity-motivated clustering algorithm utilizes an auxiliary dataset whereas simple baselines such as K-means and PCA K-means are not able to use that. Is there a way of modifying the K-means and PCA K-means so that they can also use this auxiliary dataset while still giving a sensible algorithm for the primary 2-class clustering task?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575416629538, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Reviewers"], "noninvitees": [], "tcdate": 1570237740293, "tmdate": 1575416629549, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Review"}}}, {"id": "ryxdtTj1qS", "original": null, "number": 3, "cdate": 1571958143712, "ddate": null, "tcdate": 1571958143712, "tmdate": 1572972494700, "tddate": null, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "invitation": "ICLR.cc/2020/Conference/Paper1238/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies an interesting phenomenon in neural network models that the classifier's prediction at a one input will not be significantly perturbed after the classifier is updated via sgd at another input that is dissimilar from the former one. This phenomenon is termed as the local elasticity, which provides another perspective seeking to interpret the neural networks. They present that this local elasticity characteristic does not hold for linear models. To further investigate this property, the paper introduces the relative similarity and kernelized similarity based on which a k-means like clustering algorithm is developed to further find fine-grained clusters within a coarse-grained category, like dogs and cats from the mammal category. Interpreting neural networks is a hot research topic, and a paper advancing knowledge in this area is certainly welcome. The paper is well presented (with a small typo in the definition of S_ker(x,x)). In the experiments, it will be interesting to further investigate how the local elastic property changes with large batch size in that large batch size may encourage more diversity of the examples in a batch. Furthermore, it will be even more interesting to explore how these similarities can improve the performance of a simple k-nearest neighbor classifier."}, "signatures": ["ICLR.cc/2020/Conference/Paper1238/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1238/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hangfeng@seas.upenn.edu", "suw@wharton.upenn.edu"], "title": "The Local Elasticity of Neural Networks", "authors": ["Hangfeng He", "Weijie Su"], "pdf": "/pdf/28d5679dc56726f471eb847aa741005dd33f4e0a.pdf", "abstract": "This paper presents a phenomenon in neural networks that we refer to as local elasticity. Roughly speaking, a classifier is said to be locally elastic if its prediction at a feature vector x' is not significantly perturbed, after the classifier is updated via stochastic gradient descent at a (labeled) feature vector x that is dissimilar to x' in a certain sense. This phenomenon is shown to persist for neural networks with nonlinear activation functions through extensive simulations on real-life and synthetic datasets, whereas this is not observed in linear classifiers. In addition, we offer a geometric interpretation of local elasticity using the neural tangent kernel (Jacot et al., 2018). Building on top of local elasticity, we obtain pairwise similarity measures between feature vectors, which can be used for clustering in conjunction with K-means. The effectiveness of the clustering algorithm on the MNIST and CIFAR-10 datasets in turn corroborates the hypothesis of local elasticity of neural networks on real-life data. Finally, we discuss some implications of local elasticity to shed light on several intriguing aspects of deep neural networks.", "keywords": [], "paperhash": "he|the_local_elasticity_of_neural_networks", "_bibtex": "@inproceedings{\nHe2020The,\ntitle={The Local Elasticity of Neural Networks},\nauthor={Hangfeng He and Weijie Su},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxMYANtPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4aadc7d35c76ab6e8d418804757f46b17aaac6f7.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxMYANtPH", "replyto": "HJxMYANtPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1238/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575416629538, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1238/Reviewers"], "noninvitees": [], "tcdate": 1570237740293, "tmdate": 1575416629549, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1238/-/Official_Review"}}}], "count": 11}