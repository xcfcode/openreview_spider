{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488565806968, "tcdate": 1478296700593, "number": 477, "id": "BybtVK9lg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BybtVK9lg", "signatures": ["~Akash_Srivastava1"], "readers": ["everyone"], "content": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 24, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396607406, "tcdate": 1486396607406, "number": 1, "id": "B1wa2fIdg", "invitation": "ICLR.cc/2017/conference/-/paper477/acceptance", "forum": "BybtVK9lg", "replyto": "BybtVK9lg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396607876, "id": "ICLR.cc/2017/conference/-/paper477/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BybtVK9lg", "replyto": "BybtVK9lg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396607876}}}, {"tddate": null, "tmdate": 1485022941784, "tcdate": 1481944818799, "number": 3, "id": "SJoekNG4g", "invitation": "ICLR.cc/2017/conference/-/paper477/official/review", "forum": "BybtVK9lg", "replyto": "BybtVK9lg", "signatures": ["ICLR.cc/2017/conference/paper477/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper477/AnonReviewer1"], "content": {"title": "Promising direction, but the paper needs more work", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)\n\nIn general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. \n\nCan you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?\n\nFigure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.\n\nSection 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?\n\nNone of the numbers include error bars. Are the results statistically significant?\n\n\nMinor comments:\n\nLast term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?\n\nThe idea of using an inference network is much older, cf. Helmholtz machine. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512573110, "id": "ICLR.cc/2017/conference/-/paper477/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper477/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper477/AnonReviewer3", "ICLR.cc/2017/conference/paper477/AnonReviewer4", "ICLR.cc/2017/conference/paper477/AnonReviewer1"], "reply": {"forum": "BybtVK9lg", "replyto": "BybtVK9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper477/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper477/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512573110}}}, {"tddate": null, "tmdate": 1485022926841, "tcdate": 1485022926841, "number": 3, "id": "rkvAIX-Pe", "invitation": "ICLR.cc/2017/conference/-/paper477/official/comment", "forum": "BybtVK9lg", "replyto": "S1IKDsWIx", "signatures": ["ICLR.cc/2017/conference/paper477/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper477/AnonReviewer1"], "content": {"title": "Response to author rebuttal", "comment": "Thanks for your response and the references which clarify the uni-modality question. It might be helpful to add these citations and other details to the paper. \n\nConfounding model vs inference: I see what you mean. Reading the draft, I felt a lot of success was attributed to the new inference scheme e.g. the statement \"we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling\" in the abstract. Claiming ProdLDA+NVI works better than LDA+CGS seems to confound model vs inference.  \nI like your TL;DR in the openreview page: \"We got neural variational inference to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.\" as this is a more accurate claim. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560235, "id": "ICLR.cc/2017/conference/-/paper477/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper477/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper477/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560235}}}, {"tddate": null, "tmdate": 1484063161941, "tcdate": 1484063161941, "number": 17, "id": "SJMp-Kf8e", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "BkiuiOfUx", "signatures": ["~Charles_Sutton1"], "readers": ["everyone"], "writers": ["~Charles_Sutton1"], "content": {"title": "More about comparison", "comment": "Right, and our main point is that this comparison is *not* lacking. \n\nWe present the added value of each trick separately in Figure 5 in the paper. \n\nWe pointed that out in our response, three messages above yours in this thread.\n\nThe review claims that the comparison is lacking, but the review is incorrect. (Or we simply misunderstood the review, but if so, the reviewer has not chosen to clarify.)\n\nWe're happy to add important comparisons or to clarify the paper, but to be entirely honest, at this point we're having trouble understanding what the confusion is?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1484061555388, "tcdate": 1484061555388, "number": 16, "id": "BkiuiOfUx", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "BygtE3WNe", "signatures": ["~Mohitdeep_Singh1"], "readers": ["everyone"], "writers": ["~Mohitdeep_Singh1"], "content": {"title": "RE:about comparison", "comment": "Thanks for the clarification. The main point is that you may consider to provide the added value of each trick separately, which is currently lacking."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1484007702947, "tcdate": 1484007702947, "number": 15, "id": "rJJmtoZLe", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "Hy-FkbfVl", "signatures": ["~Akash_Srivastava1"], "readers": ["everyone"], "writers": ["~Akash_Srivastava1"], "content": {"title": "response", "comment": "Thanks for the comments, which we\u2019ll incorporate. Here are the clarifications to your questions:\n\n1. Reason for not waiting for DMFVI to finish: In order to make sure the reported results were statistically significant we ran every method for multiple (40) times. This would have not been practically possible for DMFVI which was taking more than 24 hours for a single run.\n\n2. Perplexity vs Topic Coherence: J. Chang et al, (2009) in Reading tea leaves: How humans interpret topic models show that in fact perplexity correlates negatively with topic interpretability. Our results are in line with their findings.\n\n3. We agree and recently we found that a BN inspired reparametrization of the topic matrix helps a great deal in improving the topic coherence for NVLDA as well so we are in the process of extending  the training section of the paper with the effect of normalization, learning rate and momentum scheduling on the latent topics."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1484007388540, "tcdate": 1484007388540, "number": 14, "id": "HJNJuoWLe", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "SJ2TV3CQl", "signatures": ["~Akash_Srivastava1"], "readers": ["everyone"], "writers": ["~Akash_Srivastava1"], "content": {"title": "response", "comment": "Thanks for your feedback and the correction in eq 4.1. Here are a few points that we would like to clarify to make our contributions more specific.\n\n1. Sensitivity to optimization tricks: VAEs and GANs are notorious for being sensitive to optimization parameters. One of the main contributions of our paper is to work out which optimization tricks are necessary for VAEs to work with LDA (and other mixed-membership methods). Given how important a probabilistic model  LDA is, we feel that this is an important contribution.\n\nOnce the normalizations have been applied before softmax non-linearities the training method does not seem to be sensitive to the numerical parameters of the optimization scheme. In fact, we used the same learning rate and momentum across both data set and both models. \n\n2. Another key highlight is the speed of training which is known to be quite a problem area for VAEs.  The proposed method drastically speeds up inference in one of the most celebrated ML models. On 20newsgroup dataset, it takes 46 seconds compared to several minutes and hours of inference time needed for DMFVI and collapsed gibbs. In addition, our proposed model increases the topic quality three times over the state-of-art in the same 46 seconds of inference time, which in our opinion is both highly useful and exciting. \n\n4. Simple exponential family: While proLDA can be seen as an extension to SePCA (with non-gaussian priors), that would be true of several other mixed membership models simply because SePCA is fairly general architecture. Secondly, it is not clear how would one extend the proposed inference for SePCA to prodLDA. \n\n5. Vocabulary Size: 20newsgroup = 2000 whereas RCV1 = 10,000. The method works just as well for higher dimensional  sparse representations without any change or additional adjustments. \n\n6. Batch-Normalization: We have recently been able to improve the NVLDA learning via a re-parameterization scheme for the topic matrix that is inspired by batch normalization .Now it performs significantly better than NVDM and Gibbs Sampler (in some cases) in terms of the topic coherence."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1484007293967, "tcdate": 1484007293967, "number": 13, "id": "S1IKDsWIx", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "SJoekNG4g", "signatures": ["~Akash_Srivastava1"], "readers": ["everyone"], "writers": ["~Akash_Srivastava1"], "content": {"title": "response", "comment": "Thanks for the review. Here are our clarifications and answers.\n\nOur results do not conflate modelling and inference. It\u2019s true that we introduce both a new model ProdLDA and a new inference method, but we evaluate them separately. First we compare the new inference method to old inference methods on an existing model, LDA. Then we compare the new model, ProdLDA, to the old model, LDA, with the same inference method.\n\nWe included the comparison to prodLDA as an example to show how easy it is to apply our method to difficult models. ProdLDA is an interesting model for this approach precisely because it is difficult to get good baselines with other inference methods.\n\nAs far as we are aware, prodLDA is a novel topic model related to exponential family harmonium like NVDM.\n\nFigure 1 and Sparsity: theta are sampled from the posterior p(theta|doc,alpha). alpha is the concentration parameter of the prior on theta. Sparsity is shown through the amount of probability mass that the different components are allocated under the different choices of prior. For gaussian prior, it seems that all components are getting roughly the same mass, whereas, for dirichlet, fewer components get more mass and the rest of the components get hardly any. This is in fact also the motivation for the choice of dirichlet priors in the original LDA model.\n\nHyper-parameter selection:  For other LDA inference methods we used the built in hyperparameter optimization option in Mallet or scikit-learn, respectively. For NVDM we used the author\u2019s code and because our method is so fast, it\u2019s easy to embed them within an optimization scheme for alpha, like is used in mallet. So we used BO for HP selection and used it for all the experiments. \n\nUni-modal: No, Dirichlet is unimodal in the softmax basis. See Philipp Hennig, David H Stern, Ralf Herbrich, and Thore Graepel. Kernel topic models. In AISTATS, pp. 511\u2013519, 2012, Section 3.3 and David JC MacKay. Choice of basis for laplace approximation. Machine learning, 33(1):77\u201386, 1998, Section 2.\n\nStatistical significance: Yes the results are statistically significant. Across 40 different random initializations, the standard deviation is very low.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1484007052345, "tcdate": 1484007052345, "number": 12, "id": "ryVcLobLx", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "ByNv1p1Qe", "signatures": ["~Akash_Srivastava1"], "readers": ["everyone"], "writers": ["~Akash_Srivastava1"], "content": {"title": "model and the inference", "comment": "Thanks for your question. We already tease out the difference between the model and inference. The way that we do this is by comparing different inference algorithms for LDA. We compare neural inference for LDA to standard mean-field and Gibbs sampling for LDA.\n\nWe agree that it would be interesting to compare different inference methods for ProdLDA as well, but this is more difficult than it may seem. The problem is that the Dirichlet prior p(theta) is not conjugate to the product of multinomials p(w_i | theta). This means that we cannot use collapsed Gibbs sampling, and mean field methods are significantly more difficult to apply.\n\nThe reason that we introduced prodLDA was simply to show that carrying out inference in newer models does not require the same mathematical heavy lifting as traditional VI and collapsed Gibbs methods, and the results in the paper already makes this point. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1482024832681, "tcdate": 1481932664989, "number": 2, "id": "Hy-FkbfVl", "invitation": "ICLR.cc/2017/conference/-/paper477/official/review", "forum": "BybtVK9lg", "replyto": "BybtVK9lg", "signatures": ["ICLR.cc/2017/conference/paper477/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper477/AnonReviewer4"], "content": {"title": "Nice paper to read", "rating": "7: Good paper, accept", "review": "This paper proposes the use of neural variational inference method for topic models. The paper shows a nice trick to approximate Dirichlet prior using softmax basis with a Gaussian and then the model is trained to maximize the variational lower bound. Also, the authors study a better way to alleviate the component collapsing issue, which has been problematic for continuous latent variables that follow Gaussian distribution. The results look promising and the experimental protocol sounds fine.\n\nMinor comments:\nPlease add citation to [1] or [2] for neural variational inference, and [2] for VAE. \nA typo in \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) is results in the distribution\u201d, it should be \u201cThis approximation to the Dirichlet prior p(\u03b8|\u03b1) results in the distribution\u201d\n\nIn table 2, it is written that DMFVI was trained more than 24hrs but failed to deliver any result, but why not wait until the end and report the numbers?\n\nIn table 3, why are the perplexities of LDA-Collapsed Gibbs and NVDM are lower while the proposed models (ProdLDA) generates more coherent topics? What is your intuition on this?\n\nHow does the training speed (until the convergence) differs by using different learning-rate and momentum scheduling approaches shown as in figure 1?\n\nIt may be also interesting to add some more analysis on the latent variables z (component collapsing and etc., although your results indirectly show that the learning-rate and momentum scheduling trick removes this issue).\n\nOverall, the paper clearly proposes its main idea, explain why it is good to use NVI, and its experimental results support the original claim. It explains well what are the challenges and demonstrate their solutions. \n\n[1] Minh et al., Neural Variational Inference and Learning in Belief Networks, ICML\u201914\n[2] Rezende et al., Stochastic Backpropagation and Approximate Inference in Deep Generative Models, ICML\u201914", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512573110, "id": "ICLR.cc/2017/conference/-/paper477/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper477/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper477/AnonReviewer3", "ICLR.cc/2017/conference/paper477/AnonReviewer4", "ICLR.cc/2017/conference/paper477/AnonReviewer1"], "reply": {"forum": "BybtVK9lg", "replyto": "BybtVK9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper477/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper477/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512573110}}}, {"tddate": null, "tmdate": 1481913682561, "tcdate": 1481913682561, "number": 11, "id": "S1jUHn-El", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "BkpbhEvQe", "signatures": ["~Charles_Sutton1"], "readers": ["everyone"], "writers": ["~Charles_Sutton1"], "content": {"title": "replication", "comment": "Thanks for your comments. Our preprocessing steps are described in the discussion thread above. As described above, we used standard toolkits both for feature processing and for the baseline LDA inference methods, so that our results should be easy to replicate. We use the same preprocessing steps in all of the experiments that we report, so that all of the comparisons that we report are fair. We would be happy to help you reproduce these results. If you wish to ensure that your data set is processed in the same way as ours, please contact us via email and we will share our processed data with you.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1481913464430, "tcdate": 1481913464430, "number": 10, "id": "BygtE3WNe", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "HJRj9BwQx", "signatures": ["~Charles_Sutton1"], "readers": ["everyone"], "writers": ["~Charles_Sutton1"], "content": {"title": "about comparison", "comment": "In reverse order:\n\nContribution: It is true that the contribution of this paper is mostly empirical. We have made no attempt to claim otherwise. We show how to make VAE training for LDA work, by using batch normalization, a Laplace approximation to the Dirichlet prior, and careful tuning of momentum.  We have found training a VAE for LDA is more difficult than it may seem at first; if it were not, others would have already reported results on this. Given how important the LDA model is, we feel that showing exactly how to train VAEs for LDA successfully, and showing as well that a VAE beats standard inference methods by a wide margin, is an important empirical contribution.\n\nComparing different inference methods for ProdLDA: We'll respond to this question below (as the ICLR reviewer also asks this)\n\n\"it makes more sense to compare the amortised VI with other inference method for \u2018LDA\": We agree. This is why we made that comparison in the paper. We compared amortised VI with collapsed Gibbs and decoupled mean field VI. These are, by far, the two most popular, standard inference methods for LDA. Adding a comparison to collapsed variational would add no additional insight."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1481716931566, "tcdate": 1481716931559, "number": 1, "id": "SJ2TV3CQl", "invitation": "ICLR.cc/2017/conference/-/paper477/official/review", "forum": "BybtVK9lg", "replyto": "BybtVK9lg", "signatures": ["ICLR.cc/2017/conference/paper477/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper477/AnonReviewer3"], "content": {"title": "VAE model for LDA. Interesting idea, but a incremental.  ", "rating": "6: Marginally above acceptance threshold", "review": "This is an interesting paper on a VAE framework for topic models. The main idea is to train a recognition model for the inference phase which, because of so called \u201camortized inference\u201d can be much faster than normal inference where inference must be run iteratively for every document. Some comments:\nEqn 5: I find the notation p(theta(h)|alpha) awkward. Why not P(h|alpha) ?\nThe generative model seems agnostic to document length, meaning that the latent variables only generate probabilities over word space. However, the recognition model is happy to radically change the probabilities q(z|x) if the document length changes because the input to q changes. This seems undesirable. Maybe they should normalize the input to the recognition network?\nThe ProdLDA model might well be equivalent to exponential family PCA or some variant thereof: http://jmlr.csail.mit.edu/proceedings/papers/v9/li10b/li10b.pdf\nSection 4.1: error in the equation. The last term should be \nProd_i exp(delta*_r_i) * exp((1-delta)*s_i).\nLast paragraph 4.1. The increment relative to NVDM seems small: approximating the Dirichlet with a Gaussian and high momentum training. While these aspects may be important in practice they are somewhat incremental.\nI couldn\u2019t find the size of the vocabularies of the datasets in the paper. Does this method work well for very high dimensional sparse document representations?\nThe comment on page 8 that the method is very sensitive to optimization tricks like very high momentum in ADAM and batch normalization is a bit worrying to me. \nIn the end, it\u2019s a useful paper to read, but it\u2019s not going to be the highlight of the conference. The relative increment is somewhat small and seems to heavily rely optimization tricks.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512573110, "id": "ICLR.cc/2017/conference/-/paper477/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper477/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper477/AnonReviewer3", "ICLR.cc/2017/conference/paper477/AnonReviewer4", "ICLR.cc/2017/conference/paper477/AnonReviewer1"], "reply": {"forum": "BybtVK9lg", "replyto": "BybtVK9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper477/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper477/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512573110}}}, {"tddate": null, "tmdate": 1481231891444, "tcdate": 1481231014203, "number": 9, "id": "HJRj9BwQx", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "SJfvW3SXl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "comparison", "comment": "I see your point. It is important to emphasise the contribution is a new inference method for conventional topic model (LDA). From this perspective it makes more sense to compare the amortised VI with other inference method for \u2018LDA\u2019. It is desirable to include collapsed variational inference (Teh et al.,) as well. On the other hand, it seems to me you also want to propose a new topic model by \u2018changing only one line of code of LDA\u2019, it is then important to assess the relative contribution by comparing different topic models using the same inference method. I would suggest to compare ProdLDA with LDA in a conventional inference setting to tease out the contributions and then move on to neural inference. My two cents of `fairness\u2019 are mainly raised in this regard. These tricks are rather off-the-shelf and can be applied to NVDM as well. (It seems to me that 'your optimisation techniques' are just tweaking the parameters of the ADAM optimiser, batch normalisation and dropout, although you mentioned the intuition of component collapsing. Don't over claim.)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1481228294285, "tcdate": 1481227269006, "number": 8, "id": "BkpbhEvQe", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "ryb3torQl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Re: Comparison", "comment": "Thanks for the clarification. Could you please let me know if the following preprocessing steps are used: removing common stopwords, stemming, and then considering the 2000 most frequent words. Or do you mean you use the same preprocessing steps across your own models which are not in consistent with previous work?\nI ask this mainly because I am unable to reproduce the collapsed Gibbs sampling baseline either and the second author mentions (in previous posts) that your pre-processing is different from the Replicated Softmax paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1481126233630, "tcdate": 1481126233625, "number": 7, "id": "SJfvW3SXl", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "r1CgOu77l", "signatures": ["~Charles_Sutton1"], "readers": ["everyone"], "writers": ["~Charles_Sutton1"], "content": {"title": "Response to anonymous public review", "comment": "Thanks for your review. We would like to point out a few things:\n\nThe main point of our paper is to determine what approximations/optimization techniques are necessary to make VAE inference work for LDA. So the most important comparison is between the different inference methods for LDA. The results for NVDM are mostly for reference.\n\nThat said, after the submission we have verified our results for NVDM by using the code from the original author and we'll update the paper to reflect that.\n \nAlso, we actually do explore the effect of each trick separately, as the reviewer suggests (see Tables 4, 5 and Figure 1).\n\nIt is an interesting question as to whether the optimization techniques that we use for VAE+LDA would also help the performance of NVDM. We have tried this and found that our optimisation techniques do improve the topic coherence for NVDM, but it is still less than LDA+Gibbs and prodLDA. We'll update the paper to reflect this.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1481124264825, "tcdate": 1481124264817, "number": 6, "id": "ryb3torQl", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "SJ0Uv7Vmx", "signatures": ["~Charles_Sutton1"], "readers": ["everyone"], "writers": ["~Charles_Sutton1"], "content": {"title": "Re: Comparison", "comment": "Thanks for the comment. We use exactly the same preprocessing steps for all methods, so that the comparison is fair."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1481025385328, "tcdate": 1481025365744, "number": 5, "id": "SJ0Uv7Vmx", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "HJITnwwbe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Comparison to NVDM and Replicated Softmax", "comment": "It is suggested that the author could use the same pre-processing step as the Replicated Softmax paper, which NVDM follows and compares against. Otherwise the numbers are not comparable."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1480980631724, "tcdate": 1480980470522, "number": 1, "id": "r1CgOu77l", "invitation": "ICLR.cc/2017/conference/-/paper477/public/review", "forum": "BybtVK9lg", "replyto": "BybtVK9lg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Comparison to NVDM looks unfair", "rating": "5: Marginally below acceptance threshold", "review": "The comparison to NVDM looks unfair since the user introduces a couples tricks (Dirichlet prior, batch normalisation, high momentum training, etc.) which NVDM doesn't use. A more convincing experimental design is to explore the effect of each trick separately in neural variational inference. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1482512900807, "id": "ICLR.cc/2017/conference/-/paper477/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "replyto": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk", "ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs", "(anonymous)"], "cdate": 1482512900807}}}, {"tddate": null, "tmdate": 1480736604190, "tcdate": 1480736604186, "number": 1, "id": "ByNv1p1Qe", "invitation": "ICLR.cc/2017/conference/-/paper477/pre-review/question", "forum": "BybtVK9lg", "replyto": "BybtVK9lg", "signatures": ["ICLR.cc/2017/conference/paper477/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper477/AnonReviewer1"], "content": {"title": "improvements to model (ProdLDA / LDA) vs improvements to inference (NVI / CGS)", "question": "In some of the experiments, it seems like ProdLDA + neural variational inference (NVI) works best. Given that NVI is worse than collapsed Gibbs sampling for LDA, the experiments seem to confound improvements to model vs inference. Can you evaluate a difference inference scheme for ProdLDA? That'd help tease out the relative significance of the contributions. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959260251, "id": "ICLR.cc/2017/conference/-/paper477/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper477/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper477/AnonReviewer1"], "reply": {"forum": "BybtVK9lg", "replyto": "BybtVK9lg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper477/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper477/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959260251}}}, {"tddate": null, "tmdate": 1479142590027, "tcdate": 1479142590023, "number": 4, "id": "HJITnwwbe", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "HJcj5wIWe", "signatures": ["~Akash_Srivastava1"], "readers": ["everyone"], "writers": ["~Akash_Srivastava1"], "content": {"title": "Re:Re:Re: Perplexity", "comment": "Our pre-processing is different from the Replicated Softmax paper. See section 6, paragraph 2 (Essentially, we use the CountVectorizer with default settings and do not stem the tokens). Therefore, our mean wordcount and std. are not the same as the ones you mentioned. If that doesn't help, please feel free to contact us via email and we'd be happy to provide you our pre-processed data files and help figure out what is going on in more detail."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1479076514315, "tcdate": 1479076514310, "number": 3, "id": "HJcj5wIWe", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "HycAYjmbg", "signatures": ["~Erik_Holmer1"], "readers": ["everyone"], "writers": ["~Erik_Holmer1"], "content": {"title": "Re:Re: Preplexity", "comment": "Thanks for the detailed answer. Unfortunately I'm unable to reproduce the results on my dataset. I'm curious if we are using the same preprocessing. Could you confirm that your preprocessed dataset has roughly the same mean word count and standard deviation as reported in the original Replicated Softmax paper (Mean: 51.8, St. Dev: 70.8)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1478896082006, "tcdate": 1478896082000, "number": 2, "id": "HycAYjmbg", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "SJOMe3ybl", "signatures": ["~Akash_Srivastava1"], "readers": ["everyone"], "writers": ["~Akash_Srivastava1"], "content": {"title": "Response: Perplexity", "comment": "Thanks for your question. We used the standard Mallet implementation with 100,000 iterations and 'optimise interval' set to 1000 for training on 20 Newsgroup. To evaluate perplexity we used \"left-right algorithm\" (Evaluation Methods for Topic Models) with re-sampling set to True as implemented in Mallet on the entire (original) \"test set\" split.\n\nSeveral papers (for example, Neural Variational Inference for Text Processing) have reported  higher numbers for LDA perplexity on the test set than ours. It seems that many authors have repeated the numbers from (Replicated Softmax: an Undirected Topic Model). However, their results from LDA are not comparable to ours, because in their work  only 50 (randomly sampled) held out documents were used  for computing perplexities and NOT the entire test set. Therefore, those  numbers should be borrowed with caution. Indeed, many later papers that compare against those results do not clearly state whether they  run their methods on the same subset.\n\nAnother difference between the results that we report for LDA and those in the replicated softmax paper is  our use of the left-right algorithm rather than AIS. We expect that to  be a less important difference, as (Evaluation Methods for Topic Models) showed that the LR algorithm generally agrees with AIS."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}, {"tddate": null, "tmdate": 1478639861605, "tcdate": 1478635536417, "number": 1, "id": "SJOMe3ybl", "invitation": "ICLR.cc/2017/conference/-/paper477/public/comment", "forum": "BybtVK9lg", "replyto": "BybtVK9lg", "signatures": ["~Erik_Holmer1"], "readers": ["everyone"], "writers": ["~Erik_Holmer1"], "content": {"title": "Perplexity", "comment": "The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen.  Would you mind sharing the parameters you used and/or the preprocessed dataset?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoencoding Variational Inference For Topic Models", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "pdf": "/pdf/cc9de480f7ff6f42783efc6ba2b325b3a3ca106a.pdf", "TL;DR": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "paperhash": "srivastava|autoencoding_variational_inference_for_topic_models", "conflicts": ["ed.ac.uk"], "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"], "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287560379, "id": "ICLR.cc/2017/conference/-/paper477/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BybtVK9lg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper477/reviewers", "ICLR.cc/2017/conference/paper477/areachairs"], "cdate": 1485287560379}}}], "count": 25}