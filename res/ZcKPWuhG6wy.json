{"notes": [{"id": "ZcKPWuhG6wy", "original": "K-aP9GWAJw9", "number": 1249, "cdate": 1601308139859, "ddate": null, "tcdate": 1601308139859, "tmdate": 1616030046932, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "qtULpB-c9sH", "original": null, "number": 1, "cdate": 1610040390811, "ddate": null, "tcdate": 1610040390811, "tmdate": 1610473985095, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The reviews were largely split in the beginning. Although all reviewers find that the idea of diversity and affinity measures for data augmentation is intriguing and potentially useful, they also raised many concerns such as computationally expensive nature of the diversity metric, lack of clear methodology of how to utilize those metrics to design augmentation strategies in practice, and weak organization and presentation of some experiments which are seemingly less related to the main point of the paper. During the discussion phase authors made significant efforts to improve the paper, and some of the concerns are favorably addressed. As a result, two reviewers raised their initial scores, yet we still think the paper is on the borderline. \n\nOverall, this paper presents an interesting and unique idea that potentially stimulate the community, while it also has some key weaknesses and has much room for improvements. Considering both pros and cons, we decided to accept the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040390797, "tmdate": 1610473985079, "id": "ICLR.cc/2021/Conference/Paper1249/-/Decision"}}}, {"id": "tHwNQrJwwNe", "original": null, "number": 4, "cdate": 1603897363917, "ddate": null, "tcdate": 1603897363917, "tmdate": 1606688325236, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Review", "content": {"title": "Official Blind Review #3 ", "review": "This paper empirically investigates two crucial factors: affinity and diversity in useful data augmentation strategies. Through extensive experiments on existing image augmentation methods, it demonstrates that a good augmentation practice should bring high affinity and diversity for validation and training data. Specifically, it uses the accuracy gap between augmented and clean validation data to measure affinity. The diversity is measured by final training loss with the augmentations used in training.\n\nPros\n1. The paper is well-written and easy-to-follow.\n2. The proposed two measurements for affinity and diversity are easy to compute and observe. They are both model-based, making the measures more adaptive to model biases.\n3. The experiments test extensive augmentation methods to support the affinity and diversity claims.\n\nCons:\n1. Adversarial examples are perceptually similar to the original data but can result in low validation accuracy. The proposed affinity guide may not help guide robust model training.\n2. In Figure 3, the highlights of the three augmentation methods, i.e., Mixup, AutoAugment, and RandAugment, are not difficult to visualize.\n3. In Figure 4, one diversity value (x-axis) seems to correspond to multiple test accuracy measurements, which looks confusing.\n4. Although both affinity and diversity are essential to measuring augmentation quality, it is unclear how to use them to guide training. It seems that both positive and negative affinity values may help, according to Figure 3. Moreover, higher training loss indicates higher diversity. But high training loss may also mean a model still does not converge. It may be difficult for a new task with little experience to use the two metrics to select useful augmentation strategies.\n\nSummary\n\nThe paper investigates two important factors (affinity and diversity) in measuring the effectiveness of data augmentation. It also provides two simple metrics to measure them. My main concern is how to use them in practice to guide training. Moreover, the affinity measure seems not helpful for robust training since adversarial examples can easily fail a model, resulting in low validation accuracy. But using them in training can help advance model robustness. The paper would be better if it can address the two concerns. Overall, I recommend it for acceptance if it can include discussions on these two concerns.\n\nPost-rebuttal updates\n\nThank the authors for the efforts in answering the questions. The responses have addressed my concerns about the robust training and using training loss to measure diversity. Exploring the data augmentation for robust training will be interesting. Besides,  training loss is usually sensitive to some other hyper-parameters such as optimizer and learning rate. So, investigating the robustness of this metric is also meaningful.\n\nMoreover, there is still a gap from applying the proposed metrics to training guidance. The current research mainly shows there are relations between the two metrics and data augmentation effectiveness.  How to merge them into one easily observable is still missing. Therefore, I keep my original score.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123027, "tmdate": 1606915769842, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1249/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Review"}}}, {"id": "tCS3MXCOhMq", "original": null, "number": 2, "cdate": 1603533182979, "ddate": null, "tcdate": 1603533182979, "tmdate": 1606648596190, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Review", "content": {"title": "This paper is interesting and can be improved.", "review": "# Summary\n\nThis paper analyzes data augmentation for image classification using two measures: Affinity and Diversity. These measures depend on both training data and model and are easy to compute. The authors show that the performance of image classifiers depends on both of these measures through extensive experiments.\n\n# Strengths\n\n* The introduced measures consider both data and models, which are inseparable in modern deep learning.\n* The measures can explain why some data augmentation methods work and others are not, intuitively.\n* Affinity is easy to compute. To obtain Affinity, one needs a model trained on clean data and uses validation sets with a given augmentation. One can reuse the trained model to measure other augmentations.\n* The experiments are extensive.\n\n# Weaknesses\n\n* Diversity requires to train a CNN model with each augmentation configuration. This requirement restricts the crucial application of the paired measures to find better augmentation configurations given a dataset and a model under limited computational cost.\n* The slingshot effect in section 4.2 is interesting. But Fig 5 (c) shows a weak connection between the slingshot effect and the proposed measures. Instead, I think the results suggest the existence of other factors. Explaining this effect by the proposed measures only would be difficult, and I recommend removing this subsection.\n* Section 4.3 shows dynamic augmentations increase the effective training data size (compared to static augmentations), and thus the performance improves. I think this is well-known, and that's why we call data augmentation \"data augmentation.\"\n* Experiments are conducted only on CIFAR-10 and ImageNet. I know these datasets are the de-facto standard, but datasets from other domains are preferable to be included to support the claims.\n\n# Rating\n\n5. This paper includes interesting and useful insights, but its novelty is limited. Besides, its applicability is also restricted because of the computational intensity of the Diversity measure.\n\n# Feedbacks\n\n* Titles for the color bars in Fig 2 are missing.\n* It took moments to understand T and B in Figure 3 (a) are top and bottom.\n* The margin between the caption and the main text on page 7 is too small, which confused me during reviewing.\n* Reference information is old. Lim et al. 2019 is accepted at NeurIPS, and Hataya et al. 2020 is accepted at ECCV.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123027, "tmdate": 1606915769842, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1249/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Review"}}}, {"id": "05SlStMKge", "original": null, "number": 3, "cdate": 1605753612869, "ddate": null, "tcdate": 1605753612869, "tmdate": 1606265890247, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "tHwNQrJwwNe", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "Re: Official Blind Review #3 ", "comment": "We thank the reviewer for taking the time to read our work and for providing constructive feedback! We think the questions and suggestions about robustness in particular have strengthened the work. \n\n_The proposed affinity guide may not help guide robust model training._\n\nThis is a valid point. In the paper, we investigated how Affinity and Diversity related to clean performance, but as pointed out by the reviewer did not comment on robust performance for which we made no claims. Though it was not the focus of the work, it is an interesting question!\n\nWe have now taken the reviewers recommendation to expand the paper with additional experiments and a discussion of how affinity and diversity relate (or fail to relate) to robustness (see supplementary section G). In particular, we have evaluated robust performance on ImageNet-V2, CIFAR-10.1, and CIFAR-10-C. For the latter we present results for the overall CIFAR10-C robustness score, as well as accuracy for each of the composite corruptions. We think that this suggestion has strengthened the paper, but note that a truly thorough analysis of robustness is outside the scope of the current work. \n\n_In Figure 3, the highlights of the three augmentation methods, \u2026 are difficult to visualize._\n\nSorry for the difficulty. We have replotted these augmentations in the updated manuscript. Please let us know if they are still hard to see.\n\n_In Figure 4, one diversity value (x-axis) seems to correspond to multiple test accuracy measurements._\n\nThis is indeed correct! The plot illustrates that diversity alone is not a great predictor of accuracy, as the one-to-many relation shows. By looking at both affinity and diversity, this degeneracy is largely broken. \n\n_It seems that both positive and negative affinity values may help, according to Figure 3._\n\nWe apologize if this was confusing. Affinity is always negative. Any appearance of positive affinity in Figure 3 is an artifact of the marker size. \n\n_high training loss may also mean a model still does not converge_\n\nHere it is important to note that we are comparing models trained with different augmentation strategies for equal training times. The models need not have converged to compare these relative loss values. As evidence to this point, in supplementary section G we show that the training loss early in training is also a good measure of Diversity. \n\n_It may be difficult for a new task with little experience to use the two metrics to select useful augmentation strategies._\n\nThis is a valid point, and finding the most effective way to use the metrics to find successful strategies is ongoing work. We do note, however, that one or the other of these metrics are already individually being used successfully to search for novel augmentation strategies. For example, Fast AutoAugment and CTAugment focus on increasing the Affinity of an AutoAugment policy, whereas Adversarial AutoAugment focuses on increasing the diversity of an AutoAugment policy.\n\nThank you again for your feedback! Are there any other concerns we didn\u2019t address? If we have addressed your concerns, we humbly ask if you would kindly consider supporting accepting the paper by increasing the review score accordingly "}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "k-fr_pHCpl", "original": null, "number": 5, "cdate": 1605753840051, "ddate": null, "tcdate": 1605753840051, "tmdate": 1606265874925, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "tCS3MXCOhMq", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "Re: This paper is interesting and can be improved. ", "comment": "We thank the reviewer for taking the time to read our work and for providing constructive comments. We believe the feedback regarding computational cost has especially improved the manuscript! \n\n_Diversity ... restricts the crucial application of the paired measures to find better augmentation configurations ... under limited computational cost._\n\nYou are absolutely correct that evaluating diversity as defined in equation (2) for each augmentation policy requires training a new model and is costly. For this reason, in the paper we introduced an alternative measure of diversity, the entropy of an augmentation, this has the advantage that it can be computed without training any model, but has the disadvantage of being ill defined for continuous augmentations.\n\nTo remedy this shortcoming further, we have included an alternative computationally less-expensive proxy for diversity, shown in in Figures 10 and 11. We found that the training loss at a much earlier epoch correlates well with the final training loss and so can serve as an alternative measure of diversity. This still requires training a model for each augmentation policy, but only for a small fraction of the training time. In particular the results shown in Fig 10 are for a model trained for 10 epochs, a 95% reduction in computational cost.\n\nIt is also worth noting that in many cases good augmentation strategies generalize across models. This reusability can help alleviate the initial cost of finding a successful strategy.\n\n_The slingshot effect in section 4.2 is interesting. But Fig 5 (c) shows a weak connection between the slingshot effect and the proposed measures._\n\nThanks for the comment, though to our eye the slingshot effect does seem to be least for the high affinity high diversity augmentations, this can easily be explained by the lift correlating with accuracy itself, thus we agree that figure 5c does not add much additional insight. We have thus removed figure 5c and changed the tone of section 4.2 to emphasize the main points i) that the switching lift is a phenomena which is common to augmentations and other more classical forms of regularization ii) That in some cases naively deteremtal augmentations can improve over baseline performance when switched off.\n\n_Section 4.3 shows dynamic augmentations increase the effective training data size ... this is well-known_\n\nThe intuition that data augmentation increases the effective dataset size is indeed well known, however it has also been suggested that data augmentation perhaps plays an additional beneficial regularizing role, separate from the increase in dataset size. The aim of this section is to test a strong version of this hypothesis concretely. We find a negative result -- no evidence for this independent benefit of augmentation, but still feel it is a worthwhile addition to the literature.\n\n_Experiments are conducted only on CIFAR-10 and ImageNet. I know these datasets are the de-facto standard, but datasets from other domains are preferable to be included to support the claims._\n\nWe agree that investigating the utility of Affinity and Diversity for augmentations in other domains is an exciting direction! Unfortunately a thorough investigation of other domains is beyond our scope here.\n\nNonetheless, to address this point, we are performing additional experiments on street view house numbers to broaden the datasets used and will update the manuscript shortly. If the reviewer has a particular additional domain request, we would be happy to include experiments if it is within our infrastructural capabilities and time constraints.\n\n_Feedbacks_\n\nThank you for the feedback. We have corrected all of the points mentioned here in our updated manuscript. \n\nThank you again for your constructive feedback! Are there any other concerns we didn\u2019t address? If we have addressed your concerns, we humbly ask if you would kindly consider supporting accepting the paper by increasing the review score accordingly\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "JJRoyutzFa5", "original": null, "number": 14, "cdate": 1606252619614, "ddate": null, "tcdate": 1606252619614, "tmdate": 1606252619614, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "tVWcB7yJ3bk", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "re: About the definition of diversity", "comment": "Thank you for the suggestion! Variance of activations does make intuitive sense, however a first pass at this doesn\u2019t seem to correlate well with diversity, and instead correlates well with affinity (see Figure 21 in updated draft). A speculative reason for this is that the model trained only on clean data is not able to capture a meaningful notion of out of distribution diversity. But we agree making this work would be very interesting.\n\nAs to the question of how training loss captures the notion of diversity depicted in figure 1c. In the cases where we had an alternative definition, such as entropy of the augmented data, we found empirically that models had more difficulty training on more complex (higher entropy) data -- represented by a higher training loss. This also matched our intuitive picture that more diverse data should be harder to fit. This intuition and the correlation with entropy lead us to use training loss to quantify diversity. \n\nThanks again for taking the time to think about and discuss this, we sincerely appreciate the discussion, and would welcome any ideas now, or after the review period!\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "tVWcB7yJ3bk", "original": null, "number": 13, "cdate": 1606211389578, "ddate": null, "tcdate": 1606211389578, "tmdate": 1606211389578, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "About the definition of diversity", "comment": "I start this thread to potentially discuss with the authors about the definition of diversity (and affinity). This is independent from my review, it is not intended to produce any change in the manuscript, but to simply have a scientific discussion.\n\nFirst, I would like to hear from the authors some more details about how they think their definition of diversity (final loss of a model trained with data augmentation) reflects the qualitative notion of diversity, as expressed in the paper, particularly in Figure 1c.\n\nTo me, the final loss of the model trained with data augmentation _is_ somehow related to diversity, but it seems that it does not capture all details of the concept. A measure of spread or entropy (discussed by the authors) seems closer, in my opinion.\n\nOne direction that has not been discussed would be defining affinity and diversity as a function of the model's activations (towards the transformed images of a data augmentation strategy). This entails some challenges, of course, but seems interesting to me. Have the authors explored this idea or thought about it? "}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "oXR7Uwq3Ti1", "original": null, "number": 12, "cdate": 1606206860024, "ddate": null, "tcdate": 1606206860024, "tmdate": 1606206860024, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "WlmFK86CXP", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "if Acc(a) > Acc(a') then Aff(a) > Aff(a') ? Div(a) > Div(a')", "comment": "Thanks for the new clarifications.\n\nRegarding the boolean relationship between [Aff(a) > Aff(a')] and [Div(a) > Div(a')] if Acc(a) > Acc(a'), contribution 2 of the paper is the following: \"We find that performance is dependent on _both_ metrics. In the Affinity-Diversity plane, the best augmentation strategies jointly optimize the two (see Fig 1)\". From _both_ and _jointly_ this I understand that the two metrics should generally correlate with better performance, and for a majority of pairs, a and a', if Acc(a) > Acc(a') then _both_ Aff(a) > Aff(a') AND Div(a) > Div(a'). Certainly, these are noisy measures and I would not expect a value close to 100 %, but definitely a majority. From Figure 3b, it seems that this would be the case for CIFAR-10, supporting the claim, but by a much lesser extent for ImageNet (Figure 3c). This is precisely the concern raised in my original review.\n\nI concede that the claim holds true if for the same value of diversity, higher affinity leads to better accuracy (for the same value of affinity, higher diversity leads to better accuracy). This would make the conditional statement more precise with 'greater than or equal' operators. Still, that would only capture a few cases of continuously varying metrics. Perhaps a more informative summary statistic should take into account differences, not boolean relationships. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "WlmFK86CXP", "original": null, "number": 11, "cdate": 1606183713774, "ddate": null, "tcdate": 1606183713774, "tmdate": 1606183713774, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "V7Wy0Mpsz7Q", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "re: Upgraded score, though main concerns remain", "comment": "Thank you for taking the time to read the response and for the additional comments and clarification (as well as the score raise)! We have clarified and responded to specific pieces below. \n\n_I humbly think that an alternative may be to re-defining the diversity ... I am willing to elaborate on the suggestions._\n\nWe would be excited to hear the full suggestion in another thread! We have now included a preliminary analysis showing the correlation between Diversity defined as the loss at the end of training, and the variance of affinity over the dataset for some CIFAR10 augmentations. We find poor correlation between the two in this case (new Figure 20). \n\n_the authors have introduced a new result, the count of pairs of augmentations for which \"if Acc(a) > Acc(a') then either Aff(a) > Aff(a') or Div(a) > Div(a')\". However, if the claim is that performance depends on both metrics, should the condition not be Aff(a) > Aff(a') AND Div(a) > Div(a'), that is AND, not OR?_\n\nOur claim was indeed that performance depends both on Affinity and Diversity. It was not, however, intended to imply that if one augmentation was better than another it needed to have both higher affinity and higher diversity. Rather improving affinity or improving diversity both help performance, and the inequality is intended to capture this. Perhaps the message is more clear when phrased using the contrapositive: what fraction of pairs of augmentations violate this picture -- i.e. what fraction have Acc(a) >  Acc(a\u2019) but where a has both lower Affinity and lower Diversity than a\u2019. This is 1 - the numbers quoted in the text. Ie only 2.5% of pairs for ImageNet and .9% of pairs for CIFAR10 violate this. \n\n_The authors have provided some insights about how affinity and diversity may help us understand some augmentation strategies such as mixup, SpecAugment and AutoAugment. \u2026 it would be interesting to discuss this in the paper._\n\nApologies for missing this! We had intended to, and now have, expanded the analysis in the discussion.\n\n_unrelated comments_\n\nFor all of the plotting suggestions -- color scheme, relative Affinity and Diversity, grid consistency. Unless we hear strong opposing viewpoints from other reviewers, if accepted, we are happy to make these changes in the camera ready version. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "u5VDDAi_qVt", "original": null, "number": 1, "cdate": 1603354715239, "ddate": null, "tcdate": 1603354715239, "tmdate": 1606155562324, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Review", "content": {"title": "Relevant topic, interesting ideas; proposal and methods can be polished", "review": "## Edit after authors' responses\n\nI have upgraded my score (from 4 to 5) based on the clarifications provided by the authors and the updated manuscript. Please see the details in my extended comments: https://openreview.net/forum?id=ZcKPWuhG6wy&noteId=V7Wy0Mpsz7Q\n\n## Summary of the paper\n\nThis paper proposes two metrics, affinity and diversity, for assessing the value and contribution of data augmentation strategies (single transformation or combinations of them). Given a model trained on a clean data set (without data augmentation), the affinity of an augmentation strategy is defined as the difference between the accuracy of the model on augmented validation data, minus the accuracy on clean validation data. The diversity of an augmentation strategy is defined as the final training loss of a model trained with data augmented according to that strategy. The paper presents an empirical analysis of the affinity and diversity of a set of image augmentation strategies evaluated on a network architecture trained on CIFAR-10 and one trained on ImageNet. The main conclusion is that the contribution to a model's performance of an augmentation strategy is predicted by its joint affinity and diversity, but not separately.\n\n## Summary of merits and concerns\n\n### Merits\n\n+ The paper's overarching motivation of quantifying the usefulness of data augmentation strategies is interesting and definitely important, given the renewed interest in data augmentation by the machine learning community.\n+ The proposal of quantifying the data distribution shift and complexity (affinity and diversity, respectively) introduced by an augmentation strategy is reasonable, interesting and well motivated.\n+ The introduction of the problem and motivation for the proposal (Introduction) is comprehensible and interesting and the review of related work is exhaustive and relevant. This part of the paper is very well written and I quite enjoyed reading it.\n+ In the rest of the paper, the concepts and definitions introduced are easy to understand and the methods employed for empirically assessing the affinity and diversity are generally clear.\n\n### Concerns\n\n- I see some issues in the specific definition of affinity and diversity. Summarised (extended below), first, while the affinity can be easily computed for any augmentation strategy and pre-trained model, the diversity is computationally costly as it requires re-training the model;  second, in their current definition, the dependence on the specific model and data set makes the metrics hard to compare across models and data sets; third, affinity and diversity are defined in very different ways, one in terms of the accuracy of a pre-trained model, the other in terms of the training loss.\n- Although generally clear, the methodology employed falls short at demonstrating the contributions stated in the introduction and portraying a complete picture of how affinity and diversity can be used to assess the value of data augmentation strategies. We do gain some insights, but I have some concerns about the methodology and some important questions remain open.\n- One motivation for the introduction of metrics to quantify the merit and mechanisms of data augmentation strategies is that the reasons why cutout, SpecAugment and mixup work so well are not well understood. Another one is that (so-called) automatic data augmentation strategies, such as AutoAugment, are hugely computationally expensive. However, the paper does not really discuss how affinity and diversity explain the mechanisms of these augmentation strategies and how they can be used to discover new strategies more efficiently.\n- The presentation of the results, especially in the figures, can be improved, in my opinion, and hinders the clarity of the paper.\n\n## Evaluation and justification\n\nWhile acknowledging the merits and contributions of the paper, my concerns outweigh the positive aspects of the paper and hence my recommendation of rejection. I will discuss next in more depth these concerns in order to better justify my recommendation and with the intention to provide constructive feedback for potential subsequent work on the paper.\n\n### Definitions of affinity and diversity\n\nThe definitions of affinity and diversity proposed in this paper are intuitive, easy to understand and reasonable. Furthermore, as stated above, I agree that quantifying the concepts and intuitions behind these metrics is an important contribution. In fact, the main result that the value of an augmentation strategy depends on both its affinity and diversity matches my (and the author's) intuitions and expectations. However, I have several concerns about the specific way these quantities are defined and computed in the paper.\n\nFirst of all, since one of the motivations for proposing such metrics is to more efficiently discover new augmentation strategies and assess the existing ones, I think that an important feature of the metrics should be the efficiency and ease of computation. However, computing the diversity of an augmentation strategy for a model requires training the model end-to-end. This is not the case of the affinity, which can be computed for any pre-trained model, and I think a useful definition of the diversity should achieve the same goal.\n\nSecond, in the way affinity and diversity are defined in the paper, it is hard to compare augmentation strategies across models and data sets, even though they are identical, that is an image rotation is applied in the same way on CIFAR-10 and on ImageNet; on ResNet and on DenseNet. Thus, it would be desirable if comparisons across models and data sets would be possible. This mismatch is in fact reflected in the presentation of the results, hindering the clarity. For example, in Figures 3b and 3c, the colour codes represent very different things as the range varies in one case from -50 to +7 and in the other case from -70 to +0.6. Green dots in Figure 3b (CIFAR-10) represent augmentation strategies that improve the accuracy with respect to the baseline, while green dots in Figure 3c (ImageNet) correspond to strategies that hinder the performance. Furthermore, the range of affinity and diversity also differs greatly between the two plots. This has to do in part with a likely suboptimal way of presenting the results (more about this below), but also with the fact that the affinity and diversity in ways that are not comparable across models and data sets.\n\nOne reason why the metrics are not comparable is that they are defined in absolute terms, without any normalisation that cancels the dependence on specific aspects of the model and data set, such as the loss, the number of classes (which determines the loss and accuracy), etc. Moreover, the authors chose to define the affinity and diversity in terms of the (top-1) accuracy and the (cross-entropy) loss, respectively. However, other researchers that may wish to use these metrics in the future might prefer to assess their models using an alternative metric, such as the top-5 accuracy, commonly used for ImageNet, or trained their models with a different loss. Such choices would also affect the interpretation of affinity and diversity and complicate even further the comparisons. \n\nA suggestion would be to define the metrics in relative terms instead. Without claiming that these suggestions would be optimal, the affinity could be computed, for instance, as the accuracy on the augmented data divided by the baseline accuracy on the clean data (and optionally multiplied by 100 to turn it into a percentage): $\\mathcal{T} = A(m, D') / A(m, D) * 100$. This would represent the fraction of the percentage of the accuracy obtained by testing on augmented images. These would reduce the dependence on the specific metric (accuracy) and on the characteristics of the model and data set. This has been used for instance in [[1]](#references) to also compared the contribution to performance of models trained with different data augmentation strategies.\n\nThird, affinity and diversity are defined in very different ways. Intuitively, the concepts that these quantities aim to represent are both related to the data distribution: affinity is related to the shift in the data distribution introduced by an augmentation strategy; diversity is related to the complexity in the distribution introduced by the augmentation. However, the former is defined in terms of the accuracy of a pre-trained model with respect to a baseline, the other in terms of the absolute training loss achieved by training with the augmentation. These are very different, unrelated quantities. Again, without claiming that the following suggestion is optimal, one idea would be to define the diversity as measure of spread (standard deviation, variance). Have the authors considered defining the diversity along the lines of the variance of the affinity or of a related quantity? This would quantify the diversity of an augmentation strategy and at the same time remove the need to train a model end-to-end, as discussed in the first point.\n\nThe authors briefly discuss entropy as an alternative measure of diversity, despite the problem of computing it for continuously-varying transformations. This is also an interesting direction which could be worth exploring.\n\n### Results do not demonstrate all contributions\n\nThe authors list four main contributions of their paper in the introduction. The first one is the introduction of affinity and diversity as \"interpretable, easy-to-compute metrics for parametrizing augmentation performance\". This is satisfied, although I have discussed above some concerns about the definitions. \n\nThe second contribution is that \"performance is dependent on both metrics\". This is indeed reflected by the results in Figures 3b and 3c. However, I should also note in this regard that despite the large number of augmentation strategies analysed, only two network architectures, each trained on one data set, are included in the experimental setup, and some differences between the two plots could already be discussed. The claim that the performance gain introduced by an augmentation strategy increases when both the affinity and the diversity increase is well supported by the results in Figure 3b (WRN on CIFAR-10), but this is not so clear in Figure 3c (ResNet on ImageNet). As a matter of fact, the relative test accuracy in Figure 3c seems to be higher (more yellow) as diversity decreases, rather than increases, and affinity increases. It would be desirable to obtain similar results on other architectures and data sets in order to gain more evidence to support this claim. \n\nRelated to this point, I would like to note that having two sets of results (WRN on CIFAR-10 and ResNet on ImageNet), presented in Figure 3, the authors select one of them (WRN on CIFAR-10, Figure 3b), the one, out of two, that best supports the claim, for Figure 1 on the first page of the paper. This clearly introduces a selection bias that distorts the actual data (i.e. why not showing in Figure 1 the results on ImageNet?). Moreover, the range of the axes differs between Figure 1 and Figure 3b, and there is even a third version in Figure 7, at the supplementary material. I would appreciate it if the authors can comment on/clarify this.\n\nThe third contribution claims to \"connect augmentation to other familiar forms of regularization\". This point is addressed in Section 4.2, where the authors evaluate the performance of the models trained with data augmentation after turning off the augmentation partway through training. Although the phenomenon that performance sharply improves after turning off regularisers partway is interesting, this has been observed in previous works (reviewed by the authors) and the analysis in this paper, through the lens of affinity and diversity, does not provide new, significant insights, in my opinion. Further, taking into account the claim in the list of contributions, the analysis offers little insight about the connection of data augmentation with other forms of regularisation, beyond noting that the \"slingshot effect\" occurs also with data augmentation, which had been observed before.\n\nI would like to draw the attention to certain aspects of the methodology in this section that might distort the conclusions. For example, the authors conclude from Figure 5b that \"For some poor-performing augmentations, [switching off the augmentation] can actually bring the test accuracy above the baseline\". However, I would like to note that the authors report that in order to obtain these results, they tested multiple switch-off points and select the one that yields the best accuracy. This introduces a clear bias in Figure 5b towards best-case scenarios, which might give the impression that the switch-off lift is a general effect or, in the best case, the magnitude of the effect will be magnified. In order to gain a more accurate picture of the phenomenon, all the available data should be considered and ideally a statistical analysis should be carried out. Another source of bias in the visualisation of the results is that in Figure 5c, \"Where Switch-off Lift is negative, it is mapped to 0 on the color scale\".\n\nOn the other hand, the switch-off lift effect could be explained in simpler terms, at least partially. For example, the authors write \"Bad augmentations can become helpful if switched off\". First of all, this could occur by pure chance and be reflected in the reported results as an artifact of the selection bias pointed out in the previous paragraph. Second, we should simply think that it is expected that turning off a bad augmentation should improve the accuracy. As a matter of fact, the augmentation strategies used to illustrate this effect are `FlipUD(100%)` (I will assume this means vertical flip) and `Rotate(fixed, 20deg,100%)`. In both cases, with the augmentation on, the model does not see the original images, so an improvement is expected if suddenly the model does see them. If the final accuracy is actually above the baseline should be analysed through a statistical analysis, rather than focusing on the best case, which might be due to pure chance. Moreover, it is also worth questioning the accuracy on clean images is actually a good baseline, since this model sees fewer different images.\n\nFinally, the authors claim that \"Switch-off Lift varies with Affinity and Diversity\", from the results in Figure 5c. However, from this figure we observe that mainly, it varies with affinity. This makes sense: very low affinity is indicative of unrealistic or at least odd augmentations, which are detrimental if performed during the whole training procedure. If turned off, the model has time to fine tune on the actual images. \n\nThe temporal dynamics of training neural networks and its relation to regularisation is an interesting, active topic of research, but due to its complexity it should be analysed very rigorously in order to minimise the risk of leading ourselves astray.\n\nThe fourth contribution claims that affinity and diversity informs that \"performance is only improved when a transform increases the total number of unique training examples\". This aspect is addressed in Section 4.3. The authors \"seek to discriminate this increase in effective dataset size from other effects\". Again, this is an important and interesting topic, as well as hard to analyse, but in my opinion the methods fall short at justifying the conclusions. Here, the authors simply trained models with static augmentation and found that \"For almost all tested augmentations, using static augmentation yields lower test accuracy than the clean baseline\", which is not surprising because the augmented images differ from the original validation/test distribution on which the model is evaluated. However, this observation does not prove that the gain provided by data augmentation, when it does improve the performance, is due to the increase in the effective training set size. To be clear, this is likely to be the case, intuitively, but should be proven differently.\n\n### Cutout, SpecAugment, mixup are not discussed in terms of affinity and diversity\n\nGaining insights about the mechanisms that make some data augmentation techniques (Cutout, SpecAugment, mixup) work better than others would be an interesting contribution. In the introduction, the authors mention this as a motivation for proposing the affinity and diversity metrics. However, although (some of) these augmentation strategies are included in the experiments, there is no specific discussion about them anywhere in the paper. Having mentioned this in the introduction as a motivation for the paper, I did miss a discussion that provided new insights about how these methods work and when.\n\nSimilarly, the authors motivate the proposal of affinity and diversity as a way to better understand the mechanisms of data augmentation and discovering new techniques more efficiently. However, beyond presenting the empirical results in Section 4, the paper does not further discuss use cases of affinity and diversity to efficiently assess the value of new techniques, or analyse commonly used strategies in terms of these metrics. For example, a widely used data augmentation strategy in computer vision is the combination of horizontal flips and vertical and horizontal translations of about 10 % of the height and width. This has been found to provide large performance gains [[1, 2, 3]](#references), while additional transformations only marginally improved the accuracy. Knowing the effectiveness of this simple augmentation strategy, it would also be interesting to analyse its affinity and diversity.\n\n### Visualisation of results\n\nAlthough this aspect has not been decisive in my evaluation of the paper, I think there is room for improvement regarding the visual presentation of the results and hopefully the following feedback, from a careful read of the article, may help make the paper stronger.\n\n- Figure 3a: it would help to more clearly specify, perhaps directly in the plots, that the top row corresponds to CIFAR-10 and the bottom row to ImageNet.\n- Figures 3b and 3c: I have commented above on this figure specifically, about the possibility of changing the definitions of affinity and diversity that would improve the comparison across models and data sets and hence the interpretability of these figures. In any case, a confusing aspect of these figures is that the colour codes define different ranges of values in each plot, which have semantically very different interpretations. For instance, light green values on 3b correspond to augmentations that improve the accuracy with respect to the baseline, while the same colour on 3c correspond to augmentations that perform worse than the baseline. Given that there is a clear central point in the colour code, zero, where the semantic interpretation changes (positive vs. negative), I would strongly suggest to use a [perceptually uniform diverging colour palette](https://seaborn.pydata.org/tutorial/color_palettes.html#perceptually-uniform-divering-palettes).\n- Figure 4: I would suggest to colour-code the dots to reflect the probability of rotation. It would reduce the cognitive load to interpret the figure.\n- Figure 5a: the legends could be placed inside the axes to make space for larger figures\n- Figure 5b: indicate what the dash line represents\n\n## Questions\n\nThe questions I list below are mainly intended to raise awareness about relatively minor aspects of the paper that remained unclear to me while reading it, with the aim of providing constructive feedback to potentially improve the manuscript. The aspects that have been more decisive in my decision have been already commented above.\n\n- \"Random crop was also applied on all ImageNet models.\": Why is not random crop considered data augmentation in this paper?\n- How would the authors explain the strange variation of diversity with respect to the probability of rotation in Figure 4 centre? Is this a general behaviour in other augmentation strategies? Is the affinity as clear in other cases?\n- Figure 5c: how is it possible to get an improvement (Switch-Off Lift) of 50 %, while in Figure 5b all cases show smaller variation?\n- The specific version of the wide residual network reported in the paper is \"WRN 28-2\" However, 28-2 is not described in the original WRN paper, and is also not described in the github repository of AutoAugment. I suppose that it should instead read WRN 28-10. Assuming this is the case, I have an additional question: WRN 28-10 achieves, according to my own implementation, around 91.5 % accuracy on CIFAR-10 without any data augmentation, using the hyperparameters and regularisation of the original paper. If I interpret correctly Figure 5b, the baseline accuracy achieved by the authors is 89.7, which is significantly lower. I would appreciate it if the authors could clarify what I may have misunderstood.\n- In Section 4.3, what is the goal of comparing models trained with static vs dynamic data augmentation? How would models trained with static augmentation be better or have larger diversity?\n- \"transforms and hyperparameters from the AutoAugment search space [...] implicitly have high Affinity\": this is not what we see in the Figure 3b and Figure 7. Could the authors clarify this statement?\n\n## Minor comments and potential typos identified\n\n- \"some have proposed that augmentation strategies are effective because they increase the diversity of images seen by the model\": any reference where this claim is made?\n- Make sure that \"dataset\"/\"data set\" are spelled consistency throughout the article.\n- Would the authors venture any guess about the affinity and diversity of strategies in other data domains?\n+ I appreciate that the authors report (Section 3) details about how the test results were computed, including the standard errors\n- \"static training\" (Section 3): Do the authors mean \"static augmentation\"? Also, consider giving an example for better illustration.\n- It is slightly confusing that the augmentation function is denoted by $a$ in Definition 1, which is an unusual choice for a function. Would a capital letter be a better choice, since augmentations are generally stochastic functions?\n- \"KL divergence of the shifted data with respect to the original data\": consider specifying this mathematically too\n- What is the reason for the capitalisation of _Affinity_ and _Diversity_?\n- Typo: \"model-dependant\" (Section 3.1)\n- The gap between Figure 5's caption and the paragraph seems to have been manually reduced. If this is the case, it may be against the formatting guidelines and, especially, it hinders the readability.\n- Some data augmentation strategies are mentioned without previously introducing them. For example, `FlipUD` in Section 4.2\n\n## References\n\n[1] Hern\u00e1ndez-Garc\u00eda, Alex, and Peter K\u00f6nig. \"Data augmentation instead of explicit regularization.\" arXiv preprint arXiv:1806.03852, 2018.\n\n[2] Goodfellow, Ian, et al. \"Maxout networks.\" International conference on machine learning. PMLR, 2013.\n\n[3] Springenberg, Jost Tobias, et al. \"Striving for simplicity: The all convolutional net.\" arXiv preprint arXiv:1412.6806, 2014.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123027, "tmdate": 1606915769842, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1249/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Review"}}}, {"id": "V7Wy0Mpsz7Q", "original": null, "number": 10, "cdate": 1606155430422, "ddate": null, "tcdate": 1606155430422, "tmdate": 1606155430422, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "-kVTM-9UheE", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "Upgraded score, though main concerns remain", "comment": "I sincerely appreciate the effort and time the authors have employed in addressing most of the concerns of my (rather long) review, as well as updating the manuscript. I am glad if the authors agreed with some of my suggestions. In particular, I positively value the improvements regarding clarity, especially the figures. For these reasons, I have upgraded my evaluation of the submission in one point. Unfortunately, I still assess the manuscript marginally below the acceptance threshold, since my concerns regarding the definitions of affinity and diversity remain, as well as how these metrics shed light to our understanding of data augmentation. I think that my views on these issues was largely explained in my original review, but I will summarise it below, taking into account the responses of the authors.\n\nFirst of all, as stated in my original review, I see a lot of potential in the proposal of such metrics to quantify the contribution of data augmentation on generalisation. I think that the notions of affinity and diversity, as intuitively depicted in Figure 1c (of the updated submission), are meaningful and  potentially useful, at least to my intuition. However, in particular the definition of diversity as the final loss of a model, diverges from the notion reflected in Figure 1c. Furthermore, it is computational expensive (although the authors have studied a less costly alternative; see below) and of different nature than the affinity.\n\nIn their response, the authors have mentioned some new results from studying \"early diversity\", that is the loss computed after a few epochs, which is correlated with the final loss. This is interesting, but it introduces additional problems: Is the difference with respect to the final loss negligible? How many epochs are acceptable for computing the early diversity? I humbly think that an alternative may be to re-defining the diversity, more aligned with the affinity and especially with the notion from Figure 1c. I am willing to elaborate on the suggestions made in my original review, in case the authors find them useful, and discuss these ideas in a separate thread.\n\nRegarding my concern that the claim that \"performance is dependent on both metrics\", the authors have introduced a new result, the count of pairs of augmentations for which \"if Acc(a) > Acc(a') then either Aff(a) > Aff(a') or Div(a) > Div(a')\". However, if the claim is that performance depends on both metrics, should the condition not be Aff(a) > Aff(a') AND Div(a) > Div(a'), that is AND, not OR?\n\nAnother concern I raised was that the phenomenon of improved performance after turning off data augmentation (Section 4.2) is only vaguely connected to affinity and diversity. In their response, the authors have resolved some of my questions about the methodology in this section, but the main concern remains. Furthermore, the authors have not addressed my concerns regarding the fourth contribution, that the analysis in Section 4.3 falls short at answering the question of whether the gain provided by data augmentation is due to an increase in the effective training size, or by what extent.\n\nThe authors have provided some insights about how affinity and diversity may help us understand some augmentation strategies such as mixup, SpecAugment and AutoAugment. I appreciate this, but I think it would be interesting to discuss this in the paper, as  it is one of the contributions claimed in the introduction.\n\nFinally, some unrelated comments:\n\n* I agree that \"random crop is integrated into the data loading pipeline, which makes it hard to disentangle it as a separate augmentation\". And the fact that training without random crop yields bad results speaks for the importance of data augmentation, diversity, in this case. However, not considering leaving random crop as part of the training process of the baseline models, without considering it data augmentation, may strongly distort the metrics. Actually, the little improvement of in test accuracy by augmentations on ImageNet (Figure 3c) could be explained by the fact that the random crops are providing most of the data augmentation-related gains.\n* I personally prefer the diverging colour scheme of Figure 18 top.\n* In Figure 1, 1b has a grid, 1a not.\n* Absolute vs relative: \"many researchers have developed intuition for ImageNet and CIFAR10 raw accuracies\": that's right, but it doesn't mean it's the best approach, is it?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "3W2jBznBQn", "original": null, "number": 9, "cdate": 1605754757061, "ddate": null, "tcdate": 1605754757061, "tmdate": 1605754797143, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "zH_H14l31I", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "Re: Relevant topic, interesting ideas; proposal and methods can be polished (Part 4)", "comment": "_Figure 5c: how is it possible to get an improvement (Switch-Off Lift) of 50 %, while in Figure 5b all cases show smaller variation?_\n\nThe x-axis in Figure 5b uses a symlog scaling, with linear threshold -1 and +1 (now indicated in the Figure with grey dashed lines. The left-most point represents a Switching Lift of +50. We realize that in the original Figure, the x-axis was missing a label for the left-most point (Aug On accuracy 39%), that is now indicated in the figure as well. Thanks for catching this!\n\n_The network reported in the paper is WRN 28-2 \u2026 should [it] instead read WRN 28-10. _\n\nThe model we use is WRN 28-2. In the original wide resnet paper, the authors introduce the notation WRN-n-k to denote a residual network that has a total number of convolutional layers n and a widening factor k.\n\n_In Section 4.3, what is the goal of comparing models trained with static vs dynamic data augmentation? How would models trained with static augmentation be better or have larger diversity?_\n\nThe goal of training models with static augmentation is to see whether broadening the distribution off the strict training manifold without introducing additional samples provides any benefit (relative to un-augmented data). The goal of comparing static and dynamic augmentations to each other is not because we expect static augmentats to be better or more diverse, but to quantify the boost in diversity that the stochastic policy has relative to the fixed policy.\n\n_\"transforms and hyperparameters from the AutoAugment search space [...] implicitly have high Affinity\": this is not what we see in the Figure 3b and Figure 7. Could the authors clarify this statement?_\n\nWe thank the referee for bringing this up, we changed this in the text. We agree that the AutoAugment policy is not necessarily high Affinity compared to all the ops we considered. We wanted to make the point that the while Adversarial AutoAugment method aims to increase diversity, this aim alone cannot lead to effective augmentation strategies. For example, setting the pixels of all training images to black would maximize the training loss (diversity), but would lead to terrible generalization. However, Adversarial AutoAugment tries to maximize diversity while restricted to AutoAugment ops and hyperparameters, which are higher affinity than a random transformation one can apply to an image. For example, the range of rotations in AutoAugment is set to be -30 to 30 degrees, which is higher Affinity than a different range such as 30 to 60 degrees or -60 to 60 degrees. However, the referee is correct that AutoAugment ops are not necessarily as high affinity as the other high affinity operations such as horizontal flips.\n\n### Minor comments and potential typos identified\n\n_\"some have proposed that augmentation strategies are effective because they increase the diversity of images seen by the model\": any reference where this claim is made?_\n\nOne classic reference is Simard et. al. 2003\n\n_Would the authors venture any guess about the affinity and diversity of strategies in other data domains?_\n\nAt the level of speculation, we expect a similar tradeoff in other domains. SpecAugment is one concrete example in speech recognition which we expect to be the analogue of a high diversity lower affinity augmentation -- the authors note the increase in training loss and out of distribution character. In fact, one of our long term motivations for attempting to find well defined metrics for augmentation policies is to hopefully port the success of augmentations in image understanding to domains which have yet to realize such big gains. This is an active direction of future work.\n\n_It is slightly confusing that the augmentation function is denoted by  a  in Definition 1, which is an unusual choice for a function. Would a capital letter be a better choice, since augmentations are generally stochastic functions?_\n\nWe wanted to avoid the confusion created by using capital A for both Affinity and augmentation.\n\n_What is the reason for the capitalisation of Affinity and Diversity?_\n\nWe wanted to introduce precise terms with sharp definitions, and did not want the terms to be confused with their meaning in colloquial language. \n\n### Typos and other minor comments\n\nThanks for catching these, we have implemented them in the revised text.\n\nThank you again for your feedback! Are there any other concerns we didn\u2019t address?\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "zH_H14l31I", "original": null, "number": 8, "cdate": 1605754616844, "ddate": null, "tcdate": 1605754616844, "tmdate": 1605754784837, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "cteDXNkeH", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "Re: Relevant topic, interesting ideas; proposal and methods can be polished (Part 3)", "comment": "_The authors conclude from Figure 5b that \"For some poor-performing augmentations, [switching off the augmentation] can actually bring the test accuracy above the baseline\". However, I would like to note that the authors report that in order to obtain these results, they tested multiple switch-off points and select the one that yields the best accuracy. This introduces a clear bias._\n\nWe apologize if this was not clear and believe the reviewer may be misunderstanding our claim. As described in supplementary section D, we follow the standard procedure of tuning a hyperparameter (the time at which to switch off an augmentation policy) on a held out validation set and then report the corresponding test performance. The standard error on the mean over the ten independent runs are so small as to not be visible in figure 5a. This has now been clarified in the caption to Figure 5. This clearly demonstrates that there exist cases where switching off a poor-performing augmentation policy can improve over baseline performance. To make this more clear we have also included a table in supplementary section D with the performance for standard and optimally switched augmentation strategies as well as the std error on the mean for all augmentations studied 43% show a benefit from switching at at least 2X the standard error, 47% show a change relative to standard augmentation within +/- 2X the standard error, and 10% show a decrease. \n\nWe make no claim that switching of a bad augmentation is beneficial if done at an arbitrary time point during training. If published, we will release a link to the full data containing performance for these sub-optimal switching times.\n\n_we should simply think that it is expected that turning off a bad augmentation should improve the accuracy._\n\nThe interesting phenomena is not that the performance improves, but that it improves over baseline performance. We found 29 examples of this and 20 where the improvement over the baseline was greater than 2X the standard error.\n\n_If the final accuracy is actually above the baseline should be analysed through a statistical analysis, rather than focusing on the best case, which might be due to pure chance._\n\nAs discussed above, selection was performed on a held out validation set and performance is statistically significantly above the baseline, inconsistent with arising from statistical fluctuations. \n\n_Visualisation of results: hopefully the following feedback, from a careful read of the article, may help make the paper stronger._\n\nThank you for the feedback. We have incorporated all of these suggestions into the revised manuscript and agree it makes the paper stronger. In particular, Figures 1, 3, and 4 now use a color scheme with a consistent zero. As mentioned above, for the rescaled Affinity and Diversity definitions we have provided sample figures in supplementary section H. We have also provided examples with different color schemes (Fig 18) and would appreciate any feedback from the reviewers on preferences.\n\n### Questions\n_\"Random crop was also applied on all ImageNet models.\": Why is not random crop considered data augmentation in this paper?_\n\nIn common ImageNet infrastructures, random crop is integrated into the data loading pipeline, which makes it hard to disentangle it as a separate augmentation. Furthermore, models trained on ImageNet without random crop perform very poorly. In order to analyze the effect of augmentations against a more realistic baseline and due to the infrastructure constraints, we chose to keep random crop preprocessing.\n\n_How would the authors explain the strange variation of diversity with respect to the probability of rotation in Figure 4 centre? Is this a general behaviour in other augmentation strategies? Is the affinity as clear in other cases?_\n\nWe assume the reviewer is asking about the lack of monotonicity of Diversity with respect to augmentation probability. We do not have a deep understanding of this behavior, nor do we have an argument that training loss should be strictly monotonic in augmentation probability. Some degree of non-monotonicity appears common across augmentations. \n\nFor affinity there is a simple argument that ensures monotonicity with probability -- the affinity of a policy applying an augmentation with probability p takes the form A = p A_on + (1-p) A_off = p A_on, where A_on is the affinity with the augmentation applied with probability 1 and A_off is the affinity with no augmentation (which is equal to zero). This ensures the monotonicity for these single probability augmentations."}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "cteDXNkeH", "original": null, "number": 7, "cdate": 1605754434061, "ddate": null, "tcdate": 1605754434061, "tmdate": 1605754434061, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "-kVTM-9UheE", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "Re: Relevant topic, interesting ideas; proposal and methods can be polished (Part 2)", "comment": "_the paper does not really discuss how affinity and diversity explain the mechanisms of [SpecAugment, mixup, and AutoAugment] augmentation strategies and how they can be used to discover new strategies more efficiently._\n\nWe believe that our analysis sheds light on the mechanisms of vision augmentations, such as mixup, Cutout, and AutoAugment in several ways. First of all, it is worth noting that many of the important methods that built on AutoAugment focused on one of our proposed metrics. For example, Fast AutoAugment and CTAugment focus on increasing the Affinity of an AutoAugment policy, whereas Adversarial AutoAugment focuses on increasing the diversity of an AutoAugment policy. Our work is the first one to propose and study these two metrics generally, which helps put previous work in context. Furthermore, our work suggests that further improvements can be expected if one tries to increase both of the metrics, instead of just one of them as was the case in previous work. Given our work, an obvious strategy to find better policies would be to combine the losses of Adversarial AutoAugment and Fast AutoAugment, which we plan to do in future work. Finally, and knowing our insights from the vision domain, we speculate that a similar mechanism might be at play in other domains, such as speech recognition, and underlie why strategies such as SpecAugment, which appear to be relatively out of distribution, nonetheless work -- we predict that they drastically increase diversity.\n\n_combination of horizontal flips and vertical and horizontal translations ... has been found to provide large performance gains [1, 2, 3]_\n\nWe thank you for bringing this up. We actually have included this combination of augmentations, we call them FlipLR+Crop (because this operation on CIFAR-10 is commonly called Flips and Pad-and-Crop, which is the same operation you mention as horizontal flips and vertical and horizontal translations). It is worth noting that these two augmentations are two of the highest Affinity augmentations, which might explain why they have been so commonly used on CIFAR-10 and CIFAR-100. Below we list accuracy, Affinity, and Diversity values for the combination of these two augmentations. We have also added these references to the paper. \n\nAcc: 94.6%, Affinity: -2.97, Diversity: 0.105\n\n_The claim that the performance gain introduced by an augmentation strategy increases when both the affinity and the diversity increase is well supported by the results in Figure 3b (WRN on CIFAR-10), but this is not so clear in Figure 3c (ResNet on ImageNet)._\n\nThanks for raising this point. One shortcoming of the presentation in figures 3b and 3c is that it relies on visual clarity rather than a sharp metric for the take-away message.\n\nTo make this more precise we have now included a measure of the fraction of pairs of augmentations (a,a\u2019) satisfying the condition that if Acc(a) > Acc(a\u2019) then either Aff(a) > Aff(a\u2019) or Div(a) > Div(a\u2019). The ImageNet plot (Figure 3c) satisfies this for 97.5% of all pairs (compared with 99.1% of pairs for CIFAR-10 and 75% for an uncorrelated null model).\n\n_the authors select one [set of results] for Figure 1_\n\nWe have now included both CIFAR-10 and ImageNet results in Figure 1\n\n_the range of the axes differs between Figure 1 and Figure 3b_\n\nFigure 3b includes the modern augmentation policies, RandAugment, mixup, and AutoAugment, while Figure 1 does not. We adjusted the axis to accommodate these exceptionally high Affinity/Diversity augmentations."}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "-kVTM-9UheE", "original": null, "number": 6, "cdate": 1605754299068, "ddate": null, "tcdate": 1605754299068, "tmdate": 1605754299068, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "u5VDDAi_qVt", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "Re: Relevant topic, interesting ideas; proposal and methods can be polished (Part 1)", "comment": "We thank the reviewer for taking the time to provide an exceptionally in depth review of our work. Though we do not agree with all points (see below for some push back), we do feel the reviewer has raised some important points which have helped to strengthen the manuscript.\n\n_diversity is computationally costly as it requires re-training the model_\n\nYou are absolutely correct that evaluating diversity as defined in equation (2) for each augmentation policy requires training a new model and is costly. For this reason, in the paper we introduced an alternative measure of diversity, the entropy of an augmentation, this has the advantage that it can be computed without training any model, but has the disadvantage of being ill defined for continuous augmentations.\n\nTo remedy this shortcoming further, we have included an alternative computationally less-expensive proxy for diversity, shown in in Figures 10 and 11. We found that the training loss at a much earlier epoch correlates well with the final training loss and so can serve as an alternative measure of diversity. This still requires training a model for each augmentation policy, but only for a small fraction of the training time. In particular the results shown in Fig 10 are for a model trained for 10 epochs, a 95% reduction in computational cost.\n\nThe reviewer also proposed an alternative interesting, inexpensive candidate diversity measure:\n\n_one idea would be to define the diversity as a measure of the spread (standard deviation, variance). Have the authors considered defining the diversity along the lines of the variance of the affinity or of a related quantity?_\n\nThank you for the suggestion, we had not considered this, and its computational efficiency would be appealing. We were not sure if the review was suggesting variance over model initializations, or over instances of the augmented dataset. We assumed the latter, but for completeness, following your suggestion, we have implemented this measure with variance computed over model reinitializations and are currently measuring variance over dataset instances (via batches). Unfortunately, the variance over initializations does not correlate well with the other diversity measures. We have updated the manuscript with the model variance measurements (supplementary section H) and will upload a revised version when the data variance is complete. \n\n_the dependence on the specific model and data set makes the metrics hard to compare across models and data sets_\n\nIt is definitely true that the metrics vary between datasets. One way to address the variation of the metric ranges is to normalize as you have suggested. We are happy to do this and have discussed below. \n\nA higher level point is that variation in the relative ordering of augmentations between datasets is expected and a feature of this approach. Even within image classification It is not true that augmentation strategies are equally effective from dataset to dataset. For example, the AutoAugment policy found on SVHN did not perform well on CIFAR-10 (see AutoAugment and Unsupervised Data Augmentation), and Cutout does not perform well on ImageNet while being very effective on CIFAR-10 and SVHN (see Cutout, Patch Gaussian).  It is reassuring that this is reflected in Affinity and Diversity. For a fixed dataset, we expect the relative ordering of augmentations to be relatively stable to model details. This assumption has been used with success (e.g. see AutoAugment, Population Based Augmentation, Fast AutoAugment) to identify successful augmentation strategies on one architecture and use them on another.\n\n_One reason why the metrics are not comparable is that they are defined in absolute terms \u2026 A suggestion would be to define the metrics in relative terms instead_\n\nThank you for the suggestion. We initially opted for basing Affinity off of absolute accuracies as many researchers have developed intuition for ImageNet and CIFAR10 raw accuracies. Nonetheless, we do appreciate the value in the suggested relative metrics! We have presented example plots in supplementary section H based off of normalized Affinity and Diversity and would love to hear from reviewers which normalization they prefer! \n\n_affinity and diversity are defined in very different ways \u2026 these are very different, unrelated quantities_\n\nAffinity is the performance (measured by accuracy) of a clean model on an augmented test set. Diversity is the performance (measured by loss) of an augmented model on the augmented train set. Though a bit less convenient, we can consider using loss, rather than accuracy to measure affinity. In this case, Affinity and Diversity are related by exchanging train and test set, and clean and augmented models."}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "Qr6_O-zudQS", "original": null, "number": 4, "cdate": 1605753722497, "ddate": null, "tcdate": 1605753722497, "tmdate": 1605753722497, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "aVBrhnK9bM4", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment", "content": {"title": "Re: The paper introduces novel concepts to make the effect of data augmentation predictable. ", "comment": "We thank the reviewer for taking the time to read our work and for providing constructive comments. We believe the feedback regarding computational cost has especially improved the manuscript! \n\n_It seems it takes significant time to compute the proposed affinity and diversity scores. \u2026 It will be good if the authors can add experiments or discussions related to efficiency in runtime._\n\nThis is a good question and related to points raised by other reviewers.\n\nWe have included an expanded discussion of the computational cost of these metrics in section 3.2 and appendix E. Affinity is relatively inexpensive to compute. Affinity requires training a model on clean data a single time. Diversity as defined in equation (2) is indeed expensive to compute, it requires training an independent model on each potential augmentation. For this reason in Appendix E we proposed an alternative proxy for diversity, the entropy of an augmentation. This correlates well with Diversity, but has the advantage of not requiring any additional training. The downside is that entropy is ill defined for continuous augmentations.\n\nTo remedy this shortcoming further, we have included an alternative computationally less-expensive proxy for diversity, shown in in Figures 10 and 11. We found that the training loss at a much earlier epoch correlates well with the final training loss and so can serve as an alternative measure of diversity. This still requires training a model for each augmentation policy, but only for a small fraction of the training time. In particular the results shown in Fig 10 are for a model trained for 10 epochs, a 95% reduction in computational cost.\n\nIt is also worth noting that in many cases good augmentation strategies generalize across models. This reusability can help alleviate the initial cost of finding a successful strategy.\n\n_Or the authors can extend the use cases beyond model performance prediction._\n\nAside from performance prediction, we can envision that this analysis may be useful more directly when searching for better augmentation policies, especially in new domains. Although this is outside the scope of this work, we are interested in exploring this question. An early finding by Zhang et al. (2019) shows that when an augmentation strategy is guided by an increase in Diversity, performance is greatly improved. We are also optimistic that these metrics will serve as interesting ways to characterize distribution shift of the underlying input data independent of direct concerns of model performance. \n\nThank you again for your feedback! Are there any other concerns we didn\u2019t address?"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ZcKPWuhG6wy", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1249/Authors|ICLR.cc/2021/Conference/Paper1249/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923861883, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Comment"}}}, {"id": "aVBrhnK9bM4", "original": null, "number": 3, "cdate": 1603785269213, "ddate": null, "tcdate": 1603785269213, "tmdate": 1605024491298, "tddate": null, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "invitation": "ICLR.cc/2021/Conference/Paper1249/-/Official_Review", "content": {"title": "The paper introduces novel concepts to make the effect of data augmentation predictable. ", "review": "##########################################################################\n\nSummary:\n\nThis paper studies the problem of data augmentation that obtains new training examples by modifying existing ones. Data augmentation is popular in machine learning and artificial intelligence since it enhances the number of training examples. However, its effect on model performance remains unknown in practice. An augmentation operator (e.g. image rotation) can be either helpful or harmful. This paper introduces two novel metrics, named affinity and diversity, to quantify the effect of any given augmentation operator. The authors find that an operator with high affinity score and high diversity score leads to the best performance improvement. \n\n\n##########################################################################\n\nReasons for score: \n\nOverall, I like the idea of this paper about making the effects of augmentation operators tractable. The proposed affinity and diversity scores are great indicators to evaluate the usefulness of an arbitrary operator. The finding that higher scores are better is insightful. With that, practitioners are able to inspect a large number of operators and select the most effective ones. \n\nHowever, the computation of affinity and diversity scores may be very expensive. The runtime could be more than that runs an operator directly to get the performance improvement. It will be good if the authors can compare the efficiency and/or share some comments in the rebuttal.\n\n##########################################################################\n\nPros:\n\n1. The paper proposes a novel idea of quantifying the effect of data augmentation. Specifically, the idea introduces two metrics, affinity and diversity scores, to evaluate any given augmentation operator in its effect on model performance. The experiments showed that higher the scores, better the performance improvement. The finding is novel that should be the first time in the field. \n\n2. The paper justifies that either affinity or diversity alone does not predict model performance. They together can make the prediction deterministic. The finding suggests that it is nontrivial to quantify the effect of augmentation operators. The paper nicely leverages heat maps as shown in Figure 3(b) and 3(c) to reveal the finding. \n\n\n##########################################################################\n\nCons: \n\nI have a concern about computation efficiency. It seems it takes significant time to compute the proposed affinity and diversity scores. For a given operator, Definition 1 regarding affinity requires running the model once. Similarly, Definition 2 regarding diversity requires running the model another time. So it needs to run the model twice to get affinity and diversity scores. Empirically, we can just run the model once to calculate the actual model improvement of the operator. If so, it is inefficient to use affinity and diversity scores for the purpose of predicting model performance. \n\n\n##########################################################################\n\nQuestions during rebuttal: \n\nIt will be good if the authors can add experiments or discussions related to efficiency in runtime. Or the authors can extend the use cases beyond model performance prediction.  ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1249/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1249/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Tradeoffs in Data Augmentation: An Empirical Study", "authorids": ["~Raphael_Gontijo-Lopes1", "smullin-physics@stanfordalumni.org", "~Ekin_Dogus_Cubuk1", "~Ethan_Dyer1"], "authors": ["Raphael Gontijo-Lopes", "Sylvia Smullin", "Ekin Dogus Cubuk", "Ethan Dyer"], "keywords": ["Generalization", "Interpretability", "Understanding Data Augmentation"], "abstract": "Though data augmentation has become a standard component of deep neural network training, the underlying mechanism behind the effectiveness of these techniques remains poorly understood. In practice, augmentation policies are often chosen using heuristics of distribution shift or augmentation diversity. Inspired by these, we conduct an empirical study to quantify how data augmentation improves model generalization. We introduce two interpretable and easy-to-compute measures: Affinity and Diversity. We find that augmentation performance is predicted not by either of these alone but by jointly optimizing the two.", "one-sentence_summary": "We quantify mechanisms of how data augmentation works with two metrics we introduce: Affinity and Diversity.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gontijolopes|tradeoffs_in_data_augmentation_an_empirical_study", "supplementary_material": "/attachment/8940e1a67ab0f0b5f9c66643e38292a844448f30.zip", "pdf": "/pdf/3d2fc5aa6c81e18581fdf14b971478263093ec81.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngontijo-lopes2021tradeoffs,\ntitle={Tradeoffs in Data Augmentation: An Empirical Study},\nauthor={Raphael Gontijo-Lopes and Sylvia Smullin and Ekin Dogus Cubuk and Ethan Dyer},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ZcKPWuhG6wy}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ZcKPWuhG6wy", "replyto": "ZcKPWuhG6wy", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1249/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538123027, "tmdate": 1606915769842, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1249/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1249/-/Official_Review"}}}], "count": 18}