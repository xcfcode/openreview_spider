{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124433175, "tcdate": 1518460374069, "number": 197, "cdate": 1518460374069, "id": "HJCRT81DM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HJCRT81DM", "signatures": ["~Po-Sen_Huang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582681305, "tcdate": 1520726502317, "number": 1, "cdate": 1520726502317, "id": "rkC1GgfKG", "invitation": "ICLR.cc/2018/Workshop/-/Paper197/Official_Review", "forum": "HJCRT81DM", "replyto": "HJCRT81DM", "signatures": ["ICLR.cc/2018/Workshop/Paper197/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper197/AnonReviewer1"], "content": {"title": "Uses MCTS + RL for knowledge base completion", "rating": "6: Marginally above acceptance threshold", "review": "This paper uses MCTS + RL for answering queries on a knowledge base graph. The contribution of this paper is to do planning using MCTS instead of policy gradient. This indeed is important in the sparse reward setting and leads to better peformance. \n1. Given that this approach is so close to Minerva, please explicitly contrast against it at the beginning of the paper. The basic explanation is hurried and not very clear. \n2. In the line, \"we use the following formula to calculate the score for each unique candidate n. ..\" Please justify this. Isn't there a more principled way to address this issue?\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582681125, "id": "ICLR.cc/2018/Workshop/-/Paper197/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper197/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper197/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper197/AnonReviewer3"], "reply": {"forum": "HJCRT81DM", "replyto": "HJCRT81DM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper197/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper197/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582681125}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582641920, "tcdate": 1520806870425, "number": 2, "cdate": 1520806870425, "id": "r1RAiQmKf", "invitation": "ICLR.cc/2018/Workshop/-/Paper197/Official_Review", "forum": "HJCRT81DM", "replyto": "HJCRT81DM", "signatures": ["ICLR.cc/2018/Workshop/Paper197/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper197/AnonReviewer3"], "content": {"title": "Cool application of PG + MCTS to KB reasoning, potentially not novel enough", "rating": "5: Marginally below acceptance threshold", "review": "This paper introduces a method which combines Policy Gradient (or other model-free techniques such as A2C) with model-based reasoning powered by MCTS. Since the RL dynamics are known and deterministic MCTS naturally improves performance (much like in e.g. AlphaGo). The idea of using MCTS + deep RL is far from new, so the originality of this work is, in my opinion, quite weak. The only novel idea is how to deal with cycles in the knowledge graph traversal, and the fix is quite trivial. \n\nThe application and good empirical results on tough KB reasoning datasets is quite good however. \n\nOne important note: I disagree with the authors that this is a \"partially observable\" MDP, just because you don't base your decision on the current node doesn't mean the other nodes are unobserved -- they are observed just not included in your state vector. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582681125, "id": "ICLR.cc/2018/Workshop/-/Paper197/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper197/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper197/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper197/AnonReviewer3"], "reply": {"forum": "HJCRT81DM", "replyto": "HJCRT81DM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper197/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper197/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582681125}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573579301, "tcdate": 1521573579301, "number": 154, "cdate": 1521573578964, "id": "SJ7RRA0KM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HJCRT81DM", "replyto": "HJCRT81DM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1520374585865, "tcdate": 1520374434325, "number": 5, "cdate": 1520374434325, "id": "HJ5oGcndM", "invitation": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "forum": "HJCRT81DM", "replyto": "HJ7m3xiOf", "signatures": ["~Sachin_Rajoria2"], "readers": ["everyone"], "writers": ["~Sachin_Rajoria2"], "content": {"title": "Thank you", "comment": "Hi Yelong, Thank you so much for the detailed response and confirming my belief that only difference MINERVA and ReinforceWalk is the inference algorithm. I guess there is no mathematically guaranteed superiority of one inference technique over the other between REINFORCE based policy gradient inference in MINERVA and standard MCTS based inference in ReinforceWalk. The release of the code would be super helpful, I can try out both for my application and maybe MCTS based approach works out!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625087, "id": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper197/Reviewers"], "reply": {"replyto": null, "forum": "HJCRT81DM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625087}}}, {"tddate": null, "ddate": null, "tmdate": 1520288653486, "tcdate": 1520270362749, "number": 4, "cdate": 1520270362749, "id": "HJ7m3xiOf", "invitation": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "forum": "HJCRT81DM", "replyto": "rkROM8cdf", "signatures": ["~yelong_shen1"], "readers": ["everyone"], "writers": ["~yelong_shen1"], "content": {"title": "Reply to \"Please help and explain a new comer in the field\"", "comment": "Hi Sachin,  \"DeepPath\" and \"MINERVA\" are recently two approaches, which utilize RL algorithms for KB Reasoning task.  ReinforceWalk uses the same problem formulation as in \"MINERVA\". With slightly different model architectures of \"MINERVA\" and \"ReinforceWalk\", ReinforceWalk tries to target sparse reward issue, and incorporate planning mechanism in the graph walk task.  We compared \"RW\" and \"PG\" ( policy gradient : which is the training algorithm in MINERVA) in our experiments to show the advantage of mcts and planning. We also compare RW and PG in several datasets in our arxiv preprint.  It could be great if you could let us know which other datasets you are interested to give a comparison with \"RW\" and \"MINERVA\".  Meanwhile, we are going to release the source code in the next month, you could also be free to try it out. \n "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625087, "id": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper197/Reviewers"], "reply": {"replyto": null, "forum": "HJCRT81DM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625087}}}, {"tddate": null, "ddate": null, "tmdate": 1520226933599, "tcdate": 1520226933599, "number": 3, "cdate": 1520226933599, "id": "rkROM8cdf", "invitation": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "forum": "HJCRT81DM", "replyto": "HJCRT81DM", "signatures": ["~Sachin_Rajoria2"], "readers": ["everyone"], "writers": ["~Sachin_Rajoria2"], "content": {"title": "Please help and explain a new comer in the field", "comment": "I am trying to learn about the field of reasoning over KBs. I have read papers using methods like classical PRA, tensor factorization approaches, logic-based approaches, etc. I recently read the MINERVA paper from the main ICLR 2018 conference and now ReinforceWalk. According to my understanding, the MINERVA paper took a novel approach and framed the QA on KB as RL on a graph constructed from KB, and the agent was trained to navigate the environment using REINFORCE algorithm. Please correct me if I am wrong, but the problem formulation in terms of RL in ReinforceWalk seems to be exactly same as MINERVA, and the training was carried out using standard MCTS instead of REINFORCE. Can you please explain if I am missing anything? In case the difference between MINERVA and ReinforceWalk is only in the training algorithm, it would be more convincing if there are experimental evaluations over many datasets to showcase advantage/disadvantage of one training algorithm over the other or to showcase in which setting MCTS is better and in which REINFORCE is. Right now I am not clear which one to use for my application. I would be grateful for your response and would be very helpful in my learning of the field."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625087, "id": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper197/Reviewers"], "reply": {"replyto": null, "forum": "HJCRT81DM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625087}}}, {"tddate": null, "ddate": null, "tmdate": 1520219633859, "tcdate": 1520219633859, "number": 2, "cdate": 1520219633859, "id": "rJqgUEquM", "invitation": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "forum": "HJCRT81DM", "replyto": "HJfldXYuf", "signatures": ["~Po-Sen_Huang1"], "readers": ["everyone"], "writers": ["~Po-Sen_Huang1"], "content": {"title": "Reply to \"MINERVA results\"", "comment": "Thanks for the update! We got the MINERVA baselines in our paper by running the source code (and some details shared by your colleagues) you released earlier. We are now re-running experiments based on the codes you recently updated, and we will revise our report of MINERVA once the new results are confirmed. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625087, "id": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper197/Reviewers"], "reply": {"replyto": null, "forum": "HJCRT81DM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625087}}}, {"tddate": null, "ddate": null, "tmdate": 1520150553269, "tcdate": 1520150505799, "number": 1, "cdate": 1520150505799, "id": "HJfldXYuf", "invitation": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "forum": "HJCRT81DM", "replyto": "HJCRT81DM", "signatures": ["~Rajarshi_Das1"], "readers": ["everyone"], "writers": ["~Rajarshi_Das1"], "content": {"title": "MINERVA results", "comment": "Hi!,\n\nDisclosure: I am one of the authors of MINERVA.\n\nThis is a nice idea and it is good to see it performs well!.\n\n I wanted to comment on the performance of MINERVA^{a} in Table 1. We also tried MINERVA in the same reported setting (training per-relation models) and we got significantly different results. We have updated our paper (https://openreview.net/pdf?id=By1ZYf-0b) with the latest scores (Table 5, column 3). These results are very close to what RW gets. We would be happy to help figure out what is the source of the discrepancy.\n\nRajarshi"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search", "abstract": "We consider the problem of learning to walk over a graph towards a target node for a given input query and a source node (e.g., knowledge graph reasoning). We propose a new method called ReinforceWalk, which consists of a deep recurrent neural network (RNN) and a Monte Carlo Tree Search (MCTS). The RNN encodes the history of observations and map it into the Q-value, the policy and the state value. The MCTS is combined with the RNN policy to generate trajectories with more positive rewards, overcoming the sparse reward problem. Then, the RNN policy is updated in an off-policy manner from these trajectories. ReinforceWalk repeats these steps to learn the policy. At testing stage, the MCTS is also combined with the RNN to predict the target node with higher accuracy. Experiment results show that we are able to learn better policies from less number of rollouts compared to other methods, which are mainly based on policy gradient method.", "paperhash": "shen|reinforcewalk_learning_to_walk_in_graph_with_monte_carlo_tree_search", "keywords": ["Monte Carlo Tree Search", "knowledge graph", "reinforcement learning"], "_bibtex": "@misc{\n  shen2018reinforcewalk:,\n  title={ReinforceWalk: Learning to Walk in Graph with Monte Carlo Tree Search},\n  author={Yelong Shen and Jianshu Chen and Po-Sen Huang and Yuqing Guo and Jianfeng Gao},\n  year={2018},\n  url={https://openreview.net/forum?id=HJCRT81DM}\n}", "authorids": ["yeshen@microsoft.com", "jianshuc@microsoft.com", "huang.person@gmail.com", "yuqguo@microsoft.com", "jfgao@microsoft.com"], "authors": ["Yelong Shen", "Jianshu Chen", "Po-Sen Huang", "Yuqing Guo", "Jianfeng Gao"], "TL;DR": "We developed an agent that learns to walk over a graph by modeling the Q-network, the policy network, and the value network, which are combined together with a Monte Carlo Tree Search (MCTS) to search for the target node.", "pdf": "/pdf/3c2d5d5b3bc0450130c6974b4a4538bf133c1a73.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518712625087, "id": "ICLR.cc/2018/Workshop/-/Paper197/Public_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper197/Reviewers"], "reply": {"replyto": null, "forum": "HJCRT81DM", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1518712625087}}}], "count": 9}