{"notes": [{"id": "BJe55gBtvH", "original": "HJgMfk-twr", "number": 2482, "cdate": 1569439890190, "ddate": null, "tcdate": 1569439890190, "tmdate": 1583912046274, "tddate": null, "forum": "BJe55gBtvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["vaggos@cs.stanford.edu", "sai_nagarajan@mymail.sutd.edu.sg", "ioannis@sutd.edu.sg", "xiao_wang@sutd.edu.sg"], "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "authors": ["Vaggos Chatziafratis", "Sai Ganesh Nagarajan", "Ioannis Panageas", "Xiao Wang"], "pdf": "/pdf/45974c4cd154860a2542454f3e4be33d90d08215.pdf", "TL;DR": "In this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks ", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions.", "code": "https://docs.google.com/document/d/1qr-sROZ7q93OhigF6CoPde5NQ901wI17wmnBvbZRT9s/edit?fbclid=IwAR1HwkNZ1g2QgMmTGRZ0ktCYNgeKKk91tvRNLb59QJwU3dRmuGCJbTNMwj0", "keywords": ["Depth-Width trade-offs", "ReLU networks", "chaos theory", "Sharkovsky Theorem", "dynamical systems"], "paperhash": "chatziafratis|depthwidth_tradeoffs_for_relu_networks_via_sharkovskys_theorem", "_bibtex": "@inproceedings{\nChatziafratis2020Depth-Width,\ntitle={Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem},\nauthor={Vaggos Chatziafratis and Sai Ganesh Nagarajan and Ioannis Panageas and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe55gBtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d823cbbbbad647190cd8f6988102d4b64ed59b6c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "pWo2QQ3uy-", "original": null, "number": 1, "cdate": 1576798750065, "ddate": null, "tcdate": 1576798750065, "tmdate": 1576800885779, "tddate": null, "forum": "BJe55gBtvH", "replyto": "BJe55gBtvH", "invitation": "ICLR.cc/2020/Conference/Paper2482/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The article is concerned with depth width tradeoffs in the representation of functions with neural networks. The article presents connections between expressivity of neural networks and dynamical systems, and obtains lower bounds on the width to represent periodic functions as a function of the depth. These are relevant advances and new perspectives for the theoretical study of neural networks. The reviewers were very positive about this article. The authors' responses also addressed comments from the initial reviews. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vaggos@cs.stanford.edu", "sai_nagarajan@mymail.sutd.edu.sg", "ioannis@sutd.edu.sg", "xiao_wang@sutd.edu.sg"], "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "authors": ["Vaggos Chatziafratis", "Sai Ganesh Nagarajan", "Ioannis Panageas", "Xiao Wang"], "pdf": "/pdf/45974c4cd154860a2542454f3e4be33d90d08215.pdf", "TL;DR": "In this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks ", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions.", "code": "https://docs.google.com/document/d/1qr-sROZ7q93OhigF6CoPde5NQ901wI17wmnBvbZRT9s/edit?fbclid=IwAR1HwkNZ1g2QgMmTGRZ0ktCYNgeKKk91tvRNLb59QJwU3dRmuGCJbTNMwj0", "keywords": ["Depth-Width trade-offs", "ReLU networks", "chaos theory", "Sharkovsky Theorem", "dynamical systems"], "paperhash": "chatziafratis|depthwidth_tradeoffs_for_relu_networks_via_sharkovskys_theorem", "_bibtex": "@inproceedings{\nChatziafratis2020Depth-Width,\ntitle={Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem},\nauthor={Vaggos Chatziafratis and Sai Ganesh Nagarajan and Ioannis Panageas and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe55gBtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d823cbbbbad647190cd8f6988102d4b64ed59b6c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJe55gBtvH", "replyto": "BJe55gBtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710658, "tmdate": 1576800259713, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2482/-/Decision"}}}, {"id": "rJloHCKhjr", "original": null, "number": 5, "cdate": 1573850690927, "ddate": null, "tcdate": 1573850690927, "tmdate": 1573850775397, "tddate": null, "forum": "BJe55gBtvH", "replyto": "BJegW4y3iH", "invitation": "ICLR.cc/2020/Conference/Paper2482/-/Official_Comment", "content": {"title": "Response to author's comment", "comment": "I greatly appreciate the author's thorough response!  I also appreciate the inclusion of the synthetic dataset!  Unfortunately, it isn't possible for me to raise my score any higher (because it is already at the maximum).  "}, "signatures": ["ICLR.cc/2020/Conference/Paper2482/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2482/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vaggos@cs.stanford.edu", "sai_nagarajan@mymail.sutd.edu.sg", "ioannis@sutd.edu.sg", "xiao_wang@sutd.edu.sg"], "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "authors": ["Vaggos Chatziafratis", "Sai Ganesh Nagarajan", "Ioannis Panageas", "Xiao Wang"], "pdf": "/pdf/45974c4cd154860a2542454f3e4be33d90d08215.pdf", "TL;DR": "In this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks ", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions.", "code": "https://docs.google.com/document/d/1qr-sROZ7q93OhigF6CoPde5NQ901wI17wmnBvbZRT9s/edit?fbclid=IwAR1HwkNZ1g2QgMmTGRZ0ktCYNgeKKk91tvRNLb59QJwU3dRmuGCJbTNMwj0", "keywords": ["Depth-Width trade-offs", "ReLU networks", "chaos theory", "Sharkovsky Theorem", "dynamical systems"], "paperhash": "chatziafratis|depthwidth_tradeoffs_for_relu_networks_via_sharkovskys_theorem", "_bibtex": "@inproceedings{\nChatziafratis2020Depth-Width,\ntitle={Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem},\nauthor={Vaggos Chatziafratis and Sai Ganesh Nagarajan and Ioannis Panageas and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe55gBtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d823cbbbbad647190cd8f6988102d4b64ed59b6c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe55gBtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2482/Authors", "ICLR.cc/2020/Conference/Paper2482/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2482/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2482/Reviewers", "ICLR.cc/2020/Conference/Paper2482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2482/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2482/Authors|ICLR.cc/2020/Conference/Paper2482/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140697, "tmdate": 1576860532331, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2482/Authors", "ICLR.cc/2020/Conference/Paper2482/Reviewers", "ICLR.cc/2020/Conference/Paper2482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2482/-/Official_Comment"}}}, {"id": "BJegW4y3iH", "original": null, "number": 3, "cdate": 1573807095952, "ddate": null, "tcdate": 1573807095952, "tmdate": 1573809420760, "tddate": null, "forum": "BJe55gBtvH", "replyto": "HJgnwZWW5H", "invitation": "ICLR.cc/2020/Conference/Paper2482/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "First we thank Reviewer 1 for their time, positive feedback and valuable comments. Both questions the reviewer asked are very interesting and important and we answer them below. \n\nAddressing the first question regarding the bias term: \n\nIf the reviewer asks about adding a bias term in the ReLU activation unit, e.g., use \\max(v,\\epsilon) instead of  \\max(v,0) for the activation gates, where \\epsilon is a small number (positive or negative), then our results do not change; in particular our tradeoff in Theorem 4.1 still holds (since the Lemma 2.2 from Telgarsky is for general sawtooth functions). If the reviewer asks about what happens if one adds the bias term to the function f itself, then things get more interesting indeed: Suppose f has some period p where p is not a power of two; due to bifurcation phenomena (i.e., phenomena arising because we are at critical regimes of parameters like the \\mu parameter in our generalized triangle wave function), then the compositions of the function (f+bias term) with itself may give rise to different behaviours qualitatively compared to f. In particular, the function (f+bias term) might not have period p anymore. Intuitively you can think that the small bias term is amplified after many compositions and is not negligible anymore.\n\nSuch a brittle example is the triangle function f(x) = \\phi * x for 0<=x<=\u00bd and \\phi(1-x) for 1/2<=x<=1 where \\phi = (1+\\sqrt{5})/2 is the golden ratio. This function is easy to see that has period 3 (we include illustrative figures in the newer version of the paper). However, if we consider the function g(x) = (\\phi-\\epsilon)x for 0<=x<=\u00bd and (\\phi-\\epsilon)(1-x) for 1/2<=x<=1 with \\epsilon>0 (arbitrarily small positive) then g does not have period 3. In this sense, period as a property can be brittle to numerical changes if we are at the critical point. We updated the newer version as well with this brittle example.\n\nAddressing the second question regarding empirical intuition and performance for the classification error:\n\nHere we performed experiments on the synthetic dataset generated by the triangle functions and trained neural networks with different depths (both at regimes where representation is possible and at regimes where it is impossible). We plotted the classification error as a function of the depth as the reviewer suggested and we include the figures in the newer version of the paper. Some details for the experimental setup are included as well. The code together with the final figure are added to the google doc containing our code for the ICLR submission (we added a Python Script for the Neural Network experiment (To be run in ipython 3 enivronment)).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2482/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vaggos@cs.stanford.edu", "sai_nagarajan@mymail.sutd.edu.sg", "ioannis@sutd.edu.sg", "xiao_wang@sutd.edu.sg"], "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "authors": ["Vaggos Chatziafratis", "Sai Ganesh Nagarajan", "Ioannis Panageas", "Xiao Wang"], "pdf": "/pdf/45974c4cd154860a2542454f3e4be33d90d08215.pdf", "TL;DR": "In this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks ", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions.", "code": "https://docs.google.com/document/d/1qr-sROZ7q93OhigF6CoPde5NQ901wI17wmnBvbZRT9s/edit?fbclid=IwAR1HwkNZ1g2QgMmTGRZ0ktCYNgeKKk91tvRNLb59QJwU3dRmuGCJbTNMwj0", "keywords": ["Depth-Width trade-offs", "ReLU networks", "chaos theory", "Sharkovsky Theorem", "dynamical systems"], "paperhash": "chatziafratis|depthwidth_tradeoffs_for_relu_networks_via_sharkovskys_theorem", "_bibtex": "@inproceedings{\nChatziafratis2020Depth-Width,\ntitle={Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem},\nauthor={Vaggos Chatziafratis and Sai Ganesh Nagarajan and Ioannis Panageas and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe55gBtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d823cbbbbad647190cd8f6988102d4b64ed59b6c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe55gBtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2482/Authors", "ICLR.cc/2020/Conference/Paper2482/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2482/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2482/Reviewers", "ICLR.cc/2020/Conference/Paper2482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2482/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2482/Authors|ICLR.cc/2020/Conference/Paper2482/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140697, "tmdate": 1576860532331, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2482/Authors", "ICLR.cc/2020/Conference/Paper2482/Reviewers", "ICLR.cc/2020/Conference/Paper2482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2482/-/Official_Comment"}}}, {"id": "H1g-XB1hsB", "original": null, "number": 4, "cdate": 1573807385475, "ddate": null, "tcdate": 1573807385475, "tmdate": 1573807385475, "tddate": null, "forum": "BJe55gBtvH", "replyto": "ryl5620g5S", "invitation": "ICLR.cc/2020/Conference/Paper2482/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "First we thank Reviewer 2 for their time, positive feedback and valuable comments. \n\nFollowing the reviewer\u2019s suggestion, we have restructured the paper in the newer version in such a way so that the main results and the contribution to ML come first, before the technical details based on dynamical systems theory. Of course, feel free to suggest any other change you think would improve the current write-up. We also provided an example for Definition 2 and Definition 3 to make the presentation cleaner. We also added a discussion section in the Appendix with several additions/comments.\n\nRegarding the question of usefulness:\n\nIn terms of theoretical advantages, our paper in a nutshell gives a *natural* property of a function (periodic points of certain periods) and then derives depth-width trade-offs based on it. This addresses some questions raised in Telgarsky\u2019s work, but also in the paper \u201cExponential expressivity in deep neural networks through transient chaos\u201d (https://papers.nips.cc/paper/6322-exponential-expressivity-in-deep-neural-networks-through-transient-chaos) that seeks to provide a natural, general measure of functional complexity helping us understand the benefits of depth. On the contrary, many of previous depth separation results take a worst case approach for the representation question (showing that there exist functions implemented by deep networks that are hard to approximate with a shallow net). However, it is not clear whether such analysis applies to the typical instances arising in practice of neural-networks. We believe that our work together with Telgasky\u2019s and the paper \u201cThe power of depth for feedforward neural networks\u201d (by Eldan/Shamir) show a depth separation argument for very natural functions, like the triangle waves or the indicator function of the unit ball. \n\nContinuing with the question of given a specific prediction task, how could one assess the period, we agree that this would be extremely useful in practice but this is indeed a very difficult question that seems to be outside the reach of current techniques in the literature. Previous works and our work so far are able to present depth separation for representing certain functions. \n\nWe would like to point out that, intuitively, our characterization result consists of a certificate informing us qualitatively and quantitatively about which functions have complicated compositions and which not. Similar to computational problems in class NP, if one is given the certificate (the points x_1,...,x_p), then one can easily verify if the given function has a p-periodic cycle with points x_1,...,x_p (given oracle access to the function). Nevertheless, we believe that finding the certificate for arbitrary continuous functions is not a straightforward problem except maybe for restricted classes of functions. Having said that, we want to emphasize that in many prediction problems that are inspired by physics, one may a priori expect to have complicated dynamics behaviour and hence requiring deeper networks for better performance. Such examples include efforts to solve the notorious 3-body problem or turbulent flows showing empirical evidence that complex physical processes require deep networks (see for instance, \u201cReynolds averaged turbulence modelling using deep neural networks with embedded invariance\u201d (2016) and \u201cNewton vs the machine: solving the chaotic three-body problem using deep neural networks\u201d (2019) (https://arxiv.org/pdf/1910.07291.pdf) where they use a 10 layered neural network.) "}, "signatures": ["ICLR.cc/2020/Conference/Paper2482/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2482/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vaggos@cs.stanford.edu", "sai_nagarajan@mymail.sutd.edu.sg", "ioannis@sutd.edu.sg", "xiao_wang@sutd.edu.sg"], "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "authors": ["Vaggos Chatziafratis", "Sai Ganesh Nagarajan", "Ioannis Panageas", "Xiao Wang"], "pdf": "/pdf/45974c4cd154860a2542454f3e4be33d90d08215.pdf", "TL;DR": "In this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks ", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions.", "code": "https://docs.google.com/document/d/1qr-sROZ7q93OhigF6CoPde5NQ901wI17wmnBvbZRT9s/edit?fbclid=IwAR1HwkNZ1g2QgMmTGRZ0ktCYNgeKKk91tvRNLb59QJwU3dRmuGCJbTNMwj0", "keywords": ["Depth-Width trade-offs", "ReLU networks", "chaos theory", "Sharkovsky Theorem", "dynamical systems"], "paperhash": "chatziafratis|depthwidth_tradeoffs_for_relu_networks_via_sharkovskys_theorem", "_bibtex": "@inproceedings{\nChatziafratis2020Depth-Width,\ntitle={Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem},\nauthor={Vaggos Chatziafratis and Sai Ganesh Nagarajan and Ioannis Panageas and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe55gBtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d823cbbbbad647190cd8f6988102d4b64ed59b6c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe55gBtvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2482/Authors", "ICLR.cc/2020/Conference/Paper2482/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2482/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2482/Reviewers", "ICLR.cc/2020/Conference/Paper2482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2482/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2482/Authors|ICLR.cc/2020/Conference/Paper2482/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140697, "tmdate": 1576860532331, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2482/Authors", "ICLR.cc/2020/Conference/Paper2482/Reviewers", "ICLR.cc/2020/Conference/Paper2482/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2482/-/Official_Comment"}}}, {"id": "ryl5620g5S", "original": null, "number": 1, "cdate": 1572035777594, "ddate": null, "tcdate": 1572035777594, "tmdate": 1572972332627, "tddate": null, "forum": "BJe55gBtvH", "replyto": "BJe55gBtvH", "invitation": "ICLR.cc/2020/Conference/Paper2482/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper studies how the expressive power of NN depends on its depth and width. Sharkovsky's theorem is leveraged to characterize the depth-width tradeoff in the ability of ReLU networks to represent functions with periodic points. A lower bound on the depth necessary to represent periodic functions is also provided. All in all, the paper furthers the understanding on the benefit of deep nets for representing certain function classes.\n\nI found this to be a serious and well-written paper. The application of Sharkovsky's results is clever and well in place. My main criticism has to do with the structure, which I think overloads with general theory before getting to the main point the paper is making. I suggest stating Theorem 4.1 earlier, even as soon as Section 1.3, and use the discussion therein as an interpretation of the result. All the technical details, such as definition, Sharkovsky's Thm and proofs, can follows after than. The theoretical background is very interesting, but it would be better to start from the contribution to ML and get into the math later on. \n\nThe period dependent depth lower bound is nice but not very useful. Given a certain classification task, how could one assess/bound/approximate the period? This is general issue with this type of theory -- while it broadens our understanding it is hard to put it into actual use.\n\nAnother small comment: it would be useful to provide intuition for some of the definitions in the paper. For example Def. 3 lacks such."}, "signatures": ["ICLR.cc/2020/Conference/Paper2482/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2482/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vaggos@cs.stanford.edu", "sai_nagarajan@mymail.sutd.edu.sg", "ioannis@sutd.edu.sg", "xiao_wang@sutd.edu.sg"], "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "authors": ["Vaggos Chatziafratis", "Sai Ganesh Nagarajan", "Ioannis Panageas", "Xiao Wang"], "pdf": "/pdf/45974c4cd154860a2542454f3e4be33d90d08215.pdf", "TL;DR": "In this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks ", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions.", "code": "https://docs.google.com/document/d/1qr-sROZ7q93OhigF6CoPde5NQ901wI17wmnBvbZRT9s/edit?fbclid=IwAR1HwkNZ1g2QgMmTGRZ0ktCYNgeKKk91tvRNLb59QJwU3dRmuGCJbTNMwj0", "keywords": ["Depth-Width trade-offs", "ReLU networks", "chaos theory", "Sharkovsky Theorem", "dynamical systems"], "paperhash": "chatziafratis|depthwidth_tradeoffs_for_relu_networks_via_sharkovskys_theorem", "_bibtex": "@inproceedings{\nChatziafratis2020Depth-Width,\ntitle={Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem},\nauthor={Vaggos Chatziafratis and Sai Ganesh Nagarajan and Ioannis Panageas and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe55gBtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d823cbbbbad647190cd8f6988102d4b64ed59b6c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJe55gBtvH", "replyto": "BJe55gBtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877225145, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2482/Reviewers"], "noninvitees": [], "tcdate": 1570237722209, "tmdate": 1575877225160, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2482/-/Official_Review"}}}, {"id": "HJgnwZWW5H", "original": null, "number": 2, "cdate": 1572045155573, "ddate": null, "tcdate": 1572045155573, "tmdate": 1572972332579, "tddate": null, "forum": "BJe55gBtvH", "replyto": "BJe55gBtvH", "invitation": "ICLR.cc/2020/Conference/Paper2482/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In tackling a curious construction by Telgarsky regarding a certain class of functions that can be represented by deep networks (but not shallow networks (unless those shallow networks have exponentially many units)), the authors derive depth-width tradeoff conditions for when relu networks are able to represent periodic functions using dynamical systems analysis.\n\nThis paper was a delight to read.  I particularly enjoyed the motivating examples, and the clean exposition of Sharkovsky's theorem.  This result seems to cleanly answer the open question originally posed by Telgarsky, and the proofs are cleanly written, and correct to my (admittedly not perfect) knowledge.  I strongly suggest acceptance.\n\nQuestions/comments:\n\n1. Could the author speculate on how the introduction of a bias term might affect their lower bound?  Presumably, this breaks the cleanness of the characteristic polynomial for $A$, but perhaps there are limits where it's still tractable?  This analysis certainly isn't necessary for publishing--I'm simply curious.\n\n2. Could the authors provide some guiding intuition for the sharpness of their lower bound? (possibly on a synthetic dataset?) . I'm particularly imagining a plot that literally shows \"classification error\" versus \"depth\" for some fixed task.  While this is certainly a strong theoretical result, it would be nice to be able to contextualize how this result actually shines for a \"real\" model (and would help me believe the result \"in my gut\" so to speak)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2482/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2482/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vaggos@cs.stanford.edu", "sai_nagarajan@mymail.sutd.edu.sg", "ioannis@sutd.edu.sg", "xiao_wang@sutd.edu.sg"], "title": "Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem", "authors": ["Vaggos Chatziafratis", "Sai Ganesh Nagarajan", "Ioannis Panageas", "Xiao Wang"], "pdf": "/pdf/45974c4cd154860a2542454f3e4be33d90d08215.pdf", "TL;DR": "In this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks ", "abstract": "Understanding the representational power of Deep Neural Networks (DNNs) and how their structural properties (e.g., depth, width, type of activation unit) affect the functions they can compute, has been an important yet challenging question in deep learning and approximation theory. In a seminal paper, Telgarsky high- lighted the benefits of depth by presenting a family of functions (based on sim- ple triangular waves) for which DNNs achieve zero classification error, whereas shallow networks with fewer than exponentially many nodes incur constant error. Even though Telgarsky\u2019s work reveals the limitations of shallow neural networks, it doesn\u2019t inform us on why these functions are difficult to represent and in fact he states it as a tantalizing open question to characterize those functions that cannot be well-approximated by smaller depths.\nIn this work, we point to a new connection between DNNs expressivity and Sharkovsky\u2019s Theorem from dynamical systems, that enables us to characterize the depth-width trade-offs of ReLU networks for representing functions based on the presence of a generalized notion of fixed points, called periodic points (a fixed point is a point of period 1). Motivated by our observation that the triangle waves used in Telgarsky\u2019s work contain points of period 3 \u2013 a period that is special in that it implies chaotic behaviour based on the celebrated result by Li-Yorke \u2013 we proceed to give general lower bounds for the width needed to represent periodic functions as a function of the depth. Technically, the crux of our approach is based on an eigenvalue analysis of the dynamical systems associated with such functions.", "code": "https://docs.google.com/document/d/1qr-sROZ7q93OhigF6CoPde5NQ901wI17wmnBvbZRT9s/edit?fbclid=IwAR1HwkNZ1g2QgMmTGRZ0ktCYNgeKKk91tvRNLb59QJwU3dRmuGCJbTNMwj0", "keywords": ["Depth-Width trade-offs", "ReLU networks", "chaos theory", "Sharkovsky Theorem", "dynamical systems"], "paperhash": "chatziafratis|depthwidth_tradeoffs_for_relu_networks_via_sharkovskys_theorem", "_bibtex": "@inproceedings{\nChatziafratis2020Depth-Width,\ntitle={Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem},\nauthor={Vaggos Chatziafratis and Sai Ganesh Nagarajan and Ioannis Panageas and Xiao Wang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe55gBtvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d823cbbbbad647190cd8f6988102d4b64ed59b6c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJe55gBtvH", "replyto": "BJe55gBtvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2482/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877225145, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2482/Reviewers"], "noninvitees": [], "tcdate": 1570237722209, "tmdate": 1575877225160, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2482/-/Official_Review"}}}], "count": 7}