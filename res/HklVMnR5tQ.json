{"notes": [{"id": "HklVMnR5tQ", "original": "BJgqW-AcYm", "number": 1257, "cdate": 1538087948216, "ddate": null, "tcdate": 1538087948216, "tmdate": 1545355380747, "tddate": null, "forum": "HklVMnR5tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ryxzgItblN", "original": null, "number": 1, "cdate": 1544816105970, "ddate": null, "tcdate": 1544816105970, "tmdate": 1545354528735, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "HklVMnR5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Meta_Review", "content": {"metareview": "The reviewers appreciated the clarity of writing, and the importance of the problem being addressed. There was a moderate amount of discussion around the paper, but the two reviewers who responded to the author discussion were split in their opinion, with one slightly increasing their score to a 6, and the other remaining unconvinced. The scores overall are borderline for ICLR acceptance, and given that, no reviewer stepped forward to champion the paper.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Borderline, with no clear reviewer endorsement"}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1257/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353374293, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklVMnR5tQ", "replyto": "HklVMnR5tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1257/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1257/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353374293}}}, {"id": "SJgW5KK3y4", "original": null, "number": 16, "cdate": 1544489353209, "ddate": null, "tcdate": 1544489353209, "tmdate": 1544980192650, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "BkedufG31E", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "content": {"title": "About RETAIN ", "comment": "Dear Reviewer,\n\nThanks for your reply to the revision! Maybe we did not explain clearly in the previous response. \n\nWe understand and fully agree that RETAIN is innovative in calculating the contribution of each variable in each timestep. \n\n-- regarding RETAIN \n\nWhat we tried to explain is that the derivation of the attention at each time step (i.e. \u201cStep 3\u201d in the paper of RETAIN) and using the attention value to weight input (i.e. \u201cStep 4\u201d) are problematic. \n\nWe would like to draw the attention of the community and inspire some insights into designing attention or contribution measures on multi-variable data. \n\nIn particular, in the RETAIN paper, in \u201cStep 3\u201d, $\\beta_j$ over variables is derived from the hidden states of RNN_{beta}. RNN_{beta} consumes multi-variable data in a conventional way and thus the hidden states mix information from all variables. Each element of $\\beta_j$ is then derived from hidden states with mixed information and opaque data flows.\n\nOur hypothesis is that it is improper to use each element of $\\beta_j$ to represent the contribution or importance measure of corresponding variables at each timestep, since each element includes the mixed contribution of all input variables. \n\nAs for \u201cStep 4\u201d, using attention value to directly weight the input data could be problematic as well if we take into account the correlation direction and domain of the input variables.\n\n-- \u201cTherefore, when the authors were conducting the experiment in 4.5 using RETAIN, I wonder if the authors selected the variables by correctly calculating the contribution of each variable, or simply used the attention that each variable received.\u201d\n\nThe experiments in 4.5 are strictly in accordance with RETAIN paper and use the \u201ccontribution coefficient\u201d defined in Eq. (5) in RETAIN paper to select variables. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604391, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklVMnR5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1257/Authors|ICLR.cc/2019/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604391}}}, {"id": "Skegf_JRkV", "original": null, "number": 18, "cdate": 1544579079645, "ddate": null, "tcdate": 1544579079645, "tmdate": 1544579079645, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "H1gY3VZhJ4", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "content": {"title": "Thanks for the reply!", "comment": "Dear Reviewer,\n\nThanks for updating the rating!\n\nWe are continuously working on improving the manuscript both theoretically and experimentally. \n\nFeel free to post comments if you have additional advice.\n\nThanks!\n\nBest regards,\nAuthors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604391, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklVMnR5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1257/Authors|ICLR.cc/2019/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604391}}}, {"id": "BkedufG31E", "original": null, "number": 15, "cdate": 1544458863775, "ddate": null, "tcdate": 1544458863775, "tmdate": 1544458863775, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "B1ez2no9hX", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "content": {"title": "Retaining the rating", "comment": "After reading the authors' feedback, I must say that I am still not convinced of the strong novelty of this work.\nThe proposed method for deriving the importance (variable-wise or time-wise) is still, in essence, averaging the attention values. \nAnd the authors' feedback suggests that the authors may not have a clear understanding of RETAIN. What separates RETAIN from other attention-based models is that RETAIN provides a way to precisely calculate the contribution of each variable in each timestep, which is not the same as calculating the variable importance by the attention each variable receives. \nTherefore, when the authors were conducting the experiment in 4.5 using RETAIN, I wonder if the authors selected the variables by correctly calculating the contribution of each variable, or simply used the attention that each variable received.\nWith these said, I still think the paper proposes a decent approach, and the overall quality of the paper calls for a 6, and I retain my rating.\nHowever, if this paper is accepted, I suggest that the authors clarify the points I raised regarding RETAIN, as imprecise description of baselines could lower the credibility of the entire paper (even though the paper's idea itself is nice)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1257/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604391, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklVMnR5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1257/Authors|ICLR.cc/2019/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604391}}}, {"id": "H1gY3VZhJ4", "original": null, "number": 14, "cdate": 1544455345112, "ddate": null, "tcdate": 1544455345112, "tmdate": 1544455345112, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "SklwRJb52m", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "content": {"title": "I revised my rating on the basis of the improvements brought to the paper.", "comment": "In light of the improvements brought to the paper to address some of the concerns initially raised, I believe the paper will be of interest to the ICLR community."}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1257/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604391, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklVMnR5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1257/Authors|ICLR.cc/2019/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604391}}}, {"id": "SklwRJb52m", "original": null, "number": 1, "cdate": 1541177295073, "ddate": null, "tcdate": 1541177295073, "tmdate": 1544455145304, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "HklVMnR5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Official_Review", "content": {"title": "This paper explores the interpretability of LSTM with multivariable data while providing accurate forecasts in the context of time series. The paper is interesting and addresses a relevant topic. But it has several drawbacks that need to be addressed.", "review": "The contributions of this paper are in the field of LSTM, where the authors explore the interpretability of LSTM with multivariate data obtained from various and disparate applications. To this end, the authors endow their approach with tensorized hidden states and an update process in order to learn the hidden states. Furthermore, the authors develop a mixture attention mechanism and a summarization methods to quantify the temporal and variable importance in the data. They validate the forecasting  and interpretability performance of their approach with experiments. \n\nThe parer is interesting, well structured and and clearly written. Also, the addressed topic of interpretability is pertinent. However, I have several concerns.\n\n1. In the related work the authors state that \u201cIn time series analysis, prediction with exogenous variables is formulated as an auto-regressive exogenous model \u201d . This is not always right - it is not imperative to add the auto-regressive terms, this is optional and depends on the way we want to formulate our time series forecasting approach and the known constraints.  \n2. In section 3 \u2014 Interpretable Multi-Variable LSTM, by stacking exogenous time series and target series, the authors implicitly formulate their algorithms in a way to consider auto-regression. And I have several concerns with this for time series forecasting. Because, the past is not always a predictor of the future even - particularly in time series context and in industrial settings. And in the occasions where the past allows to predict the future we do not necessarily need to use LSTM to forecast (the notion of persistency in forecasting is enough).  Therefore, the power of LSTM in forecasting would have been convincing if you omit the target series in your multi-variable input.\n3. In Network Architecture section the authors develop tensorized hidden state and an update scheme. This idea is interesting, I think it would also be good to know what is the algorithmic complexity of this approach? \n4.  In section 3.3 the authors state that \"In the present paper, we choose the simple normalized summation function eq.(9). \" Could the authors justify the reason behind this choice? I am not convinced of the reason behind this, especially the authors mention, right after,  that \"It is flexible to choose alternative functions for f_{agg}\"\n\n5. In the experiment section, concerning the prediction performance the authors present a table showing their results, I believe it would have been more compelling to present the prediction results with graphs showing the normalized cumulative errors, as an example.\n\n6. With regard to the interpretation of the results, the authors show the variable importance as a function of the epoch number, it would be equally important to correlate the same figure with the associated prediction results/normalized cumulative errors as a function of the epoch number - this will allow to assess the importance of the interpretability.\n\nI think it would be important to further justify the pertinence of this work in terms of interpretability (the statement in the introduction \"the interpretability of prediction models is essential for deployment and\nknowledge extraction\" seems to be limited) for example what does it bring knowing the variance importance  as a function of the epoch number. As an example, the Pearson correlation coefficient can help select relevant features to a model, and restrict the number of inputs to the relevant ones - can we draw inspiration from this and explain what the authors are proposing in terms of interpretability... Here the idea is to have a motivation presenting the merits of this work, which I think is missing - particularly with the experiments presented here.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Official_Review", "cdate": 1542234561533, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklVMnR5tQ", "replyto": "HklVMnR5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1257/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335626249, "tmdate": 1552335626249, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1ezdEgeJE", "original": null, "number": 9, "cdate": 1543664745594, "ddate": null, "tcdate": 1543664745594, "tmdate": 1543690598877, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "SylJpO6tCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "content": {"title": " Comments on the updated version", "comment": "Dear all reviewers,\n\nIf there are still concerns not addressed in our response and the updated version, we can provide further explanation in this forum.\n\nThanks!  "}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604391, "tddate": null, "super": null, "final": null, "reply": {"forum": "HklVMnR5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1257/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1257/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1257/Authors|ICLR.cc/2019/Conference/Paper1257/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Reviewers", "ICLR.cc/2019/Conference/Paper1257/Authors", "ICLR.cc/2019/Conference/Paper1257/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604391}}}, {"id": "B1ez2no9hX", "original": null, "number": 3, "cdate": 1541221546265, "ddate": null, "tcdate": 1541221546265, "tmdate": 1541534374511, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "HklVMnR5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Official_Review", "content": {"title": "Nice work, but claims are bit much", "review": "Summary:\nThe authors propose IMV-LSTM, which can handle multi-variate time series data in a manner that enables accurate forecasting, and interpretation (importance of variables across time, and importance of each variable). The authors use one LSTM per variable, and propose two implementations: IMV-Full explicitly tries to capture the interaction between the variables before mixing the LSTM hidden layers with attention. IMV-Tensor uses separate LSTMs for each variable that remain separate, and mixes the hidden layers of the LSTMs using attention. The propose model outperforms popular interpretable models on three different datasets, and the experiments regarding the variable importance is convincing.\n\nPros:\n- The paper is clearly written, easy to understand.\n- IMV-LSTM outperforms many baselines including popular interpretable models on three different datasets, and the interpretation part is not super rigorous, but convincing enough.\n- Multi-variate time-series data are very common, therefore an interpretable, accurate models such as IMV-LSTM have a big practical impact.\n- I like the idea of using the important variables to train another model for testing how accurately the models can choose important variables\n\nIssues:\n- In the introduction: claim that attention mechanism can unveil the effect of variable to the target is tricky, potentially dangerous: Attention is attention. It is no causal, let alone correlation. Coefficients in logistic regression are correlated with the prediction target. Variables with high attention has \"some relationship\" with the prediction target. \n- The methodological novelty of IMV-LSTM is limited. Using attention mechanism on RNN to provide interpretation has been explored quite often. This paper is not so different from other works [1,2,3]\n- Claim that this is the first work to derive temporal-level & variable-level importance is not convincing: The importance calculation of this paper boils down to averaging the attention values. This can be easily done in the previous works [1,2,3], or any model that uses attention on each input channel and on the temporal axis.\n- Can't follow Eq.10. How is this justified?\n\n\n[1] Choi, E., Bahadori, M.T., Sun, J., Kulas, J., Schuetz, A. and Stewart, W., 2016. Retain: An interpretable predictive model for healthcare using reverse time attention mechanism. In Advances in Neural Information Processing Systems (pp. 3504-3512).\n[2] Zhang, J., Kowsari, K., Harrison, J.H., Lobo, J.M. and Barnes, L.E., 2018. Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record. IEEE Access.\n[3] Xu, Y., Biswal, S., Deshpande, S.R., Maher, K.O. and Sun, J., 2018, July. RAIM: Recurrent Attentive and Intensive Model of Multimodal Patient Monitoring Data. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 2565-2573). ACM.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Official_Review", "cdate": 1542234561533, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklVMnR5tQ", "replyto": "HklVMnR5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1257/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335626249, "tmdate": 1552335626249, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJxactWc3X", "original": null, "number": 2, "cdate": 1541179797018, "ddate": null, "tcdate": 1541179797018, "tmdate": 1541534374309, "tddate": null, "forum": "HklVMnR5tQ", "replyto": "HklVMnR5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1257/Official_Review", "content": {"title": "Interesting and potent interpretable LSTM without an actual interpretation in terms of the problem: the model claims to make variable/temporal variable importance but without actually interpreting the quality.", "review": "This paper describes a recurrent model (LSTM specifically, but generalizable) which can produce variable-wise hidden states that can be further used for two types of attentions: 1) variable importance for the importance of each variable (not accounting for time), and 2) temporal importance of each variable for the importance of each variable over time. The proposed NN model (IMV-LSTM) does not seem to directly provide such importance. Rather, the outputs are \u201cdecomposed\u201d for each variable/time that allows probabilistic inference on top of this.\n\nOne of my main concerns (described in Cons/Comments below) is how it is not straightforward to grasp the quality of variable importance and temporal variable importance results despite this is the key strength of this paper. If this comes from my lack of understanding, I would appreciate if the authors could provide a little more explanation.\n\nPros:\n1.\tThe overall quality of the paper is decent and mostly clear.\n2.\tThe experiments are quite extensive.\n3.\tThe fact that each variable should have different level of importance is interesting and practical.\n\nCons/Comments:\n1.\tThe term \u201ctensor\u201d is used throughout the paper to describe the stacked matrices. While this is not technically wrong to describe 2>-dimensional structures, this term could potentially imply (and make the readers to expect) tensor-based schemes such as tensor decomposition. This is not necessarily bad, but to me, \u201ctensor\u201d and \u201cvariable-wise correspondence\u201d do not seems to be associated too deeply since the \u201ctensor\u201d used in IMV-LSTM is a stack of matrices that are also independently used with respect to each other.\n\n2.\tThe variable importance experiments seem quite extensive and thorough, especially the lists of variable-wise temporal importance matrices provided in the appendix. However, the authors could provide some significance or relevance of the findings with respect to any domain knowledge or literature, it may help further appreciate and interpret the quality of the variable importance which is quite subjective to non-experts. Such information may not even need to be in the main paper; including a short description in the appendix.\n\n3.\tRelated to comments (2), the difference between IMV-Full and IMV-Tensor is hard to interpret since neither one is always better than the other (i.e., IMV-Full > IMV-Tensor in some experiments, vice versa). While the key difference is speculated to be from how the LSTM handles the variables, I am curious how this related to the differences in the results and how the differences variable importance results (i.e., Fig.3) can be in at least speculated.\n\nQuestions:\n1.\tShould \\tilde{h}_t in Figure 1 (a) be \\tilde{h}__{t-1} since this hidden state is from t-1? The figure itself currently implies that the hidden state for t is used, but this is computed from x_t using U_j. With \\tilde{h}__{t-1}, it follows Eq.(1).\n\n2.\tIn Equation set 2 for IMV-Tensor, are W and U (not W_j and U_j) also in tensor forms so each variable and hidden state get transformed correspondingly (i.e., W_1 for h^1_{t-1}, U_1 for x^1_t).\n\n3.\tThe IMV-Tensor version of IMV-LSTM (related to the question above) can be considered as a set of parallel LSTMs, one for each variable. Such independence could also be inferred from Figure 1. If that\u2019s the case, where do the variables \u201cinteract\u201d with each other? Is this happening in the later stage where the hidden states across variable/time are aggregated in the attention stage (Eq.(8) and on)?\n\n4.\tUp until Eq.(8), n was used for the variable index where n = 1,\u2026,N. In Eq.(8), it seems to be still used as the variable index (i.e., h_T^n and g^n), but it is also a set of possible values for a random variable z_{T+1}. Is n used the same way for z_{T+1} as well? I am slightly confused on how z is used. Also, (just to clarify), if we use N variables, we are using y_t as well (i.e., [x_t^1,\u2026,x_t^{N-1}, y_t])?\n\n5.\tf_agg: Is this for aggregating over instances? For \\bar{\\alpha}^n, I\u2019m guessing this is aggregated over instances for variable n for t=1,\u2026,T_1.\n\n6.\tI am not too familiar with the notion of \u201ctime-lag\u201d in the experiments. If the authors could explain this a little bit, I would appreciate it.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1257/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring the interpretability of LSTM neural networks over multi-variable data", "abstract": "In learning a predictive model over multivariate time series consisting of target and exogenous variables, the forecasting performance and interpretability of the model are both essential for deployment and uncovering knowledge behind the data.\nTo this end, we propose the interpretable multi-variable LSTM recurrent neural network (IMV-LSTM) capable of providing accurate forecasting as well as both temporal and variable level importance interpretation.\nIn particular, IMV-LSTM is equipped with tensorized hidden states and update process, so as to learn variables-wise hidden states. \nOn top of it, we develop a mixture attention mechanism and associated summarization methods to quantify the temporal and variable importance in data. \nExtensive experiments using real datasets demonstrate the prediction performance and interpretability of IMV-LSTM in comparison to a variety of baselines. \nIt also exhibits the prospect as an end-to-end framework for both forecasting and knowledge extraction over multi-variate data. ", "keywords": ["Interpretability", "recurrent neural network", "attention"], "authorids": ["tian.guo@gess.ethz.ch", "tao.lin@epfl.ch"], "authors": ["Tian Guo", "Tao Lin"], "pdf": "/pdf/64fa4234a013ce0082bf8cb3c1cad107ee16e933.pdf", "paperhash": "guo|exploring_the_interpretability_of_lstm_neural_networks_over_multivariable_data", "_bibtex": "@misc{\nguo2019exploring,\ntitle={Exploring the interpretability of {LSTM} neural networks over multi-variable data},\nauthor={Tian Guo and Tao Lin},\nyear={2019},\nurl={https://openreview.net/forum?id=HklVMnR5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1257/Official_Review", "cdate": 1542234561533, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HklVMnR5tQ", "replyto": "HklVMnR5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1257/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335626249, "tmdate": 1552335626249, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1257/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}