{"notes": [{"id": "uKhGRvM8QNH", "original": "xDsQoCh7QDc", "number": 182, "cdate": 1601308028884, "ddate": null, "tcdate": 1601308028884, "tmdate": 1615879088285, "tddate": null, "forum": "uKhGRvM8QNH", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors", "authorids": ["~Linfeng_Zhang2", "~Kaisheng_Ma1"], "authors": ["Linfeng Zhang", "Kaisheng Ma"], "keywords": ["Knowledge Distillation", "Object Detection", "Teacher-Student Learning", "Non-Local Modules", "Attention Modules"], "abstract": "Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.", "one-sentence_summary": "We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.", "pdf": "/pdf/1e6969024e2fab8681c02ff62a2dbfc4feedcff4.pdf", "supplementary_material": "/attachment/86459651e7df0e37f79b10135a42a3b6779d5666.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|improve_object_detection_with_featurebased_knowledge_distillation_towards_accurate_and_efficient_detectors", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021improve,\ntitle={Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors},\nauthor={Linfeng Zhang and Kaisheng Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=uKhGRvM8QNH}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "a8RiFt2IlY", "original": null, "number": 1, "cdate": 1610040503192, "ddate": null, "tcdate": 1610040503192, "tmdate": 1610474110215, "tddate": null, "forum": "uKhGRvM8QNH", "replyto": "uKhGRvM8QNH", "invitation": "ICLR.cc/2021/Conference/Paper182/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "After the rebuttal stage, all reviewers lean positive (in final scores and/or in comments during the discussion phase). The AC found no reason to disagree. The benefit of the proposed method is demonstrated in many diverse settings, and the authors argue novelty in that no prior work addresses both fg/bg imbalance and relation distillation. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors", "authorids": ["~Linfeng_Zhang2", "~Kaisheng_Ma1"], "authors": ["Linfeng Zhang", "Kaisheng Ma"], "keywords": ["Knowledge Distillation", "Object Detection", "Teacher-Student Learning", "Non-Local Modules", "Attention Modules"], "abstract": "Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.", "one-sentence_summary": "We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.", "pdf": "/pdf/1e6969024e2fab8681c02ff62a2dbfc4feedcff4.pdf", "supplementary_material": "/attachment/86459651e7df0e37f79b10135a42a3b6779d5666.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|improve_object_detection_with_featurebased_knowledge_distillation_towards_accurate_and_efficient_detectors", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021improve,\ntitle={Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors},\nauthor={Linfeng Zhang and Kaisheng Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=uKhGRvM8QNH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "uKhGRvM8QNH", "replyto": "uKhGRvM8QNH", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040503179, "tmdate": 1610474110198, "id": "ICLR.cc/2021/Conference/Paper182/-/Decision"}}}, {"id": "kbedPNPDFwa", "original": null, "number": 1, "cdate": 1603836705312, "ddate": null, "tcdate": 1603836705312, "tmdate": 1609867491151, "tddate": null, "forum": "uKhGRvM8QNH", "replyto": "uKhGRvM8QNH", "invitation": "ICLR.cc/2021/Conference/Paper182/-/Official_Review", "content": {"title": "This paper proposes two knowledge distillation methods to improve the performance of object detection models. With respect to other methods it proposes local distillation as well as attention-guided distillation to improve the knowledge transfer between teacher and student. ", "review": "Pros:\n\n- The different attention techniques seem to consistently improve object detectors across different models. \n- The ablation studies are important in showing the advantage and impact of each proposed module. \n- Please clarify if the student models start from random weights or are initialized after the teacher training. In this regard to what extend is the approach is transferable to students with random weights?\n\nCons:\n\n- The authors only show results when training a network of the same structure with the added modules, in an attempt to boost accuracy. However, an important use of knowledge of knowledge distillation is to train smaller models, and such results are missing from the paper. Overall, the results in the paper somewhat justify the \"accurate\" part of the title but not the \"efficient\".\n\n- The importance of high AP is a bit overstated. It is important to demonstrate the different behavior between classification and detection networks, however, the same improvement is observed irrespective of the teacher AP performance. \n\n- The structure of the paper can be improved. The related work section for example can be moved earlier in the paper to give the bigger picture and the position of this work with respect to the literature. \n\n- For a more complete comparison YOLO and SSD networks should be included.\n\n- In the qualitative analysis observation (ii) states that the proposed methods leads to single box per object compared to the baselines. In this case do the baselines use non-maximum suppression? This is a standard post processing technique that alleviates this problem. No explanation is given on why the proposed method leads to this behavior.\n\nThe authors have made clear some of my concerns and made revisions accordingly. Thus I am in a position now to recommend this paper, thus I update my initial recommendation from 5 to 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper182/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors", "authorids": ["~Linfeng_Zhang2", "~Kaisheng_Ma1"], "authors": ["Linfeng Zhang", "Kaisheng Ma"], "keywords": ["Knowledge Distillation", "Object Detection", "Teacher-Student Learning", "Non-Local Modules", "Attention Modules"], "abstract": "Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.", "one-sentence_summary": "We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.", "pdf": "/pdf/1e6969024e2fab8681c02ff62a2dbfc4feedcff4.pdf", "supplementary_material": "/attachment/86459651e7df0e37f79b10135a42a3b6779d5666.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|improve_object_detection_with_featurebased_knowledge_distillation_towards_accurate_and_efficient_detectors", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021improve,\ntitle={Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors},\nauthor={Linfeng Zhang and Kaisheng Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=uKhGRvM8QNH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uKhGRvM8QNH", "replyto": "uKhGRvM8QNH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148602, "tmdate": 1606915801004, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper182/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper182/-/Official_Review"}}}, {"id": "_B-C27rmvO1", "original": null, "number": 2, "cdate": 1603839933516, "ddate": null, "tcdate": 1603839933516, "tmdate": 1607358068765, "tddate": null, "forum": "uKhGRvM8QNH", "replyto": "uKhGRvM8QNH", "invitation": "ICLR.cc/2021/Conference/Paper182/-/Official_Review", "content": {"title": "Good improvements in empirical results, experimental analysis and discussion with related work can be improved to provide further insights.", "review": "The paper proposes a knowledge distillation method for object detection. In particular, the technical contribution is mainly two-fold: attention-guided distillation module and non-local distillation module, as shown in (a) and (b) of Figure 2. The proposed modules provide consistent improvements in detection MAP across different architectures.\n\nOn the positive side, I believe the paper has the following merits:\n- The proposed modules are well-motivated, and seem to provide a consistent boost in different architectures.\n- The authors provide very detailed information to reproduce the method. Besides, I also appreciate the authors have provided the code in the supplementary material.\n- The observation that a high-AP teacher is important for distillation is quite intriguing.\n\nOverall I believe the paper has made a decent contribution, but the following aspects can be improved:\n- As mentioned by the authors, distillation is discussed for object detection in several works (n (Chen et al., 2017; Li et al., 2017; Wang et al., 2019; Bajestani & Yang, 2020). I believe it would be helpful to summarize the key difference/similarity wrt to the previous work, thus to provide a better understanding of the relation in a larger context.\n- it's not fully clear to me what the attention in attention-guided distillation captures, would the authors provide further experimental analysis? and what are the failure cases of attention-guided distillation? \n- it would be helpful to provide results on another dataset(e.g. cityscapes) to confirm the effectiveness in a different detection setting.\n\n--- \nAfter reading the authors' response and other reviews. I still believe the paper has made a good contribution thus I would stick with my original rating.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper182/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors", "authorids": ["~Linfeng_Zhang2", "~Kaisheng_Ma1"], "authors": ["Linfeng Zhang", "Kaisheng Ma"], "keywords": ["Knowledge Distillation", "Object Detection", "Teacher-Student Learning", "Non-Local Modules", "Attention Modules"], "abstract": "Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.", "one-sentence_summary": "We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.", "pdf": "/pdf/1e6969024e2fab8681c02ff62a2dbfc4feedcff4.pdf", "supplementary_material": "/attachment/86459651e7df0e37f79b10135a42a3b6779d5666.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|improve_object_detection_with_featurebased_knowledge_distillation_towards_accurate_and_efficient_detectors", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021improve,\ntitle={Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors},\nauthor={Linfeng Zhang and Kaisheng Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=uKhGRvM8QNH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uKhGRvM8QNH", "replyto": "uKhGRvM8QNH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148602, "tmdate": 1606915801004, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper182/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper182/-/Official_Review"}}}, {"id": "AfwSl1Z3sS", "original": null, "number": 4, "cdate": 1605513916766, "ddate": null, "tcdate": 1605513916766, "tmdate": 1605678437014, "tddate": null, "forum": "uKhGRvM8QNH", "replyto": "kbedPNPDFwa", "invitation": "ICLR.cc/2021/Conference/Paper182/-/Official_Comment", "content": {"title": "Thanks for your insightful comments! We have added experiments with small backbones, experiments on Yolo and SSD, changed the structured of the paper and added illustration on qualitative analysis", "comment": "**We sincerely appreciate your detailed and thoughtful reviews and hope that our response can address your questions.**\n\n---\n\n**Response to Pros.3. About weights initialization.**\n**The weights of teachers are not used in the initialization of students**. We initialize the weights of the backbone in the students with pre-trained models in ImageNet. The other weights of students are initialized randomly with Xavier initialization [1]. \n\n[1] Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks[C] Proceedings of the thirteenth international conference on artificial intelligence and statistics\n\n\n\n**Question1. Experiments on smaller models** & **Question4. Experiments on Yolo and SSD** \nThanks for your suggestions on conducting experiments on smaller models. The following table shows the experiment results on Faster R-CNN and RetianNet with smaller backbones (ResNet18 and RegNet - 800M). Besides, we also evaluate our methods on compact detectors, including Yolo v3 and SSD. It is observed that our method also achieves significant AP improvements on smaller models. \n\nBy replacing the large detector with a small detector trained with distillation, we can achieve significant compression and acceleration with trivial AP decrement, or even AP boost. For example, by replacing a baseline RetinaNet (ResNet50 backbone) with a distilled RetinaNet (RegNet-800M backbone), we can achieve 1.3 X acceleration,  2.0 X compression, and 1.0 AP improvements. We will report more experiments on small backbones in the camera-ready vision.\n\n\n\n|Model| Backbone| Distill?| AP (MS COCO)|Inference Time (FPS)|Params (M)|\n|------------|-----------|--------|----|----|----------|\n|Faster R-CNN|ResNet18|\u00d7|34.6|28.1|30.57|\n|Faster R-CNN|ResNet18|\u221a|37.0|28.1|30.57|\n|Grid R-CNN|ResNet18|x|36.6|26.7|66.37|\n|Grid R-CNN|ResNet18|\u221a|38.8|26.7|66.37|\n|RetinaNet|ResNet18|\u00d7|33.4|25.8|23.30|\n|RetinaNet|ResNet18|\u221a|35.9|25.8|23.30|\n|RetinaNet|RegNet-800M|\u00d7|35.6|22.4|19.27|\n|RetinaNet|RegNet-800M|\u221a|38.4|22.4|19.27|\n|Yolo v3|DarkNet53|\u00d7|33.4|42.2|61.95|\n|Yolo v3|DarkNet53|\u221a|35.8|42.2|61.95|\n|SSD|VGG16|\u00d7|29.4|26.1|38.08|\n|SSD|VGG16|\u221a|31.2|26.1|38.08|\n\n\n\n**Question2. About the evaluation measurement and Student-Teacher relation.**\n\n**Evaluation Measurement**. We agree that AP is not a perfect evaluation measurement for object detection. To address this concern, we additionally report the AR (average recall) on COCO in the following table. It is observed that our distillation method also leads to significant AR improvements. Note that recall is another important evaluation metric in object detection, which measures the probability of ground truth objects being correctly detected. More results can be found in the log files in our supplementary material. We are glad to evaluate any other evaluation measurement which is required by you. \n\n|Model| Backbone|Distill?|AP (Average Precision)| AR (Average Recall)| AR small  | AR medium  | AR large  |\n|------------ | -------- | --------- |----|----|----|----|----|\n|Faster R-CNN|ResNet50|\u00d7 |38.4|52.0|32.6|55.8|66.1|\n|Faster R-CNN|ResNet50|\u221a|41.5|54.4 |34.0|58.2|69.9|\n|RetinaNet|ResNet50|\u00d7|37.4|53.9|33.1|57.7|70.2|\n|RetinaNet|ResNet50|\u221a |39.6|56.1|36.8|60.0|72.1|\n|RepPoints|ResNet50|\u00d7|38.6|55.1|34.9|59.4|70.3|\n|RepPoints|ResNet50|\u221a|40.6|56.9|37.3|60.9|71.4|\n\n**Student-Teacher relation**. As shown in Figure.7 and Section 4.2, our experiments show that in object detection, there is a strong positive correlation between the AP of teachers and students.\n\n\n\n**Question3. The structure of the paper.**\nThanks for your advice on the structure of the paper. We have moved the related work as the second section in the rebuttal revision according to your advice.\n\n**Question4. Results on Yolo v3 and SSD.**\nPlease refer to our above response to **Q1&Q4**. Note that these results have also been added in our rebuttal revision (Appendix.C).\n\n\n**Question5. About NMS in qualitative analysis.**\n\n**If NMS is used?**  Yes, we have used non-maximum suppression in both baseline and distilled models in our qualitative analysis. Note that although NMS can protect detectors from generating multiple bounding boxes for the same object, *it can not solve this problem perfectly*. \n\n**Why does knowledge distillation help?**  Since knowledge distillation provides better backbone features to the RPN layer, the RPN layer can predict better bounding boxes, which helps to address the \"multiple boxes - single object\" problem. \n\n**Reproduce qualitative analysis**.  To address your confusion, we also provide the script to reproduce the qualitative analysis as follows. \n\n```\n#\tStep-1: Install the mmdetection framework.\n#\tStep-2: Download the pre-trained faster r-cnn from the model zoo and COCO dataset.\n#\tStep-3: Conduct qualitative analysis by running the following scripts.\npython tools/test.py \\\n    configs/faster_rcnn/faster_rcnn_r50_fpn_1x.py \\\n    checkpoints/\"model_file_name\" \\\n    --show-dir \"output_folder_name\"\n```\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper182/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors", "authorids": ["~Linfeng_Zhang2", "~Kaisheng_Ma1"], "authors": ["Linfeng Zhang", "Kaisheng Ma"], "keywords": ["Knowledge Distillation", "Object Detection", "Teacher-Student Learning", "Non-Local Modules", "Attention Modules"], "abstract": "Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.", "one-sentence_summary": "We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.", "pdf": "/pdf/1e6969024e2fab8681c02ff62a2dbfc4feedcff4.pdf", "supplementary_material": "/attachment/86459651e7df0e37f79b10135a42a3b6779d5666.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|improve_object_detection_with_featurebased_knowledge_distillation_towards_accurate_and_efficient_detectors", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021improve,\ntitle={Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors},\nauthor={Linfeng Zhang and Kaisheng Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=uKhGRvM8QNH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uKhGRvM8QNH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper182/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper182/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper182/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper182/Authors|ICLR.cc/2021/Conference/Paper182/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873738, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper182/-/Official_Comment"}}}, {"id": "woshgYbs-FA", "original": null, "number": 3, "cdate": 1605513211963, "ddate": null, "tcdate": 1605513211963, "tmdate": 1605677240706, "tddate": null, "forum": "uKhGRvM8QNH", "replyto": "_B-C27rmvO1", "invitation": "ICLR.cc/2021/Conference/Paper182/-/Official_Comment", "content": {"title": "Thanks for your insightful comments! We have added more literature review, Cityscapes experiments and explanation of attention.", "comment": "**We sincerely appreciate your detailed and thoughtful reviews and hope that our response can address your questions.**\n\n---\n\n**Question1. Summary of key difference and similarity with previous works**\n\nThanks for your advice on the literature review. We have summarized the key difference and similarity of our method and previous works in the following table and added them in our rebuttal revision.\n\n|         Method         | Feature Distillation | Classification Distillation | Regression Distillation | Relation Distillation | One Stage | Two Stage | Anchor Based | Anchor Free | Instance Segment |\n| :--------------------: | :------------------: | :-------------------------: | :---------------------: | :-------------------: | :-------: | :-------: | :----------: | :---------: | :--------------: |\n|       Our Method       |\u221a|\u00d7| \u00d7|\u221a|\u221a |\u221a  |\u221a | \u221a | \u221a |\n|   Chen et al., 2017    | \u221a|\u221a|\u221a|\u00d7 |  \u00d7 |\u221a |\u221a |\u00d7| \u00d7 |\n|    Li et al., 2017     |\u221a |\u00d7|\u00d7 | \u00d7 | \u00d7|\u221a  | \u221a  |\u00d7 |\u00d7|\n|    Wang et al.2019     |\u221a  | \u00d7 |\u00d7 |\u00d7 | \u00d7  | \u221a  |      \u221a       |      \u00d7      |  \u00d7    |\n| Bajestani & Yang, 2020 |\u221a| \u00d7| \u00d7 |\u00d7 | \u221a |     \u221a     | \u221a  |      \u00d7      |  \u00d7     |\n\n\n\n\n**Comparison on Methodology and Application**\n\nKnowledge distillation on the backbone features is utilized in all these methods. However, Chen et al distill not only the feature but also the classification logits and bounding box regression results, which has limited their application scenes in one stage and anchor-free models. Li et al. and Wang et al. distill the features in the regions of proposals and near object anchor locations, respectively. As a result, their methods rely on the supervision of anchors and ground truths and can't be utilized in one stage and anchor free models. Bajestani&Yang's method is utilized for active perception on video data, which can not be utilized in image-based detection. \n\nIn contrast, in our method, the attention mask and relation information can be easily generated from the backbone features, which has no requirements on ground truths, anchors or proposals. As a result, it can be easily used in different kinds of models and tasks without any modification. \n\n**Comparison on Motivation**\n\nChen et al.'s method is a direct application of knowledge distillation on object detection. The other three methods and our method are motivated by the imbalance between foreground and background pixels and these methods try to address this issue by reweighting the distillation loss. Besides, our method is also motivated by the effect of the relation among pixels in an image, which is ignored by the other methods. \n\n| Method | Motivation 1: Imbalance between foreground and background | Motivation 2: Lack of Relation Distillation |\n| :--------------------: | :-------------------------------------------------------: | :-----------------------------------------: |\n|Our Method |\u221a|\u221a|\n|Chen et al., 2017|\u00d7|\u221a|\n|Li et al., 2017 |\u221a|\u00d7|\n|Wang et al.2019 |\u221a|\u00d7|\n|Bajestani & Yang, 2020 |\u221a| \u00d7|\n\n\n**Question2. About the attention in attention-guided distillation.**\n\n**What does it capture?** The attention captures the crucial pixels in the feature map. For example, as shown in Figure.3, most of the pixels with high attention value focus on the head, legs, and eyes of the horses while the pixels of the background have much lower attention value.\n\nThe attention here is generated by summing the absolute value of features in the channel dimension. As a result, the pixel which has a larger absolute value will have a larger attention value. Note that this observation has been also utilized and discussed in [1-2]. The visualization and distribution of attention have been provided in Figure3.\n\n**Failure case.** The attention in attention-guided distillation may fail in a few specific cases. For example, in Figure3, some pixels in the tree also have high attention value. But these failure attention cases are much less than the correct cases and thus they will not introduce much negative influence during knowledge distillation.\n\n[1] Sergey Zagoruyko et al. Paying more attention and Nikos Komodakis. attention: Improving the performance of convolutional neural networks via attention transfer. ICLR, 2017.\n\n[2] Yuenan Hou et al. Learning lightweight lane detection cnns by self attention distillation. ICCV2019.\n\nNote that these two papers have also been cited in our paper.\n\n**Question3.Results on Cityscapes.**\nThanks for your advice on conducting experiments on Cityscapes. As shown in the table, we have conducted experiments with Faster R-CNN and Mask R-CNN on Cityscapes. It is observed that our method also leads to significant AP improvements in Cityscapes. We will add more Cityscapes experiments in the camera-ready vision.\n\n| Model| Backbone | Distill ? | Box AP | Mask AP |\n| ------------ | -------- | --------- | ------ | ------- |\n| Faster R-CNN|ResNet50 |\u00d7| 40.3| N/A|\n| Faster R-CNN|ResNet50 |\u221a| 43.5| N/A|\n| Mask R-CNN|ResNet50 |\u00d7| 41.0| 35.8|\n| Mask R-CNN|ResNet50 |\u221a| 43.0 | 37.5|\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper182/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors", "authorids": ["~Linfeng_Zhang2", "~Kaisheng_Ma1"], "authors": ["Linfeng Zhang", "Kaisheng Ma"], "keywords": ["Knowledge Distillation", "Object Detection", "Teacher-Student Learning", "Non-Local Modules", "Attention Modules"], "abstract": "Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.", "one-sentence_summary": "We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.", "pdf": "/pdf/1e6969024e2fab8681c02ff62a2dbfc4feedcff4.pdf", "supplementary_material": "/attachment/86459651e7df0e37f79b10135a42a3b6779d5666.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|improve_object_detection_with_featurebased_knowledge_distillation_towards_accurate_and_efficient_detectors", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021improve,\ntitle={Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors},\nauthor={Linfeng Zhang and Kaisheng Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=uKhGRvM8QNH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uKhGRvM8QNH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper182/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper182/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper182/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper182/Authors|ICLR.cc/2021/Conference/Paper182/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873738, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper182/-/Official_Comment"}}}, {"id": "fBIfBEmWHxm", "original": null, "number": 2, "cdate": 1605511938584, "ddate": null, "tcdate": 1605511938584, "tmdate": 1605676482636, "tddate": null, "forum": "uKhGRvM8QNH", "replyto": "ob0v3MR2Dj", "invitation": "ICLR.cc/2021/Conference/Paper182/-/Official_Comment", "content": {"title": "Thanks for your insightful comments! We have added experiments on small backbones and corrected the name of models according to your advice.", "comment": "**We sincerely appreciate your detailed and thoughtful reviews and hope that our response can address your concerns.**\n\n**Question1. Experiments on smaller models**\nThanks for your suggestions on conducting experiments on smaller models. The following table shows the experiment results on Faster R-CNN and RetianNet with smaller backbones (ResNet18 and RegNet - 800M) on MS COCO2017. Besides, we also evaluate our methods on compact detectors, including Yolo v3 and SSD. It is observed that our method also achieves significant AP improvements on smaller models. We will report more experiments on small backbones in the camera-ready vision.\n\n\n| Model        | Backbone    | Distill? | AP (MS COCO)   | Inference Time (FPS)  | Params (M) |\n| ------------ | ----------- | -------- | ---- | ---- | ---------- |\n| Faster R-CNN | ResNet18    | \u00d7        | 34.6  | 28.1 | 30.57      |\n| Faster R-CNN | ResNet18    | \u221a        | 37.0 | 28.1 | 30.57      |\n| Grid R-CNN   | ResNet18    | x        | 36.6 | 26.7 | 66.37      |\n| Grid R-CNN   | ResNet18    | \u221a        | 38.8 | 26.7 | 66.37      |\n| RetinaNet    | ResNet18    | \u00d7        | 33.4 | 25.8 | 23.30      |\n| RetinaNet    | ResNet18    | \u221a        | 35.9 | 25.8 | 23.30      |\n| RetinaNet    | RegNet-800M | \u00d7        | 35.6 | 22.4 | 19.27      |\n| RetinaNet    | RegNet-800M | \u221a        | 38.4 | 22.4 | 19.27      |\n| Yolo v3      | DarkNet53   | \u00d7        | 33.4 | 42.2 | 61.95      |\n| Yolo v3      | DarkNet53   | \u221a        | 35.8 | 42.2 | 61.95      |\n| SSD          | VGG16       | \u00d7        | 29.4 | 26.1 | 38.08      |\n| SSD          | VGG16       | \u221a        | 31.2 | 26.1 | 38.08      |\n\n\n\n**Question2. Paper writing**\n\nThanks for your suggestion, we will carefully revise the manuscript to make it easier to understand.\n\n**Question3. The name of models**\n\nThanks for your suggestions on the names of models. We have addressed these issues in the rebuttal revision according to your advice.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper182/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors", "authorids": ["~Linfeng_Zhang2", "~Kaisheng_Ma1"], "authors": ["Linfeng Zhang", "Kaisheng Ma"], "keywords": ["Knowledge Distillation", "Object Detection", "Teacher-Student Learning", "Non-Local Modules", "Attention Modules"], "abstract": "Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.", "one-sentence_summary": "We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.", "pdf": "/pdf/1e6969024e2fab8681c02ff62a2dbfc4feedcff4.pdf", "supplementary_material": "/attachment/86459651e7df0e37f79b10135a42a3b6779d5666.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|improve_object_detection_with_featurebased_knowledge_distillation_towards_accurate_and_efficient_detectors", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021improve,\ntitle={Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors},\nauthor={Linfeng Zhang and Kaisheng Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=uKhGRvM8QNH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uKhGRvM8QNH", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper182/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper182/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper182/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper182/Authors|ICLR.cc/2021/Conference/Paper182/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873738, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper182/-/Official_Comment"}}}, {"id": "ob0v3MR2Dj", "original": null, "number": 3, "cdate": 1603980393824, "ddate": null, "tcdate": 1603980393824, "tmdate": 1605024745157, "tddate": null, "forum": "uKhGRvM8QNH", "replyto": "uKhGRvM8QNH", "invitation": "ICLR.cc/2021/Conference/Paper182/-/Official_Review", "content": {"title": "review", "review": "This paper explores the knowledge distillation problem in object detection. It claims that the failure of knowledge distillation in object detection is mainly caused by the imbalance between pixels of foreground and background, and the relation distillation between different pixels. The authors then propose non-local distillation to tackle the problem. Extensive experiments are conducted on MS COCO and verify the effectiveness of the proposed method. \n\n1. Strengthens\n\na. This paper explores the knowledge distillation problem in object detection and gives promising results and conclusions. \n\nb. The results with different detectors greatly prove the effectiveness of the method. \n\nc. It is good that the distillation method does not bring any extra costs during the inference time.\n\n2. Weaknesses\n\na. I recommend the authors do experiments on much smaller models. For example, in real cases, mimicking ResNet50 with larger models is meaningless. It would be great if we can observe large improvements when mimicking a small backbone with ResNet-50 or even larger models. These conclusions and experiments are crucial for the applications of the method.\n\nb. The English writing of this paper can be improved. \n\nc. The names of the models in this paper are not proper. It is weird for me to call Faster RCNN101. Generally, Faster R-CNN is Faster R-CNN, and ResNet is ResNet. You can use any backbones with Faster R-CNN. Please define it as Faster R-CNN with ResNet101. \n\nAs stated comments, I think this paper is good for knowledge distillation in object detection and publishable, but encourages the authors improve the draft according to the weaknesses.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper182/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper182/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors", "authorids": ["~Linfeng_Zhang2", "~Kaisheng_Ma1"], "authors": ["Linfeng Zhang", "Kaisheng Ma"], "keywords": ["Knowledge Distillation", "Object Detection", "Teacher-Student Learning", "Non-Local Modules", "Attention Modules"], "abstract": "Knowledge distillation, in which a student model is trained to mimic a teacher model, has been proved as an effective technique for model compression and model accuracy boosting. However, most knowledge distillation methods, designed for image classification, have failed on more challenging tasks, such as object detection. In this paper, we suggest that the failure of knowledge distillation on object detection is mainly caused by two reasons: (1) the imbalance between pixels of foreground and background and (2) lack of distillation on the relation between different pixels. Observing the above reasons, we propose attention-guided distillation and non-local distillation to address the two problems, respectively.  Attention-guided distillation is proposed to find the crucial pixels of foreground objects with attention mechanism and then make the students take more effort to learn their features. Non-local distillation is proposed to enable students to learn not only the feature of an individual pixel but also the relation between different pixels captured by non-local modules. Experiments show that our methods achieve excellent AP improvements on both one-stage and two-stage, both anchor-based and anchor-free detectors. For example, Faster RCNN (ResNet101 backbone) with our distillation achieves 43.9 AP on COCO2017, which is 4.1 higher than the baseline. Codes have been released on Github.", "one-sentence_summary": "We propose two knowledge distillation methods on object detection - attention-guided distillation and non-local distillation which lead to 4.1 AP improvements on Faster RCNN101 in MS COCO2017.", "pdf": "/pdf/1e6969024e2fab8681c02ff62a2dbfc4feedcff4.pdf", "supplementary_material": "/attachment/86459651e7df0e37f79b10135a42a3b6779d5666.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|improve_object_detection_with_featurebased_knowledge_distillation_towards_accurate_and_efficient_detectors", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhang2021improve,\ntitle={Improve Object Detection with Feature-based Knowledge Distillation: Towards Accurate and Efficient Detectors},\nauthor={Linfeng Zhang and Kaisheng Ma},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=uKhGRvM8QNH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uKhGRvM8QNH", "replyto": "uKhGRvM8QNH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538148602, "tmdate": 1606915801004, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper182/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper182/-/Official_Review"}}}], "count": 8}