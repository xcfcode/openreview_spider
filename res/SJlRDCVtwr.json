{"notes": [{"id": "SJlRDCVtwr", "original": "rJxLCqv_wS", "number": 1195, "cdate": 1569439334483, "ddate": null, "tcdate": 1569439334483, "tmdate": 1577168277706, "tddate": null, "forum": "SJlRDCVtwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Simplicial Complex Networks", "authors": ["Mohammad Firouzi", "Sadra Boreiri", "Hamed Firouzi"], "authorids": ["mfirouzi@alphabist.com", "sadra.boreiri@epfl.ch", "hfirouzi@alphabist.com"], "keywords": ["topological data analysis", "supervised learning", "simplicial approximation"], "TL;DR": "A novel method for supervised learning through subdivisioning the input space along with function approximation.", "abstract": "Universal approximation property of neural networks is one of the motivations to use these models in various real-world problems. However, this property is not the only characteristic that makes neural networks unique as there is a wide range of other approaches with similar property. Another characteristic which makes these models interesting is that they can be trained with the backpropagation algorithm which allows an efficient gradient computation and gives these universal approximators the ability to efficiently learn complex manifolds from a large amount of data in different domains. Despite their abundant use in practice, neural networks are still not well understood and a broad range of ongoing research is to study the interpretability of neural networks. On the other hand, topological data analysis (TDA) relies on strong theoretical framework of (algebraic) topology along with other mathematical tools for analyzing possibly complex datasets. In this work, we leverage a universal approximation theorem originating from algebraic topology to build a connection between TDA and common neural network training framework. We introduce the notion of automatic subdivisioning and devise a particular type of neural networks for regression tasks: Simplicial Complex Networks (SCNs). SCN's architecture is defined with a set of bias functions along with a particular policy during the forward pass which alternates the common architecture search framework in neural networks. We believe the view of SCNs can be used as a step towards building interpretable deep learning models. Finally, we verify its performance on a set of regression problems.", "pdf": "/pdf/5635fefb9533068eb43b1f628d68824431d7f884.pdf", "paperhash": "firouzi|simplicial_complex_networks", "original_pdf": "/attachment/5635fefb9533068eb43b1f628d68824431d7f884.pdf", "_bibtex": "@misc{\nfirouzi2020simplicial,\ntitle={Simplicial Complex Networks},\nauthor={Mohammad Firouzi and Sadra Boreiri and Hamed Firouzi},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlRDCVtwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LzwcVP0Qg", "original": null, "number": 1, "cdate": 1576798717063, "ddate": null, "tcdate": 1576798717063, "tmdate": 1576800919476, "tddate": null, "forum": "SJlRDCVtwr", "replyto": "SJlRDCVtwr", "invitation": "ICLR.cc/2020/Conference/Paper1195/-/Decision", "content": {"decision": "Reject", "comment": "The aper introduces simplicial complex networks, a new class of\nneural networks based on the idea of the subdivision of a simplicial\ncomplex. The paper is interesting and brings ideas of algebraic topology to inform the design of new neural network architectures. \n\nReviewer 1 was positive about the ideas of this paper, but had several concerns about clarity, scalablity and the sense that the paper might still be in an early phase. Reviewer 2 had similar concerns about clarity, comparisons, and usefulness. Although there were no responses form the author, the discussion explored the paper further, but continued to think the idea is still in its early phase.\n\nThe paper is not currently ready for acceptance, and we hope the authors will find useful feedback for their ongoing reasearch. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simplicial Complex Networks", "authors": ["Mohammad Firouzi", "Sadra Boreiri", "Hamed Firouzi"], "authorids": ["mfirouzi@alphabist.com", "sadra.boreiri@epfl.ch", "hfirouzi@alphabist.com"], "keywords": ["topological data analysis", "supervised learning", "simplicial approximation"], "TL;DR": "A novel method for supervised learning through subdivisioning the input space along with function approximation.", "abstract": "Universal approximation property of neural networks is one of the motivations to use these models in various real-world problems. However, this property is not the only characteristic that makes neural networks unique as there is a wide range of other approaches with similar property. Another characteristic which makes these models interesting is that they can be trained with the backpropagation algorithm which allows an efficient gradient computation and gives these universal approximators the ability to efficiently learn complex manifolds from a large amount of data in different domains. Despite their abundant use in practice, neural networks are still not well understood and a broad range of ongoing research is to study the interpretability of neural networks. On the other hand, topological data analysis (TDA) relies on strong theoretical framework of (algebraic) topology along with other mathematical tools for analyzing possibly complex datasets. In this work, we leverage a universal approximation theorem originating from algebraic topology to build a connection between TDA and common neural network training framework. We introduce the notion of automatic subdivisioning and devise a particular type of neural networks for regression tasks: Simplicial Complex Networks (SCNs). SCN's architecture is defined with a set of bias functions along with a particular policy during the forward pass which alternates the common architecture search framework in neural networks. We believe the view of SCNs can be used as a step towards building interpretable deep learning models. Finally, we verify its performance on a set of regression problems.", "pdf": "/pdf/5635fefb9533068eb43b1f628d68824431d7f884.pdf", "paperhash": "firouzi|simplicial_complex_networks", "original_pdf": "/attachment/5635fefb9533068eb43b1f628d68824431d7f884.pdf", "_bibtex": "@misc{\nfirouzi2020simplicial,\ntitle={Simplicial Complex Networks},\nauthor={Mohammad Firouzi and Sadra Boreiri and Hamed Firouzi},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlRDCVtwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJlRDCVtwr", "replyto": "SJlRDCVtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727477, "tmdate": 1576800279724, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1195/-/Decision"}}}, {"id": "S1lAcZpKFH", "original": null, "number": 1, "cdate": 1571570070046, "ddate": null, "tcdate": 1571570070046, "tmdate": 1572972500358, "tddate": null, "forum": "SJlRDCVtwr", "replyto": "SJlRDCVtwr", "invitation": "ICLR.cc/2020/Conference/Paper1195/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "1. Summary of the paper\n\nThis paper introduces *simplicial complex networks*, a new class of\nneural networks based on the idea of the subdivision of a simplicial\ncomplex. Using the simplicial approximation, a classical theorem in\nalgebraic topology, the paper demonstrates that the network is capable\nof learning a suitable function approximation that in turn can be used\nfor regression tasks.\n\n2. Summary of the review\n\nThis paper brings an interesting perspective to the table, viz. the use\nof concepts from algebraic topology to develop new neural network\narchitectures with improved approximation capabilities. I am\nappreciative and excited of any work in this direction, yet I deem the\ncurrent version of this paper not acceptable for publication because of\nthe following issues:\n\n- The paper suffers from a lack of clarity that makes assessing the\n  results and the proposed architecture harder than it needs to be.\n  In particular, numerous aspects such as scalability or the\n  generalization capabilities are glossed over. Even though I am\n  familiar with the underlying concepts and techniques, I found the\n  exposition not always easy to follow.\n\n- The experiments in the paper appear to be highly preliminary and need\n  more refinement. I understand that it is tough to develop a new\n  technique and 'compete'  against so many already existing ones, but\n  nevertheless I would suggest to at least extend one of the experiments\n  to a larger-scale comparison. Personally, I found the function\n  approximation experiment to be very interesting, so to me, this would\n  be a natural starting point for a more in-depth comparison.\n\nAgain, I want to stress that I think this paper has the potential to\nbecome a strong contribution but for this to happen, it requires an\nadditional revision cycle. I will outline some suggestions below.\n\n3. Clarity & technical correctness\n\nIt is my understanding that the paper is technically correct on all\nlevels. At times, imprecise language is used that might mislead or\nconfuse readers.\n\nMy overall recommendation would be to start the paper with a brief\nintuition of _why_ BCS is required and how to make use of the\napproximation theorem. After this, I would propose a high-level overview\nof the new architecture and its individual components. Only _then_ would\nI suggest describing each of these components in more detail.\n\nHere are some detailed suggestions:\n\n- The flow of the abstract could be improved; it is not clear why TDA\n  methods are suitable here.\n\n- Topological data analysis is not directly a 'geometric approach' for\n  data analysis. It incorporates specific attributes of geometry by\n  means of filtrations, for example, to 'escape' the coarseness of\n  unweighted simplicial complexes\n\n- TDA is also not _just_ restricted to point clouds\n\n- I find the difference between 'deterministic' and 'statistical'\n  methods incorrect in the introduction; the kernel-based approaches\n  that were developed to compare persistence diagrams, for example, are\n  _not_ the opposite of 'deterministic'. I would rather make\n  a distinction between 'distance-based methods' that are hard to\n  compute, and 'kernel-based methods' with improved scalability.\n\n- When analysing point clouds, TDA does not use 'an inducing' distance;\n  it can make use of the induced distance of a graph, for example, but\n  typically, the distances are pre-defined and evaluated between all\n  pairs of points.\n\n- I find the statement that hidden units are conceptually well-defined\n  only in SCNs to be highly problematic; I fully agree that\n  interpretability of architectures is relevant, but another expression\n  should be used here---FCN architectures (or in fact, all other\n  architectures in deep learning) are perfectly well-defined on the\n  conceptual level. Maybe the paper should be more clear about what it\n  expects the 'role' of a hidden unit to be; in fact, this is also not\n  discussed satisfactorily at present.\n\n- Introduce the concept of a *simplex* briefly before discussing\n  simplicial complexes.\n\n- Figure 1 cannot be understood in the context in which it is placed\n  currently; either move it back or create another figure that is more\n  high-level and explains the concepts\n\n- The requirement that all data points are within the standard\n  probability simplex seems vacuous to me; this can always be achieved\n  by scaling. Am I misunderstanding this? I would rewrite this and make\n  it clear that this is _not_ a limiting assumption.\n\n- Definition 2 is lacking formality; please add a brief description of\n  the function itself\n\n- Definition 3 is could use some more intuition; I understand that it\n  is technically correct, but a brief illustration of the iterative\n  nature of subdivision would be helpful.\n\n- After introducing the simplicial approximation theorem, please give\n  a brief connection on its relevance to the problem at hand.\n\n- The brief introduction of the Vietoris--Rips complex is relatively\n  hard to understand; please rephrase it. It is the complex in which\n  a set of points is a simplex if and only if the pairwise distances\n  of the points are less than or equal to the pre-defined threshold.\n\n- Section 2.3 is highly confusing at first because it requires readers\n  to understand Algorithm 1; please rephrase the introduction to this\n  section to make it clear what the _purpose_ is. To my understanding,\n  you want to use the BCS in order to obtain a situation as described\n  by the approximation theorem---but at this point, the utility of the\n  subdivision is not clear yet.\n\n- What does it mean that weights are 'specified to the BCS'?\n\n- When discussing the 'data-driven' approach, please add a motivation\n  for this. I understand that such a subdivision is required in order\n  to be computationally feasible, but this needs to be motivated.\n\n- Lemma 1 appears to be a direct consequence of the definition of\n  simplicial subdivision. I would suggest moving its proof to the\n  appendix.\n\n- Please give an improved explanation and intuition of the parameters of\n  the subdivision and why they require updating. It took me a while to\n  understand the relevance of the weights and what is actually updated\n  in the algorithm.\n\n- When talking about SCNs in Section 2.4, the salient points (learning\n  a subdivision with trainable vertex function values etc.) need to be\n  mentioned well before this section. When reading the paper for the\n  first time, I started understanding the method better while reading\n  the section, so I think that this intuition should become as early as\n  possible.\n\n- Please add intuition why _finding_ the position of a point is\n  required; I understand that this is necessary in order to obtain\n  weights and neighbours, from which to obtain a mapping, but at first,\n  this 'location problem' struck me as artificial.\n\n- When talking about the 'network policy', more details are needed. How\n  relevant is this choice for the network?\n\n- When discussing related work, please look at the follow-up\n  publication from Hofer et al., which discuss more architectures and\n  use cases:\n\n      Hofer et al., Connectivity-Optimized Representation Learning via Persistent Homology\n      ICML 2019\n\n- 'Topological geometry' on p. 7 is an unfamiliar term to me. What is\n  meant by this?\n\n- What about the computational complexity? This is somewhat glossed over\n  in the description of the BCS algorithm. Is this currently\n  a bottleneck for the deploying the architecture?\n\n- How easy would it be to extend this to _classification_ problems? The\n  title suggests that the architecture is highly generic, yet the paper\n  only discusses regression tasks. At the very least, this should be\n  briefly discussed.\n\n4. Experiments\n\n- The experiments are lacking a brief explanation: what are they\n  supposed to show or analyse? I find the function approximation\n  example to be very interesting, but somewhat preliminary insofar\n  they only compare *two* functions. Why not generate sequences or\n  families of functions and make this into a large-scale comparison?\n\n- I found the discussion of storage to be somewhat surprising; is this\n  a property that the paper wants to stress for SCNs? If so, more\n  experiments in that direction are required.\n\n- The observations on bias function choice are not reflected in the\n  experiments that are presented in the main text. I understand that it\n  works for the toy examples, but a more in-depth comparison is required\n  for this sort of experiment to be compelling.\n\n- Is it possible to _tune_ the number of subdivisions or provide an\n  upper bound? It would be interesting to see how such a bound might\n  change the results---this is just a suggestion, but I feel that it\n  might be interesting to discuss how much performance or approximation\n  qualities are gained by _not_ restricting the BCS algorithm.\n\n5. Minor style comments\n\nThe presentation quality of the paper could be improved. Here are some\nsuggestions.\n\n- Please use `\\citep` when citing works in the text; currently, citations are intermingled with sentences and make it harder to read the paper.\n- 'Universal approximation property' --> 'The universal approximation property'\n- 'with similar property' --> 'with similar properties'\n- 'alternates the common architecture' --> 'changes the common architecture'\n- the citation 'Bernardo et al.' should be fixed; it is missing information\n- I suggest removing the sentences 'The rest of this paper is organized as follows'...\n- 'Let f denotes a mapping' --> 'Let f denote a mapping'\n- 'As theorem 2' --> 'As theorem 1'\n- 'we can subdivision' --> 'we can subdivide'\n- 'a same' --> 'the same'\n- 'that how the position' --> 'how the position'\n- 'used to fed' --> 'used to feed'\n- 'the appendix F' --> 'appendix F'\n- 'may outputs' --> 'may output'\n- 'this phenomena' --> 'this phenomenon'\n- 'bias function observed to be' -->' bias function was observed to be'\n' 'can be continued' --> 'can be extended'"}, "signatures": ["ICLR.cc/2020/Conference/Paper1195/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1195/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simplicial Complex Networks", "authors": ["Mohammad Firouzi", "Sadra Boreiri", "Hamed Firouzi"], "authorids": ["mfirouzi@alphabist.com", "sadra.boreiri@epfl.ch", "hfirouzi@alphabist.com"], "keywords": ["topological data analysis", "supervised learning", "simplicial approximation"], "TL;DR": "A novel method for supervised learning through subdivisioning the input space along with function approximation.", "abstract": "Universal approximation property of neural networks is one of the motivations to use these models in various real-world problems. However, this property is not the only characteristic that makes neural networks unique as there is a wide range of other approaches with similar property. Another characteristic which makes these models interesting is that they can be trained with the backpropagation algorithm which allows an efficient gradient computation and gives these universal approximators the ability to efficiently learn complex manifolds from a large amount of data in different domains. Despite their abundant use in practice, neural networks are still not well understood and a broad range of ongoing research is to study the interpretability of neural networks. On the other hand, topological data analysis (TDA) relies on strong theoretical framework of (algebraic) topology along with other mathematical tools for analyzing possibly complex datasets. In this work, we leverage a universal approximation theorem originating from algebraic topology to build a connection between TDA and common neural network training framework. We introduce the notion of automatic subdivisioning and devise a particular type of neural networks for regression tasks: Simplicial Complex Networks (SCNs). SCN's architecture is defined with a set of bias functions along with a particular policy during the forward pass which alternates the common architecture search framework in neural networks. We believe the view of SCNs can be used as a step towards building interpretable deep learning models. Finally, we verify its performance on a set of regression problems.", "pdf": "/pdf/5635fefb9533068eb43b1f628d68824431d7f884.pdf", "paperhash": "firouzi|simplicial_complex_networks", "original_pdf": "/attachment/5635fefb9533068eb43b1f628d68824431d7f884.pdf", "_bibtex": "@misc{\nfirouzi2020simplicial,\ntitle={Simplicial Complex Networks},\nauthor={Mohammad Firouzi and Sadra Boreiri and Hamed Firouzi},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlRDCVtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlRDCVtwr", "replyto": "SJlRDCVtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1195/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1195/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576595077602, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1195/Reviewers"], "noninvitees": [], "tcdate": 1570237740942, "tmdate": 1576595077620, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1195/-/Official_Review"}}}, {"id": "BkgPbIw2tB", "original": null, "number": 2, "cdate": 1571743231264, "ddate": null, "tcdate": 1571743231264, "tmdate": 1572972500321, "tddate": null, "forum": "SJlRDCVtwr", "replyto": "SJlRDCVtwr", "invitation": "ICLR.cc/2020/Conference/Paper1195/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents a new neural network architecture called simplicial complex networks. The authors draw inspiration from topological data analysis in order to motivate this new architecture. The SCN architecture is motivated through a parametric sub-divisioning of the input space into a complex of d-dimensional simplices. The authors give a universal approximation theorem for a model of this type, and give a simple algorithm to perform the sub-divisioning. The resulting network output is a convex combination of vertices after l nested sub-divisions. The authors present a number of toy experiments to validate their claims.\n\nAlthough I believe the authors present an intriguing idea, I ultimately tend for a rejection of this paper. I have three major reasons for this: \n(1) Quality of writing and clarity is lacking: Citations are all done in-line, symbols are being used without proper explanation, and many sentences are just purely broken and impossible to understand. \n(2) The authors do not properly compare their approach to the most common architectures in use: Fully-connected nets and CNNs. What are the shortcomings and benefits of SCNs as compared to these architectures? \n(3) Usefulness of the approach. I believe this ties in with point (2). The authors make no explicit mention of the computational complexity of their approach, or how the number of parameters depends on the number of input dimensions. Since the simplicial complex is defined on the input space the number of parameters scales with the number of input dimensions, and becomes restrictively high. This explains why the author use only low-dimensional toy examples."}, "signatures": ["ICLR.cc/2020/Conference/Paper1195/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1195/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simplicial Complex Networks", "authors": ["Mohammad Firouzi", "Sadra Boreiri", "Hamed Firouzi"], "authorids": ["mfirouzi@alphabist.com", "sadra.boreiri@epfl.ch", "hfirouzi@alphabist.com"], "keywords": ["topological data analysis", "supervised learning", "simplicial approximation"], "TL;DR": "A novel method for supervised learning through subdivisioning the input space along with function approximation.", "abstract": "Universal approximation property of neural networks is one of the motivations to use these models in various real-world problems. However, this property is not the only characteristic that makes neural networks unique as there is a wide range of other approaches with similar property. Another characteristic which makes these models interesting is that they can be trained with the backpropagation algorithm which allows an efficient gradient computation and gives these universal approximators the ability to efficiently learn complex manifolds from a large amount of data in different domains. Despite their abundant use in practice, neural networks are still not well understood and a broad range of ongoing research is to study the interpretability of neural networks. On the other hand, topological data analysis (TDA) relies on strong theoretical framework of (algebraic) topology along with other mathematical tools for analyzing possibly complex datasets. In this work, we leverage a universal approximation theorem originating from algebraic topology to build a connection between TDA and common neural network training framework. We introduce the notion of automatic subdivisioning and devise a particular type of neural networks for regression tasks: Simplicial Complex Networks (SCNs). SCN's architecture is defined with a set of bias functions along with a particular policy during the forward pass which alternates the common architecture search framework in neural networks. We believe the view of SCNs can be used as a step towards building interpretable deep learning models. Finally, we verify its performance on a set of regression problems.", "pdf": "/pdf/5635fefb9533068eb43b1f628d68824431d7f884.pdf", "paperhash": "firouzi|simplicial_complex_networks", "original_pdf": "/attachment/5635fefb9533068eb43b1f628d68824431d7f884.pdf", "_bibtex": "@misc{\nfirouzi2020simplicial,\ntitle={Simplicial Complex Networks},\nauthor={Mohammad Firouzi and Sadra Boreiri and Hamed Firouzi},\nyear={2020},\nurl={https://openreview.net/forum?id=SJlRDCVtwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJlRDCVtwr", "replyto": "SJlRDCVtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1195/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1195/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576595077602, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1195/Reviewers"], "noninvitees": [], "tcdate": 1570237740942, "tmdate": 1576595077620, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1195/-/Official_Review"}}}], "count": 4}