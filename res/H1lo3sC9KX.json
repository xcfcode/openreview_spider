{"notes": [{"id": "H1lo3sC9KX", "original": "B1xK5SaqKm", "number": 740, "cdate": 1538087858708, "ddate": null, "tcdate": 1538087858708, "tmdate": 1545355416912, "tddate": null, "forum": "H1lo3sC9KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Asynchronous SGD without gradient delay for efficient distributed training", "abstract": "Asynchronous distributed gradient descent algorithms for training of deep neural\nnetworks are usually considered as inefficient, mainly because of the Gradient delay\nproblem. In this paper, we propose a novel asynchronous distributed algorithm\nthat tackles this limitation by well-thought-out averaging of model updates, computed\nby workers. The algorithm allows computing gradients along the process\nof gradient merge, thus, reducing or even completely eliminating worker idle time\ndue to communication overhead, which is a pitfall of existing asynchronous methods.\nWe provide theoretical analysis of the proposed asynchronous algorithm,\nand show its regret bounds. According to our analysis, the crucial parameter for\nkeeping high convergence rate is the maximal discrepancy between local parameter\nvectors of any pair of workers. As long as it is kept relatively small, the\nconvergence rate of the algorithm is shown to be the same as the one of a sequential\nonline learning. Furthermore, in our algorithm, this discrepancy is bounded\nby an expression that involves the staleness parameter of the algorithm, and is\nindependent on the number of workers. This is the main differentiator between\nour approach and other solutions, such as Elastic Asynchronous SGD or Downpour\nSGD, in which that maximal discrepancy is bounded by an expression that\ndepends on the number of workers, due to gradient delay problem. To demonstrate\neffectiveness of our approach, we conduct a series of experiments on image\nclassification task on a cluster with 4 machines, equipped with a commodity communication\nswitch and with a single GPU card per machine. Our experiments\nshow a linear scaling on 4-machine cluster without sacrificing the test accuracy,\nwhile eliminating almost completely worker idle time. Since our method allows\nusing commodity communication switch, it paves a way for large scale distributed\ntraining performed on commodity clusters.", "keywords": ["SGD", "distributed asynchronous training", "deep learning", "optimisation"], "authorids": ["roma.talyansky@gmail.com", "pavel.kisilev@huawei.com", "zach.melamed@huawei.com", "natan.peterfreund@gmail.com", "uri.verner@gmail.com"], "authors": ["Roman Talyansky", "Pavel Kisilev", "Zach Melamed", "Natan Peterfreund", "Uri Verner"], "TL;DR": "A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.", "pdf": "/pdf/6e08f084d5f9d00fc434104ad52965caee3ec0f7.pdf", "paperhash": "talyansky|asynchronous_sgd_without_gradient_delay_for_efficient_distributed_training", "_bibtex": "@misc{\ntalyansky2019asynchronous,\ntitle={Asynchronous {SGD} without gradient delay for efficient distributed training},\nauthor={Roman Talyansky and Pavel Kisilev and Zach Melamed and Natan Peterfreund and Uri Verner},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lo3sC9KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkgxb5znkE", "original": null, "number": 1, "cdate": 1544460792329, "ddate": null, "tcdate": 1544460792329, "tmdate": 1545354497978, "tddate": null, "forum": "H1lo3sC9KX", "replyto": "H1lo3sC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper740/Meta_Review", "content": {"metareview": "Improving the staleness of asynchronous SGD is an important topic. This paper proposed an algorithm to restrict the staleness and provided theoretical analysis. However, the reviewers did not consider the proposed algorithm a significant contribution. The paper still did not solve the staleness problem, and it was lack of discussion or experimental comparison with the state of the art ASGD algorithms. Reviewer 3 also found the explanation of the algorithm hard to follow.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Not significant contribution and not sufficient experiments"}, "signatures": ["ICLR.cc/2019/Conference/Paper740/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper740/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Asynchronous SGD without gradient delay for efficient distributed training", "abstract": "Asynchronous distributed gradient descent algorithms for training of deep neural\nnetworks are usually considered as inefficient, mainly because of the Gradient delay\nproblem. In this paper, we propose a novel asynchronous distributed algorithm\nthat tackles this limitation by well-thought-out averaging of model updates, computed\nby workers. The algorithm allows computing gradients along the process\nof gradient merge, thus, reducing or even completely eliminating worker idle time\ndue to communication overhead, which is a pitfall of existing asynchronous methods.\nWe provide theoretical analysis of the proposed asynchronous algorithm,\nand show its regret bounds. According to our analysis, the crucial parameter for\nkeeping high convergence rate is the maximal discrepancy between local parameter\nvectors of any pair of workers. As long as it is kept relatively small, the\nconvergence rate of the algorithm is shown to be the same as the one of a sequential\nonline learning. Furthermore, in our algorithm, this discrepancy is bounded\nby an expression that involves the staleness parameter of the algorithm, and is\nindependent on the number of workers. This is the main differentiator between\nour approach and other solutions, such as Elastic Asynchronous SGD or Downpour\nSGD, in which that maximal discrepancy is bounded by an expression that\ndepends on the number of workers, due to gradient delay problem. To demonstrate\neffectiveness of our approach, we conduct a series of experiments on image\nclassification task on a cluster with 4 machines, equipped with a commodity communication\nswitch and with a single GPU card per machine. Our experiments\nshow a linear scaling on 4-machine cluster without sacrificing the test accuracy,\nwhile eliminating almost completely worker idle time. Since our method allows\nusing commodity communication switch, it paves a way for large scale distributed\ntraining performed on commodity clusters.", "keywords": ["SGD", "distributed asynchronous training", "deep learning", "optimisation"], "authorids": ["roma.talyansky@gmail.com", "pavel.kisilev@huawei.com", "zach.melamed@huawei.com", "natan.peterfreund@gmail.com", "uri.verner@gmail.com"], "authors": ["Roman Talyansky", "Pavel Kisilev", "Zach Melamed", "Natan Peterfreund", "Uri Verner"], "TL;DR": "A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.", "pdf": "/pdf/6e08f084d5f9d00fc434104ad52965caee3ec0f7.pdf", "paperhash": "talyansky|asynchronous_sgd_without_gradient_delay_for_efficient_distributed_training", "_bibtex": "@misc{\ntalyansky2019asynchronous,\ntitle={Asynchronous {SGD} without gradient delay for efficient distributed training},\nauthor={Roman Talyansky and Pavel Kisilev and Zach Melamed and Natan Peterfreund and Uri Verner},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lo3sC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper740/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353102699, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1lo3sC9KX", "replyto": "H1lo3sC9KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper740/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper740/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper740/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353102699}}}, {"id": "SJeYoULs2m", "original": null, "number": 3, "cdate": 1541265057018, "ddate": null, "tcdate": 1541265057018, "tmdate": 1541533726463, "tddate": null, "forum": "H1lo3sC9KX", "replyto": "H1lo3sC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper740/Official_Review", "content": {"title": "Interesting paper but the contribution seems not be good enough", "review": "Overall, this paper is well written and clearly present their main contribution.\nHowever, the novel asynchronous distributed algorithm seems not be significant enough.\nThe delayed gradient condition has been widely discussed, but there are not enough comparison between these variants.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper740/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Asynchronous SGD without gradient delay for efficient distributed training", "abstract": "Asynchronous distributed gradient descent algorithms for training of deep neural\nnetworks are usually considered as inefficient, mainly because of the Gradient delay\nproblem. In this paper, we propose a novel asynchronous distributed algorithm\nthat tackles this limitation by well-thought-out averaging of model updates, computed\nby workers. The algorithm allows computing gradients along the process\nof gradient merge, thus, reducing or even completely eliminating worker idle time\ndue to communication overhead, which is a pitfall of existing asynchronous methods.\nWe provide theoretical analysis of the proposed asynchronous algorithm,\nand show its regret bounds. According to our analysis, the crucial parameter for\nkeeping high convergence rate is the maximal discrepancy between local parameter\nvectors of any pair of workers. As long as it is kept relatively small, the\nconvergence rate of the algorithm is shown to be the same as the one of a sequential\nonline learning. Furthermore, in our algorithm, this discrepancy is bounded\nby an expression that involves the staleness parameter of the algorithm, and is\nindependent on the number of workers. This is the main differentiator between\nour approach and other solutions, such as Elastic Asynchronous SGD or Downpour\nSGD, in which that maximal discrepancy is bounded by an expression that\ndepends on the number of workers, due to gradient delay problem. To demonstrate\neffectiveness of our approach, we conduct a series of experiments on image\nclassification task on a cluster with 4 machines, equipped with a commodity communication\nswitch and with a single GPU card per machine. Our experiments\nshow a linear scaling on 4-machine cluster without sacrificing the test accuracy,\nwhile eliminating almost completely worker idle time. Since our method allows\nusing commodity communication switch, it paves a way for large scale distributed\ntraining performed on commodity clusters.", "keywords": ["SGD", "distributed asynchronous training", "deep learning", "optimisation"], "authorids": ["roma.talyansky@gmail.com", "pavel.kisilev@huawei.com", "zach.melamed@huawei.com", "natan.peterfreund@gmail.com", "uri.verner@gmail.com"], "authors": ["Roman Talyansky", "Pavel Kisilev", "Zach Melamed", "Natan Peterfreund", "Uri Verner"], "TL;DR": "A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.", "pdf": "/pdf/6e08f084d5f9d00fc434104ad52965caee3ec0f7.pdf", "paperhash": "talyansky|asynchronous_sgd_without_gradient_delay_for_efficient_distributed_training", "_bibtex": "@misc{\ntalyansky2019asynchronous,\ntitle={Asynchronous {SGD} without gradient delay for efficient distributed training},\nauthor={Roman Talyansky and Pavel Kisilev and Zach Melamed and Natan Peterfreund and Uri Verner},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lo3sC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper740/Official_Review", "cdate": 1542234386858, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1lo3sC9KX", "replyto": "H1lo3sC9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper740/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335792784, "tmdate": 1552335792784, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper740/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByeWt-9S2Q", "original": null, "number": 2, "cdate": 1540886904783, "ddate": null, "tcdate": 1540886904783, "tmdate": 1541533726260, "tddate": null, "forum": "H1lo3sC9KX", "replyto": "H1lo3sC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper740/Official_Review", "content": {"title": "missing references, theory is not novel, experiments are not sufficient", "review": "The paper proposes an algorithm to restrict the staleness in ASGD (asynchronous SGD), and also provides theoretical analysis. This is an interesting and important topic. However, I do not feel that this paper solves the fundamental issue - the staleness will be still very larger or some workers need to stay idle for a long time in the proposed algorithm if there exists some extremely slow worker. To me, the proposed algorithm is more or less just one implementation of ASGD, rather than a new algorithm. The key trick in the algorithm is collecting all workers' gradients in the master machine and update them at once, while hard limiting the number of updates in each worker. The theoretical analysis is not brand new. The\nline 6 in Algorithm 1 makes the delay a random variable related to the speed of a worker. The faster a worker is, the larger the tau is, which invalidates the assumption implicitly used in the theoretical analysis.\n\nThe experiment is done with up to 4 workers, which is not sufficient to validate the advantages of the proposed algorithm compared to state of the art ASGD algorithms. The comparison to other ASGD implementations is also missing, such as Hogwild! and Allreduce.\n\nIn addition, I am so surprised that this paper only have 10 references (the last one is duplicated). The literature review is quite shallow and many important work about ASGD are missing, e.g.,\n\n- Parallel and distributed computation: numerical methods, 1989.\n- Distributed delayed stochastic optimization, NIPS 2011.\n- Hogwild!, NIPS 2011\n- Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization, NIPS 2015\n- An asynchronous mini-batch algorithm for regularized stochastic optimization, 2016.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper740/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Asynchronous SGD without gradient delay for efficient distributed training", "abstract": "Asynchronous distributed gradient descent algorithms for training of deep neural\nnetworks are usually considered as inefficient, mainly because of the Gradient delay\nproblem. In this paper, we propose a novel asynchronous distributed algorithm\nthat tackles this limitation by well-thought-out averaging of model updates, computed\nby workers. The algorithm allows computing gradients along the process\nof gradient merge, thus, reducing or even completely eliminating worker idle time\ndue to communication overhead, which is a pitfall of existing asynchronous methods.\nWe provide theoretical analysis of the proposed asynchronous algorithm,\nand show its regret bounds. According to our analysis, the crucial parameter for\nkeeping high convergence rate is the maximal discrepancy between local parameter\nvectors of any pair of workers. As long as it is kept relatively small, the\nconvergence rate of the algorithm is shown to be the same as the one of a sequential\nonline learning. Furthermore, in our algorithm, this discrepancy is bounded\nby an expression that involves the staleness parameter of the algorithm, and is\nindependent on the number of workers. This is the main differentiator between\nour approach and other solutions, such as Elastic Asynchronous SGD or Downpour\nSGD, in which that maximal discrepancy is bounded by an expression that\ndepends on the number of workers, due to gradient delay problem. To demonstrate\neffectiveness of our approach, we conduct a series of experiments on image\nclassification task on a cluster with 4 machines, equipped with a commodity communication\nswitch and with a single GPU card per machine. Our experiments\nshow a linear scaling on 4-machine cluster without sacrificing the test accuracy,\nwhile eliminating almost completely worker idle time. Since our method allows\nusing commodity communication switch, it paves a way for large scale distributed\ntraining performed on commodity clusters.", "keywords": ["SGD", "distributed asynchronous training", "deep learning", "optimisation"], "authorids": ["roma.talyansky@gmail.com", "pavel.kisilev@huawei.com", "zach.melamed@huawei.com", "natan.peterfreund@gmail.com", "uri.verner@gmail.com"], "authors": ["Roman Talyansky", "Pavel Kisilev", "Zach Melamed", "Natan Peterfreund", "Uri Verner"], "TL;DR": "A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.", "pdf": "/pdf/6e08f084d5f9d00fc434104ad52965caee3ec0f7.pdf", "paperhash": "talyansky|asynchronous_sgd_without_gradient_delay_for_efficient_distributed_training", "_bibtex": "@misc{\ntalyansky2019asynchronous,\ntitle={Asynchronous {SGD} without gradient delay for efficient distributed training},\nauthor={Roman Talyansky and Pavel Kisilev and Zach Melamed and Natan Peterfreund and Uri Verner},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lo3sC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper740/Official_Review", "cdate": 1542234386858, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1lo3sC9KX", "replyto": "H1lo3sC9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper740/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335792784, "tmdate": 1552335792784, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper740/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1lgeOmEo7", "original": null, "number": 1, "cdate": 1539745767796, "ddate": null, "tcdate": 1539745767796, "tmdate": 1541533726049, "tddate": null, "forum": "H1lo3sC9KX", "replyto": "H1lo3sC9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper740/Official_Review", "content": {"title": "I don't understand why the proposed method is an asynchronous method", "review": "This paper tries to propose a so-called hybrid algorithm to eliminate the gradient delay of asynchronous methods. The authors propose algorithm 1 and a simplified version algorithm 2 and prove the convergence of algorithm 2 in the paper.  The paper is very hard to follow, especially the algorithm description part. What I can understand is that the authors want to let the fast workers do more local updates until the computation in the slowest worker is done. The idea is similar to EASGD except that it forces the workers to communicate the server once the slowest one has completed their job.\n\nThe following are my concerns:\n1. Do you consider the overhead in constructing the communication between machines? in your method,  workers are keeping notifying servers that they are done with the computation. \n2. In Algorithm 1 line 9 and line 23, there are two assignments: x_init =x and x_init=ps.x, is there any conflict? \n3. In Algorithm 2,  at line 6 workers wait to receive ps.x, at line 20 server wait for updates. I think there is a bug, and nothing can be received at both ends.\n4. The experiments are too weak. There is no comparison between other related methods, such as downpour, easgd.\n5. The authors test resnet50 on cifar10,  however, there is no accuracy result. They show the result by using googlenet, why not resnet50? I am curious about the experimental settings.\n\nAbove all, the paper is hard to follow and the idea is very trivial. Experiments in the paper are also very weak. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper740/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Asynchronous SGD without gradient delay for efficient distributed training", "abstract": "Asynchronous distributed gradient descent algorithms for training of deep neural\nnetworks are usually considered as inefficient, mainly because of the Gradient delay\nproblem. In this paper, we propose a novel asynchronous distributed algorithm\nthat tackles this limitation by well-thought-out averaging of model updates, computed\nby workers. The algorithm allows computing gradients along the process\nof gradient merge, thus, reducing or even completely eliminating worker idle time\ndue to communication overhead, which is a pitfall of existing asynchronous methods.\nWe provide theoretical analysis of the proposed asynchronous algorithm,\nand show its regret bounds. According to our analysis, the crucial parameter for\nkeeping high convergence rate is the maximal discrepancy between local parameter\nvectors of any pair of workers. As long as it is kept relatively small, the\nconvergence rate of the algorithm is shown to be the same as the one of a sequential\nonline learning. Furthermore, in our algorithm, this discrepancy is bounded\nby an expression that involves the staleness parameter of the algorithm, and is\nindependent on the number of workers. This is the main differentiator between\nour approach and other solutions, such as Elastic Asynchronous SGD or Downpour\nSGD, in which that maximal discrepancy is bounded by an expression that\ndepends on the number of workers, due to gradient delay problem. To demonstrate\neffectiveness of our approach, we conduct a series of experiments on image\nclassification task on a cluster with 4 machines, equipped with a commodity communication\nswitch and with a single GPU card per machine. Our experiments\nshow a linear scaling on 4-machine cluster without sacrificing the test accuracy,\nwhile eliminating almost completely worker idle time. Since our method allows\nusing commodity communication switch, it paves a way for large scale distributed\ntraining performed on commodity clusters.", "keywords": ["SGD", "distributed asynchronous training", "deep learning", "optimisation"], "authorids": ["roma.talyansky@gmail.com", "pavel.kisilev@huawei.com", "zach.melamed@huawei.com", "natan.peterfreund@gmail.com", "uri.verner@gmail.com"], "authors": ["Roman Talyansky", "Pavel Kisilev", "Zach Melamed", "Natan Peterfreund", "Uri Verner"], "TL;DR": "A method for an efficient asynchronous distributed training of deep learning models along with theoretical regret bounds.", "pdf": "/pdf/6e08f084d5f9d00fc434104ad52965caee3ec0f7.pdf", "paperhash": "talyansky|asynchronous_sgd_without_gradient_delay_for_efficient_distributed_training", "_bibtex": "@misc{\ntalyansky2019asynchronous,\ntitle={Asynchronous {SGD} without gradient delay for efficient distributed training},\nauthor={Roman Talyansky and Pavel Kisilev and Zach Melamed and Natan Peterfreund and Uri Verner},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lo3sC9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper740/Official_Review", "cdate": 1542234386858, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1lo3sC9KX", "replyto": "H1lo3sC9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper740/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335792784, "tmdate": 1552335792784, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper740/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}