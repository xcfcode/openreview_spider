{"notes": [{"id": "v0g0bcWdAYd", "original": null, "number": 9, "cdate": 1588054385457, "ddate": null, "tcdate": 1588054385457, "tmdate": 1588054385457, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "ET7lzkMjg9", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment", "content": {"title": "Response", "comment": "To clarify, it is interactive imitation learning in the sense that the algorithm can collect additional data in the environment. This is in contrast to supervised behavior cloning algorithms that only use demonstrations and no additional environment roll-outs."}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgbYyHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1831/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1831/Authors|ICLR.cc/2020/Conference/Paper1831/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150267, "tmdate": 1576860542013, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment"}}}, {"id": "rkgbYyHtwB", "original": "S1lJWNA_DH", "number": 1831, "cdate": 1569439609312, "ddate": null, "tcdate": 1569439609312, "tmdate": 1583912048476, "tddate": null, "forum": "rkgbYyHtwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "ET7lzkMjg9", "original": null, "number": 3, "cdate": 1583345257135, "ddate": null, "tcdate": 1583345257135, "tmdate": 1583345257135, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "X6vMEhBtg", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Public_Comment", "content": {"title": "Minor comment", "comment": "Hi -- I just want to point out that this paper is _not_ studying interactive imitation learning. It is considering the non-interactive setting, where we cannot query the expert, but we do see the expert's actions. "}, "signatures": ["~Akshay_Krishnamurthy1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Akshay_Krishnamurthy1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgbYyHtwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504189191, "tmdate": 1576860575440, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Public_Comment"}}}, {"id": "X6vMEhBtg", "original": null, "number": 1, "cdate": 1576798733548, "ddate": null, "tcdate": 1576798733548, "tmdate": 1576800902888, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper presents an approach for interactive imitation learning while avoiding an adversarial optimization by using ensembles. The reviewers agreed that the contributions were significant and the results were compelling. Hence, the paper should be accepted.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795704526, "tmdate": 1576800252121, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Decision"}}}, {"id": "Syx2DuzwFr", "original": null, "number": 1, "cdate": 1571395684450, "ddate": null, "tcdate": 1571395684450, "tmdate": 1574046375375, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "* Summary:\nThe paper aims to address the covariate shift issue of behavior cloning (BC). The main idea of the paper is to learn a policy by minimizing a BC loss and an uncertainty loss. This uncertainty loss is defined as a variance of a policy posterior given by demonstration. To approximate this posterior, the paper uses an ensemble approach, where an ensemble of policies is learned from demonstrations. This approach leads to a method called disagreement-regularized imitation learning (DRIL). The paper proofs for a tabular setting that DRIL has a linear regret bound in terms of the horizon, which is better than that of BC which has a quadratic regret bound. Empirical evaluation shows that DRIL outperforms BC in both discrete and continuous control tasks, and it outperforms GAIL in discrete control tasks. \n\n* General comments:\nThe paper proposes a simple but effective method to address the important issue of covariate shift. The method performs well empirically and has a theoretical support (although only for a tabular setting). While there are some issues (see below), this is a good paper. I vote for acceptance.  \n\n* Major comments and questions:\n- Accuracy of posterior approximation via ensemble. \nIt is unclear whether the posterior approximated from ensemble is accurate. More specifically, these ensemble policies are trained using BC loss. Under a limited amount of data (where BC fails), these policies would also fail and are inaccurate. Therefore, it should not be expected that a posterior from these inaccurate policies is accurate. Have the authors measure or analyze accuracy of these policies or that of the posterior? This important point is not mentioned or analyzed in the paper.\n\n- Alternative approaches to posterior approximation and uncertainty computation. \nThere are other approaches to obtain a posterior besides the ensemble approach, e.g., Bayesian neural networks. Such alternatives were not mentioned in the paper. Also, there are other quantities for measuring uncertainty besides the variance such as the entropy. These approaches and quantities have different pros and cons and they should be discussed in the paper.\n\n- Sample complexity in terms of environment interactions. \nThe sample complexity in terms of environment interactions is an important criterion for IL. I suggest the authors to include this criterion in the experiments. \n\n* Minor questions:\n- Why does the minibatch size is only 4 in the experiments for all methods. This is clearly too small for a reasonable training of deep networks. Is this a typo?\n\n- It is strange to not evaluate GAIL in the continuous control experiments, since GAIL was originally evaluated in these domains. I strongly suggest the authors to evaluate GAIL (and perhaps stronger methods such as VAIL (Peng et al., 2019)) in the continuous control experiments.\n\n---After reading authors' response---\nI have read the authors' response and other reviews. The authors addressed my comments in the response and the updated paper. I keep the same rating and recommend acceptance.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574971908933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Reviewers"], "noninvitees": [], "tcdate": 1570237731674, "tmdate": 1574971908949, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Review"}}}, {"id": "ryxdF6c2sS", "original": null, "number": 7, "cdate": 1573854591628, "ddate": null, "tcdate": 1573854591628, "tmdate": 1573854591628, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment", "content": {"title": "Updates", "comment": "We have made a number of updates to the paper in response to the comments, please see our answers below. We have also changed the colors of the plots to be more color-blind friendly. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgbYyHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1831/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1831/Authors|ICLR.cc/2020/Conference/Paper1831/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150267, "tmdate": 1576860542013, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment"}}}, {"id": "rkeRVQ52ir", "original": null, "number": 4, "cdate": 1573851957979, "ddate": null, "tcdate": 1573851957979, "tmdate": 1573853728100, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "SJgLU6tycB", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for the detailed review and suggestions for improving the paper. We have made the following changes in response:\n\n- We have added references to the two suggested related works (Menda 2018 and Venkatraman 2015).\n\n1. We have clarified that Step 9 of the algorithm optimizes the expected clipped cost under the current policy. In our experiments we use A2C, which estimates the expected cost using rollouts from multiple parallel actors all sharing the same policy (16 in our case, we have added this to the experiment details in Appendix C).\n\n2-3. We have added ablation experiments in Appendix D1 showing the effect of the different choices for the cost clipping (negative vs. 0, not clipping at all). Having the range of the cost (or reward) include negative and positive values has a large impact on performance. We believe the reason is that if the cost is always positive (or reward is always negative), then an easy way to minimize the cost (or maximize reward) is for the agent to terminate the episode early. Some environments such as Mountain Car are in fact designed this way: all rewards are negative, and the optimal policy is to reach the goal (and thus terminate the episode) as soon as possible. In other environments however, terminating early is highly suboptimal (i.e. the agent dies and cannot collect any more reward). Including both positive and negative costs helps to avoid these issues. \n\n4. We train the different models in the ensemble starting from different initializations and using different bootstrap samples of the demonstration data (we have made this more clear in the text). While it is true that the degenerate case of all models converging to the same solution could potentially occur, our experiments and other works which successfully use ensembles for posterior approximation (mentioned in related work) suggest that this is rare in practice. We have also added experiments in Appendix D2 comparing ensembles to MC-dropout for posterior approximation, and found that dropout also works well - this shows that our approach is not specifically tied to the ensemble method. \n\n5. We have changed notation to use \\kappa^* for the optimum.\n\n6. We have specified the agent's start state, and changed the notation to be consistent with the original work.\n\n7. Our goal is to show that \\kappa^* is upper bounded by a constant independent of T, which translates into a better regret bound than BC when T becomes large. Since \\kappa^* is the minimum of \\kappa(U) for all subsets U of S, showing that \\kappa(U) is upper bounded by a constant for some U means that \\kappa^* is also. We have clarified this in the example.\n\n8. In Example 1, we have specified that we are using a Beta distribution to represent the posterior, whose parameters are determined by the state-action counts in the demonstration data (Beta/Dirichlets are standard choices for binomial/categorical distributions). For the state s_2 which is never visited, the Beta distribution becomes equivalent to a uniform distribution, which is where we get our value of the variance from.  \n\n9. Most of the derivations do carry over to the continuous setting, but there are two steps in the last part of the proof of Lemma 1 that use properties of discrete states/actions: that \\alpha(U) >= 1, and that \\pi(a | s) \\leq 1 (note that for continuous actions, densities can become arbitrarily peaked so the last bound, which was used to bound \\beta(U), does not hold). We are currently working on the continuous case but our current results are for the tabular case.\n\n\nAdditional feedback:\n\nWe have made a number of additional changes: fixing the citation, changing notation in the example, changing density to mass, added mention of Pinsker's inequality and changed the wording regarding the q threshold. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgbYyHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1831/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1831/Authors|ICLR.cc/2020/Conference/Paper1831/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150267, "tmdate": 1576860542013, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment"}}}, {"id": "rygcE4chiS", "original": null, "number": 6, "cdate": 1573852210056, "ddate": null, "tcdate": 1573852210056, "tmdate": 1573852210056, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "Syx2DuzwFr", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for the review. To address the questions/comments:\n\nMajor questions/comments:\n\n- It is true that we should not expect the policies in the ensemble to perform better than BC, since they are trained on the same limited data. However, the motivation is that even though they may make errors, the errors they make are likely to be different from each other. For example, if we look at several functions sampled from a Gaussian process posterior, these will tend to agree on the training data, but can look very different (both from the true function and each other) outside of the training data. Therefore we do not care so much about the quality of the ensemble policies (measured by how they would perform in the environment), but rather whether they exhibit low variance on the training data and higher variance off of it. \n\n- We have updated the text to mention that other methods for posterior approximation are also possible (Bayes by Backprop, MC-dropout), and added additional experiments comparing the ensemble approach to MC-dropout in Appendix D2. It turns out that MC-dropout also works well, similarly to the ensemble method. This shows that our approach is not specific to the ensemble method which we use in most of our experiments. \n\n- We have added the number of environment steps to the curves in Figure 2b, which shows the sample complexity. Note that since we use A2C as an RL optimizer in our experiments, we are not particularly sample efficient in terms of environment steps. Our general method is agnostic to the RL optimizer though, so more sample-efficient RL methods (such as model-based methods or others which reuse data more efficiently) could in principle be used as well.\n\n\nMinor questions:\n\n- Minibatch 4 was a typo, thanks for catching that. We use 16 parallel environments for A2C and have added this to the experiment details.\n\n- We initially did not include GAIL in the continuous control experiments because there was not much headroom for improvement over BC. We will add these experiments for the next update. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgbYyHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1831/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1831/Authors|ICLR.cc/2020/Conference/Paper1831/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150267, "tmdate": 1576860542013, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment"}}}, {"id": "S1lhsQchsB", "original": null, "number": 5, "cdate": 1573852067686, "ddate": null, "tcdate": 1573852067686, "tmdate": 1573852067686, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "S1gya8oTKB", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for the encouraging comments and we are glad you enjoyed reading the paper. Regarding the GAIL hyperparameters: this was a formatting issue and we have changed the text to refer to Table 2 where the GAIL hyperparameters are listed. We have also added the chain MDP example to the appendix and added references discussed in the previous comment thread.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgbYyHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1831/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1831/Authors|ICLR.cc/2020/Conference/Paper1831/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150267, "tmdate": 1576860542013, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment"}}}, {"id": "S1gya8oTKB", "original": null, "number": 2, "cdate": 1571825335104, "ddate": null, "tcdate": 1571825335104, "tmdate": 1572972418044, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an imitation learning algorithm that combines behavioral cloning with a regularizer that encourages the agent to visit states similar to the demonstrated states. The key idea is to use ensemble disagreement to approximate uncertainty, and use RL to train the imitation agent to visit states in which an ensemble of cloned imitation policies is least uncertain about which action the expert would take. Experiments on image-based Atari games show that the proposed method significantly outperforms BC and GAIL baselines in three games, and performs comparably or slightly better than the baselines in the remaining three games.\n\nOverall, I enjoyed reading this paper. It proposes a relatively simple imitation method with compelling empirical results.\n\nOne minor comment: on page 15, the sentence \"We initially performed a hyperparameter search on Breakout with 10 demonstrations over the following values: \" ends in a blank space, without actually providing any hyperparameter values. It would be nice if you could actually include those values, or at least how many different values were searched.\n\nThank you for addressing the comments about related work in an earlier thread (https://openreview.net/forum?id=rkgbYyHtwB&noteId=S1lv4r5qvS). Two follow-ups:\n - The chain MDP example clearly illustrates why including the BC cost is important, and how DRIL differs from support estimation methods like RED. Thank you for the clarification.\n - The focus of Sasaki et al. is on reducing the number of environment interactions, but their proposed method also addresses covariate shift: it fits a Q function that classifies whether the demonstration states are reachable from the current state, and thus encourages the agent to return to demonstrated states."}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574971908933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Reviewers"], "noninvitees": [], "tcdate": 1570237731674, "tmdate": 1574971908949, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Review"}}}, {"id": "SJgLU6tycB", "original": null, "number": 3, "cdate": 1571949902087, "ddate": null, "tcdate": 1571949902087, "tmdate": 1572972417998, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary of what the paper claims and contributes\n---\nThis paper proposes a new interactive imitation learning algorithm to address the covariate shift problem in imitation learning. It explicitly seeks to avoid settings interactive expert feedback (e.g. DAgger). The method is straightforward: 1. First, learn an ensemble of policies via KL-based Behavior Cloning 2. Then, learn a new policy via a new objective that combines the original Behavior Cloning objective with a \"disagreement\" loss, formed by computing the expected variance of the ensemble evaluated on state-action trajectories under the new policy. The intuition for the method is that by learning an ensemble, it will have low variance on in-distribution demonstration data, and high variance on out-of-distribution other data; by encouraging the policy to seek regions of low variance, it should result in a policy that more closely matches the demonstrator's state visitation distribution than Behavior-Cloning alone. Analysis in the discrete finite case shows that the algorithm achieves regret linear in \\kappa*T, where \\kappa is an environment- and expert-dependent constant. The analysis is instantiated for a simple MDP, and experiments comparing their algorithm on this restricted environment provide some evidence that the bound is achievable in practice.\n\nFurther experiments on a variety of Atari environments and continuous-control tasks from OpenAI Gym also 1) demonstrates that their algorithm outperforms Behavior Cloning in these settings 2) usually approaches expert performance with a small number of demonstrations, and 3) also shows that the uncertainty cost improves over time, indicating the final policy learns to visit states where the ensemble agrees, and that while doing so, improves performance on the underlying task.\n\nEvaluation\n---\n>Originality:\nAre the tasks or methods new?\nThe method is new.\n\nIs the work a novel combination of well-known techniques?\nYes.\n\nIs it clear how this work differs from previous contributions?\nYes.\n\nIs related work adequately cited?\nThere is some missing discussion of related works:\n1. EnsembleDAgger (Menda 2018) also uses the variance of ensembles in Imitation Learning, but instead of using it to regularize on-policy learning, it uses it as an improved decision criterion by which to query an expert demonstrator.\n2. Data as Demonstrator (Venkatraman 2015) uses on-policy learning to create \"corrections\" of time-series models (See their Fig 1), which is similar to this paper's intuition of seeking to push the learner back to places that are in-distribution of the expert demonstrations. That paper also achieves a linear regret bound under some assumptions.\n\n>Quality:\nIs the submission technically sound?\nMostly, although there are some issues:\n1. Step 9 of the algorithm is ambiguous. What is the distribution of on-policy data that is fed into the cost? E.g. how many rollouts from the policy are collected?\n2. Why is the clipped cost negative, as opposed to 0?\n3. Why was a clipped cost used at all? This cost is different from that used in the theoretical analysis. Some justification and discussion is needed for why the new cost was used, and whether the analysis still applies when it's used.\n4. Throughout most of the paper, p(\\pi | \\mathcal D) represents the model ensemble. However, no discussion was dedicated to what we should expect this distribution to look like in theory and in practice. It depends on how the ensemble is constructed / learned. A degenerate case would be if all models in the ensemble converged to the same local optima, in which case they would agree everywhere, nullifying the cost penalty. Discussion of what properties this distribution must satisfy is missing. It probably needs full support over the space of policies such that the optimal policy is nearly realizable (within \\epsilon)?\n5. \\kappa is overloaded: A. it's used as a function B. it's used as the optimal value of that same function. Consider using different notation for one of the, e.g. \\kappa^* for the optimum, or \\gamma for the function. Furthermore, it might help to make \\kappa's dependencies clearer, which would help illustrate its independence of T.\n6. Example 1: the fact that the policy always starts at s_1 is missing from the description (at least, an equivalent assumption is made in Ross 2010)\n7. Example 1: it's not clear that setting \\mathcal U = \\{s_1, s_2\\} achieves the optimum of \\kappa(\\mathcal U). Discussion of this aspect is needed.\n8. Example 1: The statement that the variance is equivalent to the variance of the uniform distribution seems to be a strong assumption about p(\\pi | \\mathcal D). This missing assumption is related to point 4. I mentioned above^\n9. The paper is missing discussion for why the analysis would not immediately extend to continuous state and action spaces.\n\nAre claims well supported by theoretical analysis or experimental results?\nYes, although the experimental results would be made stronger if related approaches were considered, e.g. Reddy 2019. Right now, there's just a single method of comparison -- BC.\n\nIs this a complete piece of work or work in progress?\nSeems complete.\n\nAre the authors careful and honest about evaluating both the strengths and weaknesses of their work?\nI believe so -- noting that BC ended up performing similar in environments where there is less drift was a good addition.\n\n>Clarity:\nIs the submission clearly written?\nYes.\n\nIs it well organized?\nYes.\n\nDoes it adequately inform the reader?\nYes.\n\n>Significance:\nAre the results important?\nYes.\n\nAre others (researchers or practitioners) likely to use the ideas or build on them?\nYes.\n\nDoes the submission address a difficult task in a better way than previous work?\nYes.\n\nDoes it advance the state of the art in a demonstrable way?\nYes.\n\nDoes it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?\nUnique theoretical approach.\n\nAdditional feedback\n---\nSec 3: \"The threshold q defines a normal range of uncertainty based on the demonstration data, and values outside of this range incur a negative cost\". The logic of this statement is confusing. 1. It's not clear what \"outside\" means from the sentence alone (i.e. it should be \"above\"). 2. A single value doesn't define a range (i.e. state the lower value is 0).\n\nSec 4.1: \"high density\" -> \"high mass\"\n\nIt would help to have a diagram of \\mathcal U, \\mathcal S - \\mathcal U, \\alpha, \\beta, \\kappa.\n\nIt would be clearer if set notation was used for the complement of \\mathcal U, rather than \\beta's definition of s\\notin \\mathcal U.\n\nExample 1: citation should be Ross 2010, not Ross 2011.\n\nExample 1 has different notation than in Ross 2010 (consider changing to match)\n\nIt's possible that copying a model from the ensemble and fine-tuning it with the loss would yield a faster Algorithm (1). Would this work? What do the training curves (i.e. like the plots in Fig 3b) look like in that case?\n\nWhy does the breakout DRIL agent outperform the expert?\n\nMention that Pinkser's inequality yields the KL bound on total variation.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574971908933, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Reviewers"], "noninvitees": [], "tcdate": 1570237731674, "tmdate": 1574971908949, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Review"}}}, {"id": "HJg9IX6v_B", "original": null, "number": 1, "cdate": 1570390865642, "ddate": null, "tcdate": 1570390865642, "tmdate": 1570390865642, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "S1lv4r5qvS", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment", "content": {"comment": "Thank you for the helpful pointers to the related work. The Random Expert Distillation (RED) method by Wang et al [1] is indeed relevant and we will include a discussion in the updated paper. Both RED and our method use an uncertainty measure derived from the demonstration data as a cost function which is minimized through RL, and which is designed to guide the policy back towards the demonstration data. There are two main differences between RED and our method: i) we use the variance of an ensemble as a measure of uncertainty, rather than random network distillation [3] and ii) we include a supervised behaviour cloning (BC) cost in addition to the uncertainty cost.\n\nIncluding the BC cost is actually quite important for our theoretical results. In Lemma 1, J_{exp}(\\pi) is broken up into two terms, one of which is bounded by the BC cost (scaled by alpha(U)) and one of which is bounded by the uncertainty cost (scaled by 1/beta(U)). By minimizing both of these costs, we minimize J_{exp}(\\pi) (scaled by kappa), which in turn translates into a regret bound. \n\nThe following example shows that minimizing the uncertainty cost alone without the BC cost can lead to highly sub-optimal policies if the demonstration data is generated by a stochastic policy which is only slightly suboptimal. Consider the following deterministic chain MDP:\n\ns0 <---> s1 <---> s2 <---> s3 \n\nSay the agent always starts in s1, and gets a reward of 1 in s3 and 0 elsewhere, and there are 2 actions: left and right (in s3, going right keeps the agent at s3, in s0 going left keeps the agent at s0). \n\nAssume the demonstration data is generated by a policy defined as follows: \n- in s0, go right with probability 1\n- in s1, go right with probability 1\n- in s2, go right with probability 0.9, left with probability 0.1\n- in s3, go right (i.e. stay at s3) with probability 1. \n\nIf both transitions (s2, right) and (s2, left) appear in the demonstration data, then (assuming realizability) RED will assign the same cost to both transitions. This means that a policy which cycles forever between s1 and s2 (always going left at s2, and never collecting reward) will have the same cost as a policy which goes right at s2 and then stays at s3 (thus collecting lots of reward). If we include a BC cost however, the policy will learn to assign a higher probability to going right at s2 and end up collecting reward. For both RED and our method, if we are realizable and optimization can be performed exactly, then the uncertainty cost will be set to zero for all transitions appearing in the demonstration data, regardless of their relative frequency. However, this problem can be avoided by combining the BC cost with the uncertainty cost. \n\nThe method of Sasaki et. al [2] is interesting and we will include a reference in related work. The focus of their work is somewhat different, i.e. reducing the number of environment interactions rather than addressing covariate shift. \n\nThank you for the recommendations regarding the description of SQIL, we will include them when we update the paper.\n\n[1] http://proceedings.mlr.press/v97/wang19d/wang19d.pdf \n[2] https://openreview.net/forum?id=BkN5UoAqF7\n[3] https://arxiv.org/pdf/1810.12894.pdf", "title": "Addressing Related work comments:"}, "signatures": ["ICLR.cc/2020/Conference/Paper1831/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgbYyHtwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1831/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1831/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1831/Authors|ICLR.cc/2020/Conference/Paper1831/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504150267, "tmdate": 1576860542013, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Official_Comment"}}}, {"id": "S1lv4r5qvS", "original": null, "number": 1, "cdate": 1569527086589, "ddate": null, "tcdate": 1569527086589, "tmdate": 1569527086589, "tddate": null, "forum": "rkgbYyHtwB", "replyto": "rkgbYyHtwB", "invitation": "ICLR.cc/2020/Conference/Paper1831/-/Public_Comment", "content": {"comment": "Thank you for the interesting paper! I have two comments about related work.\n\nThere are two prior methods \u2013 Random Expert Distillation (RED) by Wang et al. [1], and the implicit IRL method in Sasaki et al. [2] \u2013 that aren\u2019t mentioned in the paper, but are similar to the proposed method. In particular, the proposed method seems to take a similar approach to RED, except that it uses ensemble disagreement instead of random network distillation for density estimation of the demonstrations. It would be nice to discuss how the proposed method relates to the prior work.\n\nThe discussion of one of the prior methods \u2013 SQIL by Reddy et al. \u2013 mischaracterizes how SQIL works. The first paragraph on page 6 claims that SQIL requires careful reward decay and does not use a fixed reward function. In fact, SQIL uses a fixed reward function (r=+1 for demonstrations, r=0 for everything else), and does not modify or decay the rewards over time. It would be nice to adjust how SQIL is positioned in the related work. In my opinion, the proposed method differs from SQIL in that it uses a fixed reward function that is potentially less sparse and potentially easier to train the imitation agent with via RL.\n\nAgain, thank you for the interesting work. I look forward to seeing how the paper evolves, and hope that others working on imitation learning give it a read.\n\n[1] http://proceedings.mlr.press/v97/wang19d/wang19d.pdf\n[2] https://openreview.net/forum?id=BkN5UoAqF7", "title": "Related work"}, "signatures": ["~Siddharth_Reddy1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Siddharth_Reddy1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disagreement-Regularized Imitation Learning", "authors": ["Kiante Brantley", "Wen Sun", "Mikael Henaff"], "authorids": ["kdbrant@cs.umd.edu", "wen.sun@microsoft.com", "mihenaff@microsoft.com"], "keywords": ["imitation learning", "reinforcement learning", "uncertainty"], "TL;DR": "Method for addressing covariate shift in imitation learning using ensemble uncertainty", "abstract": "We present a simple and effective algorithm designed to address the covariate shift problem in imitation learning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial imitation learning.", "pdf": "/pdf/015e6a6bdb538f4bd1d42b10ac56527cb4aea07a.pdf", "paperhash": "brantley|disagreementregularized_imitation_learning", "_bibtex": "@inproceedings{\nBrantley2020Disagreement-Regularized,\ntitle={Disagreement-Regularized Imitation Learning},\nauthor={Kiante Brantley and Wen Sun and Mikael Henaff},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgbYyHtwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/49d438c9834d44e2cef4cd5e79e9d1932f5122d1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgbYyHtwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504189191, "tmdate": 1576860575440, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1831/Authors", "ICLR.cc/2020/Conference/Paper1831/Reviewers", "ICLR.cc/2020/Conference/Paper1831/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1831/-/Public_Comment"}}}], "count": 13}