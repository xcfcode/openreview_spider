{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124479152, "tcdate": 1518442346718, "number": 115, "cdate": 1518442346718, "id": "r17_wzJPM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "r17_wzJPM", "signatures": ["~Kangwook_Lee1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "SGD on Random Mixtures: Private Machine Learning under Data Breach Threats", "abstract": "We propose Stochastic Gradient Descent on Random Mixtures (SGDRM) as a simple way of protecting data under data breach threats. We show that SGDRM converges to the globally optimal point for deep neural networks with linear activations while being differentially private. We also train nonlinear neural networks with private mixtures as the training data, proving the practicality of SGDRM.", "paperhash": "lee|sgd_on_random_mixtures_private_machine_learning_under_data_breach_threats", "keywords": ["SGD on random mixtures", "SGDRM", "differential privacy"], "_bibtex": "@misc{\n  lee2018sgd,\n  title={SGD on Random Mixtures: Private Machine Learning under Data Breach Threats},\n  author={Kangwook Lee and Kyungmin Lee and Hoon Kim and Changho Suh and Kannan Ramchandran},\n  year={2018},\n  url={https://openreview.net/forum?id=r17_wzJPM}\n}", "authorids": ["kw1jjang@kaist.ac.kr", "atm13579@kaist.ac.kr", "gnsrla12@kaist.ac.kr", "chsuh@kaist.ac.kr", "kannanr@eecs.berkeley.edu"], "authors": ["Kangwook Lee", "Kyungmin Lee", "Hoon Kim", "Changho Suh", "Kannan Ramchandran"], "TL;DR": "SGDRM is the SGD algorithm run on random mixtures; it is differentially private and has convergence guarantees.", "pdf": "/pdf/4a799307316d59b97a6c68e231441b88b2177a2e.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582731301, "tcdate": 1520670524630, "number": 1, "cdate": 1520670524630, "id": "SyHHPz-tG", "invitation": "ICLR.cc/2018/Workshop/-/Paper115/Official_Review", "forum": "r17_wzJPM", "replyto": "r17_wzJPM", "signatures": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer2"], "content": {"title": "The result seems to be theoretically sound, but the setting is not clear", "rating": "7: Good paper, accept", "review": "The motivation is clear and well understood.\nThe result seems to be theoretically sound, but the setting is not clear. \nThe authors mention that the convergence result is specific to linear neural network models. If so, the resulting model is represented by a linear model. If this is correct, the analytic solution is obtainable and it is unclear to me if the results obtained in this study are meaningful. Please clarify this point in the revised version.\n ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD on Random Mixtures: Private Machine Learning under Data Breach Threats", "abstract": "We propose Stochastic Gradient Descent on Random Mixtures (SGDRM) as a simple way of protecting data under data breach threats. We show that SGDRM converges to the globally optimal point for deep neural networks with linear activations while being differentially private. We also train nonlinear neural networks with private mixtures as the training data, proving the practicality of SGDRM.", "paperhash": "lee|sgd_on_random_mixtures_private_machine_learning_under_data_breach_threats", "keywords": ["SGD on random mixtures", "SGDRM", "differential privacy"], "_bibtex": "@misc{\n  lee2018sgd,\n  title={SGD on Random Mixtures: Private Machine Learning under Data Breach Threats},\n  author={Kangwook Lee and Kyungmin Lee and Hoon Kim and Changho Suh and Kannan Ramchandran},\n  year={2018},\n  url={https://openreview.net/forum?id=r17_wzJPM}\n}", "authorids": ["kw1jjang@kaist.ac.kr", "atm13579@kaist.ac.kr", "gnsrla12@kaist.ac.kr", "chsuh@kaist.ac.kr", "kannanr@eecs.berkeley.edu"], "authors": ["Kangwook Lee", "Kyungmin Lee", "Hoon Kim", "Changho Suh", "Kannan Ramchandran"], "TL;DR": "SGDRM is the SGD algorithm run on random mixtures; it is differentially private and has convergence guarantees.", "pdf": "/pdf/4a799307316d59b97a6c68e231441b88b2177a2e.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582731109, "id": "ICLR.cc/2018/Workshop/-/Paper115/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper115/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper115/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper115/AnonReviewer3"], "reply": {"forum": "r17_wzJPM", "replyto": "r17_wzJPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper115/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582731109}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582718223, "tcdate": 1520685590093, "number": 2, "cdate": 1520685590093, "id": "Hy0MGUbYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper115/Official_Review", "forum": "r17_wzJPM", "replyto": "r17_wzJPM", "signatures": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer1"], "content": {"title": "Review of \"SGD on Random Mixtures\"", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents an interesting new concept: protection of an ML pipeline against a data breach attack. The authors propose distorting a dataset  to produce an alternate version that (a) is differentially private w.r.t. the original disclosure, (b) can be used to train a model with as close an accuracy as possible. The authors propose a methodology that mixes inputs, by computing new datapoints as follows: each datapoint is a (differentially private) average between $\\ell$ inputs randomly selected. $\\ell$ establishes a fidelity/diff. privacy tradeoff. The authors provide an accuracy guarantee in the case of what they refer to as \"Deep linear neural networks\", namely, neural networks with a linear activations. An evaluation over a 5 layer network shows that the method maintains good performance even if the linear activation is amended.\n\nThis is, overall, a nice idea, with a nice execution. The paper is formal, precise, and clear, despite the lack of space. The \"magic/surprizing\" observation here is that training over a mixture has guarantees even when loss is measured with respect to the original dataset, not the transformed dataset. \n\nI have a few concerns. First, there are no \"deep linear networks\": this is somewhat of an oxymoron. A network with a linear activation function is equivalent to linear regression. There no reason to go \"deep\" in such a case. Second, the guarantee seems straightforward (but not trivial, due to DP) in the case when the data is generated by a linear model, as linear combinations would still be helpful in training the same linear model. The result does not seem surprising in light of these two observations, and the true challenge, both in terms of a desirable algorithm as well as its analysis, seems to lie in the non-linear case.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD on Random Mixtures: Private Machine Learning under Data Breach Threats", "abstract": "We propose Stochastic Gradient Descent on Random Mixtures (SGDRM) as a simple way of protecting data under data breach threats. We show that SGDRM converges to the globally optimal point for deep neural networks with linear activations while being differentially private. We also train nonlinear neural networks with private mixtures as the training data, proving the practicality of SGDRM.", "paperhash": "lee|sgd_on_random_mixtures_private_machine_learning_under_data_breach_threats", "keywords": ["SGD on random mixtures", "SGDRM", "differential privacy"], "_bibtex": "@misc{\n  lee2018sgd,\n  title={SGD on Random Mixtures: Private Machine Learning under Data Breach Threats},\n  author={Kangwook Lee and Kyungmin Lee and Hoon Kim and Changho Suh and Kannan Ramchandran},\n  year={2018},\n  url={https://openreview.net/forum?id=r17_wzJPM}\n}", "authorids": ["kw1jjang@kaist.ac.kr", "atm13579@kaist.ac.kr", "gnsrla12@kaist.ac.kr", "chsuh@kaist.ac.kr", "kannanr@eecs.berkeley.edu"], "authors": ["Kangwook Lee", "Kyungmin Lee", "Hoon Kim", "Changho Suh", "Kannan Ramchandran"], "TL;DR": "SGDRM is the SGD algorithm run on random mixtures; it is differentially private and has convergence guarantees.", "pdf": "/pdf/4a799307316d59b97a6c68e231441b88b2177a2e.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582731109, "id": "ICLR.cc/2018/Workshop/-/Paper115/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper115/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper115/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper115/AnonReviewer3"], "reply": {"forum": "r17_wzJPM", "replyto": "r17_wzJPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper115/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582731109}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582664503, "tcdate": 1520752400976, "number": 3, "cdate": 1520752400976, "id": "S1FzwLfFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper115/Official_Review", "forum": "r17_wzJPM", "replyto": "r17_wzJPM", "signatures": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer3"], "content": {"title": "Need more work on the privacy part", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes an algorithm called SGDRM that performs SGD for convex and non-convex problems and protects privacy against data-breach attack. SGDRM constructs several random noisy mixtures of the original dataset and performs normal SGD on the mixtures. The paper shows the differential privacy guarantees and convergence guarantee for least square and linear neural network; it also shows experimental results with general neural networks, comparing SGDRM with different mixture widths. \n\nThe idea of using random noisy mixtures to protect privacy is quite interesting. It is nice to see that using mixture can sometimes lead to better results than using the original dataset, and helps with privacy as well. However, I'm not fully convinced about the differential privacy part, as the experiments are done with zero noise and thus does not guarantee differential privacy. It would be better if there is some result for what (epsilon, delta) we can get for common machine learning tasks, and how that compares with existing works. Also, to show the significance of using the random mixtures, providing more comparison with Zhang et al. (2018) may help.\n\nSome more comments:\nAbout privacy:\n- I think the data-breach attack is similar to local differential privacy, which considers publishing a noisy version of the original dataset. Is that the case? If so, you may want to say something about local dp and other ml algorithms that guarantees local dp in related work.\n- If related, it might be better if you mention how random sampling helps with privacy in general, with some intuition and previous works.\n- It would be better if you show some values of (epsilon, delta) in real examples. \n- It might be better if you show experimental results on the tradeoff between T and l, and how the accuracy/privacy compare with adding noise to the gradients (instead of to the samples).\n\nAbout the mixture of samples:\n- Can you provide some intuition on why mixing samples may help training? Does it help in a certain kind of data / NN? Is it possible for it to help with accuracy for l > 2?\n\nTypo: \n- in page 2 second last line \"Given T, the number of data points to be published T\".\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD on Random Mixtures: Private Machine Learning under Data Breach Threats", "abstract": "We propose Stochastic Gradient Descent on Random Mixtures (SGDRM) as a simple way of protecting data under data breach threats. We show that SGDRM converges to the globally optimal point for deep neural networks with linear activations while being differentially private. We also train nonlinear neural networks with private mixtures as the training data, proving the practicality of SGDRM.", "paperhash": "lee|sgd_on_random_mixtures_private_machine_learning_under_data_breach_threats", "keywords": ["SGD on random mixtures", "SGDRM", "differential privacy"], "_bibtex": "@misc{\n  lee2018sgd,\n  title={SGD on Random Mixtures: Private Machine Learning under Data Breach Threats},\n  author={Kangwook Lee and Kyungmin Lee and Hoon Kim and Changho Suh and Kannan Ramchandran},\n  year={2018},\n  url={https://openreview.net/forum?id=r17_wzJPM}\n}", "authorids": ["kw1jjang@kaist.ac.kr", "atm13579@kaist.ac.kr", "gnsrla12@kaist.ac.kr", "chsuh@kaist.ac.kr", "kannanr@eecs.berkeley.edu"], "authors": ["Kangwook Lee", "Kyungmin Lee", "Hoon Kim", "Changho Suh", "Kannan Ramchandran"], "TL;DR": "SGDRM is the SGD algorithm run on random mixtures; it is differentially private and has convergence guarantees.", "pdf": "/pdf/4a799307316d59b97a6c68e231441b88b2177a2e.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582731109, "id": "ICLR.cc/2018/Workshop/-/Paper115/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper115/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper115/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper115/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper115/AnonReviewer3"], "reply": {"forum": "r17_wzJPM", "replyto": "r17_wzJPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper115/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper115/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582731109}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573568727, "tcdate": 1521573568727, "number": 113, "cdate": 1521573568384, "id": "rJtTARCFM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "r17_wzJPM", "replyto": "r17_wzJPM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SGD on Random Mixtures: Private Machine Learning under Data Breach Threats", "abstract": "We propose Stochastic Gradient Descent on Random Mixtures (SGDRM) as a simple way of protecting data under data breach threats. We show that SGDRM converges to the globally optimal point for deep neural networks with linear activations while being differentially private. We also train nonlinear neural networks with private mixtures as the training data, proving the practicality of SGDRM.", "paperhash": "lee|sgd_on_random_mixtures_private_machine_learning_under_data_breach_threats", "keywords": ["SGD on random mixtures", "SGDRM", "differential privacy"], "_bibtex": "@misc{\n  lee2018sgd,\n  title={SGD on Random Mixtures: Private Machine Learning under Data Breach Threats},\n  author={Kangwook Lee and Kyungmin Lee and Hoon Kim and Changho Suh and Kannan Ramchandran},\n  year={2018},\n  url={https://openreview.net/forum?id=r17_wzJPM}\n}", "authorids": ["kw1jjang@kaist.ac.kr", "atm13579@kaist.ac.kr", "gnsrla12@kaist.ac.kr", "chsuh@kaist.ac.kr", "kannanr@eecs.berkeley.edu"], "authors": ["Kangwook Lee", "Kyungmin Lee", "Hoon Kim", "Changho Suh", "Kannan Ramchandran"], "TL;DR": "SGDRM is the SGD algorithm run on random mixtures; it is differentially private and has convergence guarantees.", "pdf": "/pdf/4a799307316d59b97a6c68e231441b88b2177a2e.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}