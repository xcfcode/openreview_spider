{"notes": [{"id": "rket4i0qtX", "original": "H1l7YpILKX", "number": 20, "cdate": 1538087729081, "ddate": null, "tcdate": 1538087729081, "tmdate": 1545355378035, "tddate": null, "forum": "rket4i0qtX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Syl7s1p-lE", "original": null, "number": 1, "cdate": 1544830874858, "ddate": null, "tcdate": 1544830874858, "tmdate": 1545354530953, "tddate": null, "forum": "rket4i0qtX", "replyto": "rket4i0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Meta_Review", "content": {"metareview": "The paper studies an narrowly focused but interesting problem -- if the Visual Question answering model \u201cFILM\u201d from Perez et al (2018) is able to decide if \u201cmost\u201d of the objects have a certain attribute or color. While the work itself is appreciate by the reviewers, concerns remain about the conclusion being limited in scope due to the synthetic nature of the data, and the analysis fairly narrow (a single model with a single very specific task). We encourage the authors to use reviewer feedback to make the manuscript stronger for a future deadline. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper20/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper20/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353366208, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": "rket4i0qtX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353366208}}}, {"id": "Hye9yQ3pkN", "original": null, "number": 11, "cdate": 1544565474398, "ddate": null, "tcdate": 1544565474398, "tmdate": 1544565474398, "tddate": null, "forum": "rket4i0qtX", "replyto": "rJlbj89T14", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "content": {"title": "agree", "comment": "Yes, I agree with the limitations raised by R1. These points would certainly make the paper better. I guess the main issue is a matter of deciding what should be in the paper now, vs. what could reasonably be considered to be follow up work."}, "signatures": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618896, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper20/Authors|ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618896}}}, {"id": "rke3eLqpJV", "original": null, "number": 9, "cdate": 1544558068424, "ddate": null, "tcdate": 1544558068424, "tmdate": 1544558068424, "tddate": null, "forum": "rket4i0qtX", "replyto": "Ske5xOUn6m", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "content": {"title": "Still Borderline", "comment": "On the positive side:\n+ The paper improved in the revision, improving mainly discussion and increasing clarity.\n\nRemaining weaknesses:\n- I still think for an analysis paper it is important to have a comparison of more than a single model. Even when proposing a new model we expect papers to compare to prior works, which might mean running them on new data; when proposing a new evaluation/study methodology I think is even more important to have an understanding of multiple methods. (The argument that \"FiLM alone comprises around 100 experiments\" is not a strong argument, I expect the experiments to be reasonable fast and other methods could be run just for the most important experiment/setting, i.e. training it once or twice)\n\n- The paper's conclusion remain limited due to the synthetic nature of the data.\n\n- R3 brought up the point of \"one-glance feed-forward-style networks\". The authors state that \"precise counting is an inherently recursive ability\". While this might be true for counting in general, for counting of small number, as e.g. studied in this work, the work of Zhang et al. (2018) shows an approach to do so with \"one-glance feed-forward-style networks\". Another reason to have more comprehensive evaluation of prior work. \n\nOverall I am not strongly opposed to accepting the paper, but I also think the study is limited and I remain with my border line rating of 5."}, "signatures": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618896, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper20/Authors|ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618896}}}, {"id": "Byx1LB5614", "original": null, "number": 8, "cdate": 1544557895275, "ddate": null, "tcdate": 1544557895275, "tmdate": 1544557895275, "tddate": null, "forum": "rket4i0qtX", "replyto": "r1eR5rP61V", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "content": {"title": "Still Borderline", "comment": "sorry for not discussing it earlier (it is probably too late now, but for the record/in case it is not):\n\nOn the positive side:\n+ The paper improved in the revision, improving mainly discussion and increasing clarity.\n\nRemaining weaknesses:\n- I still think for an analysis paper it is important to have a comparison of more than a single model. Even when proposing a new model we expect papers to compare to prior works, which might mean running them on new data; when proposing a new evaluation/study methodology I think is even more important to have an understanding of multiple methods. (The argument that \"FiLM alone comprises around 100 experiments\" is not a strong argument, I expect the experiments to be reasonable fast and other methods could be run just for the most important experiment/setting, i.e. training it once or twice)\n\n- The paper's conclusion remain limited due to the synthetic nature of the data.\n\n- R3 brought up the point of \"one-glance feed-forward-style networks\". The authors state that \"precise counting is an inherently recursive ability\". While these might be true for counting in general, for counting of small number, as e.g. studied in this work, the work of Zhang et al. (2018) shows an approach to do so with \"one-glance feed-forward-style networks\". Another reason to have more comprehensive evaluation of prior work. \n\nOverall I am not strongly opposed to accepting the paper, but I also think the study is limited and I remain with my border line rating of 5."}, "signatures": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618896, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper20/Authors|ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618896}}}, {"id": "r1eR5rP61V", "original": null, "number": 7, "cdate": 1544545685648, "ddate": null, "tcdate": 1544545685648, "tmdate": 1544545685648, "tddate": null, "forum": "rket4i0qtX", "replyto": "r1lu-1aBJE", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "content": {"title": "Still arguing for accept", "comment": "Having read the other reviews, rebuttal and updated paper version, I would still argue for acceptance of this paper. I believe this is important work. We need to better understand the mechanisms that are operating in our deep learning models. These mechanisms are induced by the data/training process and also partly determined by network architecture. Given this complexity, an experimental approach to this investigation seems reasonable. There are tools from experimental psychology that can help, and it's probably time our community became more familiar with these tools.\n\nLooking at the other reviews, I don't see a major flaw with this paper. The weaknesses identified in the review process fall into two categories: \n(1) Reviewers asking for 'more' - i.e. experiments with different models, experiments with distractors, experiments disentangling objects vs. ratios, etc. Yet, the paper is also criticized for being too long. It's hard to see what the authors could rationally do in response to these criticisms. There is a certain amount of cognitive psych background that needs to be covered for an ICLR audience. We can't ask for a journal paper to fit in 8 pages. \n(2) The other category of weaknesses mostly relates to correcting some imprecise expressions, particularly from R3. I substantially agree with all of these criticisms. But I believe the authors have addressed them.\n\nIn summary, why would we not publish this paper?"}, "signatures": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618896, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper20/Authors|ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618896}}}, {"id": "BJg2S_InpQ", "original": null, "number": 4, "cdate": 1542379587690, "ddate": null, "tcdate": 1542379587690, "tmdate": 1542379587690, "tddate": null, "forum": "rket4i0qtX", "replyto": "HyxpQdUhp7", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "content": {"title": "Authors' response (part 2)", "comment": "- There are a few papers focusing solely on \"most\" in psycholinguistics (like the ones cited) and linguistics in general (e.g., formal semantics), many of which talk about cardinality as a \"core concept\" of human cognition, and many of which contrast a more subconscious concept of cardinality (like the approximate number system) with the conscious algorithmic ability for precise and infinite counting.\n\n- We consistently use \"interpretation\" in the new version.\n\n- Fixed.\n\n- We think that our approach and particularly its inspiration by experiments from psychology are a substantial contribution to evaluation methodology in the context of powerful deep learning models, which are not infrequently described by anthropomorphizing words like \"understand\" and compared to \"human-level\" performance (added a sentence to the introduction). The reason for exceeding 8 pages in length is likely due to the more elaborate introduction of the various concepts from Pietroski et al.'s work and experimental methodology in psychology in general, which we assume the audience of this conference is not very familiar with."}, "signatures": ["ICLR.cc/2019/Conference/Paper20/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618896, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper20/Authors|ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618896}}}, {"id": "HyxpQdUhp7", "original": null, "number": 3, "cdate": 1542379556636, "ddate": null, "tcdate": 1542379556636, "tmdate": 1542379556636, "tddate": null, "forum": "rket4i0qtX", "replyto": "rJlfD0wcom", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "content": {"title": "Authors' response (part 1)", "comment": "Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:\n\n- We added a few sentences to the end of section 2.4 on our speculative intuition regarding what strategy a model may prefer, and we do indeed think that a pairing-based strategy is plausible for convolution-based networks. When talking in more general terms about \"deep learning models\", we refer to the proposed methodology for \"investigating deep learning models\", and don't want to claim that we actually evaluate a representative number of models. We see this methodology, as illustrated by our experiments for one model, as the central part of our contribution. The first paragraph of section 3.2 describes this FiLM model and, given the focus on methodology, we considered the description (plus reference to the paper) sufficient here.\n\n- There are a few points here:\n* Since it was shown that humans seem to follow a cardinality-based strategy, the pairing-based one would be not human-like, but nonetheless cognitively plausible. We use \"cognitively\" in the sense of \"algorithmically\" plausible, so a procedure that makes sense for solving the problem. An example for an implausible method would be to rely on color/shape cues to solve instances involving \"most\", which doesn't make sense for the abstract meaning of \"most\".\n* Regarding the question whether comparability to human behavior is indispensable: On the one hand, acceptance of and reliance on systems which follow vastly different principles can be problematic; on the other hand, if the information a system uses to arrive at its conclusion doesn't make any sense to humans (in cases where humans have an intuition what is relevant, like the above example of \"most\" and color/shape cues), we doubt that good performance alone will justify using such a model, as opposed to instead doubting the quality of the underlying benchmark data.\n* The question whether we want a VQA system which returns approximate answers is an interesting one, but we don't intend to claim that this is a desired property, just that it is desirable to know whether (and how exactly) our systems currently solve such tasks approximately. A conclusion from our findings may well be that it is worth improving VQA models with respect to their counting capability, as they seem to rely on an approximate as opposed to an exact number system.\n\n- We added a longer footnote to section 2.4 about the \"one-glance feed-forward-style networks\" for clarification. In summary, general precise counting is an inherently recursive ability, and models which don't have an architectural module for recursive computations are not expected to be able to learn this capability, while they may stil learn to subitize or represent numbers approximately (which doesn't require recursion).\n\n- You're right, this sentence was a bit vague, we rephrased it to: \"these models can indeed learn and utilize more abstract concepts (approximate numbers) than mere superficial pattern matching (\"red square\" etc)\". The differences we want to point out is that, on the one hand, (approximate) numbers are a more abstract concept than, for instance, object types like \"cat\", \"chair\", etc as they can be combined with any object type. On the other hand, being able to utilize such representations to answer practical questions like whether \"most\" applies is more interesting than just being able to classify which representation applies.\n\n- Good point, our reasoning here was mostly influenced by Pietroski et al.'s work and our intuition about the ability of convolutions to learn a local pairing strategy (see addition at the end of section 2.4). Presumably, it could be possible to observe the behavior in our paper based on a pairing-based mechanism which works approximately, independent of clustering, as predicted by Weber's Law. It's probably impossible to ultimately rule out a pairing-based strategy via experiments evaluating extrinsic behavior only, but we note that there is evidence for Weber's Law in other approximate systems where pairing-based strategies are no alternative, thus suggesting that similar mechanisms are at work here. We added a footnote on this to section 2.4."}, "signatures": ["ICLR.cc/2019/Conference/Paper20/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618896, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper20/Authors|ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618896}}}, {"id": "Ske5xOUn6m", "original": null, "number": 2, "cdate": 1542379506043, "ddate": null, "tcdate": 1542379506043, "tmdate": 1542379506043, "tddate": null, "forum": "rket4i0qtX", "replyto": "B1eHFc49nm", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "content": {"title": "Authors' response", "comment": "Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:\n\n4. Note that the training data is not constrained with respect to ratios and number of objects addressed by the caption, so the learned behavior should be independent of these aspects. Moreover, note that for most ratios there is only one combination of numbers with at most 15 objects in total, but larger images fitting a greater total number of objects would definitely be an option here. For the less close-to-balanced ratios 1:2, 2:3, 3:4 where there are multiple possibilities, performance generally is (close-to-)perfect, indicating that there is no increased difficulty of learning multiples in the presence of more close-to-balanced ratios (for instance, 6:9 vs 7:8). We hope this clarifies your concern.\n\n5. We fully agree that it would be very interesting to investigate these models. For this paper, we decided to focus on the methodology of investigating such questions in detail (the evaluation for FiLM alone comprises around 100 experiments) as opposed to focusing on the comparison of behavior of different models, and leave the latter to future work. We added a few additional sentences to section 3.2 regarding that.\n\n6. We added a few sentences to the end of section 2.4 on our speculative intuition regarding what strategy a model may prefer. We didn't think about the fact that one may want to control which strategy is learned, which would indeed be interesting, but that's why we considered FiLM as is and didn't experiment with changing architecture details. At the same time, considering that understanding \"most\" is only one of many capabilities a VQA model is supposed to learn, these results are unlikely to be an important influencing factor for architecture choice, while at least knowing about the properties of a model is nonetheless interesting.\n\n7. The evaluation is supposed to show what an architecture is capable of learning under \"ideal\" conditions. It's an interesting question whether/how this changes when gradually shifting towards \"less ideal\" setups. An advantage of using a controlled setup like ours is that this is possible to investigate, to some degree at least (for instance, add more types of captions to the training data, not just quantifier statements). At some point we may be interested in actually investigating the same for real-world data, but we think it's unclear right now what exactly such evaluation data should ideally look like, what problems are most interesting, what details to pay attention to. Artificial data allows us to investigate these questions while avoiding the elaborate and expensive process of obtaining real-world data.\n\n8. Note that the training data is far less constrained than the evaluation data, including various distracting aspects like additional shapes/colors. The evaluation data doesn't contain such distractors, but it would of course be possible (and potentially interesting) to add such. We didn't do so since we considered instances with only \"relevant\" attributes to be the most difficult setup, like a minimal pair, where a model is required to focus on all objects and both their shape and color attribute to decide correctly.\n\n9. We incorporated the changes as you suggested. Thanks!\n\n10. We didn't think about this interpretation -- our intention was to signal that we take the \"The Meaning of 'Most'\" setup and methodology of Pietroski et al. from psychology, and implement a deep learning version for visual question answering models."}, "signatures": ["ICLR.cc/2019/Conference/Paper20/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618896, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper20/Authors|ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618896}}}, {"id": "S1eF1dL3TQ", "original": null, "number": 1, "cdate": 1542379489148, "ddate": null, "tcdate": 1542379489148, "tmdate": 1542379489148, "tddate": null, "forum": "rket4i0qtX", "replyto": "SygP4mws3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "content": {"title": "Authors' response", "comment": "Many thanks for the valuable feedback! We uploaded a revised version of the paper, and in the following address the weaknesses you pointed out:\n\n- Due to space constraints, we have to refer to Pietroski et al.'s work for more elaborate reasoning regarding the cognitive implications. We think their experiments are supposed to give strong indication for the ANS as a likely explanation of human behavior, and thus in our work for the FiLM model, without ultimately ruling out the pairing-based strategy (which is probably impossible via experiments evaluating extrinsic behavior only). We are not aware of what the curves for the pairing-based strategy in figure 6 would look like, but there is definitely evidence for Weber's Law in other approximate systems (where pairing-based strategies are no alternative), thus suggesting that similar mechanisms are at work here. We added a footnote on this to section 2.4.\n\n- We added a sentence to section 5 to clarify the differences (they focus on subitizing while we focus on ANS, and their experiments follow a different methodology with specifically designed data used for training and not just evaluation).\n\n- We actually consider the pairing-based strategy as more likely to be learned. Why? You're right that the convolutions need to learn to handle all possible spatial arrangements, but we think that this is the case for both the pairing- and the cardinality-based strategy, while the latter in addition needs to learn a presumably (our intuition) more complex aggregation mechanism of the locally computed results. Anyway, we added a few sentences to the end of section 2.4 discussing our intuition in some more detail to address this point."}, "signatures": ["ICLR.cc/2019/Conference/Paper20/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618896, "tddate": null, "super": null, "final": null, "reply": {"forum": "rket4i0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper20/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper20/Authors|ICLR.cc/2019/Conference/Paper20/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers", "ICLR.cc/2019/Conference/Paper20/Authors", "ICLR.cc/2019/Conference/Paper20/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618896}}}, {"id": "SygP4mws3Q", "original": null, "number": 3, "cdate": 1541268271289, "ddate": null, "tcdate": 1541268271289, "tmdate": 1541534355397, "tddate": null, "forum": "rket4i0qtX", "replyto": "rket4i0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Review", "content": {"title": "Strong, hyper-focused contribution to VQA understanding", "review": "This paper studies how the FiLM visual question answering (VQA) model answer questions involving the quantifier \u2018most\u2019. This quantifier is chosen for study because it cannot be expressed in first order logic (i.e., high-order logic is required), and secondly because there are two different algorithmic approaches to answering questions involving \u2018most\u2019 (cardinality-based strategy and pairing-based strategy). Experiments are performed by designing abstract visual scenes with controlled numerosity and spatial layouts, and applying methodologies from pyscholinguistics. The paper concludes that the model learns an approximate number system (ANS), consistent with the cardinality-based strategy, with implications for understanding the conditions under which existing VQA models should perform well or badly (and possibly for improving VQA models). \n\nStrengths:\n- The research question is clear and well-conceived. In general, it seems there are significant opportunities for better collaboration between the experimental psychology and machine learning communities, and this is a good example of the benefits.\n- The paper is clear, highly-focused, and well-written.\n\nWeaknesses:\n- The arguments for why the experimental evidence actually supports the existance of an approximate number system (ANS) could be made more clear. For example, the section on \u201cRatios andWeber fraction\u201d argues that \u201cthese curves align well with the trend predicted by Weber\u2019s law\u201d, but does not explain how the experimental data would present if the alternative hypothesis (pairing-based strategy) was being used. What would the pairing-based strategy look like in Figure 6 right? Are there not significance tests that could be used to more carefully quantify the level of support for the two alternative strategies?\n- The experiments seem very similar to Wu et al. 2018, which is considered to be prior work under the ICLR guidelines. While this paper is acknowledged in the related work, it would be helpful to expand further on the relationship between these works, so the originality and contribution of this paper can be better evaluated.\n- In some ways it is not that surprising that the CNN more easily learns an approximate number system rather than a pairing-based algorithm, as the later would presumably need to learn a different convolutional filter for every possible spatial arrangement of the pairs (which would be very sample inefficient). Therefore, it might be interesting to consider, are there any circumstances under which the CNN would learn a pairing based algorithm? For example, what if the spatial configuration of the pairs was simplified, so they were always side-by-side at a fixed distance? If pairing-based algorithms emerged under simplified scenarios, this might have implications for the design of CNN filters (if we want models that are capable of learning these types of functions).\n\nSummary:\nI regard this as a good paper, with a couple of weakness that could be addressed as indicated.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Review", "cdate": 1542234556156, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rket4i0qtX", "replyto": "rket4i0qtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631015, "tmdate": 1552335631015, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eHFc49nm", "original": null, "number": 2, "cdate": 1541192317076, "ddate": null, "tcdate": 1541192317076, "tmdate": 1541534355151, "tddate": null, "forum": "rket4i0qtX", "replyto": "rket4i0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Review", "content": {"title": "Interesting direction and discussion to study the relationship \u201cmost\u201d with limited experimental evaluation focusing on a single model.", "review": "Problem and contribution:\nThe paper studies if the Visual Question answering model \u201cFILM\u201d from Perez et al (2018) is able to decide if \u201cmost\u201d of the objects have a certain attribute or color. \nFor this it tries to mimic the setup used to test human abilities in the study by Pietroski et al. (2009).\n\nThe main contribution of this is work is a discussion of how a model could solve the problem of deciding \u201cmost\u201d and the study which shows that the studied model has some ability to do this. From this the paper concludes that the model is likely to have some approximate number system.\n\n\nStrengths:\n1.\tThe paper looks at a new angle to study and characterize CNN models in general, and VQA models in particular by looking into the psycholinguistic literature experimental setup studied with human subjects.\n2.\tThe paper studies different variants of controlling for different factors (e.g. pairing data points, area used, different training data and pre-trained vs. trained from scratch CNN models)\n3.\tIt is interesting to see that the models performance reasonably aligns with the curve predicted by \u201cWeber\u2019s law\u201d.\n\n\nWeaknesses:\n4.\tNumber of objects vs. ratios is not disentangled: While the paper clarifies that not only a smaller number of objects are used, it would be interesting to understand if similar conclusions hold if only the same number or about the same number of total objects are used but the ratios change (at least for more extreme ratios, 1:2, this seems to be the case as they achieve 100% accuracy).\n5.\tThe paper only focusses on a single VQA model (FILM) which limits the understanding if this observation is specific to this model; what about other models such as the one from Hudson & Manning (2018), or Relation Networks (Santoro et al) or even simpler baselines: A system which two attention mechanisms (without normalizations) which are sum pooled and then compared would sort of explicitly encode the idea of the APN system. It would be valuable to compare them to see how different systems (can) solve this task. I would expect that the architecture favors certain capabilities; e.g. Relation Networks might lead more to a paring-based strategy. Or Zhang et al. (2018) might be able to exploit explicit counting to solve the task.\n6.\tThe \u201cmost\u201d ability or APN ability seems to be highly related to accumulation in neural networks. The paper FiLM uses global max-pooling and I am wondering if this affect this ability. \n7.\tThe study is only performed on symbols which a very large training set (given the difficulty of the problem) and it not clear how well this generalizes to real images or scenarios with less training data. \n7.1.\tMaybe beyond the scope of this work, but it would be interesting to understand how much training data different models need to obtain this capability.\n8.\tFor evaluation: Are there distractors, i.e. elements which don\u2019t belong to set A or B? If not, how would distractors affect it.\n9.\tClarity: \n9.1.\tThe equation between equation (1) and (2) misses a number [I will call it 1.5 for now]\n9.2.\tIn formula (1.5) \u201c<=>\u201d seems to be used at different levels (?) it would be good to use brackets to make clear which level \u201c<=>\u201d refers to.\n\nMinor:\n10.\tThe title suggests that the paper studies multiple VQA models but only a single model is studied.\n\nConclusion:\nThe paper looks into an interesting direction to study CNN models but has some limitations including studying only a single VQA model type, limited to artificially generated images. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Review", "cdate": 1542234556156, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rket4i0qtX", "replyto": "rket4i0qtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631015, "tmdate": 1552335631015, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJlfD0wcom", "original": null, "number": 1, "cdate": 1540157017941, "ddate": null, "tcdate": 1540157017941, "tmdate": 1541534354907, "tddate": null, "forum": "rket4i0qtX", "replyto": "rket4i0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper20/Official_Review", "content": {"title": "Interesting analysis of quantifier interpretation in a VQA model, but the theoretical discussion is unsatisfying", "review": "The paper analyzes the strategy that a visual question answering model (FiLM) uses to verify statements containing the quantifier \"most\" (\"most of the dots are red\"). It finds that the model is sensitive to the ratio of objects that satisfy the predicate (that are red) to objects that do not; as the ratio decreases (e.g. 10 red dots compared to 9 blue dots), the model's performance decreases too. This is consistent with human behavior.\n\nStrengths:\n* The introduction lays out an ambitious program of comparing humans to deep neural networks.\n* The experimental results are interesting (although of modest scope) and support the hypothesis that the network is not counting the objects but rather is using an approximation that is sensitive to the ratio between the red and non-red items.\n\nWeaknesses:\n* The architecture of the particular model is described very briefly, and at multiple points there\u2019s an implication that this is an investigation of \u201cdeep learning models\u201d more generally, even though those models may vary widely. While the authors are using an existing model, they shouldn't assume that the reader has read the paper describing that model. I would like to see more discussion of whether it is at all plausible for this model to acquire the pairing strategy, compared to alternative VQA models (e.g., using relation networks).\n* I found it difficult to follow the theoretical motivation for performing the work. The goal seems to be to test whether the network is performing the task in way that \"if not human-like, at least is cognitively plausible\". I don't understand what is meant by cognitively plausible but not human-like; perhaps an example of a cognitively implausible mechanism would help clarify this issue. Later in the same paragraph, the authors argue that \"in the case of a human-centered domain like natural language, ultimately, some degree of comparability to human performance is indispensable\". This assertion is not justified, and seems surprising to me; we have very useful natural language processing systems that do not perform in a way that is comparable to humans (the hedge \"some degree of\" is really neither here nor there). In general, I don't understand why we would want a visual question answering system that returns approximate answers -- isn't it better to have it count exactly how many red dots there are compared to non-red dots?\n* The authors assume that explicit counting is not \"likely to be learned by the 'one-glance' feed-forward-style neural network\" evaluated in the paper. What is this statement based on? Why would a \"one-glance\" network have trouble counting objects? (What is a \u201cone-glance network\u201d?)\n* Another vague concept that is used without clarification: it is argued that if the network implements something like the Approximate Number System, that shows that it can \"learn and utilize higher-level concepts than mere pattern matching\". What is \"pattern matching\" and how does it differ from \"higher-level concepts\"?\n* Why would the pairing strategy in a neural network be affected by the clustering of the objects? I understand why a human who needs to saccade back and forth between the two groups of objects might lose track of the objects that have been paired so far, but I don't understand why that would affect the architecture in question.\n\nMinor comments:\n* Is the definition of \"most\" really a central piece of evidence for \"the apparent importance of a cardinality concept to human cognition\"? Our ability to count seems sufficient to me. Perhaps I'm not understanding what the authors have in mind here.\n* Please use the terms \"interpretation\" and \"verification\" consistently.\n* \"One over the other strategy\" -> \"one strategy over the other\".\n* The paper is almost 9 pages long, but the contribution does not appear more substantial than a standard 8-page submission.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper20/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The meaning of \"most\" for visual question answering models", "abstract": "The correct interpretation of quantifier statements in the context of a visual scene requires non-trivial inference mechanisms. For the example of \"most\", we discuss two strategies which rely on fundamentally different cognitive concepts. Our aim is to identify what strategy deep learning models for visual question answering learn when trained on such questions. To this end, we carefully design data to replicate experiments from psycholinguistics where the same question was investigated for humans. Focusing on the FiLM visual question answering model, our experiments indicate that a form of approximate number system emerges whose performance declines with more difficult scenes as predicted by Weber's law. Moreover, we identify confounding factors, like spatial arrangement of the scene, which impede the effectiveness of this system.", "paperhash": "kuhnle|the_meaning_of_most_for_visual_question_answering_models", "keywords": ["quantifier", "evaluation methodology", "psycholinguistics", "visual question answering"], "authorids": ["aok25@cam.ac.uk", "aac10@cam.ac.uk"], "authors": ["Alexander Kuhnle", "Ann Copestake"], "TL;DR": "Psychology-inspired evaluation of quantifier understanding for visual question answering models", "pdf": "/pdf/cfa9e1cebabbdee1621e129032dbba854de511f2.pdf", "_bibtex": "@misc{\nkuhnle2019the,\ntitle={The meaning of \"most\" for visual question answering models},\nauthor={Alexander Kuhnle and Ann Copestake},\nyear={2019},\nurl={https://openreview.net/forum?id=rket4i0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper20/Official_Review", "cdate": 1542234556156, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rket4i0qtX", "replyto": "rket4i0qtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper20/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335631015, "tmdate": 1552335631015, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper20/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}