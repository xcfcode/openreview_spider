{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363763280000, "tcdate": 1363763280000, "number": 5, "id": "zkxNBUsiN6B38", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6elK6-b28q62g", "replyto": "6elK6-b28q62g", "signatures": ["Eric qiao"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Based on the reviews, a revised version will be updated on arXiv tonight. Thanks."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Behavior Pattern Recognition using A New Representation Model", "abstract": "We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for behavior recognition problems.", "pdf": "https://arxiv.org/abs/1301.3630", "paperhash": "qiao|behavior_pattern_recognition_using_a_new_representation_model", "keywords": [], "conflicts": [], "authors": ["Eric qiao", "Peter A. Beling"], "authorids": ["qifengqiao@gmail.com", "pb3a@virginia.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363762920000, "tcdate": 1363762920000, "number": 2, "id": "N6tX5S-nXZNbo", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6elK6-b28q62g", "replyto": "6elK6-b28q62g", "signatures": ["Eric qiao"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "To Reviewer 698b.\r\n---------------------\r\nResponse: We propose a new problem that aims to categorize decision-makers by learning from the samples of their sequential decision-making behavior. The first key to success of this problem is an appropriately designed feature representation constructed from observations of the actions taken by decision makers. We note that there is little systematic research addressing feature representation for this problem, however, with almost all the existing wok using heuristically selected measure of the raw behavior data. The novelty of our work is not about the classification/clustering algorithms, but rather about how to automatically learn the features that can effectively represent the behavior data. In other words, we propose solving the problem of learning to recognize decision-makers by characterizing behavior with a universal, abstract multi-dimensional feature vector, which does not rely on domain-specific expert knowledge.\r\n\r\nTo Reviewer 08b2\r\n-------------------------\r\nWe are in agreement a decision strategy is a plan or policy that maps state to action for an agent. We further agree with the reviewer that the testing of agents with different decision strategies is an interesting scenario. Indeed we have performed such experimentation, some of which was reported on in the original version of the paper. In our GridWorld test, we simulate agents using the optimal actions output by the forward planning of an MDP model. We generate two classes of agents. The decision strategies of the two classes of agents, which are generated by the forward planning of MDP models, are different; e.g., one group of agents avoids the boundary of GridWorld, while the other group does not.  The reviewer\u2019s suggestion gives us a new idea to produce another scenario in which agents are adopting different decision strategies. In this case, the agents have the same destination goals, but with different uncertainties while making decisions. Therefore, the observed decision strategies will be different. Theoretically, when we model behavior in MDP space, the recovered reward functions will be different and still provide an effective way for categorizing the agents.\r\n\u201cThe secretary problem was first tested on three different strategies that achieve the same goal. This is exactly the interesting scenario. Disappointingly, though, these results were not described in detail or shown (last paragraph on page 8 -- I don't see any details about the results of this experiment).\u201d\r\nResponse: Yes, we have conducted experiments to categorize three groups of agents with different strategies. The results are 100% accurate; therefore we did not show the accuracy charts here. Instead, it is more difficult to categorize two groups of agents whose decision strategies are formed by the same heuristic decision strategy but with different parameters. As shown in Figure 3, action space-based methods have more difficulty solving this problem. In the secretary problem, we first model this problem using a MDP model. The reward function is learned from the observed decision trajectories by using IRL. It is a vector whose entry is a reward learned for a state. To visually display the feature vectors, we project the multi-dimensional reward vector into the 2-D space by using PCA.  The discussion and display of results for these experiments have been expanded in the current version of the paper.\r\nMinor: The conclusions are not well grounded in the current work -- what data make the authors think that this method would be even more superior in real data?\r\n\r\nResponse: The reviewer\u2019s point that this conclusion outstrips the work is quite valid.  We have removed this claim.  Our initial enthusiasm was due in part to good results from another research project that uses our proposed method to analyze behavioral data from high frequency trading algorithms in real exchanges.\r\n\r\n\r\nTo Reviewer 8f06\r\n---------------------------\r\nResponse: \u201cActivity recognition, or called goal recognition, plan recognition, intent recognition in other fields, aims to recognize the actions and goals of one or more agents from a series of observations on the agents' actions and the environmental conditions.\u201d \r\n\r\nPlease note that activity recognition aims to recognize the policies or goals; however, our proposed problem aims to categorize the agents by the abstraction of their behavior, not requiring identification of goals. The reward function recovered by IRL algorithms in our problem may not only characterize goal information but also more abstraction of behavior, e.g. the information on decision strategy or more abstract behavior characteristics.  Our problem is motivated by some real-world problem that comes from domains like high frequency trading of stocks and commodities, where there is considerable interest in identifying new market players and algorithms based on observations of trading actions, but little hope in learning the precise policies employed by these agents. \r\n\r\nActivity recognition problem and our problem are also fundamentally different in the following points. First, plan recognition problem, which is formulated on MDP model, assumes that the reward function of a MDP model (or they call cost function) is known, but our IRL method is to infer the reward function, which is the variable to learn. Second, the goal recognition problem assumes that a set of possible goals is known as a prior. Given the possible goals, one can model the decision problems using MDP/POMDP with the known reward functions. However, our problem is more general and does not require inference of goals. The reward functions are considered as random variables. The existing plan recognition problem is like to infer the goal in a finite and discrete space, but our IRL model indirectly estimates goals in an infinite and continuous space.\r\n\r\n\u201cThese papers use a significantly more advanced setup, in which trajectories also need to be segmented into activities (which is more realistic). Classification or unsupervised learning could used on top of their output as well. The paper should discuss the proposed approach in a broader context. \u201c\r\n\r\nResponse: The two papers offer interesting idea about activity recognition. We are not aware of the research that has advanced setup to segment trajectories into activities, but agree the work is relevant and interesting and have added appropriate citations to the new version of the paper. As we mentioned, our problem is different from learning from demonstration that aims to recognize the goal or the policy. Given a number of trajectories for an agent, our problem is to automatically extract features that capture overall behavior pattern for this agent. It is not our purpose to find several sub-goals (or skills, activities mentioned in those two papers) for one agent. However, these two algorithms give us some idea for further research about an advanced method considering the sub-goals for categorization of the agents.\r\n\r\n\u201cSpecifically, in the grid-world case, the data is generated exactly according to the paradigm for which the algorithm was designed. Also, states are fully observable. What happens if you have more classes? Partial observability, so the environment is not really an MDP in the observations?\u201d\r\nResponse: If the states are partially observable, we can apply POMDP to solve those problems. The focus of our paper is to demonstrate that the agents\u2019 behavior can be better categorized in reward space. We agree that it would shed light on the significance of our method by adding further study in the problems with partially observable states.\r\nOur experiments with the secretary problem are perhaps closer to the spirit separating the regimes of data generation and learning than the original text would have suggested, and we have substantially changed the secretary problem section of the paper to make the correspondences clearer.  As a surrogate for the action trajectories of humans, we use agents that we generate action trajectories for randomly sampled secretary problems using the cutoff rule (CR), successive non-candidate counting rule (SNCCR), and the candidate counting rule (CCR).  For a given decision rule (CR, SNCCR, CCR), we simulate a group of agents that adopt this rule, differentiating individuals in a group by adding Gaussian noise to the rule's parameter.  The details of the process are given in Algorithm 2.  We use IRL and observed actions to learn reward functions for the MDP model given in Algorithm 2. \r\n\r\nIt is critical to understand that the state space for this MDP model captures nothing of the history of candidates, and as a consequence is wholly inadequate for the purposes of modeling SNCCR and CCR.  In other words, for general parameters, neither SNCCR nor CCR can be expressed as a policy for the MDP in Algorithm 2.  (There does exist an MDP in which all three of the decision rules can be expressed as policies, but the state space for this model is exponentially larger.) Hence, for two of the rules, the processes that we use to generate data and the processes we use to learn are distinct.\r\n\r\n\u201cAlso, the FT and FE competitor methods are quite simplistic, one would expect that better way of encoding the trajectory (e.g using PCA or other forms of dimensionality reduction) would work better.\u201d\r\nResponse: The standard and widely used feature extraction scheme calculates the statistical metrics directly on the raw observation data. This relies on application-specific expert knowledge, and there may be verities of methods to quantize the observations. We would like to see whether the standard algorithm would work better after applying advanced feature extract tools, such as PCA, on top of the FT and FE feature vectors.\r\n\r\n\u201cSince many reward functions can generate the same behavior, but some will make different behaviors easier to recognize than others, the paper should emphasize which of the algorithmic choices here are specifically designed to help the recognition. \u201c\r\nResponse: The core of this paper is to propose a universal feature representation framework that can effectively characterize the agents\u2019 behavior, instead of studying which IRL algorithm is the best choice in this framework. A major contribution is that our feature representation is automatic and does not require domain-specific expert knowledge for selecting the feature metrics. We aim to prove that the reward feature space recovered by IRL algorithms is superior to the standard methods that manually construct the statistical features on the raw observation data.\r\n\r\nHowever, other papers (see, e.g., Q. Qiao and P. Beling. Inverse reinforcement learning via Gaussian process. ACC, 2011) also find that the IRL algorithm with GP also excels in the training of an apprentice RL agent. This means that some IRL algorithms may perform better in both the recognition problem and the apprenticeship learning problem. In turn, this may show the evidence that the reward features can better characterize the decision-making behaviors. There may be some link between the problems of recognizing the behavior patterns and replicating the behavior policies.\r\n\r\n\u201cvalue of hyper-parameters for the GP and describe how these have been/can be chosen.\u201d\r\nResponse: The algorithm on page 5 mentioned how to optimize the hyper-parameters for the GP."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Behavior Pattern Recognition using A New Representation Model", "abstract": "We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for behavior recognition problems.", "pdf": "https://arxiv.org/abs/1301.3630", "paperhash": "qiao|behavior_pattern_recognition_using_a_new_representation_model", "keywords": [], "conflicts": [], "authors": ["Eric qiao", "Peter A. Beling"], "authorids": ["qifengqiao@gmail.com", "pb3a@virginia.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362703740000, "tcdate": 1362703740000, "number": 3, "id": "KK9P-lgBP7-mW", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6elK6-b28q62g", "replyto": "6elK6-b28q62g", "signatures": ["anonymous reviewer 8f06"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Behavior Pattern Recognition using A New Representation Model", "review": "Summary:\r\nThe paper presents an approach to activity recognition based on inverse reinforcement learning.  It proposes an IRL algorithm based on Gaussian Processes.  Evaluation is presented for classification and clustering of behavior in a grid-world problem and the secretaries problem.\r\n\r\nComments:\r\nThe problem called here 'behavior pattern recognition' is very actively studied currently under the name 'activity recognition', using both unsupervised and supervised methods, some quite sophisticated.  See:\r\nhttp://en.wikipedia.org/wiki/Activity_recognition\r\nand references therein.  You should clarify why you need a new term here, if somehow the problem you propose here is different.  Based on its definition, it does not seem to be any different.  \r\nMoreover, this problem has also been studied in reinforcement learning in the context of learning by demonstration.  See the recent work of George Konidaris, eg:\r\nG.D. Konidaris, S.R. Kuindersma, R.A. Grupen and A.G. Barto. Robot Learning from Demonstration by Constructing Skill Trees. The International Journal of Robotics Research 31(3), pages 360-375, March 2012. \r\nAndrew Thomaz, eg:\r\nL. C. Cobo, C.L. Isbell, and A.L. Thomaz. 'Automatic task decomposition and state abstraction from demonstration.' In Proceedings of the International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), 2012. \r\nThese papers use a significantly more advanced setup, in which trajectories also need to be segmented into activities (which is more realistic).  Classification or unsupervised learning could used on top of their output as well.  \r\nThe paper should discuss the proposed approach in a broader context.\r\n\r\nThe experiments presented in the paper are quite simplistic and need to be improved.  Specifically, in the grid-world case, the data is generated exactly according to the paradigm for which the algorithm was designed.  Also, states are fully observable.  What happens if you have more classes? Partial observability, so the environment is not really an MDP in the observations?  Also, the FT and FE competitor methods are quite simplistic, one would expect that better way of encoding the trajectory (e.g using PCA or other forms of dimensionality reduction) would work better.  \r\n\r\nNote that comparing against the other IRL methods for this task is tricky, because they are designed to recover a reward function that can be then used to train an RL agent, not a reward function which can be used to recognize future behavior.  These are different goals.  Since many reward functions can generate the same behavior, but some will make different behaviors easier to recognize than others, the paper should emphasize which of the algorithmic choices here are specifically designed to help the recognition.\r\n\r\nFor the secretaries problem, classification results should also be included.  The description of the problem is very brief and makes it hard to tell how difficult the problem is (Fig 3a seems to suggest it's not that hard).  \r\n\r\nIncluding a more realistic domain, where activities change during a trajectory, would make the paper a lot more convincing.\r\n\r\nFrom a writing point of view, there are many small grammar mistakes, especially using 'the' and 'a' and the paper requires a careful proofreading. Also, the experimental description should specify all detail necessary, eg value of hyper-parameters for the GP and describe how these have been/can be chosen.  Running times would also be useful to include, as well as error bars on the graphs.\r\n\r\nPros:\r\n- IRL would be useful to use in this setting, and the proposed approach makes sense\r\nCons:\r\n- Related references are omitted or not discussed\r\n- Novelty of the proposed approach is low\r\n- The experiments are very limited and simplistic"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Behavior Pattern Recognition using A New Representation Model", "abstract": "We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for behavior recognition problems.", "pdf": "https://arxiv.org/abs/1301.3630", "paperhash": "qiao|behavior_pattern_recognition_using_a_new_representation_model", "keywords": [], "conflicts": [], "authors": ["Eric qiao", "Peter A. Beling"], "authorids": ["qifengqiao@gmail.com", "pb3a@virginia.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362473880000, "tcdate": 1362473880000, "number": 4, "id": "PPs3ZO_pnzZTb", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6elK6-b28q62g", "replyto": "6elK6-b28q62g", "signatures": ["anonymous reviewer 08b2"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Behavior Pattern Recognition using A New Representation Model", "review": "This paper proposes a behavior pattern recognition framework that re-represents the problem of classifying behavior trajectories as a problem of classifying reward functions instead. Since the reward function of the agent that is classified is not known, it is inferred using inverse reinforcement learning (IRL). Comparison of the proposed method to standard trajectory classification methods shows that the former performs much better that the latter on a grid task and to a lesser extent (but still better) on an optimal stopping problem (the secretary problem). \r\n\r\nThe novelty here is not in the classification or IRL algorithms, but rather in the idea that it is better to classify reward functions than observed state-action sequences. The real test case for this proposal, I think, is a case in which agents differ in their behavioral strategy, but not in the (real) reward function which they were working for. It might be that in that case the proposed method would still excel as the inferred reward function would be different for those with different strategies -- and this would be a very nice demonstration. For instance, if I prefer to go to the goal using the scenic route, and someone else takes the route with less traffic (but a lower speed limit), we might both reach the destination at the same time, thus maximizing some external reward function correctly, but IRL might infer that I assign reward to scenery and the other person to not having to compete with other cars on the road. \r\n\r\nUnfortunately, such a scenario was not tested in the paper. The gridworld task involved two classes of agents that differed only in their (true) reward function, not their strategies. In that case it seems obvious that classifying based on reward functions would be a good idea (it was still nice to see that the proposed method does very well even with very short trajectories --- I am not saying there was no merit to the experiments shown, just that this was not the strongest test case for the proposed framework). \r\n\r\nThe secretary problem was first tested on three different strategies that achieve the same goal. This is exactly the interesting scenario. Disappointingly, though, these results were not described in detail or shown (last paragraph on page 8 -- I don't see any details about the results of this experiment). Instead, the authors show results for a different experiment in which all agents had the same strategy but differed in the cutoff rule (which is akin to a reward function), as well as an experiment comparing a heuristic strategy to a random one. In both cases these are not the interesting test cases. (As an aside, I also found Figure 3 which describes these results unclear: how was reward defined for these simulations? what are the axes in the different subplots?)\r\n\r\nMinor: The conclusions are not well grounded in the current work -- what data make the authors think that this method would be even more superior in real data?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Behavior Pattern Recognition using A New Representation Model", "abstract": "We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for behavior recognition problems.", "pdf": "https://arxiv.org/abs/1301.3630", "paperhash": "qiao|behavior_pattern_recognition_using_a_new_representation_model", "keywords": [], "conflicts": [], "authors": ["Eric qiao", "Peter A. Beling"], "authorids": ["qifengqiao@gmail.com", "pb3a@virginia.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362418320000, "tcdate": 1362418320000, "number": 1, "id": "kA2a1ywTaHAT3", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "6elK6-b28q62g", "replyto": "6elK6-b28q62g", "signatures": ["anonymous reviewer 698b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Behavior Pattern Recognition using A New Representation Model", "review": "I am not a huge expert in reinforcement learning but nonetheless I have to say this paper is quite confusing to me. I had a hard time understanding the point. Moreover, I think the topic of this paper has nothing to do whatsoever with the interests of this conference, namely representation learning, so I suggest the authors resubmit this work elsewhere.\r\n\r\ncons:\r\n- not clearly written\r\n- not relevant to this conference"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Behavior Pattern Recognition using A New Representation Model", "abstract": "We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for behavior recognition problems.", "pdf": "https://arxiv.org/abs/1301.3630", "paperhash": "qiao|behavior_pattern_recognition_using_a_new_representation_model", "keywords": [], "conflicts": [], "authors": ["Eric qiao", "Peter A. Beling"], "authorids": ["qifengqiao@gmail.com", "pb3a@virginia.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358404200000, "tcdate": 1358404200000, "number": 49, "id": "6elK6-b28q62g", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "6elK6-b28q62g", "signatures": ["qifengqiao@gmail.com"], "readers": ["everyone"], "content": {"decision": "reject", "title": "Behavior Pattern Recognition using A New Representation Model", "abstract": "We study the use of inverse reinforcement learning (IRL) as a tool for the recognition of agents' behavior on the basis of observation of their sequential decision behavior interacting with the environment. We model the problem faced by the agents as a Markov decision process (MDP) and model the observed behavior of the agents in terms of forward planning for the MDP. We use IRL to learn reward functions and then use these reward functions as the basis for clustering or classification models. Experimental studies with GridWorld, a navigation problem, and the secretary problem, an optimal stopping problem, suggest reward vectors found from IRL can be a good basis for behavior pattern recognition problems. Empirical comparisons of our method with several existing IRL algorithms and with direct methods that use feature statistics observed in state-action space suggest it may be superior for behavior recognition problems.", "pdf": "https://arxiv.org/abs/1301.3630", "paperhash": "qiao|behavior_pattern_recognition_using_a_new_representation_model", "keywords": [], "conflicts": [], "authors": ["Eric qiao", "Peter A. Beling"], "authorids": ["qifengqiao@gmail.com", "pb3a@virginia.edu"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 6}