{"notes": [{"id": "HJePXkHtvS", "original": "HJec6L2_Dr", "number": 1620, "cdate": 1569439519062, "ddate": null, "tcdate": 1569439519062, "tmdate": 1577168263058, "tddate": null, "forum": "HJePXkHtvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "tOhGpVMw39", "original": null, "number": 1, "cdate": 1576798728034, "ddate": null, "tcdate": 1576798728034, "tmdate": 1576800908507, "tddate": null, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a training method for deep neural networks to detect out-of-distribution samples under perspective of Gaussian discriminant analysis.\n\nReviewers and AC agree that some idea is given in the previous work (although it does not focus on training), and additional ideas in the paper are not super novel. Furthermore, experimental results are weak, e.g., comparison with other deep generative classifiers are desirable, as the paper focuses on training such deep models.\n\nHence, I recommend rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726544, "tmdate": 1576800278703, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Decision"}}}, {"id": "Sygh7IBZ5r", "original": null, "number": 2, "cdate": 1572062756314, "ddate": null, "tcdate": 1572062756314, "tmdate": 1574349932024, "tddate": null, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper proposes a metric learning-based generative model for detecting the out-of-distribution examples.  A new objective function is proposed to model class-dependent class-distribution into a Gaussian analysis models. For the proposed objective, the illustration of derived KL divergence under the Gaussian discriminative analysis assumption is well done.  The empirical results conclude the superiority of the proposed loss function in both tabular and image datasets, when comparing the plain network and one with a softmax.\n\nThis study aims is to detect out-of-distribution samples for better generalization. However, the related works need to be revised and present the novelty of the work compared to some metric and distance-based learning algorithms. For example, the proposed idea is similar to adding a regularization term to the prototypical network with Euclidean distance (Snell et al. 2016). This aspect is not very well explained.\n\nAnother issue is the lack of comparison with state-of-the-art approaches. The Related Work section (Sec. 2) show a baseline (plain) and another one based on softmax. Experimental comparison with state-of-the-art will help to position this work.\n\n** Update ** I read the authors comments and other reviews. Although some clarification were useful, I still maintain my rating of \"weak reject\", I don't get much excitment and I am not feeling there is something great with this work.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1620/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1620/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575581226766, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1620/Reviewers"], "noninvitees": [], "tcdate": 1570237734730, "tmdate": 1575581226784, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Official_Review"}}}, {"id": "rkg7luoWcS", "original": null, "number": 3, "cdate": 1572087786784, "ddate": null, "tcdate": 1572087786784, "tmdate": 1574281061285, "tddate": null, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper presents an algorithm two learn both classifier and out-of-distribution sample detector. Instead of learning softmax weights, the proposed approach learns to project the inputs to a latent space, where each class is a Gaussian distribution. Out-of-distribution samples can be detected by the distance between the learnt representation and centers. The proposed approach can be viewed as generalization of Gaussian discriminant analysis and one-class classification. The proposed approach is technically sound, and the experiments do show some improvement over previous algorithm on out-of-distribution detection, especially on tabular datasets. However I think there are some weaknesses of this paper\n\n* The novelty is a little thin. The proposed algorithm is based on just a modification of the learning objective, and there are no theoretical analysis of why the proposed approach can work better.\n* Experimental result is somewhat weak. Improvement on the image datasets seems marginal, especially on the SVHN dataset. I also doubt if classifying Cifar10 against TinyImageNet or LSUN challenging enough, because these datasets are fairly different. I am interested in whether the proposed approach can detect novel classes, such as training using only 9 of 10 classes on Cifar10.\n\nAnother question: does learning classifier as well as centers need additional optimization techniques, like special initialization?\n\nUpdate\n======= \n\nAfter a careful read of the Mahabolis baseline (Lee et al., 2018) I agree with the authors that this paper has some novelty comparing with previous works, i.e., directly learning a generative classifier instead of converting a discrimitively trained classifier into generative. Combined with the good results obtained. I will raise my score to a weak accept (though without a strong belief).", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1620/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1620/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575581226766, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1620/Reviewers"], "noninvitees": [], "tcdate": 1570237734730, "tmdate": 1575581226784, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Official_Review"}}}, {"id": "Byl13TdZsS", "original": null, "number": 5, "cdate": 1573125542532, "ddate": null, "tcdate": 1573125542532, "tmdate": 1573125632378, "tddate": null, "forum": "HJePXkHtvS", "replyto": "rkg7luoWcS", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Official_Comment", "content": {"title": "Rebuttal #2", "comment": "Thanks for your review.\n\nWe would disagree with the reviewer on the aspect of novelty. Our work is not about just a modification of the learning objective, but designing a novel objective for effectively detecting OOD samples by using deep neural networks (DNNs) in perspective of Gaussian discriminant analysis (GDA). Unlike the objective used for training the softmax classifier, our proposed objective is theoretically derived from GDA, and this theoretical background guarantees that each class-conditional distribution follows isotropic Gaussian distribution with the same variance in the latent space.\n\nNote that the latent space optimized by the softmax classifier does not guarantee that 1) the class-conditional distributions follow the Gaussian distribution and 2) they have the same covariance matrix, as shown in Figure 1. However, the state-of-the-art method [Lee et al. 2018] computes the Mahalanobis distance using the tied-covariance matrix (assuming that all the covariances are the same) in such space. For this reason, the Mahalanobis method cannot accurately capture the confidence of each sample (i.e., how likely the sample belongs to the in-distribution), and the proposed method clearly address this problem. Thereby, our generative classifier achieves higher OOD detection accuracy than the state-of-the-art method.\n\n\nAbout your question:\n\nThe proposed classifier does not need any additional optimization techniques. In the experiments, we used the Xavier weight initialization and the Adam optimizer, which are conventionally used for DNNs (or the softmax classifier). In this sense, we claim that our learning objective empirically provides the stable convergence and it can be easily employed in the deep learning framework without complicated mathematical modeling or sophisticated optimization.\n\n[Lee et al. 2018] A Simple Unified Framework for Detecting Out-of-distribution Samples and Adversarial Attacks, NIPS 2018\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1620/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJePXkHtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1620/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1620/Authors|ICLR.cc/2020/Conference/Paper1620/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153327, "tmdate": 1576860541980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Official_Comment"}}}, {"id": "HJgDWCr-oH", "original": null, "number": 4, "cdate": 1573113343502, "ddate": null, "tcdate": 1573113343502, "tmdate": 1573113343502, "tddate": null, "forum": "HJePXkHtvS", "replyto": "SJgAzmZUYr", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Official_Comment", "content": {"title": "Rebuttal #1", "comment": "Thanks for your review.\n\n1) The main motivation of our work is from the observation that the previous Mahalanobis method adopts the concept of the generative classifier under the strong assumption, which is not realistic enough. Specifically, the latent space optimized by the softmax classifier does not guarantee that each empirical class-conditional distribution follows the Gaussian distribution. In addition, the Mahalanobis detector requires the linear discriminant analysis (LDA) assumption that all the class covariances are the same, to compute the Mahalanobis distance by using tied-covariance matrix. Figure 1 clearly shows that the latent space trained by the softmax classifier is not suitable for the Mahalanobis detector in that 1) all the covariances are not the same as well as 2) ID and OOD samples are difficult to be distinguished. \n\nTo address this limitation, we optimize the latent space so that each class-conditional distribution follows the isotropic Gaussian distribution with the same covariance. By doing so, we can simply define the confidence score based on the Euclidean distance without any assumptions, and our proposed score more effectively distinguishes OOD samples from ID samples than the Mahalanobis detector that works on the latent space trained by the softmax classifier. \n\n2) Thank you for letting us know the missing related work. However, they do not define the confidence score that can be used for detecting OOD samples, so we cannot directly compare their performance. It is worth noting that all of them simply focus on ID classification, not OOD detection (please refer to our responses to the reviewer #3). Unlike the existing distance-based classifiers, our objective introduces the regularization term for OOD detection motivated by [Ruff et al. 2018] and it enables to accurately detect OOD samples. \n\nIn the research literature of OOD detection, there have been several attempts to re-train a network based on their own objectives [Malinin and Gales 2018], but they cannot avoid compromising the performance of ID classification. For this reason, the detectors that employ the pre-trained softmax classifier (including the baseline detector and Mahalanobis detector) have gained much attention. In this sense, although our main task is OOD detection, we report the ID classification results in order to emphasize that our classifier succeeds to improve the performance of OOD detection without compromising the ID classification accuracy.\n\n\nAbout your questions:\n\n1) Thanks for your suggestion, but it sounds quite challenging to learn the covariance that approximates an arbitrary matrix. By letting it be an identity matrix, our objective can be easily implemented on the deep learning framework as well as efficiently compute the confidence score.\n\n2) Our proposed objective enforces that the covariance of pre-trained features approximate an identity matrix. Thus, we think the Mahalanobis detector would hardly affect the performance, even though the actual covariance could not be an identity matrix exactly.  \n\n\n[Ruff et al. 2018] Deep One-class Classification, ICML 2018\n[Malinin and Gales 2018] Predictive Uncertainty Estimation via Prior Networks, NIPS 2018"}, "signatures": ["ICLR.cc/2020/Conference/Paper1620/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJePXkHtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1620/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1620/Authors|ICLR.cc/2020/Conference/Paper1620/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153327, "tmdate": 1576860541980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Official_Comment"}}}, {"id": "BJxYqil-iH", "original": null, "number": 3, "cdate": 1573092240525, "ddate": null, "tcdate": 1573092240525, "tmdate": 1573092240525, "tddate": null, "forum": "HJePXkHtvS", "replyto": "Sygh7IBZ5r", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Official_Comment", "content": {"title": "Rebuttal #3", "comment": "Thanks for your review.\n\n1) We want to emphasize that the most important part of our proposed classifier is the regularization term, because it plays a key role to accurately detect OOD samples. The challenge of the OOD detection task is to obtain the decision boundary between ID samples and OOD samples. To this end, we aim to learn K one-class classifiers that have sphere-shaped decision boundaries with minimum volumes by using the regularization term. DeepSVDD [Ruff et al. 2018] showed that such a sphere-shaped decision boundary is effective to detect abnormal samples in one-class setting, so we extend it to multi-class setting specifically for the OOD detection task. On the other hand, the existing distance-based classifiers including [Snell et al. 2017] only focus on the ID classification based on the distance, so their OOD detection performance would be poor. In Figure 2, the classifier trained with a very small regularization coefficient $\\lambda=10^{-3}$ (it seems to be almost the same model with [Snell et al. 2017]) achieves the poor performance in terms of OOD detection while still showing the good performance in terms of ID classification. \n\n2) In Table 2 and 3, we already compared the performances with the state-of-the-art method, which is Mahalanobis method. [Lee et al. 2018] demonstrated that the Mahalanobis method outperforms both the baseline (plain) and another one (ODIN; equipped with calibration techniques) based on softmax. Furthermore, as we mentioned in the paper, calibration techniques such as temperature scaling and input perturbation are not practical because they require OOD samples from the test distribution to find the best hyperparameter values for OOD detection. For this reason, we omit the comparison with ODIN. Note that the OOD detection performance of our proposed classifier would be much better if any calibration techniques are applied to.\n\n[Ruff et al. 2018] Deep One-class Classification, ICML 2018\n[Snell et al. 2017] Prototypical Networks for Few-shot Learning, NIPS 2017\n[Lee et al. 2018] A Simple Unified Framework for Detecting Out-of-distribution Samples and Adversarial Attacks, NIPS 2018\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1620/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJePXkHtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1620/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1620/Authors|ICLR.cc/2020/Conference/Paper1620/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153327, "tmdate": 1576860541980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Official_Comment"}}}, {"id": "SJgAzmZUYr", "original": null, "number": 1, "cdate": 1571324693554, "ddate": null, "tcdate": 1571324693554, "tmdate": 1572972445136, "tddate": null, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\n\nUnlike the softmax classifier, the authors considered the generative classifier based on Gaussian discriminative analysis and showed that such deep generative classifiers can be useful for detecting out-of-distribution samples. For various benchmark tasks, the proposed method outperforms baselines based on the softmax classifier.  \n\nDetailed comments:\n\nThe novelty of this paper is not significant due to the following reasons:\n\n1. The main message (i.e. the concept of the deep generative classifier can be useful for detecting out-of-distribution samples) is not really new because it has been explored before [Lee' 18]. Even though this paper considers training a deep generative classifier directly unlike [Lee' 18], the proposed method looks like a simple variant of [Lee' 18].\n\n2. Missing baselines for training the deep generative classifier: training the deep generative classifier directly has been studied by [Guerriero' 18] and [Pang' 18] but the authors did not compare the proposed training method with such baselines. Because of that, it is hard to say that contributions from proposing a training method are significant. \n\nQuestions:\n\n1. Could the authors consider a case without an identity covariance assumption? Most training methods for deep generative classifier assumes the identity covariance matrix because optimizing the log determinant is not easy. So, it would be interesting if the authors can handle this issue. \n\n2. Even though the authors assume the identity covariance matrix, the covariance matrix of pre-trained features can not be an identity matrix. Could the authors report the performance of Mahalanobis detector using the proposed deep generative classifier? \n\n[Lee' 18] Lee, K., Lee, K., Lee, H. and Shin, J., 2018. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems (pp. 7167-7177).\n\n[Guerriero' 18] Samantha Guerriero, Barbara Caputo, Thomas Mensink, DeepNCM: Deep Nearest Class Mean Classifiers, ICLR workshop 2018.\n\n[Pang' 18] Pang, T., Du, C. and Zhu, J.,  Max-mahalanobis linear discriminant analysis networks. In ICML, 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1620/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1620/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575581226766, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1620/Reviewers"], "noninvitees": [], "tcdate": 1570237734730, "tmdate": 1575581226784, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Official_Review"}}}, {"id": "SJx0sj7NOS", "original": null, "number": 1, "cdate": 1570155430041, "ddate": null, "tcdate": 1570155430041, "tmdate": 1570155430041, "tddate": null, "forum": "HJePXkHtvS", "replyto": "Syl8zfSZ_S", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Official_Comment", "content": {"comment": "Thanks for your comment! The minimization of our KL-divergence term does not guarantee to achieve a unit covariance matrix in the class-conditional distribution, because it is identical to minimizing the variances as you pointed out. We observed that the term \\sum ||f(x)-c_k||^2 also can be derived from KL(P_k || N(c_k, \\sigma^2 I)) in the same way, i.e., even in the case that we assume the isotropic Gaussian distribution. This phenomenon occurs because of the empirical distribution P_k based on the dirac delta function, which is not continuous but has non-zero values only at the points where the f(x) exists. In this situation, reducing the distance between each point and the class center eventually makes the empirical distribution approximate the isotropic Gaussian distribution, regardless of its assumed variance \\sigma. However, it does not affect our overall framework. To use the Euclidean distance for OOD detection and ID classification, the actual values of the variances are not important as long as they are the same for all the classes. In order to control the effect of this KL-divergence term, we introduced the hyperparameter \\lambda, which determines the final variance of the class-conditional distributions while interacting with the log posterior term in our objective.", "title": "RE: A question about how to enforce the covariance to be an identity matrix"}, "signatures": ["ICLR.cc/2020/Conference/Paper1620/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJePXkHtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1620/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1620/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1620/Authors|ICLR.cc/2020/Conference/Paper1620/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153327, "tmdate": 1576860541980, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Official_Comment"}}}, {"id": "Syl8zfSZ_S", "original": null, "number": 1, "cdate": 1569964558331, "ddate": null, "tcdate": 1569964558331, "tmdate": 1569964558331, "tddate": null, "forum": "HJePXkHtvS", "replyto": "HJePXkHtvS", "invitation": "ICLR.cc/2020/Conference/Paper1620/-/Public_Comment", "content": {"comment": "Thanks for the interesting idea! Using a generative classifier for detecting OOD makes lots of sense. It seems that using the GDA assumption while enforcing unit covariance matrix is the key step. Would you elaborate more about how the unit covariance matrix be achieved? The confusion comes from Section 2, in that the loss term derived from KL(P_k||N(c_k, I)) will have an effect of keep minimizing the variance, instead of driving its covariance to be an identity matrix. Is there an empirical observation showing that a unit covariance matrix is achieved by adding this loss term? Thanks!", "title": "A question about how to enforce the covariance to be an identity matrix"}, "signatures": ["~Yen-Chang_Hsu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yen-Chang_Hsu1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generative Classifier for Out-of-distribution Sample Detection", "authors": ["Dongha Lee", "Sehun Yu", "Hwanjo Yu"], "authorids": ["dongha0914@postech.ac.kr", "hunu12@postech.ac.kr", "hwanjoyu@postech.ac.kr"], "keywords": ["Out-of-distribution Detection", "Generative Classifier", "Deep Neural Networks", "Multi-class Classification", "Gaussian Discriminant Analysis"], "TL;DR": "This paper proposes a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks.", "abstract": "The capability of reliably detecting out-of-distribution samples is one of the key factors in deploying a good classifier, as the test distribution always does not match with the training distribution in most real-world applications. In this work, we propose a deep generative classifier which is effective to detect out-of-distribution samples as well as classify in-distribution samples, by integrating the concept of Gaussian discriminant analysis into deep neural networks. Unlike the discriminative (or softmax) classifier that only focuses on the decision boundary partitioning its latent space into multiple regions, our generative classifier aims to explicitly model class-conditional distributions as separable Gaussian distributions. Thereby, we can define the confidence score by the distance between a test sample and the center of each distribution. Our empirical evaluation on multi-class images and tabular data demonstrate that the generative classifier achieves the best performances in distinguishing out-of-distribution samples, and also it can be generalized well for various types of deep neural networks.", "pdf": "/pdf/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "paperhash": "lee|deep_generative_classifier_for_outofdistribution_sample_detection", "original_pdf": "/attachment/11ac0898cd3ebc18d54a20f8e51424f430299881.pdf", "_bibtex": "@misc{\nlee2020deep,\ntitle={Deep Generative Classifier for Out-of-distribution Sample Detection},\nauthor={Dongha Lee and Sehun Yu and Hwanjo Yu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJePXkHtvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJePXkHtvS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192177, "tmdate": 1576860575409, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1620/Authors", "ICLR.cc/2020/Conference/Paper1620/Reviewers", "ICLR.cc/2020/Conference/Paper1620/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1620/-/Public_Comment"}}}], "count": 10}