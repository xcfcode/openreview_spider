{"notes": [{"id": "S1lXnhVKPr", "original": "SygrzpCWwS", "number": 182, "cdate": 1569438890524, "ddate": null, "tcdate": 1569438890524, "tmdate": 1577168228592, "tddate": null, "forum": "S1lXnhVKPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "authors": ["Xianfeng Liang", "Shuheng Shen", "Jingchang Liu", "Zhen Pan", "Yifei Cheng", "Enhong Chen"], "authorids": ["zeroxf@mail.ustc.edu.cn", "vaip@mail.ustc.edu.cn", "jliude@cse.ust.hk", "pzhen@mail.ustc.edu.cn", "chengyif@mail.ustc.edu.cn", "cheneh@ustc.edu.cn"], "keywords": ["variance reduction", "local SGD", "distributed optimization"], "TL;DR": "We prove that the proposed algorithm can achieve linear iteration speedup with lower communication complexity than Local SGD and the experimental results verify our theoretical clarification.", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.", "pdf": "/pdf/df64e6c4b505d38fc940fdb12afd76fd73ee2502.pdf", "code": "https://github.com/VRL-SGD/VRL-SGD", "paperhash": "liang|variance_reduced_local_sgd_with_lower_communication_complexity", "original_pdf": "/attachment/d7612234b1a301f02037b934ab2b0b732e2a9df6.pdf", "_bibtex": "@misc{\nliang2020variance,\ntitle={Variance Reduced Local {\\{}SGD{\\}} with Lower Communication Complexity},\nauthor={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Yifei Cheng and Enhong Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lXnhVKPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "VhH7thgG2", "original": null, "number": 1, "cdate": 1576798689564, "ddate": null, "tcdate": 1576798689564, "tmdate": 1576800945591, "tddate": null, "forum": "S1lXnhVKPr", "replyto": "S1lXnhVKPr", "invitation": "ICLR.cc/2020/Conference/Paper182/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a novel variance reduction algorithm for SGD. The presentation is clear. But the theory is not good enough. The reivewers worry about the converge results and the technical part is not sound.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "authors": ["Xianfeng Liang", "Shuheng Shen", "Jingchang Liu", "Zhen Pan", "Yifei Cheng", "Enhong Chen"], "authorids": ["zeroxf@mail.ustc.edu.cn", "vaip@mail.ustc.edu.cn", "jliude@cse.ust.hk", "pzhen@mail.ustc.edu.cn", "chengyif@mail.ustc.edu.cn", "cheneh@ustc.edu.cn"], "keywords": ["variance reduction", "local SGD", "distributed optimization"], "TL;DR": "We prove that the proposed algorithm can achieve linear iteration speedup with lower communication complexity than Local SGD and the experimental results verify our theoretical clarification.", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.", "pdf": "/pdf/df64e6c4b505d38fc940fdb12afd76fd73ee2502.pdf", "code": "https://github.com/VRL-SGD/VRL-SGD", "paperhash": "liang|variance_reduced_local_sgd_with_lower_communication_complexity", "original_pdf": "/attachment/d7612234b1a301f02037b934ab2b0b732e2a9df6.pdf", "_bibtex": "@misc{\nliang2020variance,\ntitle={Variance Reduced Local {\\{}SGD{\\}} with Lower Communication Complexity},\nauthor={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Yifei Cheng and Enhong Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lXnhVKPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1lXnhVKPr", "replyto": "S1lXnhVKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721677, "tmdate": 1576800272816, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper182/-/Decision"}}}, {"id": "rkl57upLir", "original": null, "number": 3, "cdate": 1573472290437, "ddate": null, "tcdate": 1573472290437, "tmdate": 1573472290437, "tddate": null, "forum": "S1lXnhVKPr", "replyto": "HJelZJgJ9B", "invitation": "ICLR.cc/2020/Conference/Paper182/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Dear Reviewer 3, \n\nThank you for your suggestion to improve our experiments. \n\nWe have added the comparison between EASGD and VRL-SGD in the latest version. And the experimental results show that EASGD is not as good as VRL-SGD in non-iid case, which is reasonable in our opinions with the following reasons. \n1)\tThe local model and the global model in EASGD will never be consistent, thus EASGD is hard to converge in non-iid case.\n2)\tElastic Force does not alleviate the bias of local gradient. The local gradient in EASGD is biased as Local SGD in the non-iid case. And the \u201cElastic Force\u201d operation is only executed once in k updates, which is a special form of model averaging. \n3)\tTo our best knowledge, there is no theoretical analysis for the convergence of asynchronous EASGD in the general non-convex setting. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper182/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "authors": ["Xianfeng Liang", "Shuheng Shen", "Jingchang Liu", "Zhen Pan", "Yifei Cheng", "Enhong Chen"], "authorids": ["zeroxf@mail.ustc.edu.cn", "vaip@mail.ustc.edu.cn", "jliude@cse.ust.hk", "pzhen@mail.ustc.edu.cn", "chengyif@mail.ustc.edu.cn", "cheneh@ustc.edu.cn"], "keywords": ["variance reduction", "local SGD", "distributed optimization"], "TL;DR": "We prove that the proposed algorithm can achieve linear iteration speedup with lower communication complexity than Local SGD and the experimental results verify our theoretical clarification.", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.", "pdf": "/pdf/df64e6c4b505d38fc940fdb12afd76fd73ee2502.pdf", "code": "https://github.com/VRL-SGD/VRL-SGD", "paperhash": "liang|variance_reduced_local_sgd_with_lower_communication_complexity", "original_pdf": "/attachment/d7612234b1a301f02037b934ab2b0b732e2a9df6.pdf", "_bibtex": "@misc{\nliang2020variance,\ntitle={Variance Reduced Local {\\{}SGD{\\}} with Lower Communication Complexity},\nauthor={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Yifei Cheng and Enhong Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lXnhVKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lXnhVKPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference/Paper182/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper182/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper182/Reviewers", "ICLR.cc/2020/Conference/Paper182/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper182/Authors|ICLR.cc/2020/Conference/Paper182/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175144, "tmdate": 1576860555812, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference/Paper182/Reviewers", "ICLR.cc/2020/Conference/Paper182/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper182/-/Official_Comment"}}}, {"id": "SkgBb_6LiS", "original": null, "number": 2, "cdate": 1573472252768, "ddate": null, "tcdate": 1573472252768, "tmdate": 1573472252768, "tddate": null, "forum": "S1lXnhVKPr", "replyto": "HyxV4BPPtB", "invitation": "ICLR.cc/2020/Conference/Paper182/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Dear Reviewer 2, \nThanks for reviewing our paper. Our replies to your reviews are as follows:\n1.\tOur work mainly focuses on non-convex stochastic optimization. The theoretical analysis in convex/strong convex is also meaningful, and we will consider it in future work. It is unfair and hard to compare the convergence of VRL-SGD with EXTRA, because EXTRA is a deterministic algorithm, while VRL-SGD is a stochastic one. In the latest version (Remark 5.4), we compare the convergence of VRL-SGD with D^2, which is a stochastic variant of EXTRA. VRL-SGD has a tighter convergence rate and a lower communication complexity compared with D^2.\n\n2.\ta) Among the communication efficiency algorithms, Local SGD is simple and has great theoretical properties. We mainly focus on improving the performance of Local SGD in the non-iid case.\n       b) The variance reduction techniques in SVRG and its variants are used to reduce the variance caused by stochastic sampling, while they are time-consuming or memory consuming when applied in a distributed setting, especially training deep learning models. This is because that they need to calculate full gradients or store historical gradients. We use a lightweight variant of the variance reduction technique in SVRG and achieve the goal of eliminating the variance caused by non-iid without more calculation or memory.\n\n3.\tIt had been reported that too large mini-batch sizes would cause a poor generalization in previous studies [1,2,3].\n[1] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang: On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. ICLR 2017\n[2] Siyuan Ma, Raef Bassily, Mikhail Belkin: The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning. ICML 2018\n[3] Dong Yin, Ashwin Pananjady, Maximilian Lam, Dimitris S. Papailiopoulos, Kannan Ramchandran, Peter L. Bartlett: Gradient Diversity: a Key Ingredient for Scalable Distributed Learning. AISTATS 2018\n\n4.\tThank you for your suggestion to improve our experiments, we have \tadded more baselines, such as EASGD mentioned by Reviewer 3. There are many algorithms based on Local SGD, but most of them focus on reducing communication costs, not addressing the issue in the non-iid case.\n\nThe answer to your question: as claimed in Remark 5.6, it suffices to choose any k\\le O(T^{\\frac{1}{2}}/N^{\\frac{3}{2}}).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper182/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "authors": ["Xianfeng Liang", "Shuheng Shen", "Jingchang Liu", "Zhen Pan", "Yifei Cheng", "Enhong Chen"], "authorids": ["zeroxf@mail.ustc.edu.cn", "vaip@mail.ustc.edu.cn", "jliude@cse.ust.hk", "pzhen@mail.ustc.edu.cn", "chengyif@mail.ustc.edu.cn", "cheneh@ustc.edu.cn"], "keywords": ["variance reduction", "local SGD", "distributed optimization"], "TL;DR": "We prove that the proposed algorithm can achieve linear iteration speedup with lower communication complexity than Local SGD and the experimental results verify our theoretical clarification.", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.", "pdf": "/pdf/df64e6c4b505d38fc940fdb12afd76fd73ee2502.pdf", "code": "https://github.com/VRL-SGD/VRL-SGD", "paperhash": "liang|variance_reduced_local_sgd_with_lower_communication_complexity", "original_pdf": "/attachment/d7612234b1a301f02037b934ab2b0b732e2a9df6.pdf", "_bibtex": "@misc{\nliang2020variance,\ntitle={Variance Reduced Local {\\{}SGD{\\}} with Lower Communication Complexity},\nauthor={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Yifei Cheng and Enhong Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lXnhVKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lXnhVKPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference/Paper182/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper182/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper182/Reviewers", "ICLR.cc/2020/Conference/Paper182/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper182/Authors|ICLR.cc/2020/Conference/Paper182/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175144, "tmdate": 1576860555812, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference/Paper182/Reviewers", "ICLR.cc/2020/Conference/Paper182/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper182/-/Official_Comment"}}}, {"id": "Hkl1TwaIsH", "original": null, "number": 1, "cdate": 1573472183074, "ddate": null, "tcdate": 1573472183074, "tmdate": 1573472183074, "tddate": null, "forum": "S1lXnhVKPr", "replyto": "rJgPsOx6FS", "invitation": "ICLR.cc/2020/Conference/Paper182/-/Official_Comment", "content": {"title": "Response to Review #1 ", "comment": "Dear Reviewer 1, \nThanks for reviewing our paper. \n \nHere are our responses that would clarify your concerns.\n1)\tA)  Compared with Local SGD, VRL-SGD uses an extra variance reduction technique which can eliminate the gradient variance among workers. Thus, the impact caused by non-iid is removed in our result, which is our main contribution. A similar result can be found in existing study [1], where non-iid data converge with the same speed as iid data. \nB)  Sorry for the confusion. There was an error in the submitted version since we ignored the first period, where VRL-SGD was consistent with Local SGD without variance reduction. We have fixed this problem in the latest submission.\nC)  We also have added some additional experiments in Appendix E to support our conclusion. As shown in Figure 4 in the latest submission, the gradient variance among workers in VRL-SGD is significantly smaller than it in Local SGD. The variance in VRL-SGD asymptotically tends to zero while it is always a constant in Local SGD.\n2)\tAs shown in Corollary 5.2, VRL-SGD has the convergence rate O(1/sqrt{NT}) conditioned on T \\ge (72N^3L^2K^2)/(sigma^2). This means that, when NT is fixed, to achieve this convergence rate, k needs to satisfy k \\le O(T^{1/2}/N^{3/2}). Therefore, the convergence result is still related to k.\n3)\tAccording to 3.2 NOTATIONS, t`` = t`-k. Therefore \\Delta^{t''}_i and \\Delta^{t'}_i represent the same variable \\Delta_i of the i-th worker at different time. As shown in Eq (4), the update form of \\Delta_i is: \\Delta_i += 1/(k\\gamma) (\\hat{x}^t \u2013 x_i^t).\n4)\tWe do not agree with you. Note that \\Delta_{i}^t is not zero in the above supposition. When x^\\tau and \\hat x are x^*, from the new representation shown of \\Delta_i in Eq (9) we can see that \\Delta_i^t = \\nabla f_i(x^*) \u2013 1/N \\sum_{j=1}^N \\nablaf_j(x^*) = \\nabla f_i(x^*) - \\nabla f(x^*) = \\nabla f_i(x^*) \\neq 0. Besides, one can see from Figure 3 in the latest version, the model in VRL-SGD converges to the optimal solution even the data is non-iid.\n5)\tA)\tThank you for your suggestion to improve our experiments, we have add more experimental results to analyze the influence of parameter k in Appendix F.\nB)\tIn Remark 5.4, we have analyzed that the parameter k needs to satisfy k\\le O(T^{\\frac{1}{2}}/N^{\\frac{3}{2}}) to guarantee the linear speedup of VRL-SGD. So it suffices to choose any k\\le O(T^{\\frac{1}{2}}/N^{\\frac{3}{2}}).\n \n We are sincerely expected that you can take them into account when making the final recommendation. Great Thanks!\n\n[1] Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu. D^2 : Decentralized training over decentralized data. In ICML, pp. 4855\u20134863, 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper182/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "authors": ["Xianfeng Liang", "Shuheng Shen", "Jingchang Liu", "Zhen Pan", "Yifei Cheng", "Enhong Chen"], "authorids": ["zeroxf@mail.ustc.edu.cn", "vaip@mail.ustc.edu.cn", "jliude@cse.ust.hk", "pzhen@mail.ustc.edu.cn", "chengyif@mail.ustc.edu.cn", "cheneh@ustc.edu.cn"], "keywords": ["variance reduction", "local SGD", "distributed optimization"], "TL;DR": "We prove that the proposed algorithm can achieve linear iteration speedup with lower communication complexity than Local SGD and the experimental results verify our theoretical clarification.", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.", "pdf": "/pdf/df64e6c4b505d38fc940fdb12afd76fd73ee2502.pdf", "code": "https://github.com/VRL-SGD/VRL-SGD", "paperhash": "liang|variance_reduced_local_sgd_with_lower_communication_complexity", "original_pdf": "/attachment/d7612234b1a301f02037b934ab2b0b732e2a9df6.pdf", "_bibtex": "@misc{\nliang2020variance,\ntitle={Variance Reduced Local {\\{}SGD{\\}} with Lower Communication Complexity},\nauthor={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Yifei Cheng and Enhong Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lXnhVKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1lXnhVKPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference/Paper182/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper182/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper182/Reviewers", "ICLR.cc/2020/Conference/Paper182/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper182/Authors|ICLR.cc/2020/Conference/Paper182/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504175144, "tmdate": 1576860555812, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper182/Authors", "ICLR.cc/2020/Conference/Paper182/Reviewers", "ICLR.cc/2020/Conference/Paper182/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper182/-/Official_Comment"}}}, {"id": "HyxV4BPPtB", "original": null, "number": 1, "cdate": 1571415339942, "ddate": null, "tcdate": 1571415339942, "tmdate": 1572972628331, "tddate": null, "forum": "S1lXnhVKPr", "replyto": "S1lXnhVKPr", "invitation": "ICLR.cc/2020/Conference/Paper182/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes the variance reduction to the local SGD algorithm and shows the proposed VRL-SGD can achieve better convergence than local SGD when the data are not identical over workers. The idea is interesting and the paper is easy to follow. However, the study does not go into depth. I do not support the acceptance for current situation. \n\n1. The paper does not show the convergence rate when the function is convex/strongly convex, which make it hard to compare with previous works e.g. EXTRA [Shi et al. 2015]. \n2. There are many alternatives to achieve communication efficiency. The paper does not argue why to choose variance reduced Local SGD. A natural idea to reduce the variance is to distribute the inner loop of SVRG to different workers as proposed in [Konecny et al. 2016]. Moreover, the analysis is given in [Lee et al. 2015] and [Cen et al. 2019]. Especially, [Cen et al. 2019] suggests using a regularization term to handle the data load is not balanced over the workers. \n3. The paper states that S-SGD cannot achieve linear iteration speedup due to communication bottleneck. Can we avoid the communication bottleneck by increasing the batch size? There are vast literatures on distributed training of deep neural network by using large batch size.  \n4. The experiment comparison is not complete given there are many related work in this area.\n\nQuestion: How to determine the number of local SGD steps for each communication round? In SVRG, the number of iterations in the inner loop is related to the condition number (strongly convex case). Does the number of local SGD steps have a similar correspondence\n\n[Jason Lee, et al.]  2015. Distributed Stochastic Variance Reduced Gradient Methods and A Lower Bound for Communication Complexity.\n[Shicong Cen, et al.] 2019 Convergence of Distributed Stochastic Variance Reduced Methods without Sampling Extra Data\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper182/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper182/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "authors": ["Xianfeng Liang", "Shuheng Shen", "Jingchang Liu", "Zhen Pan", "Yifei Cheng", "Enhong Chen"], "authorids": ["zeroxf@mail.ustc.edu.cn", "vaip@mail.ustc.edu.cn", "jliude@cse.ust.hk", "pzhen@mail.ustc.edu.cn", "chengyif@mail.ustc.edu.cn", "cheneh@ustc.edu.cn"], "keywords": ["variance reduction", "local SGD", "distributed optimization"], "TL;DR": "We prove that the proposed algorithm can achieve linear iteration speedup with lower communication complexity than Local SGD and the experimental results verify our theoretical clarification.", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.", "pdf": "/pdf/df64e6c4b505d38fc940fdb12afd76fd73ee2502.pdf", "code": "https://github.com/VRL-SGD/VRL-SGD", "paperhash": "liang|variance_reduced_local_sgd_with_lower_communication_complexity", "original_pdf": "/attachment/d7612234b1a301f02037b934ab2b0b732e2a9df6.pdf", "_bibtex": "@misc{\nliang2020variance,\ntitle={Variance Reduced Local {\\{}SGD{\\}} with Lower Communication Complexity},\nauthor={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Yifei Cheng and Enhong Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lXnhVKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lXnhVKPr", "replyto": "S1lXnhVKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574946297312, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper182/Reviewers"], "noninvitees": [], "tcdate": 1570237755845, "tmdate": 1574946297324, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper182/-/Official_Review"}}}, {"id": "rJgPsOx6FS", "original": null, "number": 2, "cdate": 1571780766526, "ddate": null, "tcdate": 1571780766526, "tmdate": 1572972628298, "tddate": null, "forum": "S1lXnhVKPr", "replyto": "S1lXnhVKPr", "invitation": "ICLR.cc/2020/Conference/Paper182/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "In the paper, the authors propose a variance reduced local SGD and prove its convergence rate. In the experiments, they show that the proposed method converges faster than local SGD.  \n\nThe following are my concerns:\n1) I am concerned about the convergence result, it shows that the convergence of the proposed method has nothing to do with the extent of non-iid. However, it is not correct intuitively. It is easy to imagine that non-iid data will converge slower than iid data. \n\n2) In Corollary 5.2, the convergence result is not related to k. It is false to me.\n\n3) It is not clear in algorithm 1 how the \\delta^{t''} is updated.\n\n4) The assumption in equation (11)  \"When all local model x^t, x\\tau and the average model \\hat x converge to the local minimum x\u2217\" is not correct when data is non-iid distributed. Suppose x^t and \\hat x is x^*,  and \\Delta^{t''} = 0.  Because data is non-iid, the solution of the local problem is not equal to the global problem, therefore, x^t will go away from x^*. \n\n5) In the experiment, the setting of k should affect the experiment. However, authors don't analyze this parameter.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper182/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper182/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "authors": ["Xianfeng Liang", "Shuheng Shen", "Jingchang Liu", "Zhen Pan", "Yifei Cheng", "Enhong Chen"], "authorids": ["zeroxf@mail.ustc.edu.cn", "vaip@mail.ustc.edu.cn", "jliude@cse.ust.hk", "pzhen@mail.ustc.edu.cn", "chengyif@mail.ustc.edu.cn", "cheneh@ustc.edu.cn"], "keywords": ["variance reduction", "local SGD", "distributed optimization"], "TL;DR": "We prove that the proposed algorithm can achieve linear iteration speedup with lower communication complexity than Local SGD and the experimental results verify our theoretical clarification.", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.", "pdf": "/pdf/df64e6c4b505d38fc940fdb12afd76fd73ee2502.pdf", "code": "https://github.com/VRL-SGD/VRL-SGD", "paperhash": "liang|variance_reduced_local_sgd_with_lower_communication_complexity", "original_pdf": "/attachment/d7612234b1a301f02037b934ab2b0b732e2a9df6.pdf", "_bibtex": "@misc{\nliang2020variance,\ntitle={Variance Reduced Local {\\{}SGD{\\}} with Lower Communication Complexity},\nauthor={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Yifei Cheng and Enhong Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lXnhVKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lXnhVKPr", "replyto": "S1lXnhVKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574946297312, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper182/Reviewers"], "noninvitees": [], "tcdate": 1570237755845, "tmdate": 1574946297324, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper182/-/Official_Review"}}}, {"id": "HJelZJgJ9B", "original": null, "number": 3, "cdate": 1571909368014, "ddate": null, "tcdate": 1571909368014, "tmdate": 1572972628254, "tddate": null, "forum": "S1lXnhVKPr", "replyto": "S1lXnhVKPr", "invitation": "ICLR.cc/2020/Conference/Paper182/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tackles the problem of data-parallel (synchronous) distributed SGD, to optimize the (finite) sum of N non-convex, possibly different (in the so-called non-identical case), loss functions. This paper focuses on improving the communication efficiency compared to several existing methods tackling this problem.\n\nTo that end, the authors contribute:\n\u00b7 A novel algorithm and its asymptotic communication complexity.\n\u00b7 The proof that the common metric of the sum over the training steps of the expected squared norm of the gradient at the average of the N parameters is bounded above.\n\u00b7 Experimental results (training loss function of epoch number) comparing this algorithm with 2 existing ones, solving 3 problems under reasonable settings.\n\n  The training time per epoch of VRL-SGD is claimed to be identical to the one of Local SGD, as the algorithm only have minor differences.\n\n- strengths of the paper: \n\n\u00b7 The main paper is very easy to follow.\n\u00b7 Good effort to give intuitions on why VLR-SGD can improve the convergence rate of existing algorithms.\n  Such an effort is to be highlighted.\n\u00b7 No obvious mistake in the main.   I have not thoroughly checked the full proof though.\n\n-  weaknesses of the paper: \n\n\u00b7 The algorithm, while having differences, is quite reminiscent of Elastic Averaging SGD (EASGD) [1].\n  Indeed in both algorithms the model update at the workers consists in both descending the local gradient plus descending toward some \"moving-average\"obtained through averaging all the local models.\n  In EASGD, this \"moving-average\" is common to every worker and the master, which updates it every k steps.\n  In this paper, each worker has its own \"moving-average\", which update computations are different than in EASGD as the use the instant average of the workers' models instead of the previous \"moving-average\".\n\n[1]Sixin Zhang, Anna Choromanska, Yann LeCun, Deep learning with Elastic Averaging SGD,  NeurIPS, 2015\n\n- Questions I would like the authors to respond to during the rebuttal:\n\n\u00b7 Could Elastic Averaging SGD (in particular their fastest variant EAMSGD) be applied as-is to solve the non-identical, non-convex optimization problem at hand?\n  Despite the authors of EASGD not studying their algorithm in the non-identical case, following what is done in the intuition part of VRL-SGD (in particular Equation (8)), it seems that the update rule of the \"moving-average\" in EASGD is then equivalent to having a momentumSGD with dampening (instead of the \"generalized SGD form\" obtained with the approach of VRL-SGD).  Hence my question.\n\nI suggest acceptance. However I'm willing to change my opinion after reading other more qualified reviewers in the sub-area of variance-reduction techniques.\n\nnote: If EASGD was to be sound in the non-identical case as well, my decision would not change much."}, "signatures": ["ICLR.cc/2020/Conference/Paper182/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper182/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variance Reduced Local SGD with Lower Communication Complexity", "authors": ["Xianfeng Liang", "Shuheng Shen", "Jingchang Liu", "Zhen Pan", "Yifei Cheng", "Enhong Chen"], "authorids": ["zeroxf@mail.ustc.edu.cn", "vaip@mail.ustc.edu.cn", "jliude@cse.ust.hk", "pzhen@mail.ustc.edu.cn", "chengyif@mail.ustc.edu.cn", "cheneh@ustc.edu.cn"], "keywords": ["variance reduction", "local SGD", "distributed optimization"], "TL;DR": "We prove that the proposed algorithm can achieve linear iteration speedup with lower communication complexity than Local SGD and the experimental results verify our theoretical clarification.", "abstract": "To accelerate the training of machine learning models, distributed stochastic gradient descent (SGD) and its variants have been widely adopted, which apply multiple workers in parallel to speed up training. Among them, Local SGD has gained much attention due to its lower communication cost. Nevertheless, when the data distribution on workers is non-identical, Local SGD requires $O(T^{\\frac{3}{4}} N^{\\frac{3}{4}})$ communications to maintain its \\emph{linear iteration speedup} property, where $T$ is the total number of iterations and $N$ is the number of workers. In this paper, we propose Variance Reduced Local SGD (VRL-SGD) to further reduce the communication complexity. Benefiting from eliminating the dependency on the gradient variance among workers, we theoretically prove that VRL-SGD achieves a \\emph{linear iteration speedup} with a lower communication complexity $O(T^{\\frac{1}{2}} N^{\\frac{3}{2}})$ even if workers access non-identical datasets. We conduct experiments on three machine learning tasks, and the experimental results demonstrate that VRL-SGD performs impressively better than Local SGD when the data among workers are quite diverse.", "pdf": "/pdf/df64e6c4b505d38fc940fdb12afd76fd73ee2502.pdf", "code": "https://github.com/VRL-SGD/VRL-SGD", "paperhash": "liang|variance_reduced_local_sgd_with_lower_communication_complexity", "original_pdf": "/attachment/d7612234b1a301f02037b934ab2b0b732e2a9df6.pdf", "_bibtex": "@misc{\nliang2020variance,\ntitle={Variance Reduced Local {\\{}SGD{\\}} with Lower Communication Complexity},\nauthor={Xianfeng Liang and Shuheng Shen and Jingchang Liu and Zhen Pan and Yifei Cheng and Enhong Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lXnhVKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lXnhVKPr", "replyto": "S1lXnhVKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper182/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574946297312, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper182/Reviewers"], "noninvitees": [], "tcdate": 1570237755845, "tmdate": 1574946297324, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper182/-/Official_Review"}}}], "count": 8}