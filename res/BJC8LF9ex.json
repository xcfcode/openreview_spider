{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396618980, "tcdate": 1486396618980, "number": 1, "id": "rkXRhfI_e", "invitation": "ICLR.cc/2017/conference/-/paper496/acceptance", "forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396619518, "id": "ICLR.cc/2017/conference/-/paper496/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396619518}}}, {"tddate": null, "tmdate": 1485122132064, "tcdate": 1485122132064, "number": 2, "id": "Byn8cjMPg", "invitation": "ICLR.cc/2017/conference/-/paper496/official/comment", "forum": "BJC8LF9ex", "replyto": "B18v0flvx", "signatures": ["ICLR.cc/2017/conference/paper496/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper496/AnonReviewer5"], "content": {"title": "Last comments", "comment": "Of course, gamma... please excuse my confusion with those upside down lambdas!\nNow I understand that you didn't constrain your parameters matrix W to be square, which I assumed would be the case with the comments about not constraining the matrix to be diagonal. Thank you for the precision.\n\nIn the revised paper, you switched z_t and r_t in section 2.1 but not in section 2.2.\n\nI look forward to reading the future draft with the inclusion of the survey of the imputation literature!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552593, "id": "ICLR.cc/2017/conference/-/paper496/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper496/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper496/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552593}}}, {"tddate": null, "tmdate": 1484955242942, "tcdate": 1484955229799, "number": 8, "id": "B18v0flvx", "invitation": "ICLR.cc/2017/conference/-/paper496/public/comment", "forum": "BJC8LF9ex", "replyto": "Bk1ESnC8x", "signatures": ["~Zhengping_Che1"], "readers": ["everyone"], "writers": ["~Zhengping_Che1"], "content": {"title": "Re: A question + comments and rectifications", "comment": "\nThanks for your comments and follow-up questions. Please find our reply below.\n\n1) Question about Eq. 4-6.\nA: Firstly we just want to make it clear that the `lambda` mentioned in your question refers to the decay rate `gamma` in these equations. \nAs shown in Eq. 4, in GRU-D, gamma_x = exp(W_x \\dot delta + b_x) and gamma_h = exp(W_h \\dot delta + b_h), where `delta` is the time interval and W_x/h, b_x/h are model parameters.\nSay the numbers of input features and hidden states are |x_t| = D and |h_t| = H, respectively.\nW_x is a diagonal matrix with size [D,D], and W_h is a matrix with size [H,D].\nBy doing this, GRU-D decay the input feature based on its own time interval and decay the hidden states based on the time intervals of all input features.\nThe gamma_h (which is calculated based on delta_t) is different from the reset gate (which is calculated based on h_{t-1} and x_t) in a standard GRU.\n\n2) The homeostatic properties of the human body and Figure 4a).\nA: For some of the input features, simply forward imputation on the missing values can lead to reasonable prediction performance. However, forward imputation is not optimal for all features, otherwise the input decay in Figure 4a) would stay at 1 for all features at all time.\nFollowing the homeostatic properties, we do find a few variables have different decay rates which contribute to the model performance a lot, and many of them are validated by domain experts. Also, the plots in Figure 4a) show the input decay for time interval of <= 24 hours, and the decay values will be lower for longer time interval.\n\n3) `I still maintain that in your specific tests, not-at-random-missingness used for prediction isn't useful for an health care expert. However, GRU-D does use the presence of features as an additional clue for classification.`\nA: We agree that not-at-random-missingness may have not been properly used for health care experts. This may due to several reasons, e.g., limited time constraints to analyze the (missing) data, complex and even unknown reasons and sources of missing values, etc.\nHowever, not properly utilizing the missingness does not mean it is useless. Both the correlation validation like Fig. 1 and the performance improvements gained by GRU-D demonstrate that missingness can also be an informative resource rather than only cumbrance.\n\n4) `a review of better imputation methods would have been needed.`\n`you still ignore a whole field of study designed to solve the missing value problem.`\nA: Thanks for your suggestion on including a broader reviews of imputation methods. We'd like to include imputation literature survey on imputation methods in the upcoming draft.\nWe apologize for the confusion and we are not ignoring the research progress from the imputation field, but we are proposing a novel way to use missingness informativeness directly in the deep learning models which can help in training end-to-end ML/AI systems. As far as we know, imputation for missing-not-at-random is an open ongoing research problem.\n\n5) by combining two imputation methods in your model, which clearly suggests that imputation is a very important factor in learning a good model that handles missing values. \nA: Our model not only combines forward and mean imputations but actually utilizes time intervals and masking. The interval information is never considered in forward/mean imputation methods.\n\n6) `\"by incorporating masking [...] inside the GRU architecture\". With m_t being concatenated to x_t, this claim doesn't stand.`\nA: m_t is not only concatenated to the input and fed to get hidden state H, but also acts as the switch to control the flow of current input x_{t} and decayed input x_{t-1}. We updated figure 3b to make it clearer.\n\n7) Thanks for your detailed comments for the revised paper. We have made corresponding changes in a new version.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552796, "id": "ICLR.cc/2017/conference/-/paper496/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552796}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484955118604, "tcdate": 1478297175178, "number": 496, "id": "BJC8LF9ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJC8LF9ex", "signatures": ["~Zhengping_Che1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484862817709, "tcdate": 1482011218266, "number": 3, "id": "rJcLzVQNx", "invitation": "ICLR.cc/2017/conference/-/paper496/official/review", "forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "signatures": ["ICLR.cc/2017/conference/paper496/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper496/AnonReviewer5"], "content": {"title": "Interesting model, overreaching conclusions", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a modified gated RNN caled GRU-D that deals with time series which display a lot of missing values in their input. They work on two fronts. The first deals with the missing inputs directly by using a learned convex combination of the previous available value (forward imputation) and the mean value (mean imputation). The second includes dampening the recurrent layer not unlike a second reset gate, but parametrized according to the time elapsed since the last available value of each attributes.\n\nPositives\n------------\n- Clear definition of the task (handling missing values for classification of time series)\n- Many interesting baselines to test the new model against.\n- The model presented deals with the missing values in a novel, ML-type way (learn new dampening parameters).\n- The extensive tests done on the datasets is probably the greatest asset of this paper.\n\nNegatives\n-------------\n- The paper could use some double checking for typos.\n- The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space.\n- No mention of any methods from the statistics litterature.\n\nHere are the two main points of this review that informs my decision:\n\n1. The results, while promising, are below expectations. The paper hasn\u2019t been able to convince me that GRU-simple (without intervals) isn\u2019t just as well-suited for the task of handling missing inputs as GRU-D. In the main paper, GRU-simple is presented as the main baseline. Yet, it includes a lot of extraneous parameters (the intervals) that, according to Table 5, probably hurts the model more than it helps it. Having a third of it\u2019s parameters being of dubious value, it brings the question of the fairness of the comparison done in the main paper, especially since in the one table where GRU-simple (without intervals) is present, GRU-D doesn\u2019t significantly outperforms it.\n\n2. My second concern, and biggest, is with some claims that are peppered through the paper. The first is about the relationship with the presence rate of data in the dataset and the diagnostics. I might be wrong, but that only indicates that the doctor in charge of that patient requested the relevant analyses be done according to the patient\u2019s condition. That would mean that an expert system based on this data would always seem to be one step behind. \nThe second claim is the last sentence of the introduction, which sets huge expectations that were not met by the paper. Another is that \u201csimply concatenating masking and time interval vectors fails to exploit the temporal structure of missing values\u201d is unsubstantiated and actually disproven later in the paper. \nYet another is the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones. \nLastly is their claim to capture informative missingness by incorporating masking and time intervals directly inside the GRU architecture. While the authors did make these changes, the fact that they also concatenate the mask to the input, just like GRU-simple (without intervals), leads me to question the actual improvement made by GRU-D. \n\nGiven that, while I find that the work that has been put into the paper is above average, I wouldn\u2019t accept that paper without a reframing of the findings and a better focus on the real contribution of this paper, which I believe is the novel way to parametrize the choice of imputation method.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512565554, "id": "ICLR.cc/2017/conference/-/paper496/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper496/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper496/AnonReviewer2", "ICLR.cc/2017/conference/paper496/AnonReviewer4", "ICLR.cc/2017/conference/paper496/AnonReviewer5"], "reply": {"forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper496/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper496/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512565554}}}, {"tddate": null, "tmdate": 1484862759424, "tcdate": 1484862759424, "number": 1, "id": "Bk1ESnC8x", "invitation": "ICLR.cc/2017/conference/-/paper496/official/comment", "forum": "BJC8LF9ex", "replyto": "HyZizbPIe", "signatures": ["ICLR.cc/2017/conference/paper496/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper496/AnonReviewer5"], "content": {"title": "A question + comments and rectifications", "comment": "# Major interrogation following a perusing of the revised paper:\n- eq 4 and 6 : as the cardinality D = |delta_t| = |x_t| = |lambda| (eq 4), I understand how lambda_x is used in 5. However, other than constraining |h_t| = |x_t| = D, the definition of lambda_h makes little sense. If you use h_t instead of delta_t, that makes this a reset gate, which is already in the GRU.\n\n# Answer to comments\nI do not dispute the claim that GRU-D outperforms other algorithms in the majority of the tests made throughout the article. However, I still think that the main contribution of this paper is to learn the parameters of a convex combination of two imputation method. Since these parameters are indeed useful to extract some meaning (from figure 4), I am willing to retract part of my comment.\n\nFrom your answer : \"GRU-D model for healthcare applications is an example of how domain knowledge can be simply incorporated into the model architecture design to better handle and utilize missing values.\"\nI fail to see where you incorporated specific domain knowledge in the model. Usually, domain knowledge would be used to choose an imputation method, but since you elegantly side-step that problem by learning which method to choose, I can't find where else you use domain knowledge. If you are referring to the homeostatic properties of the human body, I would point out that Figure 4a establishes that forward imputation is always the prefered method (lambda never below 0.5).\n\nI still maintain that in your specific tests, not-at-random-missingness used for prediction isn't useful for an health care expert. However, GRU-D does use the presence of features as an additional clue for classification.\n\nAfter another look at Figure 7, I think I might have based my comment on GRU-simple, which is my mistake.\n\nFinally, I still believe that for the paper to have a greater impact, a review of better imputation methods would have been needed. Not only could GRU-D be based on more advanced methods (instead of just forward and mean imputation), but it would have made your contribution more credible. \nFrom your answer : \"it is a time series classification task with utilization of missing patterns but not a data imputation task\". Yet, you beat the model from Lipton et al. (~ GRU-simple masking only) by combining two imputation methods in your model, which clearly suggests that imputation is a very important factor in learning a good model that handles missing values. While I applaud the proposition of an integrated method that does not require a two-step approach, you still ignore a whole field of study designed to solve the missing value problem.\n\n\n# Random minor comments from reading the revised paper, in order of appearance:\n- 1.0, last paragraph : time interval -> time intervals (and in subsequent paragraphs)\n- 2.0, last line of page 2: t=0 -> t=1\n- 2.0, last paragraph : remove l_n from \\mathcal{D} as it would put your target in the training set\n- 2.1 : j, which I believe is the j-th neuron in the hidden layer, is never defined.\n- 2.1 : switching the definitions of z_t and r_t would make the formulas read more naturally\n- eq 1, 2, ... : I understand that the algorithm uses m_t^d a lot, but the paper would read more fluidly by using cases definitions.\n- 2.2 : in health care domain -> either \"in the health care domain\" or \"in health care domains\"\n- 3.3, last paragraph : both the datasets -> both datasets\n- 3.4, first paragraph : perform superior -> perform better\n- 3.4, last paragraph : add reference to fig. 7\n- 4.0 : \"by incorporating masking [...] inside the GRU architecture\". With m_t being concatenated to x_t, this claim doesn't stand.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552593, "id": "ICLR.cc/2017/conference/-/paper496/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper496/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper496/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552593}}}, {"tddate": null, "tmdate": 1484358349119, "tcdate": 1484358349119, "number": 7, "id": "H1SAfbv8l", "invitation": "ICLR.cc/2017/conference/-/paper496/public/comment", "forum": "BJC8LF9ex", "replyto": "SywGAzlEg", "signatures": ["~Zhengping_Che1"], "readers": ["everyone"], "writers": ["~Zhengping_Che1"], "content": {"title": "To AnonReviewer2", "comment": "Thanks for your comments and your pre-review suggestions for us to improve this work.\nWe'd like to clarify that our model is widely applicable and competitive as well on big datasets.\nAs shown in the experiments (Section 3.3 and Appendix A.3.5 in the updated draft), the proposed GRU-D model achieves consistent improvement in both one-layer and multi-layer RNN settings on all the datasets of varied size.\nAs far as we know, the MIMIC-III dataset we used is the largest publicly available one of its kind of tens of thousands of admission records. We agreed that the datasets in healthcare is not as many as in other mature domains such as computer vision or natural language processing, where massive public datasets are available and deep learning models are demonstrated to be powerful. Data collection and sharing in healthcare has it own myriad of challenges and is beyond the scope of this paper. \nOur methodologies work irrespective of the dataset size. Also, it is quite important to make deep learning more applicable in domains with moderate amount of available data such as healthcare. Motivated by this, our work is a reasonable approach to improve the RNN performance in such condition by resorting to the missing patterns in the data."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552796, "id": "ICLR.cc/2017/conference/-/paper496/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552796}}}, {"tddate": null, "tmdate": 1484358332765, "tcdate": 1484358332765, "number": 6, "id": "rJHpf-w8l", "invitation": "ICLR.cc/2017/conference/-/paper496/public/comment", "forum": "BJC8LF9ex", "replyto": "ry1MlUM4g", "signatures": ["~Zhengping_Che1"], "readers": ["everyone"], "writers": ["~Zhengping_Che1"], "content": {"title": "To AnonReviewer4", "comment": "Thanks for your time and thoughtful comments! We would like to address your the questions and concerns below.\n\n\nQ1: the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. \nA: We would like to clarify our contributions: we develop a reasonable and novel RNN framework for time series classification, which 1) capture missing patterns, 2) automatically imputes or accounts for missing values, and 3) consistently improves classification results. To the best of our knowledge, our work is one of the first of this kind. None of the previous works explicitly modified the RNN structure to capture and utilize missingness information for applications such as healthcare. Experiments on both synthetic and real datasets have demonstrated our proposed GRU-D model is the best among existing state-of-the-art models.\n\n\nQ2: the datasets used in this paper are small. \nA: (This question is also asked by AnonReviewer2 and we thus copied the same answer here.)\nAs far as we know, the MIMIC-III dataset we used is the largest publicly available one of its kind of tens of thousands of admission records. We agreed that the datasets in healthcare is not as many as in other mature domains such as computer vision or natural language processing, where massive public datasets are available and deep learning models are demonstrated to be powerful. Data collection and sharing in healthcare has it own myriad of challenges and is beyond the scope of this paper. \nOur methodologies work irrespective of the dataset size. Also, it is quite important to make deep learning more applicable in domains with moderate amount of available data such as healthcare. Motivated by this, we believe our work is a reasonable approach to improve the RNN performance in such condition by resorting to the missing patterns in the data.\n\n\nQ3: the decaying effect might not be able to generalize to other domains. \nA: The proposed model works in a broad set of domains where we encounter time series data missing at random or not at random. In this paper we evaluated our model for healthcare applications. Given that we're taking ICU data as example, our decay idea is very well matched to homeostatic systems (e.g., human body) that actively control themselves to revert to mean. We demonstrated it is indeed helpful in practice.\nFurthermore, our framework is flexible enough to incorporate decay functions in a variety of forms which can handle different missing mechanisms."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552796, "id": "ICLR.cc/2017/conference/-/paper496/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552796}}}, {"tddate": null, "tmdate": 1484358296740, "tcdate": 1484358296740, "number": 5, "id": "HyZizbPIe", "invitation": "ICLR.cc/2017/conference/-/paper496/public/comment", "forum": "BJC8LF9ex", "replyto": "rJcLzVQNx", "signatures": ["~Zhengping_Che1"], "readers": ["everyone"], "writers": ["~Zhengping_Che1"], "content": {"title": "To AnonReviewer5", "comment": "Thanks for your time and thoughtful comments!\n\nBefore addressing detailed comments, we would like to clarify our contribution in this work: we develop a reasonable and novel RNN framework for time series classification, which 1) captures missing patterns, 2) automatically imputes or accounts for missing values, and 3) consistently obtains the state-of-the-art results on the challenging healthcare domain classification problems. To the best of our knowledge, our work is one of the first of this kind. None of the previous works explicitly modify the RNN structure to capture and utilize missingness information for applications such as healthcare. Experiments on synthetic and real datasets have demonstrated our proposed GRU-D model is the best among existing state-of-the-art models.\n\nWe agree that GRU-simple (with and without intervals) models are strong baselines, but the proposed GRU-D model is definitely the best among all GRU baselines for handling and utilizing missingness.\nGRU-simple models often but not always get marginal improvement against other GRU baselines (i.e., GRU-mean/forward). Moreover, the masking and/or time interval are fed into the model same as other input values without considering their roles as `missingness indicators`.\nEmpirically we found GRU-simple baselines (with and without intervals) have quite close performance and are not as good as GRU-D. Thus we take GRU-simple as the representative in the main paper and show its variants in the appendix. It\u2019s worth noting that while time interval doesn\u2019t help much in GRU-simple, it not only improves the performance of GRU-D but also provides useful interpretations for the prediction tasks (as shown in Fig. 4).\n\nWe believe the superiority of GRU-D model over GRU baselines for the following reasons:\n1. It achieves consistent performance improvement on the difficult prediction tasks: ~0.02 AUC score in PhysioNet mortality prediction and ~0.01 AUC score in MIMIC-III mortality prediction over the best baselines.\n2. GRU-D model for healthcare applications is an example of how domain knowledge can be simply incorporated into the model architecture design to better handle and utilize missing values.\n3. GRU-D provides useful insights for raw input features from decay plots/distributions. These results help us understand the underlying variable impact on prediction tasks and also help domain experts understand and apply our model and discover new domain knowledge.\n\n\nOther detailed comments:\n\n\nQ: the relationship with the presence rate of data in the dataset and the diagnostics.\nA: Figure 1 shows the observations on missingness and prediction tasks and, thus demonstrates the usefulness of missingness. Missingness can be due to several factors, such as medical events, saving costs, anomalies, inconvenience and so on. Healthcare providers' judgments and actions may also lead to some missing values in the data. However, missing data is widespread in our dataset and it's hard to figure out the reason for the missing values of each variable.\n\n\nQ: the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones.\nA: As shown in Figure 7, GRU-D has comparable performance on 2k samples and beats non-GRU models with increasing margin on 10k and 19k samples. Other GRU baselines also improve with more training samples. Given these results, we expect this trend will continue if we have even larger healthcare datasets. Especially, the performance gap between the proposed GRU-D and non-RNN baselines is expected to increase.\n\n\nQ: The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space.\nA: We have revised and reorganized the contents on related baseline models as you suggested to make it more clear.\n\n\nQ: No mention of any methods from the statistics literature.\nA: The goal of this work is to make predictions on whether a patient has a disease (or what is his/her mortality status) based on the time series observations with missing values from ICU. \nIn short, it is a time series classification task with utilization of missing patterns but not a data imputation task. Also, as discussed in the paper (Wells et al., 2013), missing not at random imputation techniques lead to suboptimal performance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552796, "id": "ICLR.cc/2017/conference/-/paper496/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552796}}}, {"tddate": null, "tmdate": 1481953286593, "tcdate": 1481953286593, "number": 2, "id": "ry1MlUM4g", "invitation": "ICLR.cc/2017/conference/-/paper496/official/review", "forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "signatures": ["ICLR.cc/2017/conference/paper496/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper496/AnonReviewer4"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposed a way to deal with supervised multivariate time series tasks involving missing values. The high level idea is still using the recurrent neural network (specifically, GRU in this paper) to do sequence supervised learning, e.g., classification, but modifications have been made to the input and hidden layers of RNNs to tackle the missing value problem. \n\npros: \n1) the insight of utilizing missing value is critical. the observation of decaying effect in the healthcare application is also interesting;\n2) the experiment seems to be solid; the baseline algorithms and analysis of results are also done properly. \n\ncons:\n1) the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. \n2) the datasets used in this paper are small. \n3) the decaying effect might not be able to generalize to other domains. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512565554, "id": "ICLR.cc/2017/conference/-/paper496/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper496/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper496/AnonReviewer2", "ICLR.cc/2017/conference/paper496/AnonReviewer4", "ICLR.cc/2017/conference/paper496/AnonReviewer5"], "reply": {"forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper496/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper496/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512565554}}}, {"tddate": null, "tmdate": 1481809422819, "tcdate": 1481809422814, "number": 1, "id": "SywGAzlEg", "invitation": "ICLR.cc/2017/conference/-/paper496/official/review", "forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "signatures": ["ICLR.cc/2017/conference/paper496/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper496/AnonReviewer2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.\nIMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512565554, "id": "ICLR.cc/2017/conference/-/paper496/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper496/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper496/AnonReviewer2", "ICLR.cc/2017/conference/paper496/AnonReviewer4", "ICLR.cc/2017/conference/paper496/AnonReviewer5"], "reply": {"forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper496/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper496/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512565554}}}, {"tddate": null, "tmdate": 1481129529548, "tcdate": 1481129429873, "number": 4, "id": "r1CC63BQl", "invitation": "ICLR.cc/2017/conference/-/paper496/public/comment", "forum": "BJC8LF9ex", "replyto": "ryuwQxXmg", "signatures": ["~Zhengping_Che1"], "readers": ["everyone"], "writers": ["~Zhengping_Che1"], "content": {"title": "Re: Comparison to multi-layer RNNs", "comment": "Thanks for your suggestion on experiments for multi-layer RNNs!\n\nAs you mentioned, The methods of handling missing values in the baseline GRU-simple and the proposed GRU-D can be generalized to multi-layer RNNs.\nWe tested the following 2-layer RNN models with similar number of parameters and with more parameters to validate the generalization and efficacy of our model.\nGRU-simple-2: A 2-layer RNN model, where the first RNN layer is GRU-simple, and the second layer is a standard GRU layer.\nGRU-D-2: A 2-layer RNN model, where the first RNN layer is GRU-D, and the second layer is a standard GRU layer.\n\nOur GRU-D model consistently outperforms other baselines. Also a deeper or larger model won't always help, given that the available dataset size is limited.\nHere's a comparison on physionet mortality prediction task:\n\t\t\t\tLayer size\t\t# of params\tAUC score\nGRU-simple\t\t\t43\t\t\t18495\t\t0.8156\nGRU-simple-2\t\t\t32-32\t\t18947\t\t0.8159\nGRU-simple-2(large)\t43-64\t\t39250\t\t0.8208\nGRU-D\t\t\t\t49\t\t\t18838\t\t0.8424\nGRU-D-2\t\t\t\t34-34\t\t18599\t\t0.8420\nGRU-D-2(large)\t\t49-64\t\t40739\t\t0.8363\n\nWe observed similar result improvements on MIMIC-III dataset, where GRU-D outperforms GRU-simple by AUC score >=0.01 with 2 RNN layers.\n\nWe will include the detailed evaluation results on multi-layer RNN models in the appendix of the updated paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552796, "id": "ICLR.cc/2017/conference/-/paper496/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552796}}}, {"tddate": null, "tmdate": 1480946527973, "tcdate": 1480946527968, "number": 1, "id": "ryuwQxXmg", "invitation": "ICLR.cc/2017/conference/-/paper496/pre-review/question", "forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "signatures": ["ICLR.cc/2017/conference/paper496/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper496/AnonReviewer2"], "content": {"title": "Comparison to multi-layer RNNs", "question": "Interesting work!\n\nAnother simple baseline for dealing with missing values would be to add a standard NN layer (instead of a handcrafted one) for preprocessing the input consisting of the data and the mask, \u00a0feeding into the GRU layer. It is conceivable that if the preprocessing layer happens to be some sort of gated, recurrent layer (GRU/LSTM), that it would learn to impute the missing values and compute important statistics of the mask. Therefore, it would important to address the following question:\n \u00a0\nHow does the performance of the GRU-simple baseline change if it is given more hidden layers (with same number of parameters and with increasing number of parameters)? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959248876, "id": "ICLR.cc/2017/conference/-/paper496/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper496/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper496/AnonReviewer2"], "reply": {"forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper496/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper496/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959248876}}}, {"tddate": null, "tmdate": 1478904305731, "tcdate": 1478904305726, "number": 3, "id": "H1qx5p7-l", "invitation": "ICLR.cc/2017/conference/-/paper496/public/comment", "forum": "BJC8LF9ex", "replyto": "SyvsB0MZe", "signatures": ["~Alexey_Romanov1"], "readers": ["everyone"], "writers": ["~Alexey_Romanov1"], "content": {"title": "Re: A couple of questions", "comment": "Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552796, "id": "ICLR.cc/2017/conference/-/paper496/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552796}}}, {"tddate": null, "tmdate": 1478842864548, "tcdate": 1478841758715, "number": 2, "id": "SyvsB0MZe", "invitation": "ICLR.cc/2017/conference/-/paper496/public/comment", "forum": "BJC8LF9ex", "replyto": "SkF549MZg", "signatures": ["~Zhengping_Che1"], "readers": ["everyone"], "writers": ["~Zhengping_Che1"], "content": {"title": "Re: A couple of questions", "comment": "\nThanks for your interests in our paper and your questions!\n\n1) The best model in Lipton et al. (2016) is similar to GRU-simple (masking only) as discussed in the appendix page 12. All the input variables to our model are z-normalized to be 0 mean, so zero-filling and mean-imputation are the same.\nThe AUC score on MIMIC-III mortality prediction is 0.8367 for GRU-simple with masking only and 0.8527 for our GRU-D, and on PhysioNet2012 is 0.8226 for GRU-simple with masking only and 0.8424 for GRU-D.\n\n2) In our experiments, a model with higher AUC usually performs better in terms of other metrics. \nThe average scores of min(Se, +P) in the test folds for GRU-mean, GRU-forward, GRU-sample, and GRU-D are 0.4413, 0.4319, 0.4383, 0.4469.\n\n3) We did not explicitly handle class imbalance in our model. In evaluation, we take stratified k-fold cross validation to make sure each fold has similar percentage of samples for each class."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552796, "id": "ICLR.cc/2017/conference/-/paper496/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552796}}}, {"tddate": null, "tmdate": 1478825104607, "tcdate": 1478825104600, "number": 1, "id": "SkF549MZg", "invitation": "ICLR.cc/2017/conference/-/paper496/public/comment", "forum": "BJC8LF9ex", "replyto": "BJC8LF9ex", "signatures": ["~Alexey_Romanov1"], "readers": ["everyone"], "writers": ["~Alexey_Romanov1"], "content": {"title": "A couple of questions", "comment": "Thank you for a very interesting work! \n\n\nI have a couple of questions:\n\n1) Lipton at al. (2016) achieve the best result with zero filling and indicators. As I understood from the equation 3 and the following description, you did not experiment with zero filling and used either forward or mean imputation. Is it correct?\n\n2) It will be interesting to see not only AUC but also Sensitivity and Positive Predictivity, as well as min(Se, +P) since that was the official scoring metric in the PhysioNet Challenge 2012 (although we cannot compare these score directly, obviously)\n\n3) How did you combat such high class imbalance in case of mortality prediction?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "pdf": "/pdf/74b3010f71f7266247dc56506f38ad2d235eb359.pdf", "paperhash": "che|recurrent_neural_networks_for_multivariate_time_series_with_missing_values", "keywords": ["Deep learning"], "conflicts": ["usc.edu", "nyu.edu", "cs.nyu.edu"], "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287552796, "id": "ICLR.cc/2017/conference/-/paper496/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJC8LF9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper496/reviewers", "ICLR.cc/2017/conference/paper496/areachairs"], "cdate": 1485287552796}}}], "count": 16}