{"notes": [{"id": "HkeuD34KPH", "original": "ryx2wVmJrH", "number": 9, "cdate": 1569438815761, "ddate": null, "tcdate": 1569438815761, "tmdate": 1577168228964, "tddate": null, "forum": "HkeuD34KPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "SSE-PT: Sequential Recommendation Via Personalized Transformer", "authors": ["Liwei Wu", "Shuqing Li", "Cho-Jui Hsieh", "James Sharpnack"], "authorids": ["liwu@ucdavis.edu", "qshli@ucdavis.edu", "chohsieh@cs.ucla.edu", "jsharpna@ucdavis.deu"], "keywords": ["sequential recommendation", "personalized transformer", "stochastic shared embeddings"], "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "pdf": "/pdf/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "code": "https://github.com/SSE-PT/SSE-PT", "paperhash": "wu|ssept_sequential_recommendation_via_personalized_transformer", "original_pdf": "/attachment/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "_bibtex": "@misc{\nwu2020ssept,\ntitle={{\\{}SSE{\\}}-{\\{}PT{\\}}: Sequential Recommendation Via Personalized Transformer},\nauthor={Liwei Wu and Shuqing Li and Cho-Jui Hsieh and James Sharpnack},\nyear={2020},\nurl={https://openreview.net/forum?id=HkeuD34KPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "NO7fh8U9_Y", "original": null, "number": 1, "cdate": 1576798684783, "ddate": null, "tcdate": 1576798684783, "tmdate": 1576800950066, "tddate": null, "forum": "HkeuD34KPH", "replyto": "HkeuD34KPH", "invitation": "ICLR.cc/2020/Conference/Paper9/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes to improve sequential recommendation by extending SASRec (from prior work) by adding user embedding with SSE regularization.  The authors show that the proposed method outperforms several baselines on five datasets.\n\nThe paper received two weak accepts and one reject.  Reviewers expressed concerns about the limited/scattered technical contribution.  Reviewers were also concerned about the quality of the experiment results and need to compare against more baselines.  After examining some related work, the AC agrees with the reviewers that there is also many recent relevant work such as BERT4Rec that should be cited and discussed.  It would make the paper stronger if the authors can demonstrate that adding the user embedding to another method such as BERT4Rec can improve the performance of that model.  Regarding R3's concerns about the comparison against HGN, the authors indicates there are differences in the length of sequences considered and that some method may work better for shorter sequences while their method works better for longer sequences.  These details seems important to include in the paper. \n\nIn the AC's opinion, the paper quality is borderline and the work is of limited interest to the ICLR community.  Such would would be more appreciated in the recommender systems community.  The authors are encouraged to improve the paper with improved discussion of more recent work such as BERT4Rec, add comparisons against these more recent work, incorporate various suggestions from the reviewers, and resubmit to an appropriate venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSE-PT: Sequential Recommendation Via Personalized Transformer", "authors": ["Liwei Wu", "Shuqing Li", "Cho-Jui Hsieh", "James Sharpnack"], "authorids": ["liwu@ucdavis.edu", "qshli@ucdavis.edu", "chohsieh@cs.ucla.edu", "jsharpna@ucdavis.deu"], "keywords": ["sequential recommendation", "personalized transformer", "stochastic shared embeddings"], "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "pdf": "/pdf/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "code": "https://github.com/SSE-PT/SSE-PT", "paperhash": "wu|ssept_sequential_recommendation_via_personalized_transformer", "original_pdf": "/attachment/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "_bibtex": "@misc{\nwu2020ssept,\ntitle={{\\{}SSE{\\}}-{\\{}PT{\\}}: Sequential Recommendation Via Personalized Transformer},\nauthor={Liwei Wu and Shuqing Li and Cho-Jui Hsieh and James Sharpnack},\nyear={2020},\nurl={https://openreview.net/forum?id=HkeuD34KPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkeuD34KPH", "replyto": "HkeuD34KPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730738, "tmdate": 1576800283596, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper9/-/Decision"}}}, {"id": "SJenAyg6tH", "original": null, "number": 2, "cdate": 1571778516373, "ddate": null, "tcdate": 1571778516373, "tmdate": 1574613837120, "tddate": null, "forum": "HkeuD34KPH", "replyto": "HkeuD34KPH", "invitation": "ICLR.cc/2020/Conference/Paper9/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The manuscript proposes SSE-PT, a sequential recommendation model based on transformer and stochastic shared embedding (SST). Experiments on several datasets show that SSE-PT outperforms a number of baseline methods. Some analytical results are also provided. Overall, I think this work is not suitable for ICLR due to following reasons. \n\nThe novelty of this work is limited. This work is based on SASREC [W Kang, ICDM2018] and uses transformer to encode user-item interactions in sequential manner. The difference is that this work adds user embedding in bottom layer and utilizes SSE for regularization as well as designs SSE-PT++ by sampling. To me, there is little extension or novelty. \n\nThe experiment results are not convincing. Most of results are copied from [W Kang, ICDM2018] except HGN in Table 1. Table 1 shows SASREC is much better than HGN [C Ma, KDD2019]. However, I checked the results in HGN paper and found HGN is much better than SASREC. Even though datasets are different, most of them are from Amazon data. I was not convinced by this result due to the large difference. In addition, I did not understand why the authors change evaluation metrics in Table 3, i.e., from NDCG/Recall@10 to NDCG/Recall@5. I found SSE-PT without regularization and with different regularizations are much worse than the best result, which makes me concern about the effectiveness of personalized transformer. I did not see ablation study or discussion about this. \n\nUpdate: I have considered author rebuttal. I appreciate the extensive hyper-parameter sensitivity and ablation study in the paper, while these cannot be a key factor in evaluating paper as most of them can be done easily. I main concerns still lie in the novelty and experimental results. I still think this work is not suitable for ICLR and I keep my score. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper9/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper9/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSE-PT: Sequential Recommendation Via Personalized Transformer", "authors": ["Liwei Wu", "Shuqing Li", "Cho-Jui Hsieh", "James Sharpnack"], "authorids": ["liwu@ucdavis.edu", "qshli@ucdavis.edu", "chohsieh@cs.ucla.edu", "jsharpna@ucdavis.deu"], "keywords": ["sequential recommendation", "personalized transformer", "stochastic shared embeddings"], "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "pdf": "/pdf/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "code": "https://github.com/SSE-PT/SSE-PT", "paperhash": "wu|ssept_sequential_recommendation_via_personalized_transformer", "original_pdf": "/attachment/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "_bibtex": "@misc{\nwu2020ssept,\ntitle={{\\{}SSE{\\}}-{\\{}PT{\\}}: Sequential Recommendation Via Personalized Transformer},\nauthor={Liwei Wu and Shuqing Li and Cho-Jui Hsieh and James Sharpnack},\nyear={2020},\nurl={https://openreview.net/forum?id=HkeuD34KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkeuD34KPH", "replyto": "HkeuD34KPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576114439755, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper9/Reviewers"], "noninvitees": [], "tcdate": 1570237758464, "tmdate": 1576114439767, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper9/-/Official_Review"}}}, {"id": "S1ej5Lf7jB", "original": null, "number": 3, "cdate": 1573230226906, "ddate": null, "tcdate": 1573230226906, "tmdate": 1573230226906, "tddate": null, "forum": "HkeuD34KPH", "replyto": "B1xkMwxJYB", "invitation": "ICLR.cc/2020/Conference/Paper9/-/Official_Comment", "content": {"title": "Personalization of NLP models (such as Transformer and BERT) is an important research direction", "comment": "Hi Reviewer, thank you very much for your constructive and just-to-the-point feedback. \n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and is proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that).\n\nWe will definitely correct the typos in our final version of the paper and will include more baselines such as Fossil, MARank, and/or BERT4Rec into our final version of the paper. We think our work is orthogonal to important works like BERT4Rec, because BERT4Rec is essentially another transformer-based approach, which may also benefit from our proposed personalization scheme. We will try to see if a similar technique to ours for SASRec also works for un-personalized models such as BERT4Rec. Even BERT4Rec authors also stated in future work section: \"Another interesting direction for the future work would be introducing user component into the model for explicit user modeling when the users have multiple sessions.\" We think our work is first of this kind exploring this direction.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper9/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSE-PT: Sequential Recommendation Via Personalized Transformer", "authors": ["Liwei Wu", "Shuqing Li", "Cho-Jui Hsieh", "James Sharpnack"], "authorids": ["liwu@ucdavis.edu", "qshli@ucdavis.edu", "chohsieh@cs.ucla.edu", "jsharpna@ucdavis.deu"], "keywords": ["sequential recommendation", "personalized transformer", "stochastic shared embeddings"], "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "pdf": "/pdf/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "code": "https://github.com/SSE-PT/SSE-PT", "paperhash": "wu|ssept_sequential_recommendation_via_personalized_transformer", "original_pdf": "/attachment/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "_bibtex": "@misc{\nwu2020ssept,\ntitle={{\\{}SSE{\\}}-{\\{}PT{\\}}: Sequential Recommendation Via Personalized Transformer},\nauthor={Liwei Wu and Shuqing Li and Cho-Jui Hsieh and James Sharpnack},\nyear={2020},\nurl={https://openreview.net/forum?id=HkeuD34KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkeuD34KPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference/Paper9/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper9/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper9/Reviewers", "ICLR.cc/2020/Conference/Paper9/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper9/Authors|ICLR.cc/2020/Conference/Paper9/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177755, "tmdate": 1576860555662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference/Paper9/Reviewers", "ICLR.cc/2020/Conference/Paper9/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper9/-/Official_Comment"}}}, {"id": "rylW9BGXoS", "original": null, "number": 2, "cdate": 1573229960657, "ddate": null, "tcdate": 1573229960657, "tmdate": 1573229960657, "tddate": null, "forum": "HkeuD34KPH", "replyto": "Hyg-LKcJcS", "invitation": "ICLR.cc/2020/Conference/Paper9/-/Official_Comment", "content": {"title": "Exponential decay idea does work empirically better than Uniform one", "comment": "Hi Reviewer, Thank you very much for your insightful feedback and suggestions.\n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and is proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that).\n\nYes, we should definitely include CIKM'19 BERT4Rec in the final version of the paper, which we were not aware of. We think our work is orthogonal to important works like BERT4Rec, because BERT4Rec is essentially another transformer-based approach, which may also benefit from our proposed personalization scheme. We will try to see if a similar technique to ours for SASRec also works for un-personalized models such as BERT4Rec. Even BERT4Rec authors also stated in future work section: \"Another interesting direction for the future work would be introducing user component into the model for explicit user modeling when the users have multiple sessions.\" We think our work is first of this kind exploring this direction.\n\nAlso, your idea of sampling start index $v$ based on the recency (e.g., with exponential decay) sounds very intuitive and could be very promising. We did a quick experiment, we find using exponential decay gives slightly better results on movielen1m data when we use max length of 50. We find using combination of our idea and your idea (most of weight on the last start index but rest of times we sample start index based on recency with exponential decay) empirically performs better, giving NDCG@10 of 0.59945 versus 0.59509 and Recall@10 of 0.81109 versus 0.80414. I think our works point to a future direction that worth more explorations, both empirically and theoretically."}, "signatures": ["ICLR.cc/2020/Conference/Paper9/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSE-PT: Sequential Recommendation Via Personalized Transformer", "authors": ["Liwei Wu", "Shuqing Li", "Cho-Jui Hsieh", "James Sharpnack"], "authorids": ["liwu@ucdavis.edu", "qshli@ucdavis.edu", "chohsieh@cs.ucla.edu", "jsharpna@ucdavis.deu"], "keywords": ["sequential recommendation", "personalized transformer", "stochastic shared embeddings"], "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "pdf": "/pdf/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "code": "https://github.com/SSE-PT/SSE-PT", "paperhash": "wu|ssept_sequential_recommendation_via_personalized_transformer", "original_pdf": "/attachment/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "_bibtex": "@misc{\nwu2020ssept,\ntitle={{\\{}SSE{\\}}-{\\{}PT{\\}}: Sequential Recommendation Via Personalized Transformer},\nauthor={Liwei Wu and Shuqing Li and Cho-Jui Hsieh and James Sharpnack},\nyear={2020},\nurl={https://openreview.net/forum?id=HkeuD34KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkeuD34KPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference/Paper9/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper9/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper9/Reviewers", "ICLR.cc/2020/Conference/Paper9/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper9/Authors|ICLR.cc/2020/Conference/Paper9/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177755, "tmdate": 1576860555662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference/Paper9/Reviewers", "ICLR.cc/2020/Conference/Paper9/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper9/-/Official_Comment"}}}, {"id": "HJeSNEfXoH", "original": null, "number": 1, "cdate": 1573229613080, "ddate": null, "tcdate": 1573229613080, "tmdate": 1573229613080, "tddate": null, "forum": "HkeuD34KPH", "replyto": "SJenAyg6tH", "invitation": "ICLR.cc/2020/Conference/Paper9/-/Official_Comment", "content": {"title": "Clarifying Doubts on Experimental Section", "comment": "Hi Reviewer, thank you very much for raising your confusion to us on experiments. We will do a better job in clarifying on how we compared with HGN in experimental section. \n\nTo explain why HGN is not doing as well as SASRec in our reported results: First, it is worth noting that the HGN paper not only used completely different datasets, but also used very distinct evaluation procedures from SASRec. Instead of predicting next engaged item, it has 10% interactions in test set, such that one prediction is correct as long as it falls into the test set, while we (and SASRec) are doing another task of predicting precisely next item, in which there is only 1 correct answer. So the task they consider is an easier task than us. Moreover, If you read the HGN paper carefully, they are mainly focused on accommodating very short sequences. In paper's experiments, they use hyper-parameter $L = 5$, where $L$ is the length of sequence used for training and inference. On contrast, our method SSE-PT and SASRec uses $L = 200$ for Movielens1m and $L=50$ for other datasets. We think that mainly accounts for the difference in original paper's reported performances and our reported performances. It is very possible that for very short sequences, HGN works quite well, better than SASRec as they have shown in their paper. We will add this delicate detail to the final version of the paper to avoid any confusions for future readers. Moreover, we modified original HGN codes to make HGN's evaluation the same as that of SASRec and open sourced at: https://github.com/SSE-PT/SSE-PT/tree/master/HGN_baseline. You have a look at our codes for both our SSE-PT and HGN baseline.\n\nAs to your other comments. \n1. Yes, it is correct that first few rows (A to F and H, I) of results in Table 1 are from SASRec paper, the reason is that we use the exactly same experimental settings on exactly same datasets. So we decided to trust the results reported in SASRec paper for older methods. We include those earlier baselines for completeness but those are not as important as SASRec because SASRec has been shown to outperform those methods. We did re-run SASRec and got slightly better results in Table 1 than the ones originally reported in SASRec paper.\n\n2. Yes, Table 3 we use different metrics than Table 1, because we realize the NDCG@10, Recall@10 does not accurately reflect how bad over-fitting is as NDCG@5 and Recall@5. The percentages of improvement for using a well-suited regularization are much more dramatic once you switch the metrics to top 5 from top 10. This means good regularization are extremely important for top k ranking results, especially when $k$ is small. The results in Table 3 would still hold for top 10 but less dramatic for percentages of gains. On the other hand, because we want to make a fair comparison with SASRec on the same datasets, we chose to use same top-10 metrics in Table 1. This is our reasoning as to why metrics used in Table 1 and 3 are different.  \n\n3. As to the ablation study, the ablation study of personalization is done in Table 10 in Appendix and we have had a dedicated section 4.3 for different ablation studies done for each component of the model.\n\nWhile our paper does build on previous work, we think that the paper is an important contribution for 2 reasons:\n1. In the SASRec paper, they come to the conclusion that they \"empirically find that adding an explicit user embedding doesn\u2019t improve performance (presumably because the model already considers all of the user\u2019s actions).\"  Contrary to this, we find that personalization is actually possible for transformer-based models and \nis proving to be very useful for recommendation systems in terms of both performance and interpretation.\n\n2. We show that coming up with models that can incorporate long sequences should be an important research direction (our simple extension SSE-PT++ proved that)."}, "signatures": ["ICLR.cc/2020/Conference/Paper9/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSE-PT: Sequential Recommendation Via Personalized Transformer", "authors": ["Liwei Wu", "Shuqing Li", "Cho-Jui Hsieh", "James Sharpnack"], "authorids": ["liwu@ucdavis.edu", "qshli@ucdavis.edu", "chohsieh@cs.ucla.edu", "jsharpna@ucdavis.deu"], "keywords": ["sequential recommendation", "personalized transformer", "stochastic shared embeddings"], "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "pdf": "/pdf/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "code": "https://github.com/SSE-PT/SSE-PT", "paperhash": "wu|ssept_sequential_recommendation_via_personalized_transformer", "original_pdf": "/attachment/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "_bibtex": "@misc{\nwu2020ssept,\ntitle={{\\{}SSE{\\}}-{\\{}PT{\\}}: Sequential Recommendation Via Personalized Transformer},\nauthor={Liwei Wu and Shuqing Li and Cho-Jui Hsieh and James Sharpnack},\nyear={2020},\nurl={https://openreview.net/forum?id=HkeuD34KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkeuD34KPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference/Paper9/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper9/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper9/Reviewers", "ICLR.cc/2020/Conference/Paper9/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper9/Authors|ICLR.cc/2020/Conference/Paper9/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177755, "tmdate": 1576860555662, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper9/Authors", "ICLR.cc/2020/Conference/Paper9/Reviewers", "ICLR.cc/2020/Conference/Paper9/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper9/-/Official_Comment"}}}, {"id": "B1xkMwxJYB", "original": null, "number": 1, "cdate": 1570862855257, "ddate": null, "tcdate": 1570862855257, "tmdate": 1572972650082, "tddate": null, "forum": "HkeuD34KPH", "replyto": "HkeuD34KPH", "invitation": "ICLR.cc/2020/Conference/Paper9/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "In this paper, the authors study an important recommendation problem, i.e., sequential recommendation, and design a novel and improved model called SSE-PT (Stochastic Shared Embedding - Personalized Transformer). Specifically, the authors mainly follow the previous works of the Transformer model and the stochastic shared embedding (SSE) regularization technique. For the part of the personalized transfer (PT), the authors introduce the user embedding for each user $i$, i.e., $u_i$, shown in Eq.(2) and illustrated in Figure 1. For the part of regularization, the authors find that the SSE technique works well in terms of avoid overfiting in context of other regularization techniques.\n\nExtensive empirical studies on five datasets show the effectiveness of the proposed approach compared with other related methods.\n\nOverall, the paper is very well presented, in particular of the introduction and discussion about the related works, and the analysis of the experimental results. \n\nMy major concern is that the technical novelty is somehow limited in terms of the two closely related works of Transformer and stochastic shared embedding (SSE). I thus recommend weak acceptance.\n\nSome suggestion: Some important baseline methods may be included to make the results more convincing, e.g., Fossil, MARank, and/or BERT4Rec.\n\nSome minors:\nTypo: in the paragraph below Eq.(3): user $l$ -> user $i$\nTypo: FPMF, PFMC in different places\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper9/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper9/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSE-PT: Sequential Recommendation Via Personalized Transformer", "authors": ["Liwei Wu", "Shuqing Li", "Cho-Jui Hsieh", "James Sharpnack"], "authorids": ["liwu@ucdavis.edu", "qshli@ucdavis.edu", "chohsieh@cs.ucla.edu", "jsharpna@ucdavis.deu"], "keywords": ["sequential recommendation", "personalized transformer", "stochastic shared embeddings"], "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "pdf": "/pdf/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "code": "https://github.com/SSE-PT/SSE-PT", "paperhash": "wu|ssept_sequential_recommendation_via_personalized_transformer", "original_pdf": "/attachment/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "_bibtex": "@misc{\nwu2020ssept,\ntitle={{\\{}SSE{\\}}-{\\{}PT{\\}}: Sequential Recommendation Via Personalized Transformer},\nauthor={Liwei Wu and Shuqing Li and Cho-Jui Hsieh and James Sharpnack},\nyear={2020},\nurl={https://openreview.net/forum?id=HkeuD34KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkeuD34KPH", "replyto": "HkeuD34KPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576114439755, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper9/Reviewers"], "noninvitees": [], "tcdate": 1570237758464, "tmdate": 1576114439767, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper9/-/Official_Review"}}}, {"id": "Hyg-LKcJcS", "original": null, "number": 3, "cdate": 1571952969221, "ddate": null, "tcdate": 1571952969221, "tmdate": 1572972650047, "tddate": null, "forum": "HkeuD34KPH", "replyto": "HkeuD34KPH", "invitation": "ICLR.cc/2020/Conference/Paper9/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes SSE-PT for sequential recommendation, which is an extension of previous work SASRec by adding user embedding with SSE  regularization [Wu et al. 2019] . They further extend SSE-PT to SSE-PT++ to handle longer sequence. Experiments on five datasets show that the SSE-PT and SSE-PT++ outperform several baseline approaches.\n\nDetailed comments:\n\n1)\tThe technical contribution seems to be scattered: user embedding is introduced, effect of different types of regularization is studied and sampling based approach is added to address long sequence. It could be better if the author could make clear what the major contribution of this paper is. Also, SSE [Wu et al. 2019] is existing technique and simply applying it to sequential recommendation is a bit incremental.\n\n2)    In addition to SASRec, there are some other transformer based model (e.g., [1]) for sequential recommendation and the paper discuss how the proposed method differ from them.\n\n3)\tIn SSE-PT++, would sampling start index v based on the recency (e.g., with exponential decay) make more sense than uniform probability?\n\n4\uff09 Overall, experiments look comprehensive: The baseline methods include both non-deep-learning methods and recent deep learning based methods for sequential recommendation; ablation study is conducted; case study is performed on MovieLens to show how the attention weights differ from SASRec; running time is compared against baselines and sensitivity analysis on hyper-parameters are also provided. \n\n\nTo summarize, the paper is a bit incremental/scattered in terms of technical contribution but the execution of this paper looks solid. I would give a \u201cweak accept\u201d to this paper given the reasons listed above.\n\n\n[1] F. Sun et. al. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper9/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper9/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSE-PT: Sequential Recommendation Via Personalized Transformer", "authors": ["Liwei Wu", "Shuqing Li", "Cho-Jui Hsieh", "James Sharpnack"], "authorids": ["liwu@ucdavis.edu", "qshli@ucdavis.edu", "chohsieh@cs.ucla.edu", "jsharpna@ucdavis.deu"], "keywords": ["sequential recommendation", "personalized transformer", "stochastic shared embeddings"], "abstract": "Temporal information is crucial for recommendation problems because user preferences are naturally dynamic in the real world. Recent advances in deep learning, especially the discovery of various attention mechanisms and newer architectures in addition to widely used RNN and CNN in natural language processing, have allowed for better use of the temporal ordering of items that each user has engaged with. In particular, the SASRec model, inspired by the popular Transformer model in natural languages processing, has achieved state-of-the-art results. However, SASRec, just like the original Transformer model, is inherently an un-personalized model and does not include personalized user embeddings. To overcome this limitation, we propose a Personalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in terms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some random users' engagement history, we find our model not only more interpretable but also able to focus on recent engagement patterns for each user. Moreover, our SSE-PT model with a slight modification, which we call SSE-PT++, can handle extremely long sequences and outperform SASRec in ranking results with comparable training speed, striking a balance between performance and speed requirements. Our novel application of the Stochastic Shared Embeddings (SSE) regularization is essential to the success of personalization. Code and data are open-sourced at https://github.com/SSE-PT/SSE-PT.", "pdf": "/pdf/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "code": "https://github.com/SSE-PT/SSE-PT", "paperhash": "wu|ssept_sequential_recommendation_via_personalized_transformer", "original_pdf": "/attachment/9c1b2027800347a39632fba04cdd23bfd6d4d07d.pdf", "_bibtex": "@misc{\nwu2020ssept,\ntitle={{\\{}SSE{\\}}-{\\{}PT{\\}}: Sequential Recommendation Via Personalized Transformer},\nauthor={Liwei Wu and Shuqing Li and Cho-Jui Hsieh and James Sharpnack},\nyear={2020},\nurl={https://openreview.net/forum?id=HkeuD34KPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkeuD34KPH", "replyto": "HkeuD34KPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper9/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576114439755, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper9/Reviewers"], "noninvitees": [], "tcdate": 1570237758464, "tmdate": 1576114439767, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper9/-/Official_Review"}}}], "count": 8}