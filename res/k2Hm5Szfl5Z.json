{"notes": [{"id": "k2Hm5Szfl5Z", "original": "ORZA0gg0XMi", "number": 3725, "cdate": 1601308414570, "ddate": null, "tcdate": 1601308414570, "tmdate": 1614985777348, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6e2xsLRPt5-", "original": null, "number": 1, "cdate": 1610040353467, "ddate": null, "tcdate": 1610040353467, "tmdate": 1610473942808, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper studies the tensor principal component analysis problem, where we observe a tensor T = \\beta v^{\\otimes k} + Z where v is a spike and Z is a Gaussian noise tensor. The goal is to recover an accurate estimate to the spike for as small a signal-to-noise ratio \\beta as possible. There has been considerable interest in this problem, mainly coming from the statistics and theoretical computer science communities, and the best known algorithms succeed when \\beta \\geq n^{k/4} where n is the dimension of v. The main contribution of this paper is to leverage ideas from theoretical physics and build a matrix whose top eigenvector is correlated with v for sufficiently large \\beta using trace invariants. On synthetic data, the algorithms achieve better performance than existing methods.\n\nThe main negative of this paper is that it is not so clear how tensor PCA is relevant in machine learning applications. The authors gave some references to applications of tensor methods, but I want to point out that all of those works are about using tensor decompositions, which despite the fact that they are both about tensors, are rather different sorts of tools. Many of the reviewers also found the paper difficult to follow. I do think exposition is particularly challenging when making connections between different communities, as this work needs to introduce several notions from theoretical physics. I am also not sure how novel the methods are, since a somewhat recent paper Moitra and Wein, \"Spectral Methods from Tensor Networks\", STOC 2019 also uses tensor networks to build large matrices whose top eigenvalue is correlated with a planted signal, albeit for a different problem called orbit retrieval. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040353452, "tmdate": 1610473942790, "id": "ICLR.cc/2021/Conference/Paper3725/-/Decision"}}}, {"id": "x7qLBJhL7VD", "original": null, "number": 8, "cdate": 1606219448622, "ddate": null, "tcdate": 1606219448622, "tmdate": 1606219448622, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment", "content": {"title": "General comment", "comment": "We would thank all reviewers for the valuable comments and constructive feedback which help us to significantly improve the quality of the presentation of this work.\n\nWe have uploaded a revisited version, in order to take into account the reviewer's comments and to clarify the notations and experimental details. More specifically, our main changes in the revision include:\n\n* We paid a careful attention to clearly define, in an explicit way, all the introduced notations. We also add figures to illustrate the important ones. These definitions include:\n\n    * A formal definition of the contraction of indices (Section 2.1, end of page 3)\n\n    * A formal definition of tensor invariants (Section 2.1, end of page 3)\n\n    * A formal definition of the orthogonal group (Section 2.1, second paragraph) with a warning to not confuse it with the computational complexity (the context should make the distinction simple).\n\n    * A formal correspondence between the graphs and the trace invariants and how to obtain one from the other (section 2.1, beginning of the page 4).\n\n    * A formal definition of the matrix associated to a trace invariant and an edge (section 2.2). We also added an illustration to make the construction easier to comprehend (Figure 2). \n\n    * We defined the variance of the graph (Appendix C) and how to compute it.\n\n    * A formal definition of the intermediate graphs in the beginning of the appendix E.\n\n    * A clearer appendix on the perfect one-factorization (appendix C)\n\n* We added details (number of independent experiments, number of power iterations, the time and memory requirements of each method, etc.) to the numerical simulations in the main text and in the appendix D.\n\n* We added details about the complexity and the parallelization (appendix D section D.3)\n\n* We put a complete version of the proofs and added details and graphs to make them more easily readable.\n\n* We give more references concerning the practical applications of tensor PCA and tensor decomposition (multiple spikes recovery).\n\n* We added more extended descriptions in the last sections of the main text for a smoother reading.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Hm5Szfl5Z", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3725/Authors|ICLR.cc/2021/Conference/Paper3725/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834470, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment"}}}, {"id": "zlSqZaj0Irw", "original": null, "number": 7, "cdate": 1606148794895, "ddate": null, "tcdate": 1606148794895, "tmdate": 1606148794895, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "NvIwXUMyrB", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment", "content": {"title": "Thank you very much for your feedback [Part 3]", "comment": "\n* Let's take for example the algorithm associated to the melon diagram. It consists in i) calculating the n-dimensional matrix $M_{{i_1},{i_2}}=\\sum_{jk} T_{i_1 jk} T_{i_2 jk}$ and then ii) find its leading eigenvector. Since the matrix is n-dimensional, the time complexity of the second step will be negligible (for example just reading the tensor has a complexity of $n^3$ because that is its size.)\n    For the first step, which we would want to focus our optimization, we can calculate each element of $M$ independently of the others. And in the end we put them all together in the matrix. That is why we stated that it is suitable for parallel structure. We run some of our experiments on a cluster where we used without any issue parallelization to fasten our calculations.\n    We added a small comment about this in the appendix D about the speed of the experiments in our updated version.\n    \n* We attempted to add several details and figures for the proofs, since such combinatorial proofs can sometimes be hard to follow."}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Hm5Szfl5Z", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3725/Authors|ICLR.cc/2021/Conference/Paper3725/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834470, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment"}}}, {"id": "EyrylnDj4To", "original": null, "number": 6, "cdate": 1605680369054, "ddate": null, "tcdate": 1605680369054, "tmdate": 1606148768468, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "NvIwXUMyrB", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment", "content": {"title": "Thank you very much for your feedback [Part 2]", "comment": "* We added more graphs with the hope it will become clearer and put more explicitely the correspondance in Section 2.1 end of page 3/ beginning of page 4. We have stated in the updated version: \"A trace invariant of degree $d$ of a tensor $\\mathbf{T}$ of order $k$ admits a practical graphical representation as an edge colored graph $\\mathcal{G}$ obtained by following two steps: we first draw $d$ vertices representing the $d$ different copies of $\\mathbf{T}$. The indices of each copy is represented by $k$ half-edges with a different color for each index position as shown in Figure 1.a. Then, when two different indices are contracted in the tensor invariant, we connect their corresponding half-edges in $\\mathcal{G}$. Reciprocally, to obtain the tensor invariant associated to a graph $\\mathcal{G}$ with $d$ vertices, we take $d$ copies of $\\mathbf{T}$ (one for each vertex), we associate a color for each index position, and we contract the indices of the $d$ copies of $\\mathbf{T}$ following the coloring of the edges connecting the vertices. We denote this invariant $I_\\mathcal{G}(\\mathbf{T})$ .\"\n    \n    So if, for instance, we take the invariant $\\sum_{ijk} T_{ijk} T_{ijk}$. i) We have a product of two copies of the tensor $T$, so we draw two vertices. ii) Then since the tensor is of order 3, it has three indices. So, for each vertex we will draw three half edges with different colors (each color will represent an index position: 1st, 2nd or 3rd) to represent the three indices.\n    iii) For each contraction of a pair of indices in the invariant expression, we will connect the half edges corresponding to these indices. For example the invariant $\\sum_{ijk} T_{ijk} T_{ijk}$. contracts the first index of the first copy of $T$ with the first index of the second copy of $T$, the second with the second, and the third with the third. Thus we obtained the melon diagram.\n    \n    For the other way around, if we have a graph and we want to find the invariant associated. We search for the degree of the graph (the number of vertices). It will give us the number of copies of the tensor $T$. Then we count for each vertex how many half edges it has, this will give us the order of the tensor (How many indices it has). Then we assign a color to each index position. In the end, for each edge connecting two vertices, we contract the indices associated to the two half edges of this edge.\n    \n* We made more explicit the definition of the matrix $M_{\\mathcal{G},e}$ and we added the new figure 2 to try to provide a better explanation to how we obtain the matrix from a graph $\\mathcal{G}$ and an edge $e$, which is a crucial part of our work. \n    If we take the precedent example of the melon, that we consider our graph $\\mathcal{G}$, corresponding to the invariant $\\sum_{ijk} T_{ijk} T_{ijk}$. Cutting an edge will be equivalent to removing a contraction. Let's say we cut the edge of the color associated to the first position. It means that we are no longer setting the two first indices equal and summing over them. We will have $\\sum_{jk} T_{i_1 jk} T_{i_2 jk}$. We have two free indices $i_1$ and $i_2$, so we can define a matrix $M_{{i_1},{i_2}}=\\sum_{jk} T_{i_1 jk} T_{i_2 jk}$.\n    \n* $I_G(T)$ is the tensor invariant associated to the graph $G$ calculated for the tensor $T$. We added a more explicit introduction of $I_G(T)$  in the updated version.\n    \n* The Loss funtion was $1-<v,v_0>$ where $<,>$ is the scalar product, $v$ is the vector output by the algorithm and $v_0$ the signal vector we aim to recover. We removed completely this notation in the updated version since it was not essential.\n    \n* By dominating we meant that its operator norm was much larger than the others, we removed this notation and put a more explicit formula in the section 2.3 of the updated version.\n    \n* The polynomial complexity is inherent to the definition of the trace invariants and the matrices associated to them. A trace invariant is a sum of product of tensor elements (like $\\sum_{ijk} T_{ijk}  T_{kij}$. So it is by definition polynomial. The input is a tensor of size $n^3$, and the sum in the melonic invariant $\\sum_{ijk} T_{ijk}  T_{ijk}$ is over three indices varying from 1 to n. So it makes $O(n^3)$ operations. Thus the time linearity. The linearity of the recovery algorithm was proven in a previous reference that we precised. We added a precision about the time linearity of the two algorithms for the melon diagram in the updated version.\n    "}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Hm5Szfl5Z", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3725/Authors|ICLR.cc/2021/Conference/Paper3725/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834470, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment"}}}, {"id": "bRqAc8z_ky4", "original": null, "number": 5, "cdate": 1605680271190, "ddate": null, "tcdate": 1605680271190, "tmdate": 1606148037461, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "NvIwXUMyrB", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment", "content": {"title": "Thank you very much for the feedback! [Part 1]", "comment": "The theoretical physics community has developed in the last decade new tools to study problems involving tensors [1]. This paper aimed to show that these tools are easily adaptable and are of great interest in tackling concrete machine learning problems. Indeed, we showed in this paper that existent well-studied tools (trace invariant) and developed new ones (matrices associated to the trace invariants) are able to tackle the important problem of tensor PCA and proven that they performed better than state-of-the-art in the usual important settings and were even able to tackle more general settings closer to real life applications like asymmetric tensorial data (video, image with color, etc.). However, one well-known difficulty in approaching such interdisciplinary subjects is the dictionary of vocabulary between communities, and we think it may have been an important source of confusion. That is why we attempted in the updated version to completely revise the notations throughout the paper. We want to reiterate our appreciations for the reviewers and their very helpful comments and feedback to help us improve the clarity of the paper.\n\n\n* We added in the new version, at the end of Section 2.1, a more precise definition of the contraction: Let's define a contraction of a pair of indices as setting them equal to each other and summing over them, as in  calculating the trace of a matrix ( $\\mathbf{A}_{ij} \\rightarrow \\sum_{i=1}^n \\mathbf{A}_{ii}$ )\n    \n* O(n) refers here to the orthogonal group, we added a more precise definition, at the second paragraph of the section 2.1, in the updated version:  $\\mathrm{O}(n)$ is the $n$-dimensional orthogonal group (i.e. the group of real matrices that satisfies $\\mathbf{O} \\mathbf{O}^\\top=\\mathbf{I}_n$). \n    $\\mathbf{O}$ refers to an orthogonal matrix.\n    We used $\\mathcal{O}()$ to refer to the computational complexity.\n    Unfortunately these different objects with similar symbols added a lot of confusion.\n    So in the updated version, we used $\\mathrm{O}(n)$ for the orthogonal group (and we added its definition) and $O(n)$ for the complexity (to match the custom of the community), we also hope that the context helps in distinguishing the two objects.\n  \n[1]  R.  Gurau,  \u201cUniversality  for  Random  Tensors,\u201dAnn. Inst. H. PoincareProbab. Statist., vol. 50, no. 4, pp. 1474\u20131525, 2014.\n  "}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Hm5Szfl5Z", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3725/Authors|ICLR.cc/2021/Conference/Paper3725/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834470, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment"}}}, {"id": "zhKZTsZCE2Z", "original": null, "number": 4, "cdate": 1605679685801, "ddate": null, "tcdate": 1605679685801, "tmdate": 1605680796235, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "pRAsk8Oa1Tk", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment", "content": {"title": "Thank you very much for the feedback!", "comment": "\nWe want to thank the reviewer for his valuable feedback and comments. We reply to each of the reviewer's questions in the original order below:\n\n\n* We incorporated various clarifications and definitions in the new version, like $\\langle . \\rangle$ which corresponds to a scalar product. We also tried to explain carefully all the notations introduced by the theoretical physics community (and non standard in machine learning) in the beginning of the paper. \n* It is a typo, we addressed it in the new version.\n* The variance of a graph is the variance of its associated trace invariant. It is given by $E((I_\\mathcal{G} - E(I_\\mathcal{G}))^2)$. We added many details and many graph illustrations in the demonstrations in order to clarify them (in particular the theorem $4$). We hope that makes the demonstrations more easy to read.    \n* We added clarifications in the new version and changed notations that may have been confusing in the algorithm 1: i) We first calculate theoretically the expectation and the variance of the invariant $I$ for a random gaussian model (we denote them $E(I^{(N)})$ and $\\sigma(I^{(N)})$), they depend only on $n$. ii) Then we compute the value (that we denote $\\alpha$) of this invariant for our tensor T (from which we want to detect the presence of a signal). iii) Chebyshev's theorem provides the probability that $T$ is a random tensor based on the distance of the value $\\alpha$ to the mean (thus the comparison with the variance). The variance of the noise model is not important since we can factor it out from the tensor: if $Z'$ is a random gaussian tensor with variance $\\sigma$, first we can find $\\sigma$ by plotting the distribution of the components of $Z'$, then we can introduce $Z$ such that $Z'=\\sigma Z $  . Thus, $Z$ would be a tensor whose components follow a standard normal distribution. So changing the variance of the model just adds a constant factor ($1/\\sigma$) to the detection and recovery threshold ($T=\\beta v^{\\otimes k} + Z'=\\beta v^{\\otimes k} + \\sigma Z= \\sigma (\\beta/\\sigma+Z)$.\n* This was a typo, we addressed this issue in the new version of the paper.\n* We added clarification to the theorem 2. Since the model is inherently probabilistic, it is mathematically impossible to have a detection or recovery with probability strictly equal to 1. The common procedure is to prove the theorems at the large $n$ limit ([1] and the other papers used for their proofs random matrix theory at large $n$) and to use the empirical results to check when this approximation of the large $n$ is valid ([1] noted that empirically it was valid above $n=25$, which is also what we observe with our experiments). We clarify this important point in the paper and thank the reviewer for bringing it to our attention.\n\n* We introduced more carefully what we call the intermediate graphs in the new version. They are the graphs which has both a contribution from the noise random tensor **and** from the signal vector. Note that, the two other kinds of graphs are the pure noise graph and the pure signal graph. We also added more clarifications and an illustration to the appendix C discussing perfect one-factorization. \n* We agree that the summands may have been confusing because we wrote some coefficients implicitly in the '$\\dots$' while we kept some others. In order to clarify it, we make all the coefficients implicit and we add the decomposition of the tetrahedral matrix by drawing its 16 contributions. The reviewer is perfectly right that what is mainly used in the proofs is the coefficient $\\beta^d$ next to $v^{\\otimes(k)}$. \n* We added details about the experiments in the appendix D. For instance, concerning the recovery methods, we repeated in 50 independent instances the following settings: i) We generate randomly the n components of the signal vector $v_0$ and then normalize it.\n    ii) We generate randomly the $n^3$ components of the random tensor $Z$. If we are in the symmetric case, we symmetrize it with the same normalization than [1].\n    iii) We compute the tensor $T=Z+\\beta v_0^{\\otimes 3}$.\n    iv) We compute the matrix constructed from contracting multiple copies of $T$ (for example associated to the melon: $M_{i_1 i_2} = \\sum_{j,k} T_{i_1 jk} T_{i_2 jk}$) as described in Figure 2. To compute it, we use the numpy tensordot function in Python. v) We find its respective leading eigenvector $v$.\n    vi) We draw the correlation between the obtained vector $v$ with the initial signal vector $v_0$.\n\n[1]  E. Richard and A. Montanari, \u201cA statistical model for tensor pca,\u201d inAd-vances in Neural Information Processing Systems, pp. 2897\u20132905, 2014.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Hm5Szfl5Z", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3725/Authors|ICLR.cc/2021/Conference/Paper3725/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834470, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment"}}}, {"id": "oMQmeSamrxp", "original": null, "number": 3, "cdate": 1605678920419, "ddate": null, "tcdate": 1605678920419, "tmdate": 1605679145827, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "ARRUyGmhYcy", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment", "content": {"title": "Thank you very much for the feedback!", "comment": "\nWe want to thank the reviewer for his valuable feedback and comments.\n\nWe answer to the reviewer concerns below:\n\n\n\n*  **Applications for ML/AI/Language processing:**\nTensor PCA and tensor decomposition (the recovery of many spikes addressed in Section 3.5) is motivated by the increasing number of problems in which it is crucial to exploit the tensorial structure [1]. Recently it was successfully used to address important problems in unsupervised learning (learning latent variable models, in particular latent Dirichlet allocation [2], [3]), supervised learning (training of two-layer neural networks, \\cite{janzamin2015beating}) and reinforcement learning ([4]). Moreover, we note that some of our results tends to generalize the applications of the methods to more practical settings like the case of a tensor with axes of different dimensions (adequate for data which are inherently asymmetric like a video). We added these elements in the introduction just before the related work paragraph.\n\n    \n*  **Experiments on real data and detailed comparison of the methods:**\nWe agree with the reviewer that experiments on real data would have been very interesting. However, this paper has a more theoretical leaning and primarily aims to introduce a new framework where we derive new algorithmic results.\nFor a fair comparison to other existent methods, we favored synthetic data. We added the applications on real data as potential perspective.    \n    \n\n\n[1]  N. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E. Papalexakis,and C. Faloutsos, \u201cTensor decomposition for signal processing and machinelearning,\u201dIEEE Transactions on Signal Processing, vol. 65, no. 13, pp. 3551\u20133582, 2017.\n\n[2]  A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky, \u201cTensordecompositions  for  learning  latent  variable  models,\u201dJournal of MachineLearning Research, vol. 15, pp. 2773\u20132832, 2014.\n\n[3]  A. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y.-K. Liu, \u201cA spec-tral algorithm for latent dirichlet allocation,\u201dAlgorithmica, vol. 72, no. 1,pp. 193\u2013214, 2015.\n\n[4]  M. Janzamin, H. Sedghi, and A. Anandkumar, \u201cBeating the perils of non-convexity:  Guaranteed training of neural networks using tensor methods,\u201darXiv preprint arXiv:1506.08473, 2015.\n\n[5]  K. Azizzadenesheli, A. Lazaric, and A. Anandkumar, \u201cReinforcement learn-ing of pomdps using spectral methods,\u201darXiv preprint arXiv:1602.07764,2016."}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "k2Hm5Szfl5Z", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3725/Authors|ICLR.cc/2021/Conference/Paper3725/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834470, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Comment"}}}, {"id": "ARRUyGmhYcy", "original": null, "number": 3, "cdate": 1604116142051, "ddate": null, "tcdate": 1604116142051, "tmdate": 1605036571719, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Review", "content": {"title": "A review of the paper \"A new framework for tensor PCA based on trace invariants\"", "review": "Summary:\n \nThe paper provides an interesting algorithm for tensor PCA, which is based on trace invariants. The problem consists of recovering a (single-spike/multiple orthogonal spikes) tensor corrupted by a Gaussian noise tensor. The authors proposed a new algorithm which allows recovering a signal for a sufficiently small signal to noise ratio. \n\n##########################################################################\nReasons for score: \n \nOverall, I vote for accepting. I like the idea, and the proofs seem to be coherent and correct. The problem has clear importance for the theoretical/statistical physics community; however, I am not convinced of the importance of the problem considered here for the ICLR community and appreciate the author\u2019s comments on this. I also have a few minor concerns, which, hopefully, can be addressed by the authors in the rebuttal period. \n \n##########################################################################\n\nPros: \n1. The paper takes an interesting question about tensor PCA and proposes a promising approach to solve it based on the trace invariants. For me, the problem is encouraging, while I would appreciate a discussion about possible machine learning/AI applications (learning latent variable models? anything else?)\n \n2. The mathematical justification of the statements seems to be correct for me and ok to follow.  \n\n3. It is claimed in the paper that the algorithm improves the state-of-the-art (signal to noise ratio requirements) in several cases, while a brief survey/table of the recent results is missing.  Unfortunately, I am not working in this area and probably not familiar with recent results\n \n##########################################################################\nCons: \n \n1. Applications for ML/AI/Language processing are not very clear for me, and I would appreciate a discussion on this in the paper. \n\n2. Empirical justification. I would highly appreciate having more experiments on real data (if any) and a detailed comparison of the methods in terms of accuracy/memory/time. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070776, "tmdate": 1606915759373, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3725/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Review"}}}, {"id": "NvIwXUMyrB", "original": null, "number": 1, "cdate": 1603841043079, "ddate": null, "tcdate": 1603841043079, "tmdate": 1605023948675, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Review", "content": {"title": "Review", "review": "Summary:\n\nThis paper studies the detection and recovery problem in spiked tensor models in the form T = \\beta v0^\\otimes k + Z, where v0 is the underlying spike signal and Z is a Gaussian noise.  The authors claim that they propose a new framework to solve the problem, by looking at the trace invariants of tensors. The authors provide a detection algorithm (Algorithm 1) and a recovery algorithm (Algorithm 2), as well as the corresponding phases. The authors claim that: 1) they \"build tractable algorithms with polynomial complexity\", \"a detection algorithm linear in time\"; 2) the algorithms are very suitable for parallel architectures; 3) an improvement of the state of the art for the symmetric tensor PCA experimentally. The authors furthermore discuss the asymmetric case and the multiple spike case.\n\n\nRecommendation:\n\nAt the current stage I vote for rejection. I am not able to follow the proofs in this paper due to missing definitions of terms and notations. Also some claims are not proved. See below for details.\n\n\nPros:\n\n- The methods used in the paper seem new for spiked tensor models.\n\n- Some experimental results are provided. \n\n\nCons: \n\n- The readability of this paper severely suffers from its writing. At the current stage, filled with undefined or inconsistent notations and terms, this paper is not self-contained and hard to follow. This becomes worse considering the fact that this paper studies tensor problems -- many tensor-related terms have multiple definitions (e.g., eigenvalues, ranks). It will be very hard to follow the proofs if the definitions are unclear. Here is an incomprehensive list:\n\t- Middle of Page 3: what is the *formal* definition of contracting (instead of saying \"equivalent to a matrix multiplication\")? Also, trace invariants are never formally defined in this paper.\n\t- eq.(2),(3): what is O(n) here? Also, what does the bold O refer to? Right before eq.(4) the authors use another notation \\mathcal{O}(n). Is this the same as the first O(n)? In the abstract the authors use \\mathcal{O}(1) to refer to the constant order. Why the inconsistency?\n\t- End of Page 3: how is \\mathcal{G} related to trace invariants formally?\n\t- Section 2.2: this is not clear. What are the matrices here? What is the definition of M_{G,e}?\n\t- Section 2.3: what is the definition of I_G(T)?\n\t- Theorem 3: what is the Loss function here?\n\t- Top of Page 5: what is the exact definition of \"dominating\" here?\n\n- It should be noted that, without clear definitions of I_G(T) and M_{G,e}(T), there is no way to verify Algorithm 1 and 2.\n\n- The authors claim \"polynomial complexity\" at the beginning of the paper, but it is never proved. Theorem 7 claims that Algorithm 1 and 2 run in linear time. I cannot find that in the proof.   \n\n- It is unclear why the algorithms \"are very suitable for parallel architectures\", as the authors have claimed. Have the authors tried running the experiments in parallel?\n\n- Theorem 4, 5, 9, 10 do not have complete proofs. \n\n\nMinor comments:\n\n- Page 2 Notations: typeface of v is not consistent. \n\n- Page 8: \"eg\" should be \"e.g.\"\n\n\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070776, "tmdate": 1606915759373, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3725/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Review"}}}, {"id": "pRAsk8Oa1Tk", "original": null, "number": 2, "cdate": 1603846712873, "ddate": null, "tcdate": 1603846712873, "tmdate": 1605023948610, "tddate": null, "forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "invitation": "ICLR.cc/2021/Conference/Paper3725/-/Official_Review", "content": {"title": "Review of \"A new framework for tensor PCA based on trace invariants\"", "review": "The paper presents a pair of interesting algorithms using trace invariants to detect the signal in the signal-plus-noise tensor PCA framework.  The algorithms function by considering cutting an edge in the graph representation of the trace invariant, yielding a matrix whose leading eigenvector provides a (up to a rotation) estimate of the signal vector $v$.  This algorithm appears to be very interesting and works well in a series of simulations. \n\nUnfortunately, the presentation of the paper makes it very difficult to assess the importance of the contribution.  The introduction is well-written and well-motivated, though the later segmentation of the paper into many small subsections without much exposition makes the flow of the paper and its results hard to follow.  In addition, the notation and terminology in the paper are imprecise and, with important terminology and symbology introduced without definition or background citation.\n\nPros:\n- The proposed algorithm is clever and appears to do well compared to existing approaches in experiments.\n\n- Well written introduction (with the only complaint being some minor grammatical errors).\n\n\n\nCons:\n- Important notation is introduced, and is not defined; Equation 4 is an example of this, where $\\langle \\cdot \\rangle$ (I assume this means $\\mathbb{E}$?), $\\bar{\\mathbf{T}}$, and $\\mathcal{E}^0(\\mathcal{G})$ are all undefined.  This occurs often in the paper and in the appendix.\n\n- In the $\\bullet, \\times, \\bullet$ decomposition at the start of Section 2.3, what is $\\sqrt{N}$?\n\n- What is the variance of a graph (as in Theorem 4)?  The proof sketch of this theorem is very hard to follow.\n\n- Algorithm 1 is imprecise; what does \"compare $\\alpha$ to $\\sigma(I^{(N)}(T))$ mean?  If $\\alpha>\\sigma(I^{(N)}(T))$ then a spike is detected?  How do you compute the variance of $I^{(N)}(T)$?  How would you compute this if the noise model did not have unit variance)?  \n\n- Both algorithms are only presented for 3-way tensors, but the Theoretical claims are for higher order tensors?\n\n- The proofs of the theorems and the statement of the theorems are, in general, a bit imprecise.  For example, in the proof of Theorem 2, Chebyshev's inequality will not guarantee disjointness everywhere, but only with high probability.  This is the case if $\\beta_{det}$ is finite.  This is a finite $\\beta_{det}$ result, with a claim only holding in the limit.\n\n- In Theorem 5, what are the intermediate graphs/matrices?  In addition, this section (and Appendix C discussing perfect one-factorization) are a bit opaque.\n\n- Is the decomposition after equation 5 only for the melon graph?  For more complex graphs (i.e., the tetrahedral), I believe you will have additional trace-like coefficients on all terms.  In any event, I am confused about the summands.  I do not see why the all $Z$ sum would have a $\\beta$, while the cross-terms would not.  Furthermore, why would the all $v$ sum not have a $\\beta^d$ coefficient?  This is what is implicitly being used in the proofs?\n\n- In the experiments, important details are left out.  What is the setup here: what are the $v$'s, how many iterations of tensor power method are applied, how many MC replicates are run to produce the error bars, what is the y-axis, what are the runtimes here, what is Random in Figure 6?  More detail would help a lot to understand how your new approach compares (it appears well) with the current literature.\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3725/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3725/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A new framework for tensor PCA based on trace invariants", "authorids": ["~Mohamed_Ouerfelli1", "~mohamed_Tamaazousti1", "vincent.rivasseau@gmail.com"], "authors": ["Mohamed Ouerfelli", "mohamed Tamaazousti", "Vincent Rivasseau"], "keywords": ["Tensor", "Principal Component Analysis", "Tensor decomposition", "trace invariant"], "abstract": "We consider the Principal Component Analysis (PCA) problem for tensors $T \\in (\\mathbb{R}^n)^{\\otimes k}$ of large dimension $n$ and of arbitrary order $k\\geq 3$. It consists in recovering a spike $v_0^{\\otimes k}$ (related to a signal vector $v_0 \\in \\mathbb{R}^n$) corrupted by a Gaussian noise tensor $Z \\in (\\mathbb{R}^n)^{\\otimes k}$ such that $T=\\beta v_0^{\\otimes k} + Z$ where $\\beta$ is the signal-to-noise ratio. In this paper, we propose a new framework based on tools developed by the theoretical physics community to address this important problem. They consist in trace invariants of tensors built by judicious contractions (extension of matrix product) of the indices of the tensor $T$. Inspired by these tools, we introduce a new process that builds for each invariant a matrix whose top eigenvector is correlated to the signal for $\\beta$ sufficiently large. Then, we give examples of classes of invariants for which we demonstrate that this correlation happens above the best algorithmic threshold ($\\beta\\geq n^{k/4}$) known so far. This method has many algorithmic advantages: (i) it provides a detection algorithm linear in time and that has only $O(1)$ memory requirements (ii) the algorithms are very suitable for parallel architectures and have a lot of potential of optimization given the simplicity of the mathematical tools involved (iii) experimental results show an improvement of the state of the art for the symmetric tensor PCA. Furthermore, this framework allows more general applications by being able to theoretically study the recovery of a spike in the form of $v_1 \\otimes \\dots \\otimes v_k$ with different dimensions ($T \\in \\mathbb{R}^{n_1\\times n_2\\times \\dots \\times n_k}$ with $n_1,\\dots, n_k \\in \\mathbb{N}$) as well as the recovery of a sum of different orthogonal spikes. We provide experimental results to these different cases that match well with our theoretical findings.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ouerfelli|a_new_framework_for_tensor_pca_based_on_trace_invariants", "pdf": "/pdf/d155f51d91d2a2073134b4ed85949e958c848d13.pdf", "supplementary_material": "", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=O48b1USIZK", "_bibtex": "@misc{\nouerfelli2021a,\ntitle={A new framework for tensor {\\{}PCA{\\}} based on trace invariants},\nauthor={Mohamed Ouerfelli and mohamed Tamaazousti and Vincent Rivasseau},\nyear={2021},\nurl={https://openreview.net/forum?id=k2Hm5Szfl5Z}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "k2Hm5Szfl5Z", "replyto": "k2Hm5Szfl5Z", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3725/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070776, "tmdate": 1606915759373, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3725/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3725/-/Official_Review"}}}], "count": 11}