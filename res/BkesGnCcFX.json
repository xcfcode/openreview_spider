{"notes": [{"id": "BkesGnCcFX", "original": "HkxAheGEYX", "number": 1298, "cdate": 1538087955226, "ddate": null, "tcdate": 1538087955226, "tmdate": 1545355405492, "tddate": null, "forum": "BkesGnCcFX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkgUkUexxN", "original": null, "number": 1, "cdate": 1544713694462, "ddate": null, "tcdate": 1544713694462, "tmdate": 1545354507969, "tddate": null, "forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Meta_Review", "content": {"metareview": "This manuscript presents a reinterpretation of hindsight experience replay which aims to avoid recomputing the reward function, and investigates Floyd-Warshall RL in the function approximation setting.\n\nThe paper was judged as relatively clear. The authors report a slight improvement in computational cost, which some reviewers called into question. However, all of the reviewers pointed out that the experimental evidence for the method's superiority is weak. Two reviewers additionally raised that this wasn't significantly different than the standard formulation of Hindsight Experience Replay, which doesn't require the computation of rewards for relabeled goals.\n\nUltimately, reviewers were in agreement that the novelty of the method and quality of the obtained results rendered the work insufficient for publication. The Area Chair concurs, and urges the authors to consider the reviewers' pointers to the existing literature in order to clarify their contribution for subsequent submission.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Important subject matter but novelty & results insufficient for acceptance."}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1298/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352890104, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352890104}}}, {"id": "Syeog5bvg4", "original": null, "number": 10, "cdate": 1545177587454, "ddate": null, "tcdate": 1545177587454, "tmdate": 1545177587454, "tddate": null, "forum": "BkesGnCcFX", "replyto": "HJgCgQUm1E", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "content": {"title": "Response", "comment": "We address your comments point by point.\n\n> I maintain that they key idea behind this paper is not new. On top of that, the way it is presented obfuscates what is really going on. What is the justification for adding the 1-step loss to Q-learning with a constant reward for all transitions? Why will this converge at all? HER just applies Q-learning with modified goals/reward, and Q-learning comes with theoretical guarantees. \n\nThe surprising result that a constant reward for all transition converges at all is the main message behind our work. The reason why it converges is because we estimate the path-reward (as done by Kaelbling 1993) instead of future-reward (as done by Q-learning/HER). Our work builds upon Kalebling's work which does not have theoretical guarantees yet, but I do not see any reason why the formulation is averse to theoretical guarantees.\n\n>  I am not proposing any alternative solutions to the problem. Again, to me the key idea behind HER is that if you've reached state s, then you've achieved the goal of reaching state s. This means that the agent can get a reward of of 1 in the 0/1 reward formulation or a reward of 0 in the -1/0 formulation and the state is considered terminal. There is no need to check for equality of states and time indices. \n\nWhat you are describing is the \"final\" strategy described in HER paper Section 4.5 which we performs worse than \"future\" strategy. We use \"future\" strategy in all our experiments. In \"future\" strategy you have to either compare against the time-index or the goal itself. Moreover, 0 goal reward is different from no-goal reward which is what we propose.\n\n>  I am not proposing any modifications of HER. I am simply pointing out that the idea that you can do goal-based learning without recomputing rewards is both in the \u201cHindsight Experience Replay\u201d paper and in the \u201cLearning to Achieve Goals\u201d paper. To me it is the key idea behind HER. If you've reached a state s then you've achieved the goal of reaching state s. \n\nThe idea is there in \"Learning to Achieve Goals\" paper but not in \"Hindsight Experience Replay\" paper. The idea is not whether you have achieved the goal of reaching state s, but the idea is whether you should get a high-goal-reward on reaching the state s. We maintain that R(s, a, g) = 0 if s == g else -1 is unnecessary and R(s, a) = -1 is enough because only the path-rewards to reach the goal matter, not the eventual \"0\" reward that you get on reaching the goal. \n\nSince our experiments establish that triangular inequality from \"Learning to Achieve Goals\" is not helpful but the one-step loss is helpful, we bring the useful ideas from \"Learning to Achieve Goals\" to forefront in deep learning context. This is another way to look at our contributions.\n\nWe again thank you for your detailed comments and discussion."}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper1298/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617471, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1298/Authors|ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617471}}}, {"id": "HJgCgQUm1E", "original": null, "number": 9, "cdate": 1543885558399, "ddate": null, "tcdate": 1543885558399, "tmdate": 1543885558399, "tddate": null, "forum": "BkesGnCcFX", "replyto": "H1lj7OnYAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "content": {"title": "clarification", "comment": "> We agree that your proposed modification to HER would negate the reward re-computation requirement. However, HER does not do so. This perspective was influenced by the observation that reward re-computations are redundant. This observation must be non-trivial because HER and its many extensions have not accounted for it. \n\nI am not proposing any modifications of HER. I am simply pointing out that the idea that you can do goal-based learning without recomputing rewards is both in the \u201cHindsight Experience Replay\u201d paper and in the \u201cLearning to Achieve Goals\u201d paper. To me it is the key idea behind HER. If you've reached a state s then you've achieved the goal of reaching state s. \n\n> While your solution would work, we think our solution is much simpler to implement because it does not require checking whether sampled t == T. The replay buffer is sampled as is, with only a loss term added to the loss function. These are different ways to instantiate the same ideas which we believe to be non-trivial.\n\nI am not proposing any alternative solutions to the problem. Again, to me the key idea behind HER is that if you've reached state s, then you've achieved the goal of reaching state s. This means that the agent can get a reward of of 1 in the 0/1 reward formulation or a reward of 0 in the -1/0 formulation and the state is considered terminal. There is no need to check for equality of states and time indices. \n\nI maintain that they key idea behind this paper is not new. On top of that, the way it is presented obfuscates what is really going on. What is the justification for adding the 1-step loss to Q-learning with a constant reward for all transitions? Why will this converge at all? HER just applies Q-learning with modified goals/reward, and Q-learning comes with theoretical guarantees. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1298/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617471, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1298/Authors|ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617471}}}, {"id": "H1lj7OnYAQ", "original": null, "number": 8, "cdate": 1543256098858, "ddate": null, "tcdate": 1543256098858, "tmdate": 1543256098858, "tddate": null, "forum": "BkesGnCcFX", "replyto": "HJgCIDZICX", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "content": {"title": "There are multiple ways of instantiating an idea and ours is one", "comment": "We agree that your proposed modification to HER would negate the reward re-computation requirement. However, HER does not do so. This perspective was influenced by the observation that reward re-computations are redundant. This observation must be non-trivial because HER and its many extensions have not accounted for it. \n\n> I think it is clear that by replacing the goal with the final (reached) state s_T one can just give a reward of 0 at the final transition and -1 to the preceding ones. There is no need to compute rewards or compare any states.\n\nWhile your solution would work, we think our solution is much simpler to implement because it does not require checking whether sampled t == T. The replay buffer is sampled as is, with only a loss term added to the loss function. These are different ways to instantiate the same ideas which we believe to be non-trivial.\n\nWe thank you for your detailed comments and feedback."}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617471, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1298/Authors|ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617471}}}, {"id": "HJgCIDZICX", "original": null, "number": 7, "cdate": 1543014229904, "ddate": null, "tcdate": 1543014229904, "tmdate": 1543014229904, "tddate": null, "forum": "BkesGnCcFX", "replyto": "SkgVLVMZTm", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "content": {"title": "response", "comment": "Thank you for the clarifications. I think they confirmed my understanding of the paper.\n\nI maintain that the idea that you can do goal-based learning without rewards is both in the \u201cHindsight Experience Replay\u201d paper and in the \u201cLearning to Achieve Goals\u201d paper.\n\nHere\u2019s what the HER paper says about a trajectory s_1, \u2026, s_T for a goal g (top of page 4):\n\u201cThe pivotal idea behind our approach is to re-examine this trajectory with a different goal \u2014 while this trajectory may not help us learn how to achieve the state g, it definitely tells us something about how to achieve the state s_T . This information can be harvested by using an off-policy RL algorithm and experience replay where we replace g in the replay buffer by s_T\u201d.\nI think it is clear that by replacing the goal with the final (reached) state s_T one can just give a reward of 0 at the final transition and -1 to the preceding ones. There is no need to compute rewards or compare any states.\n\nThis is exactly what your one-step loss does. It is equivalent to a Q-learning update on each transition s,a,s\u2019 with the goal relabeled to s\u2019. The transition becomes terminal since the goal is reached. Presenting the one-step loss as something new is not accurate.\n\nHaving said that, there are multiple ways of instantiating this idea. The HER paper chooses to relabel goals for a trajectories. So in a sense the one-step loss is applied only to the last transition. You propose to apply the relabeling to all transitions. \u201cLearning to Achieve Goals\u201d performs all goal updating so it will also relabel each transition with the achieved state s\u2019 as the goal.\n\nComparing these approaches in terms of performance could be interesting, but as your results suggest there is not really a difference between HER and your approach in terms of data efficiency. I don\u2019t buy the comparison in terms of \u201creward computations\u201d because HER can also be implemented in a way where rewards don\u2019t need to be recomputed."}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1298/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617471, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1298/Authors|ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617471}}}, {"id": "rkxw606xAQ", "original": null, "number": 5, "cdate": 1542672063190, "ddate": null, "tcdate": 1542672063190, "tmdate": 1542672063190, "tddate": null, "forum": "BkesGnCcFX", "replyto": "SkgRQQbMa7", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "content": {"title": "One-step loss is applicable to all transitions not just terminal condition", "comment": "> The main contribution of the paper appears to be ...  equal to the reward at that timestep.\n\nThe one-step loss is, in fact, incorporated for every transition between states, not just the termination condition when the goal is achieved. An alternative perspective of one-step loss is one-step-episode Q-Learning. In other words, the one-step loss function is equivalent to treating every state transition as a full episode and the terminating condition. In the paper we have updated the \"one-step loss\" section to include this perspective. \n\n> It's not clear to me how this is fundamentally different than HER ... the transition achieves the resampled goal.\n\nAll our comparisons are already with \"sparse reward\" R(s,a,g) = (0 if s == g else -1) implementation of HER. As far as we can understand, in your proposed formulation the reward should be R(s,a,g) = (1 if s == g else 0) which is shifted by a constant factor. The sparse reward formulation still possesses the unnecessary dependence on the goal whose redundancy and removal is the emphasis of our work.\n\n> Is this not essentially identical to the proposal in this paper? ... deserves an entire paper.\n\nNo, this is not identical to the paper. At no point in our algorithm do we check the condition s == g. The proposed one-step loss that learns one-step reward Q(s_t, a_t, g=s_{t+1}) = r_t as we apply one-step loss to every transition. One-step loss is therefore task independent. As mentioned previously, this can also be thought of as one-step hindsight experience replay where the achieved goal at every step is treated as the desired goal.\n\n> The authors claim the main advantage here is avoiding recomputation of the reward function for resampled goals ... worth avoiding?\n\nIn machine learning, the sample complexity is always distinguished from computation complexity. The only case where the two are comparable is when the samples are generated from simulations which is, admittedly, true for our experiments. However, our proposed improvement is general enough to be applicable to non-simulation experiments.\n\nIt is a consequence of this task-dependent reward formulation that it can be re-sampled cheaply, hence, the computational cost is improvement is marginal. But we eliminate a redundancy common to the HER algorithm and its derivatives. With the massive popularity of HER (107 citations and counting), we believe that this is a worthwhile contribution to bring to the attention of the RL community. \n\nConsider the example of an agent navigating a maze where the goal is specified in the form of an image. The semantic comparison of the observed image with the goal image is an expensive operation that will require separate training for goal-dependent reward formulation [1]. However, in our proposed formulation, the comparison operation (s == g) in the reward formulation is not needed thereby eliminating the need of another learning module. \n\n\n> All of the experiments in this paper use a somewhat unusual task setup where every timestep has a reward of -1. \n\nThis unusual reward formulation is possible because of our contribution (one-step loss). Hence, it is only true for the experiments that are referred to with \"Ours\" label. All the baselines (HER and FWRL) and \"Ours (goal rewards)\" operate on the reward structure for HER which is R=(0 if s==g else -1).\n\n> Have the authors considered other reward structures, such as the indicator function R=(1 if s==g else 0) or a distance-based dense reward?\n> Would this proposal work in these cases? If not, how significant is a small change to HER if it can only work for one specific reward function?\n\nWe have considered and we are advocating against such reward structures because of their goal dependence. In fact in one experiment we run our algorithm with the reward structure R=(0 if s==g else -1) which is equivalent to yours with a constant shift. These results can be found in Fig. 4(b), labeled as \"Ours (goal rewards)\". \n\nDistance-based dense reward is by definition goal dependent. Our contribution is to eliminate this dependence. RL on dense rewards is easier than sparse rewards. Hence, we do not believe that distance-based reward adds much to our contribution. We do note that our method does work with goal based sparse rewards(Fig. 4b) and hence we would expect to continue to work with dense rewards. \n\n> The reconsideration of Floyd-Warshall RL ... recommend this for publication.\n\nWe analyze FWRL and added the ablation study of loss function in Appendix Figure 6.\nIt is clear that FWRL inspired loss function do not contribute to better\nlearning. Instead, they hurt the performance. We think this is because Bellman inspired loss already captures the information that FWRL inspired constraints intend to capture.\n\n[1] Nikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun. \"Semi-Parametric Topological Memory for Navigation\". In ICLR 2018"}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617471, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1298/Authors|ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617471}}}, {"id": "SkgRQQbMa7", "original": null, "number": 3, "cdate": 1541702437809, "ddate": null, "tcdate": 1541702437809, "tmdate": 1541702437809, "tddate": null, "forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Review", "content": {"title": "Review", "review": "This paper presents a reinterpretation of hindsight experience replay (HER) that avoids recomputing the reward function on resampled hindsight goals in favor of simply forcing the terminal state flag for goal-achieving transitions, referred to by the authors as a \"step loss\".\nThe new proposal is evaluated on two goal-conditioned tasks from low-dimensional observations, and show modest improvements over HER and a function-approximation version of Floyd-Warshall RL, mostly as measured against the number of times the reward function needs to be recomputed.\n\nPros:\n- minor improvement in computational cost\n- investigation of classical FWRL technique in context of deep RL\n\nCons:\n- computational improvement seems very minor\n- sparse-reward implementations of HER already do essentially what this paper proposes\n\nComments:\n\nThe main contribution of the paper appears to be the addition of what the authors refer to as a \"step loss\", which in this case enforces the Q function to correctly incorporate the termination condition when goals are achieved. I.E. the discounted sum of future rewards for states that achieve termination should be exactly equal to the reward at that timestep.\n\nIt's not clear to me how this is fundamentally different than HER. One possible \"sparse reward\" implementation of HER involves no reward function recomputation at all, instead simply replacing the scalar reward and termination flag for resampled transitions with the indicator function for whether the transition achieves the resampled goal.\nIs this not essentially identical to the proposal in this paper? I would consider this a task-dependent implementation detail for an application of HER rather than a research contribution that deserves an entire paper.\n\nThe authors claim the main advantage here is avoiding recomputation of the reward function for resampled goals.\nI do not find this particularly compelling, given that all of the evaluations are done in low-dimensional state space: reward recomputation here is just a low-dimensional euclidean distance computation followed by a simple threshold.\nIn a world where we're doing millions of forward and backward passes of large matrix multiplications, is this a savings that really requires investigation?\nIt is somewhat telling that the results are compared primarily in terms of \"# of reward function evaluations\" rather than wall time. If the savings were significant, I expect a wall time comparison would be more compelling.\nMaybe the authors can come up with a situation in which reward recomputation is truly expensive and worth avoiding?\n\nAll of the experiments in this paper use a somewhat unusual task setup where every timestep has a reward of -1. Have the authors considered other reward structures, such as the indicator function R=(1 if s==g else 0) or a distance-based dense reward?\nWould this proposal work in these cases? If not, how significant is a small change to HER if it can only work for one specific reward function?\n\nConclusion:\n\nIn my view, the main contribution is incremental at best, and potentially identical to many existing implementations of HER.\nThe reconsideration of Floyd-Warshall RL in the context of deep neural networks is a refreshing idea and seems worth investigating, but I would need to see much more careful analysis before I could recommend this for publication.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Review", "cdate": 1542234260732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335916179, "tmdate": 1552335916179, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxW9NfW6m", "original": null, "number": 3, "cdate": 1541641352549, "ddate": null, "tcdate": 1541641352549, "tmdate": 1541641352549, "tddate": null, "forum": "BkesGnCcFX", "replyto": "r1l2zvNt3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "content": {"title": "Our method consistent performs better than baselines when computered in terms of distance from the goal and reward computation", "comment": "Thank you for your feedback. \n\n1. The experimental results are mixed, and do not convincingly demonstrate the\n   effectiveness/superiority of the proposed method.\n\nThe results are mixed when the learning curves are compared with respect to the epochs (the number of transition samples) that intentionally does not take reward-recomputation in to account. \n\nWhen this computation is taken in to account, our algorithm comprehensively improves upon the baselines in 6 out of 8 experiments. To further highlight these differences we magnify our reward-recomputation plots to eliminate sections where curves overlap and are non-informative. These changes can be found in Figure 2 and 3.  \n\nWe further reiterate that reward recomputation\ncost can be significant dependent upon the environment and setup. In cases when\nthe reward-computation depends upon collisions and haptic feedback of real\nrobots, the reward recomputation may even be impossible without re-running the\nexperiment. Hence reducing reward-computation based on a simple loss term is an\nimportant contribution.\n\n\n2. The idea of the proposed method is relatively simple, and is not theoretically justified.\n\nThe main contribution of this paper is to show that goal-conditioned value functions can be learned without requiring goal-reward.\nWe believe that the simplicity of this proposed idea is the beauty of the method leading to significant changes in performance of the algorithm when reward recomputation is taken in to account.\n\nOur algorithm builds upon HER which does not itself possess theoretical guarantees. We would be able to addess this point specifically if the reviewer could clarify what kind of theoretical justification they would expect to see. \n   \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617471, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1298/Authors|ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617471}}}, {"id": "SkgVLVMZTm", "original": null, "number": 2, "cdate": 1541641291573, "ddate": null, "tcdate": 1541641291573, "tmdate": 1541641291573, "tddate": null, "forum": "BkesGnCcFX", "replyto": "S1loBx0K27", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "content": {"title": "In our reward formulation we do not get 0 reward, it is always -1", "comment": "There are two main reasons for the confusion about the contributions of\nthis work. \n\nFirst, our reward formulation is different from that of Hindsight Experience Replay\n(HER). In HER, the agent receives -1 reward for all state transitions except on\nreaching the goal when it receives 0 reward. In contrast, for our reward\nformulation the agent receives -1 reward for all state transitions including\nwhen agent reaches and continues to stay at the goal.\n\nSecond, our reward formulation is atypical with respect to conventional\nReinforcement Learning (RL). In conventional RL, a high reward is used to\nspecify the desired goal (goal-reward). However, this goal-reward is not\nnecessary in goal-conditioned RL because the goal specification is already given\nat the start of every episode. We believe that this result is counter-intuitive\nand will be interesting to the RL community.\n\nWe clarify the reviewer's concerns and\nedit our draft to minimize chances of similar confusion.\n\nClarity:\n1. The main difference between HER, FWRL and our algorithm lies in the choice of\n   loss terms used. HER uses Eq (3), FWRL uses Eq (3) + L_up + L_lo, and Our\n   algorithm uses Eq (3) + L_step as shown in the pseudo-code. Another difference\n   is due to reward formulation. Because our rewards are independent of reaching\n   the goal, we do not need to recompute rewards. We have added the description\n   about these differences in the appendix to highlight them.\n\n2. We have introduced the requested citations at appropriate places in the\n   paper. \n\n   Since [1] introduced the idea of FWRL before Dhiman et. al. 2018,\n   we replace the attributions accordingly in the paper. We further add\n   discussion points specific to their algorithm in the Related Work and\n   One-Step Loss section.\n\nNovelty and Significance\n1. Our contribution is learning *without* using goal-rewards *using* the\n   shortest path perspective. Our secondary contribution is to extend [1] to\n   deep neural networks.\n\n2. As mentioned above, our reward is always -1 *even when* current state is same\n   as the goal state. \n\n   Similar to HER, our goal states are not absorbing/terminal. Instead the\n   episodes are of fixed number of steps and the agent is encouraged to stay in\n   the goal state for as long as possible. This is how the replay buffer is\n   populated and how the average episode reward is computed. However, the\n   objective maximized is equivalent to treating this fixed length episode\n   problem as if the episodes are terminating on reaching the goal.\n\n   To further clarify this in the paper, we have added reward structure\n   details to the Introduction (section 1, paragraph 3) and the\n   Experiments section (section 5, end of paragraph 1).\n\n3. One-step loss is different from both the terminal step of both Q-Learning and [1].\n\n   One-step loss is different from terminal step of Q-Learning because it is\n   applied to every state transition rather than just the terminal step of the\n   episode. Having said that it is indeed equivalent to Q-Learning, if every\n   state transition is viewed as a one-step episode with the reached state as\n   the pseudo-goal. Correspondingly we have updated the manuscript in both the\n   introduction and the one-step loss section to include one-step-episode\n   Q-Learning perspective.\n\n   One-step loss is also different from the terminal step of [1].\n   Referring to Section 3 of [1], we see the one-step loss (Eq. 8)\n   as an alternative of the terminal condition DG*(s, a, g) = 0 if s = g in the\n   recursive definition of DG*(s, a, g). As an alternative, one-step loss \n   translates to DG*(s_t, a_t, g_{t+1}) = -1, for all transitions (s_t, a_t ->\n   g_{t+1}) i.e. it removes the dependence of checking s=g. Although it serves\n   the same purpose of terminal condition in recursive definition but the\n   condition is mathematically different and requires the different\n   assumption that one-step path is the highest reward path between s_t and g_{t+1}. \n   \n \n4. As stated earlier, our reward independent of desired goal. The reward\n   re-computation for the pseudo-goals becomes unnecessary because the reward\n   does depend upon the check if current state is same as the desired goal.\n\n   To further highlight saved reward computation, we magnify on our\n   reward-computation plots removing the uninformative parts of the plots where\n   the curves overlap.\n\nOverall quality:\n\n(A) Novelty: As argued above our proposed one-step loss is novel and so is the\nextension of [1] from tabular domain to deep learning.\n\n(B) Significance \n  (a) The counter-intuitive result that goal-conditioned RL does not need goal\n  reward is worth bringing to the attention of the RL community\n  (b) The absence of the requirement of reward-recomputation is significant\n  because in real robotics experiments, the reward computation may not be\n  possible without re-running the entire experiment.\n\n[1]: Kaelbling, Leslie Pack. \"Learning to achieve goals.\" IJCAI. 1993.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617471, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1298/Authors|ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617471}}}, {"id": "S1loBx0K27", "original": null, "number": 2, "cdate": 1541165122824, "ddate": null, "tcdate": 1541165122824, "tmdate": 1541533256518, "tddate": null, "forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Review", "content": {"title": "Review", "review": "This paper aims to improve on Hindsight Experience Replay by removing the need to compute rewards for reaching a goal. The idea is to frame goal-reaching as a shortest path problem where all rewards are -1 until the goal is reached, removing the need to compute rewards. While similar ideas were explored in a recent arxiv tech report, this paper claims to build on these ideas with new loss functions. The experimental results do not seem to be any better compared to baselines when measured in terms of data efficiency, but the proposed method requires fewer \u201creward computations\u201d.\n\nClarity:\nWhile the ideas in the paper were easy to follow, there are a number of problems with the writing. The biggest problem is that it wasn\u2019t clear exactly what algorithms were evaluated in the experiments. There is an algorithm box for the proposed method in the appendix, but it\u2019s not clear how the method differs from the FWRL baseline.\n\nAnother major problem is that the paper does a poor job of citing earlier related work on RL. DQN is introduced without mentioning or citing Q-learning. Experience replay is mentioned without citing the work of Long-Ji Lin. There\u2019s no mention of earlier work on shortest-path RL from LP Kaelbling from 1993. \n\nNovelty and Significance:\nAfter reading the paper I am not convinced that there\u2019s anything substantially new in this paper. Here are my main concerns:\n\n1) The shortest path perspective for goal-reaching was introduced in \u201cLearning to Achieve Goals\u201d by LP Kaelbling [1]. This paper should be cited and discussed.\n\n2) I am not convinced that the proposed formulation is any different than what is in Hindsight Experience Replay (HER) paper. Section 3.2 of the HER paper defines the reward function as -1 if the current state is not the same as the goal and 0 if the current state is the same as the goal. Isn\u2019t this exactly the cost-to-go/shortest path reward structure that is used in this paper?\n\n3) This paper claims that the one-step loss (Equation 8) is new, but it is actually the definition of the Q-learning update for transitioning to a terminal state. Since goal states are absorbing/terminal, any transition to a goal state must use the reward as the target without bootstrapping. So the one-step loss is just Q-learning and is not new. This is exactly how it is described in Section 3 of [1].\n\n4) The argument that the proposed method requires fewer reward evaluations than FWRL or HER seems flawed. HER defines the reward to be -1 if the current state and the goal are different and 0 if they are the same. As far as I can tell this paper uses the same reward structure, so how is it saving any computation?\n\nCan the authors comment on these points and clarify what they see as the novelty of this work?\n\nOverall quality:\nUnless the authors can convince me that the method is not equivalent to existing work I don\u2019t see enough novelty or significance for an ICLR paper.\n\n[1] \u201cLearning to Achieve Goals\u201d LP Kaelbling, 1993.\n", "rating": "1: Trivial or wrong", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Review", "cdate": 1542234260732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335916179, "tmdate": 1552335916179, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1l2zvNt3X", "original": null, "number": 1, "cdate": 1541125907804, "ddate": null, "tcdate": 1541125907804, "tmdate": 1541533256312, "tddate": null, "forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Review", "content": {"title": "Evaluation and judgement", "review": "The paper presents an approach for an approach to addressing multi-goal reinforcement learning, based on what they call \"one-step path rewards\" as an alternative to the use of goal conditioned value function. \nThe idea builds on an extension of a prior work on FWRL. \nThe paper presents empirical comparison of the proposed method with two baselines, FWRL and HER. \nThe experimental results are mixed, and do not convincingly demonstrate the effectiveness/superiority of the proposed method. \nThe idea of the proposed method is relatively simple, and is not theoretically justified. \n\nBased on these observations, the paper falls short of the conference standard. ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Review", "cdate": 1542234260732, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335916179, "tmdate": 1552335916179, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJl_G5df5m", "original": null, "number": 1, "cdate": 1538587152277, "ddate": null, "tcdate": 1538587152277, "tmdate": 1538587152277, "tddate": null, "forum": "BkesGnCcFX", "replyto": "H1gzrfOGcQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "content": {"title": "We will update the attribution", "comment": "Thank you for your comment. We were made aware of this paper recently. We will replace the attribution for the tabular version of the path-rewards idea with Kaelbling (1993) in an updated version of the manuscript."}, "signatures": ["ICLR.cc/2019/Conference/Paper1298/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617471, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkesGnCcFX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1298/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1298/Authors|ICLR.cc/2019/Conference/Paper1298/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617471}}}, {"id": "H1gzrfOGcQ", "original": null, "number": 1, "cdate": 1538585145653, "ddate": null, "tcdate": 1538585145653, "tmdate": 1538585176736, "tddate": null, "forum": "BkesGnCcFX", "replyto": "BkesGnCcFX", "invitation": "ICLR.cc/2019/Conference/-/Paper1298/Public_Comment", "content": {"comment": "The paper cites a recent arXiv paper for the concept of employing the Floyd-Warshall algorithm in goal-based reinforcement learning. This was actually introduced into the reinforcement learning literature 25 years ago in \"Learning to Achieve Goals\" https://people.csail.mit.edu/lpk/papers/ijcai93.ps\n by Leslie Pack Kaelbling, in IJCAI 93. However, the extension to the non-tabular case presented here does sound interesting.", "title": "Floyd-Warshall & RL"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1298/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards", "abstract": "Multi-goal reinforcement learning (MGRL) addresses tasks where the desired goal state can change for every trial. State-of-the-art algorithms model these problems such that the reward formulation depends on the goals, to associate them with high reward. This dependence introduces additional goal reward resampling steps in algorithms like Hindsight Experience Replay (HER) that reuse trials in which the agent fails to reach the goal by recomputing rewards as if reached states were psuedo-desired goals. We propose a reformulation of goal-conditioned value functions for MGRL that yields a similar algorithm, while removing the dependence of reward functions on the goal. Our formulation thus obviates the requirement of reward-recomputation that is needed by HER and its extensions. We also extend a closely related algorithm, Floyd-Warshall Reinforcement Learning, from tabular domains to deep neural networks for use as a baseline. Our results are competetive with HER while substantially improving sampling efficiency in terms of reward computation. \n", "keywords": ["Floyd-Warshall", "Reinforcement learning", "goal conditioned value functions", "multi-goal"], "authorids": ["dhiman@umich.edu", "shurjo@umich.edu", "qobi@purdue.edu", "jjcorso@umich.edu"], "authors": ["Vikas Dhiman", "Shurjo Banerjee", "Jeffrey M Siskind", "Jason J Corso"], "TL;DR": "Do Goal-Conditioned Value Functions need Goal-Rewards to Learn?", "pdf": "/pdf/d1d75927837e92ab56e67019bb8f4a00e1d3097e.pdf", "paperhash": "dhiman|learning_goalconditioned_value_functions_with_onestep_path_rewards_rather_than_goalrewards", "_bibtex": "@misc{\ndhiman2019learning,\ntitle={Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards},\nauthor={Vikas Dhiman and Shurjo Banerjee and Jeffrey M Siskind and Jason J Corso},\nyear={2019},\nurl={https://openreview.net/forum?id=BkesGnCcFX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1298/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311631011, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkesGnCcFX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1298/Authors", "ICLR.cc/2019/Conference/Paper1298/Reviewers", "ICLR.cc/2019/Conference/Paper1298/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311631011}}}], "count": 14}