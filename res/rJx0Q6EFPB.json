{"notes": [{"id": "aS9OWz9A11w", "original": null, "number": 18, "cdate": 1606266242680, "ddate": null, "tcdate": 1606266242680, "tmdate": 1606266699236, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "rkgdlMtKoH", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Related code has been released for more than one year", "comment": "*****************\nAs promised the related code has been released for more than one year, \n\nhttps://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT "}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference", "ICLR.cc/2020/Conference/Paper469/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "SkxMgDKtsS", "original": null, "number": 10, "cdate": 1573652202160, "ddate": null, "tcdate": 1573652202160, "tmdate": 1578656285199, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Summary of Submission Changes (1/10/2020)", "comment": "#Update(1/10/2020)\n===============================================================================================\nWe have released our code and models at the link  https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT and welcome to use them. Our 6-layer TinyBERT (BERT-base as teacher) can obtain almost same performances with BERT-base: GLUE score 79.4 vs 79.5.\n\n\n#Update(12/2/2019)\n===============================================================================================\n\nWe would like to thank the reviewers for the helpful comments! \n\nWe updated a new version including the changes as follows:\n\n1.We added more complete comparisons with same student architecture in the Appendix E of new version, for the easy and direct comparisons with prior works.\n\n2.We rephrased some claims more precisely and added the inference time of BERT$_{SMALL}$ based on the reviewers\u2019 suggestions."}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "rJx0Q6EFPB", "original": "SJeo87-DvB", "number": 469, "cdate": 1569439014322, "ddate": null, "tcdate": 1569439014322, "tmdate": 1577168214598, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "r2-uZTcpRb", "original": null, "number": 1, "cdate": 1576798697372, "ddate": null, "tcdate": 1576798697372, "tmdate": 1576800938368, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a new distillation-based method for using large pretrained models like BERT to produce much *smaller* fine-tuned target-task models. \n\nThis paper is low-borderline: It has merit and meets our basic standards, but owing to capacity limitations we had to give preference to papers we see as having a higher potential impact. Reviewers had some concerns about experimental design, but those seem to have been fully resolved after discussion. Reviewers were not convinced, even after some discussion, that the method and results were sufficiently novel and effective to have a substantial impact on the state of practice in this area.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718299, "tmdate": 1576800268760, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper469/-/Decision"}}}, {"id": "S1xrV0aqFB", "original": null, "number": 2, "cdate": 1571638829486, "ddate": null, "tcdate": 1571638829486, "tmdate": 1573752740066, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper proposes a new knowledge distillation method for BERT models. A number of modifications to the vanilla knowledge distillation method of Hinton et al (2015) are proposed. First, authors suggest adding L2 loss functions between alignment matrices, embedding layer values and prediction layer values. Second, authors propose run knowledge-distillation twice, once with the original pre-trained BERT model as teacher, and then again with task specific fine-tuned BERT as a new teacher. Third, authors emphasize the use of data augmentation for successful knowledge distillation. In Table 2, authors claim a significant lift across GLUE benchmarks with respect to other baseline methods with comparable model size.\n\nWhile the main contribution of this paper is the proposal of empirically useful techniques than theoretical development, the empirical results reported in this paper are somewhat puzzling. \n\nFirst of all, GLUE benchmark scores reported in Table 2 don't seem to be consistent with Table 1 of Sun et al (2019) for BERT-PKD ( https://arxiv.org/pdf/1908.09355.pdf ) or DistilBERT ( https://medium.com/huggingface/distilbert-8cf3380435b5 ). Indeed, BERT-PKD in Sun et al seems to significantly outperform TinyBERT on QNLI (89.0 vs 87.7) and RTE (65.5 vs 62.9), and the gap between BERT-PKD and TinyBERT on other tasks are much smaller if we take numbers reported in the original paper.\n\nIn Table 6, ablation studies with different distillation objectives are reported. Quite surprisingly, without Transformer-layer distillation (No Trm) the performance drops quite significantly. This is unexpected, because baselines such as Sun et al and DistilBERT do not use the Transformer-layer distillation but much more competitive to full TinyBERT than TinyBERT without Transformer-layer distillation. Would there be a reason why TinyBERT is so critically dependent on Transformer-layer distillation? Similarly, the removal of data augmentation (Table 5, No DA) is so detrimental to the performance of the model that it makes me to suspect whether the most of gain is from successful data augmentation. Indeed, 'No DA' row of Table 5 is very close to the performance BERT-PKD in Table 4, although the number of layers is different (4 vs 6). \n\nIn order for the research community to understand the contribution of proposed techniques more thoroughly, I suggest authors to conduct ablation studies with the simplest baseline. That is, rather than starting with the full TinyBERT model, start with a simple but competitive baseline like BERT-PKD, and only add one technique (DA, GD, Transformer-layer distillation) at a time so that readers shall understand what technique is the most important to be added to the baseline, and also whether some of the proposed techniques should always be used in combination.\n\n--- \nAfter Author Rebuttal: authors have addressed all of my concerns quite clearly. Additional experiments which targeted a specific design choice at a time made me much more convinced that the techniques proposed in this paper are useful not only for this particular context but also more broadly applicable.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575050152219, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper469/Reviewers"], "noninvitees": [], "tcdate": 1570237751675, "tmdate": 1575050152229, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Review"}}}, {"id": "S1lFQyzjoB", "original": null, "number": 12, "cdate": 1573752608624, "ddate": null, "tcdate": 1573752608624, "tmdate": 1573752608624, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "HkxCHLKKsH", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Good clarifications", "comment": "Thanks a lot for clarifications. I realized that I missed some of the important details in experiments such as the difference of hidden unit sizes/# of layers or how student layers were initialized. These experiments have quite convincingly resolved concerns I raised, and I will reflect my score accordingly."}, "signatures": ["ICLR.cc/2020/Conference/Paper469/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "HkxCHLKKsH", "original": null, "number": 9, "cdate": 1573652037831, "ddate": null, "tcdate": 1573652037831, "tmdate": 1573652062374, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "SJxEmUtYjr", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Response to Reviewer #1 [3/3]", "comment": "Q4: Study the importance of the proposed techniques.\n\nA4:\n Thanks for the good suggestion. Following the suggestion, we studied the importance of our proposed techniques under different experimental settings, which includes the \u201cBERT-PKD + DA\u201d, \u201cBERT + GD\u201d, \u201cBERT + GD +TD\u201d and the combination of \u201cBERT-PKD + TD + GD + DA\u201d. Note that we perform transformer distillation at both pre-training stage and fine-tuning stage, meaning that the suggested setting \u201cBERT-PKD + transformer distillation\u201d is equal to the setting of \u201cBERT-PKD + GD + TD\u201d. All the results are presented as follows:\n\nTable: the effects of different proposed techniques on the baseline BERT-PKD.\n------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t                         Architecture                   CoLA        MNLI        MNLI-mm     MRPC \n                                                                                                   mcc           acc                acc             acc    \n--------------------------------------------------------------------------------------------------------------------------------------\nBERT-PKD                             (M=6;d\u2019=768;d\u2019_i=3072)          43.1           80.9              80.9            83.1\n--------------------------------------------------------------------------------------------------------------------------------------\nBERT-PKD + DA                    (M=6;d\u2019=768;d\u2019_i=3072)         47.7           82.8               82.9           85.5\n--------------------------------------------------------------------------------------------------------------------------------------\nBERT-PKD + GD                   (M=6;d\u2019=768;d\u2019_i=3072)          45.7           82.0              82.2            83.6\n--------------------------------------------------------------------------------------------------------------------------------------\nBERT-PKD                             (M=6;d\u2019=768;d\u2019_i=3072)         48.4           83.8               84.0            85.5\nTransformer-layer                        \nDistillation (GD+TD)\n--------------------------------------------------------------------------------------------------------------------------------------\nBERT-PKD+GD+TD+DA      (M=6;d\u2019=768;d\u2019_i=3072)         53.5            84.1               84.0            87.0\n--------------------------------------------------------------------------------------------------------------------------------------\n\nFrom the upper table, with iteratively adding the proposed techniques (DA, GD, or TD+GD) to BERT-PKD, the augmented BERT-PKD can obtain more competitive results, which are significantly better than the original BERT-PKD. \n\n\u201cBERT-PKD + GD\u201d achieves slightly better performances than BERT-PKD, which confirms that the proposed GD can provide a relatively better initialization to BERT-PKD. In the original BERT-PKD, it is initialized by the first 6 layers of teacher BERT-base, thus BERT-PKD has the limitations that it should have the same hidden size, feedforward / filter sizes as its teacher. In our framework, initialized by GD, so it has the advantage of being more flexible in model size selection. By continuing performing the TD, BERT-PKD (GD + TD) can further capture more task-specific knowledge and achieve better results, which demonstrates the effectiveness of two-stage learning.\n\nSo we can conclude that: (a) all the proposed techniques DA, GD and transformer distillation (TD + GD) are crucial for improving the performances of baseline BERT-PKD, (b) GD can provide a good initialization to BERT-PKD, (c) we can get the best performances with combining all the proposed techniques GD+TD+DA. "}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "SJxEmUtYjr", "original": null, "number": 8, "cdate": 1573651996053, "ddate": null, "tcdate": 1573651996053, "tmdate": 1573651996053, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "SkxhJLKKir", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Response to Reviewer #1 [2/3]", "comment": "Q2: Without Transformer-layer distillation, the performance drops quite significantly.\n\nA2: \nThe reason is about the initialization of student BERT. Based on the Table 1, we know that the ablation study of \u201cNo-Trm\u201d (without transformer-layer distillation) as shown in Table 6, means we only do embedding-layer distillation (Embd) at the pre-training stage, and do the prediction-layer (Pred) and embedding-layer distillation (Embd) at the fine-tuning stage. At the pre-training stage, without Transformer-layer distillation, the important transformer-layers are randomly initialized and there is no supervision signal from upper layers to update their parameters during the pre-training stage.\n\nAt the pre-training stage, (1) BERT-PKD_k are directly initialized with the first k layers of their pre-trained teacher BERT to preserve the intermediate structures of their teacher BERT; (2) DistilBERT are also firstly initialized by its teacher BERT by taking one layer out of two, then trained with the self-supervised objective over large-scale corpus to finally get a good initialization for student BERT. \n\nGood initialization of student BERT is very crucial for the distillation of transformer-based models in NLP tasks, and our initial motivation for general distillation (Embd+Attn+Hidn) is to make the student TinyBERT learn the intermediate structures of teacher BERT at the pre-training stage to finally get a good initialization. In the Appendix D, we also studied other initialization strategies, e.g. initializing with BERT_SMALL and obtained some interesting observations.\n\nQ3: \u2018No DA\u2019 row of Table 5 is very close to the performance BERT-PKD.\nA3: The results of \u2018No DA\u2019 row in Table 5 and the results BERT-PKD in Table 4 cannot be compared directly, because they used different layer numbers, hidden sizes and feedforward/filter sizes. Thus, for a fair comparison, we evaluated the TinyBERT (No DA) with the same architecture (M=6;d\u2019=768;d\u2019_i=3072) as the BERT-PKD in Table 4, and all results are evaluated on DEV set and presented as follows:\n\nTable: the comparisons between TinyBERT (No DA) and BERT-PKD with the same architecture, and the results are evaluated on DEV set.\n---------------------------------------------------------------------------------------------------\n\t\t\t\t                    CoLA        MNLI-m     MNLI-mm      MRPC   \n                                                     mcc             acc                acc               acc   \n---------------------------------------------------------------------------------------------------\nBERT-PKD\n(M=6;d\u2019=768; d\u2019_i=3072)         43.1            80.9                80.9             83.1\n---------------------------------------------------------------------------------------------------\nTinyBERT *No DA*\n(M=6;d\u2019=768;d\u2019_i=3072)          49.1            84.0                84.4             86.0\n---------------------------------------------------------------------------------------------------\nTinyBERT \n(M=6;d\u2019=768;d\u2019_i=3072)          54.0            84.5                84.5             86.3\n---------------------------------------------------------------------------------------------------\n\nFrom the table, we can find that TinyBERT under the \u201cNo DA\u201d setting, which performs general distillation and task-specific distillation on original training dataset, consistently outperforms the BERT-PKD with the same architecture. The results indicate that the GD (general distillation) and TD (task-specific distillation) are crucial for TinyBERT learning, and the combination of GD, TD, and DA can further increase the performances. \n\nAnother interesting observation is that DA has bigger effect on 4-layer architecture than 6-layer architecture. We hypothesize that since the bigger 6-layer architecture has relatively larger model capacity, it can obtain better generalization capability than smaller 4-layer architecture through the pre-training over large-scale unsupervised corpus. Thus, the effect of data augmentation on the 6-layer architecture is relatively less than on the 4-layer architecture at the task-specific learning stage.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "SkxhJLKKir", "original": null, "number": 7, "cdate": 1573651940098, "ddate": null, "tcdate": 1573651940098, "tmdate": 1573651940098, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "S1xrV0aqFB", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Response to Reviewer #1 [1/3]", "comment": "Thank you for the helpful comments!\n\nQ1: GLUE scores reported in Table 2 don\u2019t seem to be consistent.\n\nA1:\nIn Table2, all the results of TinyBERT, BERT-PKD and DistilBERT are based on 4-layer architectures and evaluated on the TEST set of official GLUE benchmark. The results of Table 1 of Sun et al (2019) for BERT-PKD and the results of DistilBERT[1] are based on 6-layer architectures, and DistilBERT only reported their results on the DEV set of GLUE.\n\nAs described in Appendix B (\u201cBaseline setup\u201d), to ensure the correct implementations of BERT-PKD and DistilBERT, we firstly re-reproduced the reported results of baselines with 6-layer architecture, then we trained the baselines with 4-layer architecture by following the confirmed correct implementations, and evaluated them on the TEST set of official GLUE benchmarks. \n\nFor a direct comparisons with BERT-PKD and DistilBERT, we here also present the results of 6-layer TinyBERT with the same architecture as the original BERT-PKD (Sun et al., 2019) and original DistilBERT [1], and directly use the reported results of BERT-PKD and DistilBERT. As BERT-PKD and DistilBERT are evaluated on the test and dev set of GLUE, respectively. Thus, we present the results in the following two tables separately, and the results have been added to the Appendix E of our paper.\n\nTable: the comparisons between TinyBERT and BERT-PKD, and the results are evaluated on the test set of official GLUE tasks.\n-------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t                                SST-2         MRPC           QQP        MNLI-m    MNLI-mm    QNLI        RTE \n                                                                (67k)          (3.7k)          (364k)        (393k)         (393k)       (105k)      (2.5k)\n                                                                 acc           f1/acc          f1/acc          acc               acc             acc          acc\n-------------------------------------------------------------------------------------------------------------------------------------------------------\nBERT_6-PKD (Sun et al., 2019)           92.0        85.0/79.9     70.7/88.9      81.5              81.0           89.0         65.5\n(M=6; d\u2019=768;d\u2019_i=3072)                                                                                                                               \n------------------------------------------------------------------------------------------------------------------------------------------------------- \nTinyBERT                                               93.1        87.3/82.6     71.6/89.1      84.6              83.2           90.4         66.0 \n(M=6; d\u2019=768;d\u2019_i=3072)                                                                                                                             \n--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\nTable: the comparisons between TinyBERT with DistilBERT, and the results are evaluated on the dev set of GLUE tasks. Mcc refers to Matthews correlation and pear/spea refer to pearson/spearman.\n---------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t                 CoLA     MNLI     MNLI-mm      MRPC         QNLI        QQP         RTE      SST-2        STS-B \n                                                 mcc        acc            acc              f1/acc          acc         f1/acc        acc        acc       pear/spea \n--------------------------------------------------------------------------------------------------------------------------------------------------------------\nDistilBERT [1]                         42.5        81.6            81.1         88.3/82.4       85.5     87.7/90.6     60.0      92.7       84.5/85.0\n(M=6; d\u2019=768;d\u2019_i=3072)\n--------------------------------------------------------------------------------------------------------------------------------------------------------------\nTinyBERT                                54.0        84.5            84.5         90.6/86.3       91.1     88.0/91.1     70.4      93.0       90.1/89.6\n(M=6; d\u2019=768;d\u2019_i=3072)\n--------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[1] https://medium.com/huggingface/distilbert-8cf3380435b5 Accessed on 7 November 2019.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "rkgdlMtKoH", "original": null, "number": 6, "cdate": 1573650927789, "ddate": null, "tcdate": 1573650927789, "tmdate": 1573650927789, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "BJe6c-KFoB", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Response to Reviewer #3 [2/2]", "comment": "Q6. Why not reduce the parameter size by the vocabulary size?\n\nA6: Thanks for the good suggestion. Reducing vocabulary size and reducing hidden size are two orthogonal methods to reduce the number of parameters in embedding layers. In this paper, we focused on the technique of reducing hidden size, and it would be very interesting to combine the suggested methods to further reduce the size of embedding layers.\n\n\nQ7: clarify the construction of Table 2.\n\nA7: \nAll results in the Table 2 are evaluated on the TEST set of official GLUE benchmark. Both our TinyBERT and DistilBERT have the same number of layers 4 (M=4). The reported results of the original DistilBERT [1] are based on a 6-layer architecture and evaluated on the DEV set of GLUE tasks. And in the Table4, we compared to DistilBERT by directly using the reported results of DistilBERT paper on the DEV set, and our 6-layer TinyBERT has an average value, which is significantly better than that of DistilBERT (77.3 vs 71.9).  \n\nAs described in the Appendix B (Baselines setup), to obtain the results of 4-layer DistilBERT as reported in our paper, we firstly re-implemented the reported results of 6-layer DistilBERT with its released code to ensure our implementation procedure is correct. Then following the verified procedure, we trained a 4-layer DistilBERT as the baselines and evaluated it on the TEST set of official GLUE benchmark. \n\n[1] https://medium.com/huggingface/distilbert-8cf3380435b5 Accessed on 7 November 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "BJe6c-KFoB", "original": null, "number": 5, "cdate": 1573650836701, "ddate": null, "tcdate": 1573650836701, "tmdate": 1573650836701, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "HJeTtyRPtr", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Response to Reviewer #3 [1/2]", "comment": "Thank you for the helpful comments!\n\nReproducibility\n\n*** We will release the source code, all the models (including the general TinyBERT variants and task-specific TinyBERT models for each task in GLUE and SQuAD, so other researchers can easily reproduce the results in the paper), and all the training details for reproducibility, as soon as possible. ***\n\nQ1: Details for how the model has been trained.\n\nA1: \nWe have presented all the training details of TinyBERT and baselines in the Appendix B (TinyBERT setup and Baselines setup), which includes the datasets used, the number of update steps, the batch size as well as the settings for fine-tuning. \n\nOur TinyBERT and baselines use the same hyper-parameters and datasets at both the pre-training and fine-tuning stages. We here list the main setting details as follows and other details can be referred in Appendix B.\n\nTable: the hyper-parameters of DistilBERT, TinyBERT and BERT_small; BERT_PKD does not include the pre-training stage, we use the BERT_base released by google as the teacher.\n-----------------------------------------------------------------------------------------------------\n\t\t\t\t \t        At Pre-training Stage\n-----------------------------------------------------------------------------------------------------\nDataset\t\t             English Wikipedia (2,500 M words)\n-----------------------------------------------------------------------------------------------------\nTraining steps         \t\t     ~350k (3epoch)\n-----------------------------------------------------------------------------------------------------\nBatch size\t\t\t\t\t   256\n-----------------------------------------------------------------------------------------------------\nLearning rate\t\t\t          1e-4\n-----------------------------------------------------------------------------------------------------\nWeight decay \t\t\t\t  1e-4\n-----------------------------------------------------------------------------------------------------\n\n\nTable: the fine-tuning hyper-parameters of DistilBERT, TinyBERT, BERT_PKD, BERT_small and BERT_base. Max_seq_length refers to the maximum sequence length.\n---------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t \t\tLearning rate         batch size\t   Epoch      Max_seq_length\n---------------------------------------------------------------------------------------------------------------------------------\nMNLI (392k)\t\t\t\t     3e-5\t\t\t\t32\t\t\t3                  128\n--------------------------------------------------------------------------------------------------------------------------------\nQQP (363k)\t\t\t\t     3e-5\t\t\t\t32\t\t\t3                  128\n--------------------------------------------------------------------------------------------------------------------------------\nQNLI (108k) \t\t\t \t     3e-5\t\t\t\t32\t\t\t3                  128\n--------------------------------------------------------------------------------------------------------------------------------\nSST-2 (67k)\t\t\t             3e-5\t\t\t\t32\t\t\t3                   64\n--------------------------------------------------------------------------------------------------------------------------------\nCoLA (8.5k)\t\t\t             3e-5\t\t\t\t32\t\t\t3                   64\n-------------------------------------------------------------------------------------------------------------------------------\nSTS-B (5.7k) \t\t\t\t     3e-5\t\t\t\t32\t\t\t3                   64\n-------------------------------------------------------------------------------------------------------------------------------\nMRPC (3.5k)\t\t\t\t     3e-5\t\t\t\t32\t\t\t3                  128\n------------------------------------------------------------------------------------------------------------------------------\nRTE (2.5k) \t\t\t             3e-5\t\t\t\t32\t\t\t3                  128\n------------------------------------------------------------------------------------------------------------------------------\n\n\nQ2: Is the learning of the distilled model only done on the training dataset\n\nA2: As shown in the figure 2 and section 3.2, we described the methods of doing data augmentation in TinyBERT learning, the augmented dataset is merged with the original training dataset for the task-specific distillation. The effect of TinyBERT without DA (data augmentation) is presented in the Table 5 (the \u201cNo DA\u201d row) and section 4.4.\n\n\nQ3: Claim that model achieves comparable performance to BERT_base.\n\nA3: Thanks for the suggestion, we have changed the related claims.\n\n\nQ4: Was the BERT_small model tuned, or the same learning paramters from BERT_Base were used?\n\nA4: As described in the subsection \u201cBaselines setup\u201d of Appendix B, BERT_small and BERT_Base use the same hyper-parameters for learning.\n\n\nQ5: Can the authors clarify the inference time of BERT Small? \n\nA5: Yes, BERT_small and TinyBERT have the same architecture, thus they have the same inference time. We have added the inference time of BERT_small in Table 3.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "HygE_AOFiB", "original": null, "number": 4, "cdate": 1573650027892, "ddate": null, "tcdate": 1573650027892, "tmdate": 1573650027892, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "rJl7VCdFiH", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Response to Reviewer #2 [2/2]", "comment": "Thus, from the direct comparisons with the reported results in the original papers, we can see the TinyBERT outperforms the baselines (DistilBERT and BERT-PKD) under the same settings of architecture and evaluation, the effectiveness of TinyBERT is confirmed. \n\nMoreover, since BERT-PKD and DistilBERT need to initialize their student models with some layers of pre-trained teacher BERT, they have the limitations that the student models have to keep the same size settings of hidden size and feedforward/filter size as their teacher BERT. TinyBERT is initialized by general distillation, so it has the advantage of being more flexible in model size selection.\n\n[1] https://medium.com/huggingface/distilbert-8cf3380435b5 Accessed on 7 November 2019.\n\n\n\nQ2: Did authors try other values of lambda.\n\nA2: Yes, we have tried a different strategy at the early stage, which is formatted as $\\lambda_{m}=0.2*(1 + m), 0<=m<=M$, assigning larger weights to higher layers. The results are presented in the following table and show that the performances obtained by the proposed strategy on datasets CoLA, MNLI-m/mm and MRPC are worse than the ones obtained by the uniform strategy ($\\lambda_{m}=1$). We also find the knowledge distillation in CV often uses the uniform strategy [Romero et al., 2014], thus we move to the uniform strategy.\n\nTable: the comparisons between different values of lambda and the results are evaluated on dev set.\n----------------------------------------------------------------------------------------------------\n\t\t\t\t                        CoLA       MNLI-m      MNLI-mm       MRPC\n                                                         mcc          acc                 acc                 acc    \n----------------------------------------------------------------------------------------------------\nTinyBERT                                        49.7          82.8               82.9                85.8\n(M=4; d\u2019=312;d\u2019_i=1200)\n$\\lambda_{m}=1$\n---------------------------------------------------------------------------------------------------\nTinyBERT                                       43.7          80.7               81.4                 83.8\n(M=4; d\u2019=312;d\u2019_i=1200) \n$\\lambda_{m}=0.2*(1 + m)$\n---------------------------------------------------------------------------------------------------\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "rJl7VCdFiH", "original": null, "number": 3, "cdate": 1573649962844, "ddate": null, "tcdate": 1573649962844, "tmdate": 1573649962844, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "S1xOS2mL9r", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"title": "Response to Reviewer #2 [1/2]", "comment": "Thank you for the helpful comments!\n\nQ1: Experimental results are not easily comparable to prior work.\n\nA1:\n*** Comparison results as shown in Table 2, Table 3 and Table4 ***\nThe comparison results as shown in the Table 2 are all evaluated on the test set of the official GLUE tasks. As shown in the Table 3, our TinyBERT, baselines BERT-PKD and DistilBERT, all have the same number of layers (M=4), and our TinyBERT has a relatively challenging setting with smaller hidden size (d\u2019=312 vs d\u2019=768) and feedforward/filter size (d\u2019_i=1200 vs d\u2019_i=3072). If we increase the hidden size and feedforward/filter size of TinyBERT, it can obtain better performances, which is validated in our experiments in the Table 4 (wider TinyBERT variants achieve better results).\n\nIn the Table 4, we also directly compared the performances of TinyBERT, BERT-PKD and DistilBERT with the same architecture settings (M=6; d\u2019=768; d\u2019i=3072), and TinyBERT has significantly better performances. \n\n***More complete comparisons with the same student architecture ***\nFor complete and direct comparisons with prior works, we here also present the results of TinyBERT (M=6; d\u2019=768; d\u2019_i=3072) with the same architectures as used in the original BERT-PKD (Sun et al., 2019) and DistilBERT [1] papers. \n\nSince in the original papers, the BERT-PKD is evaluated on the TEST set, and the DistilBERT is evaluated on the DEV set. Thus, for a clear illustration, we present the results in the following two tables, separately, and the results have been added to the Appendix E of our paper.\n\nTable: the comparisons between TinyBERT and BERT-PKD, and the results are evaluated on the test set of official GLUE tasks.\n-------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t                                SST-2         MRPC           QQP        MNLI-m    MNLI-mm    QNLI        RTE \n                                                                (67k)          (3.7k)          (364k)        (393k)         (393k)       (105k)      (2.5k)\n                                                                 acc           f1/acc          f1/acc          acc               acc             acc          acc\n-------------------------------------------------------------------------------------------------------------------------------------------------------\nBERT_6-PKD (Sun et al., 2019)           92.0        85.0/79.9     70.7/88.9      81.5              81.0           89.0         65.5\n(M=6; d\u2019=768;d\u2019_i=3072)                                                                                                                               \n------------------------------------------------------------------------------------------------------------------------------------------------------- \nTinyBERT                                               93.1        87.3/82.6     71.6/89.1      84.6             83.2            90.4         66.0 \n(M=6; d\u2019=768;d\u2019_i=3072)                                                                                                                             \n--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n\nTable: the comparisons between TinyBERT with DistilBERT, and the results are evaluated on the dev set of GLUE tasks. Mcc refers to Matthews correlation and pear/spea refer to pearson/spearman.\n---------------------------------------------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t                 CoLA     MNLI     MNLI-mm      MRPC         QNLI        QQP         RTE      SST-2        STS-B \n                                                 mcc        acc            acc              f1/acc          acc         f1/acc        acc        acc       pear/spea \n--------------------------------------------------------------------------------------------------------------------------------------------------------------\nDistilBERT [1]                        42.5        81.6            81.1         88.3/82.4       85.5     87.7/90.6     60.0      92.7       84.5/85.0\n(M=6; d\u2019=768;d\u2019_i=3072)\n--------------------------------------------------------------------------------------------------------------------------------------------------------------\nTinyBERT                               54.0        84.5            84.5         90.6/86.3       91.1     88.0/91.1     70.4      93.0       90.1/89.6\n(M=6; d\u2019=768;d\u2019_i=3072)\n--------------------------------------------------------------------------------------------------------------------------------------------------------------"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "HJeTtyRPtr", "original": null, "number": 1, "cdate": 1571442564745, "ddate": null, "tcdate": 1571442564745, "tmdate": 1572972591431, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors propose TinyBERT, a smaller version of BERT that is trained with knowledge distillation. The authors evaluate on the GLUE benchmark.\n\nOverall, I find the direction of this work exciting and making these large models smaller for practical use is an important research area. The authors provide various ablation experiments that provide insight into their method. The main contribution is experiments comparing various existing distillation methods to different parts of the model (embeddings, layers, prediction layer), so is not particularly novel in contributing new techniques for distillation. That being said, there is importance in contributing these results as they are very useful for others working in the area and on making smaller models. But I would expect the authors to be much more detailed in their experimental description and make it clear in the paper that the comparative baselines are fair and well tuned. \n\nComments:\n\n1. Can the authors please add details for how the model has been trained, such as the datasets used, the number of update steps, the batch size, etc. as well as the finetuning parameters that were cross validated for GLUE? It is difficult to tell in the current setting if the models are comparable to the baselines. The current paper doesn't seem like it could be reproduced. It is particularly important to detail how the finetuning was done, as this is very important for the smaller datasets in GLUE.\n\n2. Is the learning of the distilled model only done on the training dataset, or there is data augmentation beyond the training set? What is the effect without data augmentation?\n\n3. Unfortunately, the performance drop on the GLUE benchmark as shown in Table 2 is fairly large. The authors compare to BERT Small and DistilBERT and I like the baselines, but the claim that the model achieves comparable performance to BERT Base is not true. \n\n4. Was the BERT Small model tuned, or the same learning parameters from BERT Base were used? \n\n5. Can the authors clarify the inference time of BERT Small? The speed improvement of TinyBERT should be the same as BERT Small based on parameter size.\n\n6. The authors experiment with distilling the embedding layer to reduce the number of parameters, why not reduce the parameter size by reducing the vocabulary size? Existing approaches to BERT training use BPE with ~30k vocabulary size or RoBERTa with ~50k vocabulary size, but large gains could be applied here by reducing the size or using softmax reduction techniques that were popular on full vocabulary language modeling datasets like wikitext-103 or billion word. \n\n7. Can the authors please clarify the construction of Table 2? Are those results on the test set (e.g. evaluated on the official GLUE benchmark), or on the dev set? Where are the DistilBERT numbers on the test set coming from, as it is not reported in their paper? "}, "signatures": ["ICLR.cc/2020/Conference/Paper469/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575050152219, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper469/Reviewers"], "noninvitees": [], "tcdate": 1570237751675, "tmdate": 1575050152229, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Review"}}}, {"id": "S1xOS2mL9r", "original": null, "number": 3, "cdate": 1572383807617, "ddate": null, "tcdate": 1572383807617, "tmdate": 1572972591398, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "What is the task?\nKnowledge distillation of BERT\n\nWhat has been done before?\nUnlike prior works such as Distilled BiLSTMSOFT (Tang et al., 2019), BERT-PKD (Sun et al., 2019) and DistilBERT, this work\n\ni) Do knowledge distillation at pre training stage also in addition to fine tuning stage.\nii) Student learns from all - embedding layers, attention matrices, hidden states, and final prediction layers. \n\nIn BERT-PKD, student learns from the [CLS]  hidden states of the teacher.\n\nWhat are the main contributions of the paper?\nNovel Transformer distillation method that is specially designed for knowledge distillation of the Transformer-based models.\nNovel two-stage learning framework which performs Transformer distillation at both the pre-training and task-specific learning stages\nResulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state-of-the-art baselines on BERT distillation.\n\nWhat are the key techniques used to tackle this task?\nNovel Transformer distillation method that is specially designed for knowledge distillation of the Transformer-based models.\nNovel two-stage learning framework which performs Transformer distillation at both the pre-training and task-specific learning stages\n\nWhat are the main results? Are they significant?\nResulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state-of-the-art baselines on BERT distillation with only \u223c28% parameters and \u223c31% inference time of them.\n\nResults show that three key procedures: TD (Task-specific Distillation), GD (General Distillation) and DA (Data Augmentation) are crucial for the proposed KD method.\n\nProposed distillation objectives - Transformer-layer distillation (attention matrices and hidden states), embedding-layer distillation and prediction layer distillation  are crucial for the proposed KD method.\n\nWeaknesses\nexperimental results were not easily comparable to prior work so it is hard to say if claims are well-supported experimental results\n\nQuestions\nDid authors try other values of lambda\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575050152219, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper469/Reviewers"], "noninvitees": [], "tcdate": 1570237751675, "tmdate": 1575050152229, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Review"}}}, {"id": "HJgUIGFkOH", "original": null, "number": 1, "cdate": 1569849934161, "ddate": null, "tcdate": 1569849934161, "tmdate": 1569849934161, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "BJlsn066wH", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment", "content": {"comment": "Thanks for your comments!\nIn the proposed two-stage learning framework, four different distillations (including the prediction-layer distillation) are considered at both the pre-training and fine-tuning stage, more details can be found in Table-1. Our initial motivation for general distillation is to make the student TinyBERT learn the intermediate structures of teacher BERT at the pre-training stage. Moreover, from our preliminary experiments, we also found that conducting \u201cprediction-layer distillation\u201d at the pre-training stage would not bring extra improvements on downstream tasks, when the Transformer-layer distillation (Attn and Hidn distillation) and Embedding-layer distillation have already been performed. Therefore, we did not include the prediction-layer distillation at the pre-training stage and this setting is followed in the ablation study.", "title": "Why not conducting prediction-layer distillation at pre-training stage"}, "signatures": ["ICLR.cc/2020/Conference/Paper469/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper469/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper469/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper469/Authors|ICLR.cc/2020/Conference/Paper469/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171027, "tmdate": 1576860561284, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Official_Comment"}}}, {"id": "BJlsn066wH", "original": null, "number": 1, "cdate": 1569738419309, "ddate": null, "tcdate": 1569738419309, "tmdate": 1569738419309, "tddate": null, "forum": "rJx0Q6EFPB", "replyto": "rJx0Q6EFPB", "invitation": "ICLR.cc/2020/Conference/Paper469/-/Public_Comment", "content": {"comment": "The general distillation doesn't use prediction-layer distillation loss. Why? The pre-training objectives seem also suitable for it.\nIn ablation study, if Transformer-layer distillation is removed in the general distillation, the student only learns from the teacher's embedding layer and can learn nearly nothing, which definitely hurts the pre-training of student a lot and is not fair.", "title": "The ablation study on Transformer-layer distillation"}, "signatures": ["~Xianghao_Tang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xianghao_Tang1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jiaoxiaoqi@hust.edu.cn", "yinyichun@huawei.com", "shang.lifeng@huawei.com", "jiang.xin@huawei.com", "chen.xiao2@huawei.com", "lynn.lilinlin@huawei.com", "wangfang@hust.edu.cn", "qun.liu@huawei.com"], "title": "TinyBERT: Distilling BERT for Natural Language Understanding", "authors": ["Xiaoqi Jiao", "Yichun Yin", "Lifeng Shang", "Xin Jiang", "Xiao Chen", "Linlin Li", "Fang Wang", "Qun Liu"], "pdf": "/pdf/78cd235085142ae08db1f2413dd4ff31e2157f79.pdf", "abstract": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be well transferred to a small \u201cstudent\u201d TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general domain as well as the task-specific knowledge in BERT. TinyBERT is empirically effective and achieves comparable results with BERT on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only \u223c28% parameters and \u223c31% inference time of them.\n", "keywords": ["BERT Compression", "Transformer Distillation", "TinyBERT"], "paperhash": "jiao|tinybert_distilling_bert_for_natural_language_understanding", "original_pdf": "/attachment/a570ba3dbb4f34154564206b123cb818859a8013.pdf", "_bibtex": "@misc{\njiao2020tinybert,\ntitle={Tiny{\\{}BERT{\\}}: Distilling {\\{}BERT{\\}} for Natural Language Understanding},\nauthor={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rJx0Q6EFPB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJx0Q6EFPB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504208664, "tmdate": 1576860594323, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper469/Authors", "ICLR.cc/2020/Conference/Paper469/Reviewers", "ICLR.cc/2020/Conference/Paper469/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper469/-/Public_Comment"}}}], "count": 17}