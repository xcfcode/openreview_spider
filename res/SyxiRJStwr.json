{"notes": [{"id": "SyxiRJStwr", "original": "r1lO0w1YPH", "number": 2038, "cdate": 1569439699446, "ddate": null, "tcdate": 1569439699446, "tmdate": 1577168273565, "tddate": null, "forum": "SyxiRJStwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Dynamic Scale Inference by Entropy Minimization", "authors": ["Dequan Wang", "Evan Shelhamer", "Bruno Olshausen", "Trevor Darrell"], "authorids": ["dqwang@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "baolshausen@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": ["unsupervised learning", "dynamic inference", "equivariance", "entropy"], "TL;DR": "Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.", "abstract": "Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "pdf": "/pdf/51de7d9941984832652175cb0737adfed5c11a97.pdf", "paperhash": "wang|dynamic_scale_inference_by_entropy_minimization", "original_pdf": "/attachment/51de7d9941984832652175cb0737adfed5c11a97.pdf", "_bibtex": "@misc{\nwang2020dynamic,\ntitle={Dynamic Scale Inference by Entropy Minimization},\nauthor={Dequan Wang and Evan Shelhamer and Bruno Olshausen and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxiRJStwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LhxIGI1UJB", "original": null, "number": 1, "cdate": 1576798738895, "ddate": null, "tcdate": 1576798738895, "tmdate": 1576800897444, "tddate": null, "forum": "SyxiRJStwr", "replyto": "SyxiRJStwr", "invitation": "ICLR.cc/2020/Conference/Paper2038/-/Decision", "content": {"decision": "Reject", "comment": "This paper constitutes interesting progress on an important problem.  I urge the authors to continue to refine their investigations, with the help of the reviewer comments; e.g., the quantitative analysis recommended by AnonReviewer4.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Scale Inference by Entropy Minimization", "authors": ["Dequan Wang", "Evan Shelhamer", "Bruno Olshausen", "Trevor Darrell"], "authorids": ["dqwang@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "baolshausen@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": ["unsupervised learning", "dynamic inference", "equivariance", "entropy"], "TL;DR": "Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.", "abstract": "Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "pdf": "/pdf/51de7d9941984832652175cb0737adfed5c11a97.pdf", "paperhash": "wang|dynamic_scale_inference_by_entropy_minimization", "original_pdf": "/attachment/51de7d9941984832652175cb0737adfed5c11a97.pdf", "_bibtex": "@misc{\nwang2020dynamic,\ntitle={Dynamic Scale Inference by Entropy Minimization},\nauthor={Dequan Wang and Evan Shelhamer and Bruno Olshausen and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxiRJStwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SyxiRJStwr", "replyto": "SyxiRJStwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725731, "tmdate": 1576800277689, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2038/-/Decision"}}}, {"id": "ryxrlQ_Lcr", "original": null, "number": 3, "cdate": 1572401900639, "ddate": null, "tcdate": 1572401900639, "tmdate": 1574383166877, "tddate": null, "forum": "SyxiRJStwr", "replyto": "SyxiRJStwr", "invitation": "ICLR.cc/2020/Conference/Paper2038/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "Summary:\nThe following work proposes a test-time optimization over scales to improve semantic segmentation. Specifically, at test time, they iteratively optimize over the score and scale parameters of Shellhamer et al 2019, where a Gaussian receptive field is used to allow for dynamic scale adaptation of each convolutional layer. They optimize the parameters with respect to an entropy minimization objective. Experiments on PASCAL VOC, reported at multiple scales, demonstrate improvements in IOU over the vanilla architecture.\n\nStrengths:\n-The work was well-motivated\n-Formulation is pretty elegant and outperforms the baseline\n\nWeaknesses:\nWhile I liked the somewhat elegant formulation of dynamic test-time scaling proposed by the following work, I don't think this work introduced many novel results nor insights\n-Multiscale test-time inference is already standard in state-of-the-art architectures such as DeepLab. Specifically, DeepLab runs inference at multiple scales, then max pools the logit responses across all scales. (see Table 3 of https://arxiv.org/pdf/1511.03339.pdf)\n-Using entropy as a measure of network uncertainty is a good idea, but also not a novel finding.\n-COCO and Cityscapes would probably have been better choices for datasets with larger variations in scale\n\nImprovements:\n-Try comparing against the multiscale logit-max-pooling inference procedure as a baseline -- or demonstrate that the proposed technique can further improve upon the results of the logit-pooling technique.\n-Some details weren't very well explained. Specifically, what is the $\\theta_{score}$ task parameter for?\n\n** Post Rebuttal Response\nI'd like to thank the authors for clarifying some points in their response. Overall, I maintain that I think the optimization-based scale inference solution they present is interesting from an implementation standpoint, but the findings in this work did not yield sufficient new insight for the task. While I agree that their approach is fairly different from common approaches such as discrete image pyramids, a thorough quantitative comparison of these differences would make this work a lot stronger. As such, I maintain my original rating.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2038/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2038/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Scale Inference by Entropy Minimization", "authors": ["Dequan Wang", "Evan Shelhamer", "Bruno Olshausen", "Trevor Darrell"], "authorids": ["dqwang@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "baolshausen@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": ["unsupervised learning", "dynamic inference", "equivariance", "entropy"], "TL;DR": "Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.", "abstract": "Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "pdf": "/pdf/51de7d9941984832652175cb0737adfed5c11a97.pdf", "paperhash": "wang|dynamic_scale_inference_by_entropy_minimization", "original_pdf": "/attachment/51de7d9941984832652175cb0737adfed5c11a97.pdf", "_bibtex": "@misc{\nwang2020dynamic,\ntitle={Dynamic Scale Inference by Entropy Minimization},\nauthor={Dequan Wang and Evan Shelhamer and Bruno Olshausen and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxiRJStwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyxiRJStwr", "replyto": "SyxiRJStwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783331531, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2038/Reviewers"], "noninvitees": [], "tcdate": 1570237728662, "tmdate": 1574783331542, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2038/-/Official_Review"}}}, {"id": "SkeTFntooH", "original": null, "number": 4, "cdate": 1573784709191, "ddate": null, "tcdate": 1573784709191, "tmdate": 1573784709191, "tddate": null, "forum": "SyxiRJStwr", "replyto": "rJeSrHy0FB", "invitation": "ICLR.cc/2020/Conference/Paper2038/-/Official_Comment", "content": {"title": "Multi-scale Training and Deformation are Included and Improved On (Plus: How to End Optimization)", "comment": "Thank you for your feedback and consideration of alternatives for addressing scale.\n\n> this issue can be addressed through multi-scale training/testing or the deformable kernels\n\nWe do multi-scale training with data augmentation (see \"w/ aug\" results). The improvement by optimization during inference is on top of a base model trained this way. Multi-scale testing requires knowledge of the testing scale, or enumeration of several scales, while the purpose of our method is to select the unknown scale.\n\nThe base method \"scale regression\" does include deformable kernels, in particular scale deformations with Gaussian structure (Shelhamer et al. 2019). The improvement due to optimization is on top of this adaptation by prediction, showing the need for further adaptation by optimization. Please see Figure 4 for the comparison of scale deformation by prediction and optimization.\n\n> optimization process may take [more time] when compared to other scale processing methods like deformable kernels\n\nWe appreciate the accuracy and efficiency of deformable kernels. Our experiments show that they are however limited, in that the predictor for the deformation can only generalize so far. Optimization takes more time, but it does so to improve the results.\n\nNote that multi-scale testing also takes more time. Since computation scales as the product of input dimensions,, a 2x larger input would cost ~4x the time, and be comparable or more expensive than our optimization.\n\n> The number of optimization steps may be hard to control\n> [is there] a more elegant way to decide when to end the optimization process?\n\nWe agree that a more adaptive rule for ending optimization would be preferable. In a rebuttal experiment on a simpler ResNet-50 FCN, we achieved similar results by choosing the number of iterations or choosing a relative tolerance on the change in the entropy across iterations. This tolerance could be more transferrable than a fixed number of steps."}, "signatures": ["ICLR.cc/2020/Conference/Paper2038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Scale Inference by Entropy Minimization", "authors": ["Dequan Wang", "Evan Shelhamer", "Bruno Olshausen", "Trevor Darrell"], "authorids": ["dqwang@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "baolshausen@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": ["unsupervised learning", "dynamic inference", "equivariance", "entropy"], "TL;DR": "Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.", "abstract": "Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "pdf": "/pdf/51de7d9941984832652175cb0737adfed5c11a97.pdf", "paperhash": "wang|dynamic_scale_inference_by_entropy_minimization", "original_pdf": "/attachment/51de7d9941984832652175cb0737adfed5c11a97.pdf", "_bibtex": "@misc{\nwang2020dynamic,\ntitle={Dynamic Scale Inference by Entropy Minimization},\nauthor={Dequan Wang and Evan Shelhamer and Bruno Olshausen and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxiRJStwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyxiRJStwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference/Paper2038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2038/Reviewers", "ICLR.cc/2020/Conference/Paper2038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2038/Authors|ICLR.cc/2020/Conference/Paper2038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147223, "tmdate": 1576860538169, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference/Paper2038/Reviewers", "ICLR.cc/2020/Conference/Paper2038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2038/-/Official_Comment"}}}, {"id": "HygQr3tosr", "original": null, "number": 3, "cdate": 1573784634567, "ddate": null, "tcdate": 1573784634567, "tmdate": 1573784673558, "tddate": null, "forum": "SyxiRJStwr", "replyto": "S1x7EQ7AFr", "invitation": "ICLR.cc/2020/Conference/Paper2038/-/Official_Comment", "content": {"title": "Optimizing Scale and Score, State-of-the-Art, and Simple Parameter Transformations", "comment": "Thank you for the feedback and careful consideration of the optimization problem.\n\n> the prior reason for adapting \\theta_{score} is less obvious and would require further explanation\n\nFor completeness, we try adapting both scale and score. The purpose of adapting score is to further fit the model to the appearance of each test input. This can be seen as a way to add modeling capacity, since every test input has its own classifier parameters, instead of sharing the same parameters across all inputs.\n\n> current or recent state-of-the-art for the considered dataset\n\nThe current state-of-the-art for PASCAL VOC is DeepLabv3+ [1] at a commanding 89% IoU. Many factors contribute to its high accuracy, including supervision, optimization, and inference-time augmentation that are independent of scale.\n\nWe choose DLA for the relevance of its skip connections and multi-scale feature pyramid, to show that further scale adaptation still helps.\n\n> Wouldn\u2019t simply multiplying \\theta_{score} by a large number decrease the entropy\n> Similarly, couldn\u2019t the adversary simply rotate \\theta_{score} to reduce IoU?\n\nWhile there are simple parameter transformations to decrease entropy or reduce accuracy, that does not mean they are simple to optimize, and empirically these cases do not happen. We do not regularize by weight decay during inference to keep from multiplying the score parameters. We do optimize the adversary until the loss converges (but note that IoU might not drop to zero becauses ties are broken by class order, with background first, so if the parameters are driven to zero the output will be fully background and be partially correct).\n\n> improvement is ~2 points on average, not ~2 points for each scale\n\nThank you for your precision. We will rephrase this accordingly.\n\n[1] https://arxiv.org/abs/1802.02611"}, "signatures": ["ICLR.cc/2020/Conference/Paper2038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Scale Inference by Entropy Minimization", "authors": ["Dequan Wang", "Evan Shelhamer", "Bruno Olshausen", "Trevor Darrell"], "authorids": ["dqwang@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "baolshausen@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": ["unsupervised learning", "dynamic inference", "equivariance", "entropy"], "TL;DR": "Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.", "abstract": "Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "pdf": "/pdf/51de7d9941984832652175cb0737adfed5c11a97.pdf", "paperhash": "wang|dynamic_scale_inference_by_entropy_minimization", "original_pdf": "/attachment/51de7d9941984832652175cb0737adfed5c11a97.pdf", "_bibtex": "@misc{\nwang2020dynamic,\ntitle={Dynamic Scale Inference by Entropy Minimization},\nauthor={Dequan Wang and Evan Shelhamer and Bruno Olshausen and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxiRJStwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyxiRJStwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference/Paper2038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2038/Reviewers", "ICLR.cc/2020/Conference/Paper2038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2038/Authors|ICLR.cc/2020/Conference/Paper2038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147223, "tmdate": 1576860538169, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference/Paper2038/Reviewers", "ICLR.cc/2020/Conference/Paper2038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2038/-/Official_Comment"}}}, {"id": "HyxxXnYssB", "original": null, "number": 2, "cdate": 1573784600246, "ddate": null, "tcdate": 1573784600246, "tmdate": 1573784600246, "tddate": null, "forum": "SyxiRJStwr", "replyto": "ryxrlQ_Lcr", "invitation": "ICLR.cc/2020/Conference/Paper2038/-/Official_Comment", "content": {"title": "Multi-scale Testing, Novelty of Entropy for Inference-Time Optimization, and Task Parameters", "comment": "Thank you for your feedback and careful consideration of inference across scales.\n\n> Multiscale test-time inference is already standard in state-of-the-art architectures\n\nThe choice of enumeration by pyramid or selection by adaption is both practical and philosophical. Practically speaking, pyramids are common for deep learning in vision, but selection is not, so a point of this work is to show that selection (especially by optimization) is worthy of more attention. In the paper referenced by the review, Table 3 shows a 1-2 point gain by pyramid, which is comparable to our 2 point gain by a substantially different route. More philosophically, a pyramid is discrete and fixed while our optimization is continuous and adaptive.\n\n> Using entropy as a measure of network uncertainty is a good idea, but also not a novel finding\n\nEntropy is certainly a well-established measure of uncertainty. The novelty of our method is its use for optimization during inference: to the best of our knowledge, ours is the first method to adapt to each testing input by entropy minimization.\n\n> what is the task parameter for?\n\nThe task parameters are the parameters of the output layer (also known as the \"score\" layer). By optimizing over scale and score parameters, the method can adjust geometry and appearance to minimize entropy. In Table 3 we evaluate optimizing only scale, only score, or both and find that both can help."}, "signatures": ["ICLR.cc/2020/Conference/Paper2038/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Scale Inference by Entropy Minimization", "authors": ["Dequan Wang", "Evan Shelhamer", "Bruno Olshausen", "Trevor Darrell"], "authorids": ["dqwang@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "baolshausen@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": ["unsupervised learning", "dynamic inference", "equivariance", "entropy"], "TL;DR": "Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.", "abstract": "Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "pdf": "/pdf/51de7d9941984832652175cb0737adfed5c11a97.pdf", "paperhash": "wang|dynamic_scale_inference_by_entropy_minimization", "original_pdf": "/attachment/51de7d9941984832652175cb0737adfed5c11a97.pdf", "_bibtex": "@misc{\nwang2020dynamic,\ntitle={Dynamic Scale Inference by Entropy Minimization},\nauthor={Dequan Wang and Evan Shelhamer and Bruno Olshausen and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxiRJStwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SyxiRJStwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference/Paper2038/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2038/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2038/Reviewers", "ICLR.cc/2020/Conference/Paper2038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2038/Authors|ICLR.cc/2020/Conference/Paper2038/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147223, "tmdate": 1576860538169, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2038/Authors", "ICLR.cc/2020/Conference/Paper2038/Reviewers", "ICLR.cc/2020/Conference/Paper2038/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2038/-/Official_Comment"}}}, {"id": "rJeSrHy0FB", "original": null, "number": 1, "cdate": 1571841340810, "ddate": null, "tcdate": 1571841340810, "tmdate": 1572972391243, "tddate": null, "forum": "SyxiRJStwr", "replyto": "SyxiRJStwr", "invitation": "ICLR.cc/2020/Conference/Paper2038/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper focused on the problem of semantic segmentation. The author proposed to minimize the output entropy to dynamically predict the scales when doing inference. The entropy minimization strategy is achieved by iterative optimization. Experimental results are reported on the PASCAL VOC dataset.\n\nClarity:\nI think this paper is moderate. The idea of dynamically predicting the scale or receptive field is interesting. However, this issue can be addressed through multi-scale training/testing or the deformable kernels. The experimental results are not that convincing. The method is only evaluated on one dataset and one backbone. The paper could be improved with more convincing experiments.\n\nLimitations:\nThe optimization process may take a certain number of forward and backward steps. In Sec 3.2 the author shows this will introduce 3x more time, this will much decrease its popularity when compared to other scale processing methods like deformable kernels.\n\nExperiments:\n1. The proposed method is evaluated on the PASCAL VOC dataset with the DLA segmentation backbone. The chosen backbone is not as strong as the most popular frameworks like DeepLab and PSPNet. Thus the baseline results as shown in Table 1 are not that high. I would like to see the relative improvements introduced by the proposed method over a stronger baseline.\n\n2. The experimental dataset is PASCAL VOC only. I would be more convinced with more datasets like Cityscapes or ADE20K.\n\n3. The reported experimental results are with models trained on a narrow range of scales. What the results and relative improvements would be if trained with regular multi-scales like [0.5, 2.0]? Will the scale issue be easily addressed by a multi-scales training strategy?\n\n4. The number of optimization steps may be hard to control, 32 is used for DLA on PASCAL VOC. Will this number be changed for different models on different datasets? If yes, can the author find a more elegant way to decide when to end the optimization process?\n\nMisc:\nIt is better to give a brief introduction of structure parameters scale and dynamic Gaussian receptive fields as in Sec 2.3."}, "signatures": ["ICLR.cc/2020/Conference/Paper2038/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2038/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Scale Inference by Entropy Minimization", "authors": ["Dequan Wang", "Evan Shelhamer", "Bruno Olshausen", "Trevor Darrell"], "authorids": ["dqwang@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "baolshausen@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": ["unsupervised learning", "dynamic inference", "equivariance", "entropy"], "TL;DR": "Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.", "abstract": "Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "pdf": "/pdf/51de7d9941984832652175cb0737adfed5c11a97.pdf", "paperhash": "wang|dynamic_scale_inference_by_entropy_minimization", "original_pdf": "/attachment/51de7d9941984832652175cb0737adfed5c11a97.pdf", "_bibtex": "@misc{\nwang2020dynamic,\ntitle={Dynamic Scale Inference by Entropy Minimization},\nauthor={Dequan Wang and Evan Shelhamer and Bruno Olshausen and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxiRJStwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyxiRJStwr", "replyto": "SyxiRJStwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783331531, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2038/Reviewers"], "noninvitees": [], "tcdate": 1570237728662, "tmdate": 1574783331542, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2038/-/Official_Review"}}}, {"id": "S1x7EQ7AFr", "original": null, "number": 2, "cdate": 1571857195060, "ddate": null, "tcdate": 1571857195060, "tmdate": 1572972391198, "tddate": null, "forum": "SyxiRJStwr", "replyto": "SyxiRJStwr", "invitation": "ICLR.cc/2020/Conference/Paper2038/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose a method to dynamically adapt some structural features of a semantic image segmentation model at inference time based on the entropy of the predictions.\n\nUsing a model that explicitly controls the size of the filters at each layer, they show that running a small number of SGD steps on the scale and final prediction parameters in the last layer to minimize the entropy of least confident predictions for a specific example leads to better performance overall, and especially better generalization when there is a size discrepancy between training and test set.\n\nStrengths: The method is inspired, and leads to significant improvements. The dynamic inference setup is clearly explained, and well motivated for the case of the scale parameters. Extensive ablation experiments and the inclusion of an oracle system help understand the contributions of each component of the setup, and the potential of inference-time optimization of the considered parameters.\n\nWeaknesses: Some information is missing from the description of the experimental setting. A quick review of the DLA model would be welcome, to get a better sense of the roles of \\theta_{scale} and \\theta_{score}. The authors should include published numbers for a relevant baseline and the current or recent state-of-the-art for the considered dataset (Table 1should also report 1x numbers in both settings). Finally, while the authors make a strong case for dynamic adaptation of the scale parameters, the prior reason for adapting \\theta_{score} is less obvious and would require further explanation.\n\nQuestions and miscellaneous remarks:\n\u201cAs reported in Table 1, our method consistently improves on the baseline by \u223c2 points for all scales\u201d > this statement is a little misleading, since the improvement is ~2 points on average, not ~2 points for each scale.\n\nWouldn\u2019t simply multiplying \\theta_{score} by a large number decrease the entropy of the predictions? Do you do anything to prevent that from happening?\n\nSimilarly, couldn\u2019t the adversary simply rotate \\theta_{score} to reduce IoU? Is the adversary optimized for long enough?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2038/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2038/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Scale Inference by Entropy Minimization", "authors": ["Dequan Wang", "Evan Shelhamer", "Bruno Olshausen", "Trevor Darrell"], "authorids": ["dqwang@eecs.berkeley.edu", "shelhamer@cs.berkeley.edu", "baolshausen@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": ["unsupervised learning", "dynamic inference", "equivariance", "entropy"], "TL;DR": "Unsupervised optimization during inference gives top-down feedback to iteratively adjust feedforward prediction of scale variation for more equivariant recognition.", "abstract": "Given the variety of the visual world there is not one true scale for recognition: objects may appear at drastically different sizes across the visual field. Rather than enumerate variations across filter channels or pyramid levels, dynamic models locally predict scale and adapt receptive fields accordingly. The degree of variation and diversity of inputs makes this a difficult task. Existing methods either learn a feedforward predictor, which is not itself totally immune to the scale variation it is meant to counter, or select scales by a fixed algorithm, which cannot learn from the given task and data. We extend dynamic scale inference from feedforward prediction to iterative optimization for further adaptivity. We propose a novel entropy minimization objective for inference and optimize over task and structure parameters to tune the model to each input. Optimization during inference improves semantic segmentation accuracy and generalizes better to extreme scale variations that cause feedforward dynamic inference to falter.", "pdf": "/pdf/51de7d9941984832652175cb0737adfed5c11a97.pdf", "paperhash": "wang|dynamic_scale_inference_by_entropy_minimization", "original_pdf": "/attachment/51de7d9941984832652175cb0737adfed5c11a97.pdf", "_bibtex": "@misc{\nwang2020dynamic,\ntitle={Dynamic Scale Inference by Entropy Minimization},\nauthor={Dequan Wang and Evan Shelhamer and Bruno Olshausen and Trevor Darrell},\nyear={2020},\nurl={https://openreview.net/forum?id=SyxiRJStwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SyxiRJStwr", "replyto": "SyxiRJStwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2038/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783331531, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2038/Reviewers"], "noninvitees": [], "tcdate": 1570237728662, "tmdate": 1574783331542, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2038/-/Official_Review"}}}], "count": 8}