{"notes": [{"id": "HyxAfnA5tm", "original": "rJ0ngAqF7", "number": 1315, "cdate": 1538087958132, "ddate": null, "tcdate": 1538087958132, "tmdate": 1550863696002, "tddate": null, "forum": "HyxAfnA5tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.", "keywords": ["meta-learning", "model-based", "reinforcement learning", "online learning", "adaptation"], "authorids": ["nagaban2@berkeley.edu", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Anusha Nagabandi", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/e94b677836dd7a8a5812d22eedbaf788394bf433.pdf", "paperhash": "nagabandi|deep_online_learning_via_metalearning_continual_adaptation_for_modelbased_rl", "_bibtex": "@inproceedings{\nnagabandi2018deep,\ntitle={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},\nauthor={Anusha Nagabandi and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxAfnA5tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJgjx5y-xE", "original": null, "number": 1, "cdate": 1544776179332, "ddate": null, "tcdate": 1544776179332, "tmdate": 1545354522821, "tddate": null, "forum": "HyxAfnA5tm", "replyto": "HyxAfnA5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1315/Meta_Review", "content": {"metareview": "The reviewers appreciated this contribution, particularly its ability to tackle nonstationary domains which are common in real-world tasks. \n\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Solid contribution, relevant to some interesting real world settings "}, "signatures": ["ICLR.cc/2019/Conference/Paper1315/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1315/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.", "keywords": ["meta-learning", "model-based", "reinforcement learning", "online learning", "adaptation"], "authorids": ["nagaban2@berkeley.edu", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Anusha Nagabandi", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/e94b677836dd7a8a5812d22eedbaf788394bf433.pdf", "paperhash": "nagabandi|deep_online_learning_via_metalearning_continual_adaptation_for_modelbased_rl", "_bibtex": "@inproceedings{\nnagabandi2018deep,\ntitle={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},\nauthor={Anusha Nagabandi and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxAfnA5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1315/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352882982, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxAfnA5tm", "replyto": "HyxAfnA5tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1315/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1315/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1315/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352882982}}}, {"id": "Syx2MNoERQ", "original": null, "number": 4, "cdate": 1542923284432, "ddate": null, "tcdate": 1542923284432, "tmdate": 1542923284432, "tddate": null, "forum": "HyxAfnA5tm", "replyto": "Hkg_xYA3n7", "invitation": "ICLR.cc/2019/Conference/-/Paper1315/Official_Comment", "content": {"title": "Thank you for your review!", "comment": "Thank you for your review. We added an appendix to the paper that addresses your question, and we have also added this information (as well as illustrative videos) to the project website. To illustrate results with less meta-training data, we have evaluated the test-time performance of models from various meta-training iterations, showing that performance does indeed improve with more meta-training data. To clarify, this statement of performance improving with meta-training data is different from the statement in the text regarding online updating the meta-learner not improving results. We meant that incorporating the EM weight updates during meta-training did not improve results, but we did not mean that additional meta-learning was harmful. We added text at the end of section 5 in the updated paper to reduce the potential for confusion. \n\nRegarding the amount of data used, the number of datapoints used during metatraining on each of the agents in our experiments is 382,000: This is 12 iterations of alternating model training plus on-policy rollouts, where each iteration collects data from 16 different environment settings, and each setting consists of 2000 datapoints. At a simulator timestep of 0.02sec/step, this sample complexity converts to around only 2 hours of real-world data."}, "signatures": ["ICLR.cc/2019/Conference/Paper1315/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1315/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.", "keywords": ["meta-learning", "model-based", "reinforcement learning", "online learning", "adaptation"], "authorids": ["nagaban2@berkeley.edu", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Anusha Nagabandi", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/e94b677836dd7a8a5812d22eedbaf788394bf433.pdf", "paperhash": "nagabandi|deep_online_learning_via_metalearning_continual_adaptation_for_modelbased_rl", "_bibtex": "@inproceedings{\nnagabandi2018deep,\ntitle={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},\nauthor={Anusha Nagabandi and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxAfnA5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1315/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615328, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxAfnA5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference/Paper1315/Reviewers", "ICLR.cc/2019/Conference/Paper1315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1315/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1315/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1315/Authors|ICLR.cc/2019/Conference/Paper1315/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1315/Reviewers", "ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference/Paper1315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615328}}}, {"id": "HylGj6OV0m", "original": null, "number": 3, "cdate": 1542913433655, "ddate": null, "tcdate": 1542913433655, "tmdate": 1542913433655, "tddate": null, "forum": "HyxAfnA5tm", "replyto": "rygBgFLi3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1315/Official_Comment", "content": {"title": "Thank you for your review!", "comment": "Thank you for your review. We have corrected the typo in the test in the middle of Algorithm 1: it should have been argmin instead of argmax. We have also clarified the caption of figure 3 to indicate that the two plots simply illustrate two different runs for the indicated agent, showing that our method chooses to assign only a single task variable even throughout runs including changing terrain slopes."}, "signatures": ["ICLR.cc/2019/Conference/Paper1315/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1315/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.", "keywords": ["meta-learning", "model-based", "reinforcement learning", "online learning", "adaptation"], "authorids": ["nagaban2@berkeley.edu", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Anusha Nagabandi", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/e94b677836dd7a8a5812d22eedbaf788394bf433.pdf", "paperhash": "nagabandi|deep_online_learning_via_metalearning_continual_adaptation_for_modelbased_rl", "_bibtex": "@inproceedings{\nnagabandi2018deep,\ntitle={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},\nauthor={Anusha Nagabandi and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxAfnA5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1315/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615328, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxAfnA5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference/Paper1315/Reviewers", "ICLR.cc/2019/Conference/Paper1315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1315/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1315/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1315/Authors|ICLR.cc/2019/Conference/Paper1315/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1315/Reviewers", "ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference/Paper1315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615328}}}, {"id": "HJg0BTu4C7", "original": null, "number": 2, "cdate": 1542913349826, "ddate": null, "tcdate": 1542913349826, "tmdate": 1542913349826, "tddate": null, "forum": "HyxAfnA5tm", "replyto": "BJgYeRFP2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1315/Official_Comment", "content": {"title": "Thank you for your review!", "comment": "Thank you for your review. We have corrected the typo in both places of Algorithm 1: it should indeed have been the opposite inequality sign, and argmin instead of argmax. \n\nWe definitely agree with your comment that a mixture model that grows with time can sometimes be considered quite heavyweight. This is precisely where we plan to focus the efforts of our future work, by introducing a refreshing scheme where an offline retraining step can periodically condense the mixture model into fewer components (perhaps in a batch-mode training setting, so not all past data needs to be saved). We are also interested in goals such as making this mixture only as big as the agent \u201cneeds\u201d it to be, allowing for better and more compressed sharing and organization of seen data. The performance of this current method makes us hopeful and excited to work toward such future work in this area."}, "signatures": ["ICLR.cc/2019/Conference/Paper1315/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1315/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.", "keywords": ["meta-learning", "model-based", "reinforcement learning", "online learning", "adaptation"], "authorids": ["nagaban2@berkeley.edu", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Anusha Nagabandi", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/e94b677836dd7a8a5812d22eedbaf788394bf433.pdf", "paperhash": "nagabandi|deep_online_learning_via_metalearning_continual_adaptation_for_modelbased_rl", "_bibtex": "@inproceedings{\nnagabandi2018deep,\ntitle={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},\nauthor={Anusha Nagabandi and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxAfnA5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1315/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615328, "tddate": null, "super": null, "final": null, "reply": {"forum": "HyxAfnA5tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference/Paper1315/Reviewers", "ICLR.cc/2019/Conference/Paper1315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1315/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1315/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1315/Authors|ICLR.cc/2019/Conference/Paper1315/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1315/Reviewers", "ICLR.cc/2019/Conference/Paper1315/Authors", "ICLR.cc/2019/Conference/Paper1315/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615328}}}, {"id": "Hkg_xYA3n7", "original": null, "number": 3, "cdate": 1541363952277, "ddate": null, "tcdate": 1541363952277, "tmdate": 1541533241423, "tddate": null, "forum": "HyxAfnA5tm", "replyto": "HyxAfnA5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1315/Official_Review", "content": {"title": "Nice work", "review": "The authors proposed a new method to learn streaming online updates for neural networks with meta-learning and applied it to multi-task reinforcement learning. Model-agnostic meta-learning is used to learn the initial weight and task distribution is learned with the Chinese restaurant process. It sounds like an interesting idea and practical for RL. Extensive experiments show the effectiveness of the proposed method.\n\nThe authors said that online updating the meta-learner did not improve the results, which is a bit surprised. Also how many data are meta-trained is not clearly described in the paper. Maybe the authors can compare the results with less data for meta-training.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1315/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.", "keywords": ["meta-learning", "model-based", "reinforcement learning", "online learning", "adaptation"], "authorids": ["nagaban2@berkeley.edu", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Anusha Nagabandi", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/e94b677836dd7a8a5812d22eedbaf788394bf433.pdf", "paperhash": "nagabandi|deep_online_learning_via_metalearning_continual_adaptation_for_modelbased_rl", "_bibtex": "@inproceedings{\nnagabandi2018deep,\ntitle={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},\nauthor={Anusha Nagabandi and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxAfnA5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1315/Official_Review", "cdate": 1542234256854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxAfnA5tm", "replyto": "HyxAfnA5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1315/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920009, "tmdate": 1552335920009, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1315/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rygBgFLi3m", "original": null, "number": 2, "cdate": 1541265644565, "ddate": null, "tcdate": 1541265644565, "tmdate": 1541533241222, "tddate": null, "forum": "HyxAfnA5tm", "replyto": "HyxAfnA5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1315/Official_Review", "content": {"title": "This was a nice proposal of a nonparametric mixture model of NNs initialized with meta-learning for supervised learning under nonstationary distributions.", "review": "The paper presents a nonparametric mixture model of neural networks for learning in an environment with a nonstationary distribution. The problem setup includes having access to only a few \"modes\" of the distribution. Training of the initial model occurs with MAML, and distributional changes during test/operation are handled by a combination of online adaptation and creations of new mixture components when necessary. The mixture is nonparametric and modeled with a CRP. The application considered in the paper is RL, and the experiments compare proposed model against baselines that do not utilize meta-learning (achieved in the proposed method with MAML), and baselines which utilize only a single model component.\n\nI thought the combination of meta-learning and a CRP was a neat way to tackle the problem of modeling and learning the \"modes\" of a nonstationary distribution. Applications in other domains would have been nice, but the presented results in RL sufficiently demonstrate the benefits of the proposed method.\n\n* Questions/Comments\n\nFigure 3 left vs right?\n\nIs the test in the middle of Algorithm 1 correct?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1315/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.", "keywords": ["meta-learning", "model-based", "reinforcement learning", "online learning", "adaptation"], "authorids": ["nagaban2@berkeley.edu", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Anusha Nagabandi", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/e94b677836dd7a8a5812d22eedbaf788394bf433.pdf", "paperhash": "nagabandi|deep_online_learning_via_metalearning_continual_adaptation_for_modelbased_rl", "_bibtex": "@inproceedings{\nnagabandi2018deep,\ntitle={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},\nauthor={Anusha Nagabandi and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxAfnA5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1315/Official_Review", "cdate": 1542234256854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxAfnA5tm", "replyto": "HyxAfnA5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1315/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920009, "tmdate": 1552335920009, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1315/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJgYeRFP2X", "original": null, "number": 1, "cdate": 1541017073225, "ddate": null, "tcdate": 1541017073225, "tmdate": 1541533241016, "tddate": null, "forum": "HyxAfnA5tm", "replyto": "HyxAfnA5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1315/Official_Review", "content": {"title": "Useful method for online adaptation to sudden changes in the modeled environment", "review": "The paper introduces a method for online adaptation of a model that is expected to adapt to changes in the environment the model models. The method is based on a mixture model, where new models are spawned using a Chinese restaurant process, and where each newly spawned model starts with weights that have been trained using meta-learning to quickly adapt to new dynamics. The method is demonstrated on model-based RL for a few simple benchmarks.\n\nThe proposed method is well justified, clearly presented, and the experimental results are convincing. The paper is generally clear and well written. The method is clearly most useful for situations where the environment suddenly changes, which is relevant in some real-world problems. As a drawback, using a mixture model (that also grows with time) for such modelling can be considered quite heavy in some situations. Nevertheless, the idea of combining a spawning process with meta-learned priors is neat, and clearly works well.\n\nMinor comments:\n- Algorithm 1: is the inequality correct, and is T* supposed to be an argmin instead of argmax?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1315/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL", "abstract": "Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. We apply our method to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that our online learning via meta-learning algorithm outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.", "keywords": ["meta-learning", "model-based", "reinforcement learning", "online learning", "adaptation"], "authorids": ["nagaban2@berkeley.edu", "cbfinn@eecs.berkeley.edu", "svlevine@eecs.berkeley.edu"], "authors": ["Anusha Nagabandi", "Chelsea Finn", "Sergey Levine"], "pdf": "/pdf/e94b677836dd7a8a5812d22eedbaf788394bf433.pdf", "paperhash": "nagabandi|deep_online_learning_via_metalearning_continual_adaptation_for_modelbased_rl", "_bibtex": "@inproceedings{\nnagabandi2018deep,\ntitle={Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based {RL}},\nauthor={Anusha Nagabandi and Chelsea Finn and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HyxAfnA5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1315/Official_Review", "cdate": 1542234256854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HyxAfnA5tm", "replyto": "HyxAfnA5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1315/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335920009, "tmdate": 1552335920009, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1315/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}