{"notes": [{"id": "H1gL-2A9Ym", "original": "H1lMzvScFX", "number": 1175, "cdate": 1538087934169, "ddate": null, "tcdate": 1538087934169, "tmdate": 1551262900974, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Hyx7QsljBE", "original": null, "number": 4, "cdate": 1550678811252, "ddate": null, "tcdate": 1550678811252, "tmdate": 1550678811252, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "SkgQgtR9BN", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Public_Comment", "content": {"comment": "Referenced it on the Pytorch github repo. Loved this paper.", "title": "Thank You!"}, "signatures": ["~Benedek_Rozemberczki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Benedek_Rozemberczki1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311661049, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1gL-2A9Ym", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311661049}}}, {"id": "BkxZow25SE", "original": null, "number": 3, "cdate": 1550661529425, "ddate": null, "tcdate": 1550661529425, "tmdate": 1550678775348, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "BkxZGnQNE4", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Public_Comment", "content": {"comment": "I tried to reproduce the results and created an implementation.\n\nhttps://github.com/benedekrozemberczki/APPNP", "title": "Attempt to reproduce results."}, "signatures": ["~Benedek_Rozemberczki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Benedek_Rozemberczki1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311661049, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1gL-2A9Ym", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311661049}}}, {"id": "SkgQgtR9BN", "original": null, "number": 17, "cdate": 1550670058832, "ddate": null, "tcdate": 1550670058832, "tmdate": 1550670158096, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "BkxZow25SE", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Reference implementation published", "comment": "Thank you for your interest and effort in reimplementing our model in PyTorch!\n\nWe've just published a reference implementation at https://github.com/klicperajo/ppnp ."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "BkxZGnQNE4", "original": null, "number": 16, "cdate": 1549184008709, "ddate": null, "tcdate": 1549184008709, "tmdate": 1549184008709, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "rke40msQE4", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Later this month", "comment": "We're planning to release source code for the model along with the camera ready version later this month."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "rke40msQE4", "original": null, "number": 2, "cdate": 1549149131863, "ddate": null, "tcdate": 1549149131863, "tmdate": 1549149131863, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Public_Comment", "content": {"comment": "Is there publicly available code for the paper?", "title": "Code"}, "signatures": ["~Benedek_Rozemberczki1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Benedek_Rozemberczki1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311661049, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1gL-2A9Ym", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311661049}}}, {"id": "HJxbLZhZx4", "original": null, "number": 1, "cdate": 1544827208924, "ddate": null, "tcdate": 1544827208924, "tmdate": 1545354530035, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Meta_Review", "content": {"metareview": "There were several ambivalent reviews for this submission and one favorable one. Although this is a difficult case, I am recommending accepting the paper.\n\nThere were two main questions in my mind.\n1. Did the authors justify that the limited neighborhood problem they try to fix with their method is a real problem and that they fixed it? If so, accept.\n\nHere I believe evidence has been presented, but the case remains undecided.\n\n2. If they have not, is the method/experiments sufficiently useful to be interesting anyway?\n\nThis question I would lean towards answering in the affirmative.\n\nI believe the paper as a whole is sufficiently interesting and executed sufficiently well to be accepted, although I was not convinced of the first point (1) above. One review voting to reject did not find the conceptual contribution very valuable but still thought the paper was not severely flawed. I am partly down-weighting the conceptual criticism they made. I am more concerned with experimental issues. However, I did not see sufficiently severe issues raised by the reviewers to justify rejection.\n\nUltimately, I could go either way on this case, but I think some members of the community will benefit from reading this work enough that it should be accepted.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "borderline paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1175/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352939268, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352939268}}}, {"id": "rylzwtw3JV", "original": null, "number": 15, "cdate": 1544481114217, "ddate": null, "tcdate": 1544481114217, "tmdate": 1544483540596, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "ryl2um9KJV", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Re: thanks for your response", "comment": "Dear reviewer,\n\nThank you for clarifying your review and reconsidering and upgrading your score!\n\nWe would like to point out that Laplacian feature propagation is just that very basic PPR-based baseline you wanted to see -- it uses PPR-like feature propagation in combination with logistic regression.\n\nSince we both agree that LASAGNE falls into a different category of methods and that we use PPR in a very different way (to propagate information instead of sampling contexts for a skip-gram model), we are not quite sure what work you are referring to that reduces the novelty value of our method. Our model's simplicity might make it seem like a minor contribution but it also makes the model easy to implement, train, optimize, extend and scale. E.g. note that GNNs with many layers suffer from difficulties in gradient-based training, while our method (thanks to the decoupling of the propagation step) does not, making it more suitable to use in practice."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "ryeRGUehk4", "original": null, "number": 14, "cdate": 1544451606009, "ddate": null, "tcdate": 1544451606009, "tmdate": 1544451606009, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "HkxoIiqdk4", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "The issues of limited range and oversmoothing", "comment": "** Issue of Limited Range **\n\nEvidence for showing that larger neighborhoods are beneficial is shown e.g. in Figures 4 and 5 of the paper. Figure 4 shows how the accuracy increases dramatically on Cora-ML and PubMed as we increase the number of propagation steps beyond 2. Figure 5 shows that the optimal \u03b1 lies between 0.05 and 0.2. For these values, between 86% and 51% of the influence comes from neighborhoods using more than 2 propagation steps.\n\nFurthermore, larger neighborhoods are especially important in the sparsely labelled setting, as shown by Li, Han and Wu (AAAI 2018) and in Figure 3 of our paper. This figure shows that our method can handle small training sets best and outperforms GCN by 6 percentage points in this setting.\n\nXu et al. (ICML 2018) have also found the limited range to be an issue, especially for nodes in the periphery. Very little information will reach these nodes with only 2 hops and a higher range is therefore critical for classifying these.\n\n** Oversmoothing and attention-like mechanisms **\n\nAn attention-like mechanism for working with multiple different neighborhood sizes was already investigated in previous work by Xu et al. in the jumping knowledge networks (JK) model. However, for most experiments they still achieved best performance when using only 2-3 layers. In our own experiments we have found JK to perform best with only 3 layers and in the paper we show that our new model significantly outperforms it.\n\nIn earlier experiments we have also tested attention over different neighborhood sizes in combination with our model, but found that learning the attention weights is problematic and mostly overfits on the node itself. Please note that our personalized PageRank uses an *implicit* attention scheme on the different neighborhoods with weights \u03b1(1-\u03b1)^k (for the k-step neighborhood), which we have found to perform significantly better than any other weighting scheme we have tested. This implicit attention mechanism might be one reason why our model performs so well.\n\nWe have also experimented with increasing the number of layers in GAT (which uses attention for its node aggregation function), but were not able to successfully increase its number of layers beyond the original 2. \n\nFinally, different node aggregation functions were used by e.g. GraphSAGE, which also performs best when using no more than 2 layers and therefore shows the same problem of limited range."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "ryl2um9KJV", "original": null, "number": 13, "cdate": 1544295283631, "ddate": null, "tcdate": 1544295283631, "tmdate": 1544295283631, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "HkeSQM8Oa7", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "thanks for your response", "comment": "Dear authors, \n\nI would like to thank you for the detailed response(s) to the review(s you have received). I would also like to make a related comment: I agree with your comments overall, modulo that there has been any confusion. Your experimental setup was clear from the first time I read your nice paper, that is why I mentioned in one of comments that \" While according to the authors\u2019 categorization of the existing methods in the intro, LASAGNE falls under the \u201crandom walk\u201d family  of methods\".    Perhaps I should have made it more clear in my review, that personally as a reviewer I would have liked to see some basic classification baseline that is related to PPR, that was my main point and why I made  two possible suggestions.\n\nI have upgraded my score. I want to clarify that my non-acceptance score as my review title summarized from early on was not due to this baseline comparison fact (besides you compared with other state-of-the-art related methods), but due to the fact that I personally found the contribution to be (on the one hand *interesting* but on the other hand) limited from a novelty perspective.   "}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "SygzJkminQ", "original": null, "number": 3, "cdate": 1541250777964, "ddate": null, "tcdate": 1541250777964, "tmdate": 1544294747223, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Review", "content": {"title": "Interesting but limited contribution", "review": "The thurst behind this paper is that graph convolutional networks (GCNs) are constrained by construction\nto focus on small neighborhoods around any given node. Large neighborhoods introduce in principle\na large number of parameters (while as the authors point out, weight sharing is an option to avoid this issue), \nplus even worse oversmoothing may occur. Specifically, Xu et al. (2018) showed that for a k-layer GCN one can \nthink of the influence score of a node x on node y as the probability  that a walker that starts at x, \nlands on y after k steps of random walk (modulo some details). \n\nTherefore, as k increases the random walks reaches its stationary distribution, forgetting any local information that is useful, \ne.g., for node classification. To avoid this problem, the authors propose the following: use personalized Pagerank\ninstead of the standard Markov chain of Pagerank. In PPR there is a restart probability, which allows \ntheir algorithm to avoid \u201cforgetting\u201d the local information around a walk, thus allowing for an arbitrary \nnumber of steps in their random walk. The authors define two methods PEP, and PEPa based on PPR. The latter \nmethod is faster in practice since it approximates the PPR.   \n\nA key advantage of the proposed method is the separation of the node embedding part from the propagation scheme. In this sense, \nfollowing the categorization of existing methods into three categories, PEP is a hybrid of message passing algorithms,\nand random walk based node embeddings. The experimental evaluation tests certain basic properties of the proposed method. One interesting performance feature of \nPEP and PEPa is that they can perform well using few training examples. This is valuable especially when obtaining labeled\nexamples is expensive.  Finally, the authors compare their proposed methods against state-of-the-art GCN-based methods.  \n\nSome remarks follow. \n\n- The idea of using PPR for node embeddings has been suggested in recent prior work \u201cLASAGNE: Locality and structure aware graph node embeddings\u201d \nBy Faerman et al.  While according to the authors\u2019 categorization of the existing methods in the intro, LASAGNE \nfalls under the \u201crandom walk\u201d family  of methods, the authors should compare against it. \n \n- Continuing the previous point,  even simpler baselines would be desirable. How inferior is for instance \nan approach on one-vs-all classification using the approximate personalized Pagerank node embedding and \nsupport vector machines?  \n \n- Also, the authors mention \u201csince our datasets are somewhat similar\u2026\u201d. Please clarify with respect to \nwhich aspects? Also, please use datasets that are different. For instance, see the LASAGNE paper for \nmore datasets that have different number of classes.  \n\n- In the experiments the authors use two layers for fair comparison. Given that one of the advantages of the \nproposed method is the  ability to have more layers without suffering from the GCN shortcomings \nwith large neighborhood exploration, it would be interesting to see an experiment where the number of layers is a variable. \n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Review", "cdate": 1542234288430, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335889176, "tmdate": 1552335889176, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkxoIiqdk4", "original": null, "number": 11, "cdate": 1544231762848, "ddate": null, "tcdate": 1544231762848, "tmdate": 1544231762848, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "B1xn8s4o6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "clarification", "comment": "I believe the reviewer here meant \"substantial and practically meaningful\" and not \"statistically significant.\" \n\nYour point about graph diameter is a good one. However I am wondering if you can elaborate a bit on your argument in section 2 where you say:\n\n\"There are essentially two reasons why a message passing algorithm like GCN can\u2019t be trivially expanded to use a larger neighborhood. First, aggregation by averaging causes oversmoothing if too many layers are used. It, therefore, loses its focus on the local neighborhood (Li et al., 2018). Second, most common aggregation schemes use learnable weight matrices in each layer. Therefore, using a larger neighborhood necessarily increases the depth and number of learnable parameters of the neural network (the second aspect can be circumvented by using weight sharing, which is typically not the case, though).\"\n\nIt seems fine to use weight sharing to deal with the second issue and I believe it isn't that uncommon. However, the oversmoothing issue could be a larger problem. Couldn't this be dealt with using attention-like mechanisms or different aggregation functions like max instead of sum (or intermediate functions)?\n\nAn average diameter of 10, the largest for datasets you explore, might not be enough to be problematic. Keeping in mind that I have not carefully read the paper, only skimmed it, can you succinctly summarize what evidence you have that limited range is an important issue in practice? I agree with the premise that it could be (because of the tying network depth or recurrent sequence length to neighborhood size is somewhat arbitrary), but I am wondering how best to demonstrate this is an issue and your approach is a successful solution on an important problem of practical interest."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "B1xn8s4o6m", "original": null, "number": 7, "cdate": 1542306644138, "ddate": null, "tcdate": 1542306644138, "tmdate": 1542306644138, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "HJg43sI_6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Re: Reviewer2", "comment": "Thank you for your quick response!\n\nIf we understand you correctly, all your points above are referring to the study of larger graphs to ensure a large diameter (since, as mentioned in your first comment, a large diameter requires more propagation steps). Note, however, that the graph diameter usually shrinks with graph size (see e.g. Leskovec 2005). Thus, instead of studying even larger graphs one should analyze graphs with sufficiently large diameter. Indeed, the graphs we have already studied in our paper have an average diameter between 5 and 10 (see Table 1 of the revised version). Thus, a few GCN layers can not cover the entire graph.\n \nOur experiments further show that denser graphs with a smaller diameter (e.g. Microsoft Academic) require a higher alpha (see Figure 5). Your discussion actually prompted us to adjust alpha on this dataset to better reflect the graph\u2019s underlying characteristics (see Section 6 of the revised version).\n\nFurthermore, we are not sure what exactly you mean with \u2018significant\u2019 -- and why you have the impression that our results are not significant. In our paper and comments we use the term significant in the mathematical sense of statistical significance. The results clearly show that our method\u2019s improvements are significant with a p-value of 0.05, as we have shown in our rigorous evaluation (for small and large graphs as well as graphs with different diameters)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "HJg43sI_6Q", "original": null, "number": 6, "cdate": 1542118316155, "ddate": null, "tcdate": 1542118316155, "tmdate": 1542118316155, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "BkxnHzUdaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Re: Re: Reviewer2", "comment": "Thanks for your reply!\n\nTo reiterate my questions:\n\n1) The graph with ~10k nodes would be the limit for your exact algorithm, as the results are missing in Table 2. But since you have the approximation with power-iteration like layers, it would be better if you can target on large graphs. \n\n2) And I expect your algorithm would benefit more on large graphs. This is the case where the pagerank could be more effective in propagating information, than parameterized message passing operators. So that's why it is important to do large scaled experiments to show the truly 'significant' gains. \n\n3) Here are several good large datasets you may want to take a look: https://snap.stanford.edu/data/"}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "S1lvvM8OTX", "original": null, "number": 5, "cdate": 1542115934862, "ddate": null, "tcdate": 1542115934862, "tmdate": 1542116018268, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "Bkxy1bhPnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Re: Reviewer3", "comment": "Thank you for your review and feedback!\n\nYou are right, nothing prevents the model from using the standard transition matrix. During model development, however, we have found that the added self-loops of the GCN-matrix are beneficial to performance. The symmetrical normalization actually doesn't make any difference in the limit k->infinity. However, we found this style of normalization to be beneficial for the finite-step approximation. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "BkxnHzUdaQ", "original": null, "number": 4, "cdate": 1542115907758, "ddate": null, "tcdate": 1542115907758, "tmdate": 1542116005870, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "S1e-m_U5nX", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Re: Reviewer2", "comment": "Thank you for your review and feedback!\n\nThe connection to the GNN-framework is certainly interesting and we\u2019ve added it in the revised version of the paper (in Section 3, after introducing APPNP). However, our main contribution is not the usage of fixed-point iterations for node classification, which has already been used e.g. in label propagation and belief propagation algorithms. Our contribution is the improvement of GCN-like models by solving the limited range problem through the development and thorough evaluation of an end-to-end trained model utilizing one specific fixed-point iteration.\n\nAs you correctly noticed, the exact model is not applicable to larger data -- this is exactly the reason why we have developed its approximation. The discussion can be found under \"efficiency analysis\" in Section 3. We have edited the experimental section to make this more clear. Furthermore, we would like to highlight that we have already performed an analysis on large graphs. As shown in Table 1, our experimental evaluation includes two graphs with 20k nodes, which follows the suggestion you gave (>10k nodes).\n\nPlease note that we have already compared our model to jumping knowledge networks (JK), which is similar to the GNN that uses proper gating/skip connections you suggested. As we show in the experimental section, we significantly outperform this model.\n\nYou state that we show \"some marginal gains\". However, we show that our results are significant. Previous methods have reported \u201clarge\u201d gains that actually were not statistically significant and vanish when thoroughly evaluated, as we show in the paper. We paid a lot of attention to performing a fair comparison and a rigorous statistical analysis of our results, which shows that we significantly outperform previous models. The different evaluation may make the improvements seem smaller. But in fact they are larger than those reported in previous, less careful evaluations. We have edited the section to further clarify this. Furthermore, we\u2019ve included a reference to the work by Dai et al."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "HkeSQM8Oa7", "original": null, "number": 3, "cdate": 1542115868611, "ddate": null, "tcdate": 1542115868611, "tmdate": 1542115995157, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "SygzJkminQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Re: Reviewer1", "comment": "Thank you for your review and feedback!\n\nWe want to clarify that the principle and task performed by LASAGNE is fundamentally different to ours. The LASAGNE method learns individual node embeddings in an unsupervised setting. Our goal is not to learn individual node embeddings but to learn a transformation from attributes to class labels in the semi-supervised setting, as graph convolutional network (GCN)-like models do. Moreover, LASAGNE only considers structural information. Generally, it has been shown that approaches that consider both structure and attributes outperform methods that only consider the structure (see e.g. Kipf Welling 2017). Therefore, we only compare with methods that consider both, but we added a reference to LASAGNE in the paper.\n\nWe feel that this confusion was due to a bad framing of our model. To make things clearer we have decided to rename the model and replace the term \u201cembedding\u201d with \u201cprediction\u201d in the revised version (see also our general comment).\n\nWe cannot run the proposed baseline, since as we clarified above we do not learn any personalized pagerank embeddings to begin with. However, we do already include a comparatively simple baseline which is the bootstrapped Laplacian feature propagation. This method propagates features in a similar way as we do and then uses a one-vs-all classifier. We significantly outperform this baseline.\n\nIn the revised version of the paper we clarified that the datasets are similar in that they contain bag-of-words features and use scientific networks. However, these graphs have very different numbers of nodes, edges, features, and classes, and different topology, as shown in Table 1. The datasets you suggested from the LASAGNE paper are not suitable for the kind of semi-supervised classification we consider since they do not contain node attributes.\n\nThank you for suggesting the interesting experiment of varying neural network depth! The investigated datasets do not benefit from deeper networks. You can find the results in Figure 11 of the updated version of the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "S1ga6ZIuT7", "original": null, "number": 2, "cdate": 1542115781131, "ddate": null, "tcdate": 1542115781131, "tmdate": 1542115781131, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "content": {"title": "Title and name change", "comment": "Dear reviewers, dear commenters,\nWe feel that the term \"embedding\" that we used in our work (and paper\u2019s title) might be a source of confusion, which is why we have decided to replace it with \u201cprediction\u201d and rename the model. We want to clarify that we do NOT learn individual node embeddings as done in node embedding methods. We propagate the predictions as part of the end-to-end trained model. Please keep in mind that we did NOT change any part of the model except for the name."}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619570, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1gL-2A9Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1175/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1175/Authors|ICLR.cc/2019/Conference/Paper1175/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers", "ICLR.cc/2019/Conference/Paper1175/Authors", "ICLR.cc/2019/Conference/Paper1175/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619570}}}, {"id": "S1e-m_U5nX", "original": null, "number": 2, "cdate": 1541199896828, "ddate": null, "tcdate": 1541199896828, "tmdate": 1541533359183, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Review", "content": {"title": "review on \"Personalized Embedding Propagation: Combining Neural Networks on Graphs with Personalized PageRank\"", "review": "This paper proposed a variant of graph neural network, which added additional pagerank-like propagations (with constant aggregation weights), in additional to the normal message-passing like propagation layers. Experiments on some benchmark transductive node classification tasks show some empirical gains.\n\nUsing more propagations with constant aggregation weights is an interesting idea to help propagate the information in a graph. However, this idea is not completely new. In the very first graph neural network [1], the propagation is done until convergence. If the operator in each layer is a contraction map, then according to the Banach Fixed Point theorem [2], a unique solution can be guaranteed. The constant operator used in this paper is thus a special case of this contraction map.\n\nAlso, the closed form solution in (3) is not practical. It may not be suitable for large graphs (e.g., graphs with >10k nodes). And that\u2019s why this approach is not suitable for Pubmed and Microsoft dataset. The PEP_A is more practical. However, in this case I\u2019m curious how it would compare with a GNN having same number of layers, but with proper gating/skip connections like ResNet. \n\nThe experiments show some marginal gains on the small graphs. However, I think it would be important to test on large graphs. Since small graphs typically have small diameter, thus several GNN layers would already cover the entire graph, and the additional propagation done by pagerank here might not be super helpful. \n\nFinally, I think the author should properly cite another relevant paper [3], which uses fixed point iteration to help propagate the local information. \n\n[1] Scarselli et.al, \u201cThe Graph Neural Network Model\u201d, IEEE Transactions on Neural Networks, 2009\n[2] Mohamed A. Khamsi, An Introduction to Metric Spaces and Fixed Point Theory\n[3] Dai et.al, Learning Steady-States of Iterative Algorithms over Graphs, ICML 2018", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Review", "cdate": 1542234288430, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335889176, "tmdate": 1552335889176, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bkxy1bhPnX", "original": null, "number": 1, "cdate": 1541026006880, "ddate": null, "tcdate": 1541026006880, "tmdate": 1541533358978, "tddate": null, "forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1175/Official_Review", "content": {"title": "Idea is interesting; experiments are convincing", "review": "This paper proposes a GCN variant that addresses a limitation of the original model, where embedding is propagated in only a few hops. The architectural difference may be explained in the following: GCN interleaves the individual node feature transformation and the single-hop propagation, whereas the proposed architecture first transforms the node features, followed by a propagation with an (in)finite number of hops. The propagation in the proposed method follows personalized PageRank, where in addition to following direct links, there is a nonzero probably jumping to a target node.\n\nI find the idea interesting. The experiments are comprehensive, covering important points including data split, training set size, number of hops, teleport probability, and ablation study. Two interesting take-home messages are that (1) GCN-like propagation without teleportation leads to degrading performance as the number of hops increases, whereas propagation with teleportation leads to converging performance; and (2) the best-performing teleport probability generally falls within a narrow range.\n\nQuestion: The current propagation approach uses the normalized adjacency matrix proposed by GCN, which is, strictly speaking, not the transition matrix used by PageRank. What prevents from using the transition matrix? Note that this matrix naturally handles directed graphs.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1175/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predict then Propagate: Graph Neural Networks meet Personalized PageRank", "abstract": "Neural message passing algorithms for semi-supervised classification on graphs have recently achieved great success. However, for classifying a node these methods only consider nodes that are a few propagation steps away and the size of this utilized neighborhood is hard to extend. In this paper, we use the relationship between graph convolutional networks (GCN) and PageRank to derive an improved propagation scheme based on personalized PageRank. We utilize this propagation procedure to construct a simple model, personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP. Our model's training time is on par or faster and its number of parameters on par or lower than previous models. It leverages a large, adjustable neighborhood for classification and can be easily combined with any neural network. We show that this model outperforms several recently proposed methods for semi-supervised classification in the most thorough study done so far for GCN-like models. Our implementation is available online.", "keywords": ["Graph", "GCN", "GNN", "Neural network", "Graph neural network", "Message passing neural network", "Semi-supervised classification", "Semi-supervised learning", "PageRank", "Personalized PageRank"], "authorids": ["klicpera@in.tum.de", "a.bojchevski@in.tum.de", "guennemann@in.tum.de"], "authors": ["Johannes Klicpera", "Aleksandar Bojchevski", "Stephan G\u00fcnnemann"], "TL;DR": "Personalized propagation of neural predictions (PPNP) improves graph neural networks by separating them into prediction and propagation via personalized PageRank.", "pdf": "/pdf/e2710dc14afc145596dd4f4cee5983a9efd02cd3.pdf", "paperhash": "klicpera|predict_then_propagate_graph_neural_networks_meet_personalized_pagerank", "_bibtex": "@inproceedings{\nklicpera2018combining,\ntitle={Combining Neural Networks with Personalized PageRank for Classification on Graphs},\nauthor={Johannes Klicpera and Aleksandar Bojchevski and Stephan G\u00fcnnemann},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1gL-2A9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1175/Official_Review", "cdate": 1542234288430, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1gL-2A9Ym", "replyto": "H1gL-2A9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1175/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335889176, "tmdate": 1552335889176, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1175/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 20}