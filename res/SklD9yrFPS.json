{"notes": [{"id": "SklD9yrFPS", "original": "r1xos90uDB", "number": 1882, "cdate": 1569439631426, "ddate": null, "tcdate": 1569439631426, "tmdate": 1583912033853, "tddate": null, "forum": "SklD9yrFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "DCC3FVzwnE", "original": null, "number": 1, "cdate": 1576798734958, "ddate": null, "tcdate": 1576798734958, "tmdate": 1576800901414, "tddate": null, "forum": "SklD9yrFPS", "replyto": "SklD9yrFPS", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This paper presents a software library for dealing with neural networks either in the (usual) finite limit or in the infinite limit. The latter is obtained by using the Neural Tangent Kernel theory. \n\nThere is variance in the reviewers' scores, however there has also been quite a lot of discussion, which has been facilitated by the authors' elaborate rebuttal. The main points in favor and against are clear: on the positive side, the library is demonstrated well (especially after rebuttal) and is equipped with desirable properties such as usage of GPU/TPU, scalability etc. On the other hand, a lot of the key insights build heavily on prior work of Lee et al, 2019. However, judging novelty when it comes to a software paper is more tricky to do, especially given that not many such papers appear in ICLR and therefore calibration is difficult. This has been discussed among reviewers. \n\nIt would help if some further theoretical insights were included in this paper; these insights could come by working backwards from the implementation (i.e. what more can we learn about infinite width networks now that we can experiment easily with them?).\n\nOverall, this paper should still be of interest to the ICLR community.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SklD9yrFPS", "replyto": "SklD9yrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709351, "tmdate": 1576800258086, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Decision"}}}, {"id": "H1emYsn6Yr", "original": null, "number": 2, "cdate": 1571830651202, "ddate": null, "tcdate": 1571830651202, "tmdate": 1574473947140, "tddate": null, "forum": "SklD9yrFPS", "replyto": "SklD9yrFPS", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "POST-REBUTTAL COMMENTS\n\nI appreciate the response from the authors. \n\nI particularly like the comparison table in the response to the other reviewer and ought to be highlighted in the paper.\n\nIf I were to start this line of research, I would be inclined to expand on the codebase. The contribution is significant. Hence, I am bumping up my score to accept.\n\n\nPRIOR FEEDBACK\n\nThe contribution of this work lies in providing a library for working with the existing variants of infinite-width neural networks and avoiding the need to derive the NNGP and NT kernels for each architecture by hand. The authors have firstly shown performance comparisons between inferences between finite vs. infinitely wide neural networks. The authors then go into some implementation details with their library. The authors have provided the code and cookbook in the links provided in the abstract. On the overall, I like this effort which is timely.\n\nSome additional suggestions below:\n\nI would like to see an additional metric for performance comparison of probabilistic models, which is often used in the GP literature: mean negative log probability.\n\nIt would also be interesting to see how the posterior variance (e.g., Fig. 1 right) evolves over the entire space during training. \n\nI would have preferred a more detailed discussion about the implementation on transforming tensor ops to kernel ops in Section 3.\n\nFor the summary of contributions, can you give the corresponding section number to refer to when you demonstrate each feature? For example, is the 4th feature (i.e., exploring weight space perspective) demonstrated in the paper?\n\nCan the authors elaborate on the ease of expanding their library for the new developments in this field?\n\n\nMinor issues:\n\nPage 1: Gaussian Procesesses?\nPage 4: it\u2019s infinite?\nFig. 4: I would have preferred the indices to be placed as subscripts instead of superscripts.\nPage 8: it\u2019s order of dimensions?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklD9yrFPS", "replyto": "SklD9yrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575736852184, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Reviewers"], "noninvitees": [], "tcdate": 1570237730921, "tmdate": 1575736852198, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Review"}}}, {"id": "ryxYhr0aKr", "original": null, "number": 3, "cdate": 1571837360776, "ddate": null, "tcdate": 1571837360776, "tmdate": 1574351295926, "tddate": null, "forum": "SklD9yrFPS", "replyto": "SklD9yrFPS", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Summary: A Jax based neural tangents kernel library is introduced, with native GPU, TPU, and XLA support. Due to the correspondences with infinite neural network kernels (NNGPs), these kernels are also able to be computed for (essentially) free. Layers which do not emit an analytical form (e.g. Tanh or Softplus) can be implemented using Monte Carlo estimates. Several engineering-based experiments are performed demonstrating the potential scalability of their library.\n\nWhile I really enjoyed reading the paper and believe that this library could be extremely practically useful, I vote to reject this paper because I do not feel that it has sufficient novelty to be a paper on its own in light of Lee et al, 2019. \n\nEdit: post rebuttal, I'm bumping my score to a weak reject but would have minimal qualms if this paper were to be accepted. I find the further experiments performed by the authors of very good quality overall, but I'm still not particularly satisfied by their `argument that the codebase itself is distinct enough from separate related work. It's unfortunately a bit hard, from a machine learning researcher side, to review the quality of a codebase in and of itself. \n\nSignificance: Having played around with the code a bit, I find that the library itself is of very high quality and is pretty straightforward to use. I could definitely see myself using this library in the future for research work.\n\nHowever, my primary concern with this paper is that it\u2019s not sufficiently distinct from the previous work of Lee et al, 2019. After all, most of the experiments in that paper would have required the type of implementation that is described in greater detail in this paper. \n\nTo be able to vote to accept this paper, I will have to see an experiment that is practically performed with the current library in order to distinguish it from previous work (specifically Lee et al, 2019). In recent work, Arora et al, 2019 (Note: I do not consider this reference in my review other than to be mentioned as an example of an experiment that could be run with your library) run neural tangent kernels on tabular data using kernel SVMs. One other potential example would be a kernel SVM in this manner on CIFAR-10. An alternative example would be to exploit the Gaussian process representation and test out both NTKs and NNGPs in comparison to standard kernels for GPs and NNs on UCI regression tasks.\n\nOriginality: Again, a very efficient and easy to use implementation of neural tangent kernels would be a great boost to the community. This is doubly so as Jax is easy and pretty straightforward to use and is quite numpy like. \n\nAgain, I am very concerned with originality in comparison to Lee et al, 2019. Even checking out the link to their codebase provides a github repo that is quite similar to this one. Given that ICLR is a venue of similar domain to NeurIPS, it\u2019s not clear to me why this paper ought to be anything other than a separate supporting tech report. If this paper had been submitted to something like SysML (edit: or JMLR MLOSS), I would see the distinctness instead.\n\nClarity: I find the paper to be extremely well-written and easy to follow. The addition of code snippets throughout is very well done, even if it\u2019s a bit overkill. I don\u2019t know what adding a half page long description of an infinitely wide WideResNet adds to the paper when that space could be better used by another experiment. \n\nQuality: I find the experiments performed to be very well constructed. Below are a few mostly minor comments on the experiments:\n\nIn Figure 1 on the right, I would have liked to have seen the posterior predictive for a NNGP with the same kernel as well. \n\nIn Figure 2, why is the NNGP slower to converge to the analytic values here? Obviously, the rates of convergence are the same, but the constants seem different.\n\nIn Figure 3 (and throughout the experiments), does \u201cfull Bayesian inference for CIFAR10\u201d mean that you treated the classification labels as regression targets? If so, how was classification error measured.\n\nIn Section 3.1, you mention that the library \u201cleverages block-diagonal structure\u201d to invert the CIFAR10 covariance matrix for each class (still 50k x 50k). Possibly this is because I haven\u2019t had the chance to use TPUs, but I\u2019m currently struggling to see how one could form and invert (via Choleskys) matrices of this size (50k x 50k) on a standard GPU (or CPU). Could the authors please clarify how they did this (whether through iterative methods, another structure exploiting trick, lots of memory, etc.)?\n\nsecond edit: I was also unable to respond to the final comment about the UCI experiments in its own comment, but thank you for providing the estimated depths. These results definitely show the potential software promise of the codebase and open some interesting new research questions as a result.\n\nReferences:\n\nArora, S., et al., Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks, https://arxiv.org/abs/1910.01663\n\nLee, J., et al., Wide Neural Networks of any Depth Evolve as Linear Models Under Gradient Descent, NeurIPS, 2019, https://arxiv.org/abs/1902.06720\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklD9yrFPS", "replyto": "SklD9yrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575736852184, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Reviewers"], "noninvitees": [], "tcdate": 1570237730921, "tmdate": 1575736852198, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Review"}}}, {"id": "BylarJj2iH", "original": null, "number": 16, "cdate": 1573855045510, "ddate": null, "tcdate": 1573855045510, "tmdate": 1573858376934, "tddate": null, "forum": "SklD9yrFPS", "replyto": "ryl8AD42oB", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "Thank you for the quick response! A bit more on novelty", "comment": "Dear Reviewer 3,\n\nThank you for your very quick reply and for updating your score. We would still like to push back on the novelty aspect.\n\n>>> I agree that the current version of the codebase is considerably more developed than it was in early May but am still not particularly convinced that it's a distinct work given that it is fundamentally based in a similar language/functionality.\n\nWhile we certainly understand the initial confusion about the overlap with Lee et al, 2019, we believe that we have provided a substantial and precise summary of contributions that are specific to this submission, in both the rebuttal and updated text, backed up by a code diff (see https://openreview.net/forum?id=SklD9yrFPS&noteId=B1gt-wb3or ).\n\nWe do not understand how using the same programming (or mathematical) language as Lee et al. 2019 can be an issue, since this is standard practice. We wish to reiterate that the key feature of the library (\u201cnt.stax\u201d, specification and computation of exact infinite-width kernels: https://github.com/neural-tangents/neural-tangents/blob/master/neural_tangents/stax.py ) is completely new in our library, was not released, was not used, and was not necessary for Lee et al. 2019. It was not \u201cmore developed\u201d, but designed and implemented essentially from scratch.\n\nIn this light, and again appreciating the time and thought you have given to our work, we would ask you to reconsider your score again. Thank you!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "rygwzyihjB", "original": null, "number": 15, "cdate": 1573854991322, "ddate": null, "tcdate": 1573854991322, "tmdate": 1573857879581, "tddate": null, "forum": "SklD9yrFPS", "replyto": "ryxYhr0aKr", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "Reply", "comment": ">>> Edit: post rebuttal, I'm bumping my score to a weak reject but would have minimal qualms if this paper were to \n>>> be accepted. I find the further experiments performed by the authors of very good quality overall, but I'm still not \n>>> particularly satisfied by their `argument that the codebase itself is distinct enough from separate related work. It's \n>>> unfortunately a bit hard, from a machine learning researcher side, to review the quality of a codebase in and of itself. \n\nWe appreciate that it is difficult to review a codebase. To help with this we\u2019d like to discuss the development of \u201cneural_tangents.stax\u201d which is the main focus of this work and was developed entirely after May. There has been a significant amount of work on NNGP and NTK methods over the past few years to compute NNGP and NT kernels for a growing set of architectures. In addition to noting that components ought to arbitrarily compose with one another (which I believe was not widely understood prior to this) development of \"neural_tangents.stax\" required arriving at efficient implementations for FC [1,2,3], CNN [4, 5, 6, 7], and pooling [5,7] kernels which were known in the literature along with FanOut, FanInSum, LayerNorm, parallel, and serial which were not explicitly known. As far as we are aware we are also the first to implement convolutions with arbitrary padding, shapes, and strides. Moreover, we wrote code to automatically parallelize this over large datasets. Finally, we note that our contribution also includes the neural tangent cookbook notebook which, as far as we are aware, includes the first computation of the mean and (now, thanks to reviewer 2) variance posterior prediction of the MSE loss. We believe that this represents a substantial research contribution.\n\n[1] Exponential expressivity in deep neural networks through transient chaos\nPoole et al. ; NeurIPS 2016\n[2] Deep Neural Networks as Gaussian Processes\nLee et al. ; ICLR 2018\n[3] Neural Tangent Kernel: Convergence and Generalization in Neural Networks\nJacot et al. ; NeurIPS 2018\n[4] Dynamical Isometry and a Mean Field Theory of CNNs\nXiao et al. ; ICML 2018\n[5] Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes\nNovak et al.; ICLR 2019\n[6] Deep Convolutional Networks as shallow Gaussian Processes\nGarriga-Alonso et al.; ICLR 2019\n[7] On Exact Computation with an Infinitely Wide Neural Net\nArora et al.; NeurIPS 2019 \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "HyxHSX5hjr", "original": null, "number": 14, "cdate": 1573851965491, "ddate": null, "tcdate": 1573851965491, "tmdate": 1573851965491, "tddate": null, "forum": "SklD9yrFPS", "replyto": "HyxjHFEnsr", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "Great questions!", "comment": ">> A couple of follow-up questions arise though that may be useful for further questions\n>> 1) What depths of networks were typically found throughout the different datasets? \n\nGreat question! We looked at it for FC Relu and Resnet + Fixup NTKs. While we do not think it really belongs in the paper, for your interest here is the plot: https://github.com/neural-tangents/neural-tangents/blob/master/iclr_figures/optimal_depth.pdf. We see that mostly single layer networks were selected for both architectures. It would be interesting to do future research, in Neural Tangents, to see if we can understand this!\n\n>> 2) Is there any suggestion as to why fixup initialization schemes ought to perform better?\n \nResidual networks cause gradients to explode which translates to poor conditioning of the NTK. It\u2019s likely that fixup-style initialization schemes improve this conditioning. Again, this would be interesting research to perform using Neural Tangents.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "HJe9z9V3iH", "original": null, "number": 13, "cdate": 1573829137694, "ddate": null, "tcdate": 1573829137694, "tmdate": 1573829137694, "tddate": null, "forum": "SklD9yrFPS", "replyto": "HJxkA_b3sS", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "Thank you for the revisions", "comment": "Will number these for clarity:\n\n1) Thanks for the updates on the comparison to the fully bayesian setting, I appreciate this comparison.\n\n2) Thanks for specifically spelling out what's being done here - it makes the paper considerably more legible.\n\n3) I had thought that might be the case - thank you for being specific as to how these matrices are being inverted."}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "HyxjHFEnsr", "original": null, "number": 12, "cdate": 1573828931043, "ddate": null, "tcdate": 1573828931043, "tmdate": 1573828931043, "tddate": null, "forum": "SklD9yrFPS", "replyto": "Bke8siZhiB", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "Thank you for the further experiment", "comment": "Thanks for providing the experiment with the UCI datasets, I find it to be a quite interesting demonstration of what your library can do, and as such I've bumped my score to weak reject. \n\nA couple of follow-up questions arise though that may be useful for further questions\n1) What depths of networks were typically found throughout the different datasets? \n2) Is there any suggestion as to why fixup initialization schemes ought to perform better?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "ryl8AD42oB", "original": null, "number": 11, "cdate": 1573828557810, "ddate": null, "tcdate": 1573828557810, "tmdate": 1573828557810, "tddate": null, "forum": "SklD9yrFPS", "replyto": "S1g-lnZhir", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "Thank you for the clarification", "comment": "Thank you for the clarification here - interesting that Hayou et al, 2019 cite the library as its own paper by late May (in my understanding).\n\nI agree that the current version of the codebase is considerably more developed than it was in early May but am still not particularly convinced that it's a distinct work given that it is fundamentally based in a similar language/functionality.\n\nApologies for the terse reply."}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "BkgVyRg2jB", "original": null, "number": 5, "cdate": 1573813723869, "ddate": null, "tcdate": 1573813723869, "tmdate": 1573825853886, "tddate": null, "forum": "SklD9yrFPS", "replyto": "BJgNczUaFB", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "AnonReviewer2 Rebuttal", "comment": "Thank you for the careful review, we\u2019re happy you found our library useful and beneficial to the ML community. Below we believe we have addressed your concerns, and we hope you can increase your score as a result.\n\n\n----------------------------------------------------------------------------------------------------\n>>> 1. The theory and formulae of NTKs and NNGPs were well developed. This work mostly consists of implementing and modularizing them. The research contribution is relatively low.\n\nICLR explicitly calls for \u201cimplementation issues, parallelization, software platforms, hardware\u201d (https://iclr.cc/, bottom). We believe that Neural Tangents will unlock qualitatively new avenues for research by making computations on infinite networks tractable for non-experts and orders of magnitude easier for theoretical practitioners. This is increasingly true as work on infinite networks continues to attract interest from the community.\n\nOn a separate note, a significant intellectual effort went into designing and efficiently implementing the library while keeping it scalable, flexible, and easy to use (see section 3.2 [new revision number]). By means of analogy, there is an immense gap between knowing the mathematical formulae of a convolutional layer and having a general and user-/accelerator-friendly implementation. We believe this kind of gaps should not be underestimated, and the novelty of our approach is itself a research contribution.\n\n\n----------------------------------------------------------------------------------------------------\n>>> 2. As commented in the paper and if I understand correctly, the current library cannot scale to large datasets for CNNs with pooling. This would make the computation much more expensive (and probably infeasible without additional techniques and huge computing power) as mentioned in [Novak et al. 2019] and [Arora et al. 2019]. However pooling seems extremely useful for NTKs and NNGPs on image datasets. I think this makes this work somewhat less exciting than it may sound.\n\nYou are correct that CNN-GPs/NTKs with pooling are _very_ compute-hungry. However, we would like to highlight that \n\n1) We did successfully run experiments on 8K CIFAR10 subsets for a WideResNet with pooling in Figure 3, and we have further run pooling experiments on the 45K CIFAR10 training set and achieved a slight improvement over the prior state of the art in [Arora et al. 2019a] with our library (see the table below, GAP = global average pooling, best values marked with **).\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Model                                               \u2551 NNGP acc \u2551 NTK acc \u2551 NNGP loss  \u2551 NTK loss   \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 WResNet-LayerNorm-depth_28   \u2551 73.7           \u2551 72.8        \u2551 0.0501         \u2551 0.0501      \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CNN-GAP-Relu-depth_10               \u2551 78.84        \u2551 *77.84*  \u2551 0.0454         \u2551 *0.0462*  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CNN-GAP-Relu-depth_20               \u2551 *79.38*    \u2551 76.98      \u2551 *0.0447*    \u2551 *0.0462*  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 CNN-GAP-Erf-depth_10                  \u2551 71.32        \u2551 71.3        \u2551 0.0538         \u2551 0.054         \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Arora et al. 2019 [GAP]                  \u2551 -                 \u2551 77.43      \u2551 -                   \u2551 -                 \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Li et al. 2019b [GAP]                       \u2551 78.49         \u2551 77.63      \u2551 -                   \u2551 -                 \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n[Arora et al. 2019] https://arxiv.org/pdf/1904.11955v2.pdf\n[Li et al. 2019] https://arxiv.org/pdf/1911.00809v1.pdf\n\n\n2) Our library provides the [parallelizable] \"nt.monte_carlo_fn\" method to Monte-Carlo estimate compute-heavy kernels, and we have established reasonable convergence for a WideResNet in Figure 2. The question of how good of a tradeoff between accuracy and time / memory the MC method provides is still admittedly open and left for future work.\n\n3) Pooling CNN kernels is arguably an emerging field of study, and we believe that as groups with large computing power demonstrate their good performance, studying these kernels (e.g. on small datasets) and developing novel approximation / mimicking / \u201cinspired-by\u201d techniques will attract a lot of research attention. We believe our library will facilitate such research greatly, and serve as a platform to deliver new results to the users."}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "S1eqydg2iB", "original": null, "number": 4, "cdate": 1573812193943, "ddate": null, "tcdate": 1573812193943, "tmdate": 1573823541082, "tddate": null, "forum": "SklD9yrFPS", "replyto": "H1emYsn6Yr", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "Suggestions implemented!", "comment": "Thank you for the careful review and great suggestions! We believe to have addressed all your comments, and hope that you could increase the score as a result.\n\n\n--------------------------------------------------------------------------------------------------\n>>> I would like to see an additional metric for performance comparison of probabilistic models, which is often used in the GP literature: mean negative log probability.\n\nThank you for the suggestion, we have added negative log likelihood (NLL) measurement in the updated version. With model\u2019s marginal likelihood (train) we did model selection across different depths and plotted accuracy/mean squared error/marginal NLL. In the appendix (Figure 7), we included test NLL\u2019s for fully connected and convolutional models. Since the predictive covariance of the WideResNet kernel has high condition number (due to the pooling layers, see https://openreview.net/pdf?id=Bkx1mxSKvB section C), obtaining numerical stable NLL measures was more challenging.\n\n\n--------------------------------------------------------------------------------------------------\n>>> It would also be interesting to see how the posterior variance (e.g., Fig. 1 right) evolves over the entire space during training. \n\nThank you for the suggestion, done in the new revision!\n\n\n--------------------------------------------------------------------------------------------------\n>>> I would have preferred a more detailed discussion about the implementation on transforming tensor ops to kernel ops in Section 3.\n\nAgreed - in the new revision, we have expanded the text with section 3.1. demonstrating the tensor-to-kernel ops translation.\n\n\n--------------------------------------------------------------------------------------------------\n>>> For the summary of contributions, can you give the corresponding section number to refer to when you demonstrate each feature? For example, is the 4th feature (i.e., exploring weight space perspective) demonstrated in the paper?\n\nGreat suggestion, done in the new revision. We have also added an experiment demonstrating linearization / taylor expansion (4th feature, section B.6, Figure 6). Please also see the existing example in `examples/weight_space.py` and https://github.com/neural-tangents/neural-tangents#weight-space.\n\n\n--------------------------------------------------------------------------------------------------\n>>> Can the authors elaborate on the ease of expanding their library for the new developments in this field?\n\nThank you for the question, we have elaborated on the process of extending the library to new layers in the new revision in section B.7 (see also new section 3.1 for the mathematical aspect of deriving new NTK/NNGP results). In general, we believe the process to be fairly straightforward, apart from the cases of:\n\n- Certain nonlinearities: to derive the layer kernel propagation expression, the user has to compute the covariance of the nonlinearity (and its derivative, for NTK) applied to correlated Gaussian variables. As discussed in section E (new revision), some such nonlinearities may not have known exact expressions for these covariances, and either \"nt.empirical_kernel_fn\" or other specialized approximations need to be employed.\n\n- Weight sharing between different layers in the network is not currently supported and may require some nontrivial work, but it is on our radar.\n\nFinally, once we de-anonymize the repository, we will be using the Github issue and project tracker to inform and engage the community in the library development and planning, and provide support for users and developers!\n\n\n-------------------------------------------------------------------------------------------------\n>>> Minor issues:\n\n>>> Page 1: Gaussian Procesesses?\n>>> Page 4: it\u2019s infinite?\n>>> Fig. 4: I would have preferred the indices to be placed as subscripts instead of superscripts.\n>>> Page 8: it\u2019s order of dimensions?\n\nThank you, all fixed in the new revision except for the Figure indices: we stick to superscript usage to follow an established tradition in prior work [1-4, ...] of using superscript for layer numbers and subscript for hidden units / channels.\n\n[1] Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and JaschaSohl-dickstein.  Deep neural networks as gaussian processes. https://arxiv.org/pdf/1711.00165.pdf\n\n[2] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani.Gaussian process behaviour in wide deep neural networks. https://arxiv.org/pdf/1804.11271.pdf\n\n[3] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia,Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. https://arxiv.org/pdf/1810.05148\n\n[4] Adri\u00e0 Garriga-Alonso, Carl Edward Rasmussen, and Laurence Aitchison. Deep convolutional networks as shallow gaussian processes. https://arxiv.org/pdf/1808.05587.pdf"}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "Bke8siZhiB", "original": null, "number": 8, "cdate": 1573817245842, "ddate": null, "tcdate": 1573817245842, "tmdate": 1573822643197, "tddate": null, "forum": "SklD9yrFPS", "replyto": "ryxYhr0aKr", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "[2/4] Addressing Major Comments", "comment": "----------------------------------------------------------------------------------------------------\n>>> To be able to vote to accept this paper, I will have to see an experiment that is practically performed with the current library in order to distinguish it from previous work (specifically Lee et al, 2019). In recent work, Arora et al, 2019 (Note: I do not consider this reference in my review other than to be mentioned as an example of an experiment that could be run with your library) run neural tangent kernels on tabular data using kernel SVMs. One other potential example would be a kernel SVM in this manner on CIFAR-10. An alternative example would be to exploit the Gaussian process representation and test out both NTKs and NNGPs in comparison to standard kernels for GPs and NNs on UCI regression tasks.\n\nPlease note that _almost all_ the experiments in our paper used features specific to our library only. Precisely:\n\n- Figures 2, 3 require computing exact kernels for the infinitely WideResNet. \n- Figure 5 demonstrates scalability to multi-gpu machines.\n- All code listings demonstrate the main feature of our library which is seamless definition of any neural network architecture of both finite and infinite widths at no extra mental/typing cost.\n- Figure 6 (new revision) uses higher-order taylor expansion of a neural network.\n\nWe stress that _none_ of the above were open-sourced or used in / necessary for Lee et al, 2019 [P.S. on a minor note, Figure 1 used the analytic Erf kernel, derived but not released / used in Lee et al, 2019]\n\nAll this having been said, we agree that it would be nice to have a practical demonstration of the convenience provided by Neural Tangents on the example of Arora et al.\u2019s results on the UCI dataset, as you suggested. Arora et al. provided clean code to reproduce their experiments. We seamlessly substituted the NTK implementation of Arora et al. with Neural Tangents. As a result, we were able to consider a wider range of architectures, finding that by selecting models on a per-experiment basis we were able to provide a marginal improvement of the Arora et al. result from 81.95% to 82.03%. We include a discussion of this experiment in Appendix C.\n\n\n----------------------------------------------------------------------------------------------------\n[...] >>> Again, I am very concerned with originality in comparison to Lee et al, 2019. Even checking out the link to their codebase provides a github repo that is quite similar to this one. Given that ICLR is a venue of similar domain to NeurIPS, it\u2019s not clear to me why this paper ought to be anything other than a separate supporting tech report. If this paper had been submitted to something like SysML (edit: or JMLR MLOSS), I would see the distinctness instead.\n\nPlease see replies above. TL;DR code released with the submission of Lee et al. 2019 had a tiny fraction of the functionality our library offers, and (from personal correspondence) their paper neither used nor had to use the main features of our library (flexible, general, and efficient specifications and evaluation of exact NNGP/NTK kernels). We again stress that this can be verified by the diff link mentioned above (https://github.com/neural-tangents/neural-tangents/compare/Lee_et_al_2019..master), and we are happy to clarify any other questions regarding the overlap.\n\n\n----------------------------------------------------------------------------------------------------\n>>> Clarity: I find the paper to be extremely well-written and easy to follow. The addition of code snippets throughout is very well done, even if it\u2019s a bit overkill. I don\u2019t know what adding a half page long description of an infinitely wide WideResNet adds to the paper when that space could be better used by another experiment. \n\nWe have decided to highlight the WideResNet snippet since it:\n\n1) Presents exactly the use-case that would be extremely tedious / not practical at all to implement without our library. Our library handles all the topology, striding, padding, performance optimizations etc, while allowing to specify the infinite networks simultaneously with the finite model, at _absolutely no_ extra mental effort.\n\n2) Gives the reader a non-trivial, practical example of using our library for complex models.\n\nNonetheless, we agree that results on the UCI dataset might be useful as well and so we have added them in section C in the new revision!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "S1g-lnZhir", "original": null, "number": 9, "cdate": 1573817321057, "ddate": null, "tcdate": 1573817321057, "tmdate": 1573822164598, "tddate": null, "forum": "SklD9yrFPS", "replyto": "ryxYhr0aKr", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "[1/4] Addressing Major Comments", "comment": "Thanks for your careful review of our work. We\u2019re happy that you enjoyed the paper, found the library easy to use, and that you might use it in the future! We hope that this fact alone helps to convince you that researchers in the community might benefit from learning about Neural Tangents at ICLR.\n\n----------------------------------------------------------------------------------------------------\n>>> While I really enjoyed reading the paper and believe that this library could be extremely practically useful, I vote to reject this paper because I do not feel that it has sufficient novelty to be a paper on its own in light of Lee et al, 2019. [...]\n\n>>> However, my primary concern with this paper is that it\u2019s not sufficiently distinct from the previous work of Lee et al, 2019. After all, most of the experiments in that paper would have required the type of implementation that is described in greater detail in this paper.\n\nWe would like to clarify the relationship between this work and Lee et al. 2019. We have added a summary of this discussion to section A in the revised version of the manuscript. \n\nTL;DR \n\n1) Neural Tangents (NT) is emphatically different from the code of Lee et al, 2019 at the time of their paper submission (larger by thousands of lines of code (LOC)), and \n\n2) The features in NT extend far beyond what was open-sourced with Lee et al, 2019 at the time of submission _and_ what could have been necessary for that paper.\n\n\nSpecifically:\n\n1) Raw code difference\n- Lee et al, 2019 at the time of submission: https://github.com/google/neural-tangents/tree/d42cc0f0281001d5885ed3969b61d69c8ccf4a15\n- Our codebase at HEAD: https://github.com/neural-tangents/neural-tangents\n- Most importantly, the +9,500/-2,500 LOC diff: https://github.com/neural-tangents/neural-tangents/compare/Lee_et_al_2019..master (we imported their code into a separate branch of our repo to show the difference)\n\n2) Feature difference: at the time of the submission of Lee et al. 2019, their open-sourced code only had the following features (see github link above):\n\n- Linearization (equivalent of \"nt.linearize\"),\n- Single-sample estimate of the empirical ntk (equivalent of \"nt.empirical_ntk_fn\"),\n- Finite-time output mean evolution (equivalent of \"nt.predict.gradient_descent_mse\", \"nt.predict.gradient_descent\", \"nt.predict.momentum\").\n\nOne can easily check that the vast majority of experiments in Lee et al. only used this functionality of Neural Tangents. Only a few experiments used very simple fully-connected ReLU kernels, which were not produced with Neural Tangents but an internal Tensorflow implementation (known via personal correspondence with the authors). These kernels are not unto themselves specific to Lee et al 2019; for example the arccosine kernel dates back to Cho and Saul in 2009. \n\nAny post-submission developments in their repository that were neither used nor necessary for their paper should not be considered published results but rather work concurrent to ours. [P.S. on an unrelated note, one could argue that even treating the original Lee et al, 2019 codebase as published is debatable, since they themselves, along with at least one other paper (https://arxiv.org/pdf/1905.13654v2.pdf) cited the code as a separate unpublished work]\n\nNT has all the features of Lee at al 2019 (with authors\u2019 permission) at the time of submission, and:\n- Most notably, a high-level modular library \"nt.stax\" to specify and do inference with infinite NTK/NNGPs analytically for many NN layers. This is the highlight of the paper and was not used in / released with / necessary for Lee et. al, 2019.\n- Multi-device GPU/TPU support.\n- Parallelizable Monte-Carlo sampling of NTK and NNGP kernels.\n- Taylor series function expansion.\n- A richer suite of prediction functions including finite/infinite time NTK/NNGP mean/covariance prediction.\n- Unification of all of these features to work together seamlessly.\n\nWe hope this, together with the updates in the text, helps clarify the contributions of our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "B1gt-wb3or", "original": null, "number": 6, "cdate": 1573816065358, "ddate": null, "tcdate": 1573816065358, "tmdate": 1573821792225, "tddate": null, "forum": "SklD9yrFPS", "replyto": "ryxYhr0aKr", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "[4/4] Summary Diff Table", "comment": "For your convenience, we provide in this comment the diff\nhttps://github.com/neural-tangents/neural-tangents/compare/Lee_et_al_2019..master\n\nand the brief table of differences between the code released by Lee et al., 2019 at the time of their submission, and our work.\n\nThank you again for the careful review. We hope, having addressed your concerns regarding differences between this work and Lee et al. that you will consider increasing your score.\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551 Codebase                                    \u2551 Released with Lee et al., 2019 \u2551 Ours                                                             \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Line of code                                \u2551 1400+                                          \u2551 6600+                                                            \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Empirical kernel                        \u2551 NTK                                               \u2551 NTK/NNGP                                                  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Weight space linearization      \u2551 Yes                                                \u2551 Yes                                                               \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Higher-order Taylor series expansion\u2551 No                                   \u2551 Yes                                                                \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Monte-carlo sampling for empirical NTK/NNGP \u2551 No                \u2551 Yes                                                                \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Multi-device parallelization     \u2551 No                                                \u2551 Yes                                                                \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Dense Layer                               \u2551 No                                                 \u2551 Yes                                                               \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Nonlinearities                            \u2551 No                                                \u2551 ReLU, Erf, Abs, LeakyRelu, ABReLU         \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Convolution                                \u2551 No                                                \u2551 Any paddings, strides, filter shapes       \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Average pooling                        \u2551 No                                                \u2551 Global, local, any strides / shapes          \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Flattening                                   \u2551 No                                                 \u2551 Yes                                                               \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 LayerNorm                                 \u2551 No                                                 \u2551 Yes                                                               \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Skip-connection                         \u2551 No                                                \u2551 Yes                                                                \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Global self-attention                 \u2551 No                                                \u2551 Yes                                                                \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Finite-time inference of the posterior \u2551 NTK, Mean                    \u2551 Mean, covariance, NNGP/NTK                \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Infinite-time inference of the posterior \u2551 No                               \u2551 Mean, covariance, NNGP/NTK                 \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Dropout                                      \u2551 No                                                 \u2551 Coming soon                                              \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551 Standard (non-ntk) parameterization \u2551 No                                  \u2551 Coming soon                                              \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d"}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "HJxkA_b3sS", "original": null, "number": 7, "cdate": 1573816519313, "ddate": null, "tcdate": 1573816519313, "tmdate": 1573816561656, "tddate": null, "forum": "SklD9yrFPS", "replyto": "ryxYhr0aKr", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment", "content": {"title": "[3/4] Addressing Minor Comments", "comment": "----------------------------------------------------------------------------------------------------\n>>> In Figure 1 on the right, I would have liked to have seen the posterior predictive for a NNGP with the same kernel as well. \n\nGreat idea, done in the new revision (Figure 1, left in red).\n\n\n----------------------------------------------------------------------------------------------------\n>>> In Figure 2, why is the NNGP slower to converge to the analytic values here? Obviously, the rates of convergence are the same, but the constants seem different.\n\nCurrently we are not aware of any rigorous results explaining the respective rates or constants. A very naive take is that empirical NTK, as an outer product of Jacobians, sums over a larger number of [admittedly dependent] random entries (\"O(N^2 * d)\", where \"N\" is width and \"d\" is depth) than NNGP, which is an outer product of the activations (\"O(N)\"). However, since the same random variables are involved in the computation of the NTK and NNGP, we are not certain the observed effect is not architecture / dataset dependent.\n\n\n----------------------------------------------------------------------------------------------------\n>>> In Figure 3 (and throughout the experiments), does \u201cfull Bayesian inference for CIFAR10\u201d mean that you treated the classification labels as regression targets? If so, how was classification error measured.\n\nYou are correct, the targets were converted to mean-zero vectors like \"[-0.1, \u2026, 0.9, \u2026, -0.1]\", where 0.9 is assigned to the correct class index, and -0.1 to all others. The error was computed as the \"1-accuracy\" where accuracy is the fraction of samples where the argmax of the model output is at the correct class index. We have updated the description of Figure 3 in the new revision.\n\n\n----------------------------------------------------------------------------------------------------\n>>> In Section 3.1, you mention that the library \u201cleverages block-diagonal structure\u201d to invert the CIFAR10 covariance matrix for each class (still 50k x 50k). Possibly this is because I haven\u2019t had the chance to use TPUs, but I\u2019m currently struggling to see how one could form and invert (via Choleskys) matrices of this size (50k x 50k) on a standard GPU (or CPU). Could the authors please clarify how they did this (whether through iterative methods, another structure exploiting trick, lots of memory, etc.)?\n\nThank you for the question, \n\n1) A 32-bit 50k x 50k matrix has a size of ~10Gb, which (together with auxiliary variables like targets and train-test kernel matrix) is pushing the limit of many modern GPUs/TPUs and inference is indeed not feasible on these accelerators. \n\n2) However, calling \"[jax.]scipy.linalg.solve(..., sym_pos=True)\" is perfectly doable on a CPU, and runs in about 3 minutes on a laptop with 2.9 Ghz Intel Core i9 (6 cores) and 32 Gb of RAM for a 50k x 50k training set kernel matrix and 50k x 10 training targets. [P.S. due to a technical bug in JAX/XLA (https://github.com/google/jax/issues/1644) at the time of writing \"jax.scipy.linalg.solve\" fails for matrices larger than 46,340 x 46,340, but the issue is unrelated to compute/memory and the original \"scipy.linalg.solve\" works fine for 50k.]\n\n3) Our library allows to effortlessly leverage both fast GPUs and typically larger amount of CPU RAM by computing the kernel on [multiple] GPUs and performing inference on CPU. For this the user only needs to pass \"store_on_device=False\" to the \"batch\" decorator (https://github.com/neural-tangents/neural-tangents/blob/408c07d938458bbe80da3e66e420eb1fb84cbe33/neural_tangents/utils/batch.py#L395), i.e. making the kernel be computed in batches on [multiple] GPUs and collected into a single matrix for further inference in the CPU RAM.\n\nWe have expanded the discussion in the new revision (section B.3) to mention the above. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SklD9yrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1882/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1882/Authors|ICLR.cc/2020/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149519, "tmdate": 1576860545755, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Authors", "ICLR.cc/2020/Conference/Paper1882/Reviewers", "ICLR.cc/2020/Conference/Paper1882/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Comment"}}}, {"id": "BJgNczUaFB", "original": null, "number": 1, "cdate": 1571803787946, "ddate": null, "tcdate": 1571803787946, "tmdate": 1572972411384, "tddate": null, "forum": "SklD9yrFPS", "replyto": "SklD9yrFPS", "invitation": "ICLR.cc/2020/Conference/Paper1882/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work develops a library for working with a class of infinitely wide neural networks, in particular those corresponding to neural tangent kernels (NTKs) and neural network Gaussian processes (NNGPs). The theory for these two kernels was well developed in a series of recent papers, and this library provides an automatic way to transform any appropriate neural net architecture into its corresponding NTK and NNGP.\n\nInfinitely wide neural networks have been a popular subject of theoretical research and have been observed to have highly nontrivial performance on a variety of tasks (e.g. CIFAR-10 classification). It's really nice to see the development of such a library, which I believe could benefit the deep learning community a lot, especially for theoretical research on NTK.\n\nI appreciate this work a lot. Currently I can only give weak accept instead of accept for a couple of reasons:\n1. The theory and formulae of NTKs and NNGPs were well developed. This work mostly consists of implementing and modularizing them. The research contribution is relatively low.\n2. As commented in the paper and if I understand correctly, the current library cannot scale to large datasets for CNNs with pooling. This would make the computation much more expensive (and probably infeasible without additional techniques and huge computing power) as mentioned in [Novak et al. 2019] and [Arora et al. 2019]. However pooling seems extremely useful for NTKs and NNGPs on image datasets. I think this makes this work somewhat less exciting than it may sound."}, "signatures": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1882/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["romann@google.com", "xlc@google.com", "jh2084@cam.ac.uk", "jaehlee@google.com", "alemi@google.com", "jaschasd@google.com", "schsam@google.com"], "title": "Neural Tangents: Fast and Easy Infinite Neural Networks in Python", "authors": ["Roman Novak", "Lechao Xiao", "Jiri Hron", "Jaehoon Lee", "Alexander A. Alemi", "Jascha Sohl-Dickstein", "Samuel S. Schoenholz"], "pdf": "/pdf/ea2e2c780e8e418571914368ffe2a7b563d91e06.pdf", "TL;DR": "Keras for infinite neural networks.", "abstract": "Neural Tangents is a library for working with infinite-width neural networks. It provides a high-level API for specifying complex and hierarchical neural network architectures. These networks can then be trained and evaluated either at finite-width as usual or in their infinite-width limit. Infinite-width networks can be trained analytically using exact Bayesian inference or using gradient descent via the Neural Tangent Kernel. Additionally, Neural Tangents provides tools to study gradient descent training dynamics of wide but finite networks in either function space or weight space.\n\nThe entire library runs out-of-the-box on CPU, GPU, or TPU. All computations can be automatically distributed over multiple accelerators with near-linear scaling in the number of devices. \n\nIn addition to the repository below, we provide an accompanying interactive Colab notebook at\nhttps://colab.research.google.com/github/google/neural-tangents/blob/master/notebooks/neural_tangents_cookbook.ipynb\n", "code": "https://www.github.com/google/neural-tangents", "keywords": ["Infinite Neural Networks", "Gaussian Processes", "Neural Tangent Kernel", "NNGP", "NTK", "Software Library", "Python", "JAX"], "paperhash": "novak|neural_tangents_fast_and_easy_infinite_neural_networks_in_python", "_bibtex": "@inproceedings{\nNovak2020Neural,\ntitle={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},\nauthor={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SklD9yrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/4f54edeea3da487e22d5e3bb201aec9b4ca4aaef.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SklD9yrFPS", "replyto": "SklD9yrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575736852184, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1882/Reviewers"], "noninvitees": [], "tcdate": 1570237730921, "tmdate": 1575736852198, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1882/-/Official_Review"}}}], "count": 17}