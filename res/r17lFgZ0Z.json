{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730171470, "tcdate": 1509128426755, "number": 608, "cdate": 1518730171457, "id": "r17lFgZ0Z", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "r17lFgZ0Z", "original": "B1xgtxZC-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.", "pdf": "/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf", "paperhash": "sharma|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation", "_bibtex": "@misc{\nsharma2018relevance,\ntitle={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\nauthor={Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer},\nyear={2018},\nurl={https://openreview.net/forum?id=r17lFgZ0Z},\n}", "keywords": ["task-oriented dialog", "goal-oriented dialog", "nlg evaluation", "natural language generation", "automated metrics for nlg"], "authors": ["Shikhar Sharma", "Layla El Asri", "Hannes Schulz", "Jeremie Zumer"], "authorids": ["shikhar.sharma@microsoft.com", "layla.elasri@microsoft.com", "hannes.schulz@microsoft.com", "jeremie_zumer@hotmail.com"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260082877, "tcdate": 1517249977455, "number": 664, "cdate": 1517249977431, "id": "SJW6SJpHz", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "r17lFgZ0Z", "replyto": "r17lFgZ0Z", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper tackles a very important problem: evaluating natural language generation. The paper presents an overview of existing unsupervised metrics, and looks at how they correlate with human evaluation scores. This is important work and the empirical conclusions are useful to the community, but the datasets used are too limited and the authors agree it would be better to use newer bigger and more diverse datasets suggested by reviewers for drawing more general conclusions. This work would indeed be much stronger if it relied on better, more recent datasets; therefore publication as is seems premature.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.", "pdf": "/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf", "paperhash": "sharma|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation", "_bibtex": "@misc{\nsharma2018relevance,\ntitle={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\nauthor={Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer},\nyear={2018},\nurl={https://openreview.net/forum?id=r17lFgZ0Z},\n}", "keywords": ["task-oriented dialog", "goal-oriented dialog", "nlg evaluation", "natural language generation", "automated metrics for nlg"], "authors": ["Shikhar Sharma", "Layla El Asri", "Hannes Schulz", "Jeremie Zumer"], "authorids": ["shikhar.sharma@microsoft.com", "layla.elasri@microsoft.com", "hannes.schulz@microsoft.com", "jeremie_zumer@hotmail.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642478482, "tcdate": 1511171318125, "number": 1, "cdate": 1511171318125, "id": "Hy0xrQegf", "invitation": "ICLR.cc/2018/Conference/-/Paper608/Official_Review", "forum": "r17lFgZ0Z", "replyto": "r17lFgZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper608/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Useful conclusion; not enough novel contribution to warranty publication", "rating": "4: Ok but not good enough - rejection", "review": "This paper's main thesis is that automatic metrics like BLEU, ROUGE, or METEOR is suitable for task-oriented natural language generation (NLG). In particular, the paper presents a counterargument to \"How NOT To Evaluate Your Dialogue System...\" where Wei et al argue that automatic metrics are not correlated or only weakly correlated with human eval on dialogue generation. The authors here show that the performance of various NN models as measured by automatic metrics like BLEU and METEOR is correlated with human eval.\n\nOverall, this paper presents a useful conclusion: use METEOR for evaluating task oriented NLG. However, there isn't enough novel contribution in this paper to warrant a publication. Many of the details unnecessary: 1) various LSTM model descriptions are unhelpful given the base LSTM model does just as well on the presented tasks 2) Many embedding based eval methods are proposed but no conclusions are drawn from any of these techniques.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.", "pdf": "/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf", "paperhash": "sharma|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation", "_bibtex": "@misc{\nsharma2018relevance,\ntitle={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\nauthor={Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer},\nyear={2018},\nurl={https://openreview.net/forum?id=r17lFgZ0Z},\n}", "keywords": ["task-oriented dialog", "goal-oriented dialog", "nlg evaluation", "natural language generation", "automated metrics for nlg"], "authors": ["Shikhar Sharma", "Layla El Asri", "Hannes Schulz", "Jeremie Zumer"], "authorids": ["shikhar.sharma@microsoft.com", "layla.elasri@microsoft.com", "hannes.schulz@microsoft.com", "jeremie_zumer@hotmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642478383, "id": "ICLR.cc/2018/Conference/-/Paper608/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper608/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper608/AnonReviewer3", "ICLR.cc/2018/Conference/Paper608/AnonReviewer1", "ICLR.cc/2018/Conference/Paper608/AnonReviewer2"], "reply": {"forum": "r17lFgZ0Z", "replyto": "r17lFgZ0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642478383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642478445, "tcdate": 1511823512671, "number": 2, "cdate": 1511823512671, "id": "SJWidMceG", "invitation": "ICLR.cc/2018/Conference/-/Paper608/Official_Review", "forum": "r17lFgZ0Z", "replyto": "r17lFgZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper608/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "borderline", "rating": "5: Marginally below acceptance threshold", "review": "1) This paper conducts an empirical study of different unsupervised metrics' correlations in task-oriented dialogue generation. This paper can be considered as an extension of Liu, et al, 2016 while the later one did an empirical study in non-task-oriented dialogue generation.  \n\n2)My questions are as follows:\ni) The author should give the more detailed definition of what is non-task-oriented and task-oriented dialogue system. The third paragraph in the introduction should include one use case about non-task-oriented dialogue system, such as chatbots.\nii) I do not think DSTC2 is good dataset here in the experiments. Maybe the dataset is too simple with limited options or the training/testing are very similar to each other, even the random could achieve very good performance in table 1 and 2. For example, the random solution is only 0.005 (out of 1) worse then d-scLSTM, and it also has a close performance compared with other metrics. Even the random could achieve 0.8 (out of 1) in BLEU, this is a very high performance.\niii) About the scatter plot Figure 3, the authors should include more points with a bad metric score (similar to Figure 1 in Liu 2016). \niv) About the correlations in figure b, especially for BLEU and METEOR, I do not think they have good correlations with human's judgments. \nv) BLEU usually correlates with human better when 4 or more references are provided. I suggest the authors include some dataset with 4 or more references instead of just 2 references.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.", "pdf": "/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf", "paperhash": "sharma|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation", "_bibtex": "@misc{\nsharma2018relevance,\ntitle={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\nauthor={Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer},\nyear={2018},\nurl={https://openreview.net/forum?id=r17lFgZ0Z},\n}", "keywords": ["task-oriented dialog", "goal-oriented dialog", "nlg evaluation", "natural language generation", "automated metrics for nlg"], "authors": ["Shikhar Sharma", "Layla El Asri", "Hannes Schulz", "Jeremie Zumer"], "authorids": ["shikhar.sharma@microsoft.com", "layla.elasri@microsoft.com", "hannes.schulz@microsoft.com", "jeremie_zumer@hotmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642478383, "id": "ICLR.cc/2018/Conference/-/Paper608/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper608/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper608/AnonReviewer3", "ICLR.cc/2018/Conference/Paper608/AnonReviewer1", "ICLR.cc/2018/Conference/Paper608/AnonReviewer2"], "reply": {"forum": "r17lFgZ0Z", "replyto": "r17lFgZ0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642478383}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642478407, "tcdate": 1511859212182, "number": 3, "cdate": 1511859212182, "id": "rkEMEscxf", "invitation": "ICLR.cc/2018/Conference/-/Paper608/Official_Review", "forum": "r17lFgZ0Z", "replyto": "r17lFgZ0Z", "signatures": ["ICLR.cc/2018/Conference/Paper608/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Useful but incremental contribution to NLG/dialog metrics research", "rating": "5: Marginally below acceptance threshold", "review": "The authors present a solid overview of unsupervised metrics for NLG, and perform a correlation analysis between these metrics and human evaluation scores on two task-oriented dialog generation datasets using three LSTM-based models. They find weak but statistically significant correlations for a subset of the evaluated metrics, an improvement over the situation that has been observed in open-domain dialog generation.\nOther than the necessarily condensed model section (describing a model explained at greater length in a different work) the paper is quite clear and well-written throughout, and the authors' explication of metrics like BLEU and greedy matching is straightforward and readable. But the novel work in the paper is limited to the human evaluations collected and the correlation studies run, and the authors' efforts to analyze and extend these results fall short of what I'd like to see in a conference paper.\nSome other points:\n1. Where does the paper's framework for response generation (i.e., dialog act vectors and delexicalized/lexicalized slot-value pairs) fit into the landscape of task-oriented dialog agent research? Is it the dominant or state-of-the-art approach?\n2. The sentence \"This model is a variant of the \u201cld-sc-LSTM\u201d model proposed by Sharma et al. (2017) which is based on an encoder-decoder framework\" is ambiguous; what is apparently meant is that Sharma et al. (2017) introduced the hld-scLSTM, not simply the ld-scLSTM.\n3. What happens to the correlation coefficients when exact reference matches (a significant component of the highly-rated upper right clusters) are removed?\n4. The paper's conclusion naturally suggests the question of whether these results extend to more difficult dialog generation datasets. Can the authors explain why the datasets used here were chosen over e.g. El Asri et al. (2017) and Novikova et al. (2016)?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.", "pdf": "/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf", "paperhash": "sharma|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation", "_bibtex": "@misc{\nsharma2018relevance,\ntitle={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\nauthor={Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer},\nyear={2018},\nurl={https://openreview.net/forum?id=r17lFgZ0Z},\n}", "keywords": ["task-oriented dialog", "goal-oriented dialog", "nlg evaluation", "natural language generation", "automated metrics for nlg"], "authors": ["Shikhar Sharma", "Layla El Asri", "Hannes Schulz", "Jeremie Zumer"], "authorids": ["shikhar.sharma@microsoft.com", "layla.elasri@microsoft.com", "hannes.schulz@microsoft.com", "jeremie_zumer@hotmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642478383, "id": "ICLR.cc/2018/Conference/-/Paper608/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper608/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper608/AnonReviewer3", "ICLR.cc/2018/Conference/Paper608/AnonReviewer1", "ICLR.cc/2018/Conference/Paper608/AnonReviewer2"], "reply": {"forum": "r17lFgZ0Z", "replyto": "r17lFgZ0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642478383}}}, {"tddate": null, "ddate": null, "tmdate": 1515186716907, "tcdate": 1515186716907, "number": 3, "cdate": 1515186716907, "id": "HyBXcDamM", "invitation": "ICLR.cc/2018/Conference/-/Paper608/Official_Comment", "forum": "r17lFgZ0Z", "replyto": "Hy0xrQegf", "signatures": ["ICLR.cc/2018/Conference/Paper608/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper608/Authors"], "content": {"title": "response from authors", "comment": "Thank you for the valuable feedback!\n\n1) Our results on metrics contradict previous work by Wen et al. (2015) which observed much lower BLEU scores for the same datasets. We presented all the base models so that we could have a clearer comparison between their results and ours.\n2) We couldn't find any clear correlation trends for the embedding based metrics. We will report that in the paper. In future work, we will look at larger datasets as mentioned above to AnonReviewer2 and AnonReviewer1 where there might possibly be clearer trends."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.", "pdf": "/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf", "paperhash": "sharma|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation", "_bibtex": "@misc{\nsharma2018relevance,\ntitle={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\nauthor={Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer},\nyear={2018},\nurl={https://openreview.net/forum?id=r17lFgZ0Z},\n}", "keywords": ["task-oriented dialog", "goal-oriented dialog", "nlg evaluation", "natural language generation", "automated metrics for nlg"], "authors": ["Shikhar Sharma", "Layla El Asri", "Hannes Schulz", "Jeremie Zumer"], "authorids": ["shikhar.sharma@microsoft.com", "layla.elasri@microsoft.com", "hannes.schulz@microsoft.com", "jeremie_zumer@hotmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730693, "id": "ICLR.cc/2018/Conference/-/Paper608/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r17lFgZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper608/Authors|ICLR.cc/2018/Conference/Paper608/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper608/Authors|ICLR.cc/2018/Conference/Paper608/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper608/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper608/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper608/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper608/Reviewers", "ICLR.cc/2018/Conference/Paper608/Authors", "ICLR.cc/2018/Conference/Paper608/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730693}}}, {"tddate": null, "ddate": null, "tmdate": 1515186629371, "tcdate": 1515186629371, "number": 2, "cdate": 1515186629371, "id": "Bk66KPaQz", "invitation": "ICLR.cc/2018/Conference/-/Paper608/Official_Comment", "forum": "r17lFgZ0Z", "replyto": "SJWidMceG", "signatures": ["ICLR.cc/2018/Conference/Paper608/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper608/Authors"], "content": {"title": "response from authors", "comment": "Thank you for the valuable feedback!\n\n2)\n(i) We will make these changes.\n(ii) We agree with this analysis. We also found the DSTC2 dataset to be very simple for the NLG task. In future work we will be using the datasets from El Asri et al. (2017) and Novikova et al. (2016) as mentioned by AnonReviewer2 as well.\n(iii) We also include all the points but due to these task-oriented dialog datasets datasets being very simple most of these are overlapping in the upper right cluster.\n(iv) Most of the points being in the upper right cluster does distort the correlation values.\n(v) The dataset by Novikova et al. (2016) has larger number of references. We will use that dataset in future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.", "pdf": "/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf", "paperhash": "sharma|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation", "_bibtex": "@misc{\nsharma2018relevance,\ntitle={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\nauthor={Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer},\nyear={2018},\nurl={https://openreview.net/forum?id=r17lFgZ0Z},\n}", "keywords": ["task-oriented dialog", "goal-oriented dialog", "nlg evaluation", "natural language generation", "automated metrics for nlg"], "authors": ["Shikhar Sharma", "Layla El Asri", "Hannes Schulz", "Jeremie Zumer"], "authorids": ["shikhar.sharma@microsoft.com", "layla.elasri@microsoft.com", "hannes.schulz@microsoft.com", "jeremie_zumer@hotmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730693, "id": "ICLR.cc/2018/Conference/-/Paper608/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r17lFgZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper608/Authors|ICLR.cc/2018/Conference/Paper608/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper608/Authors|ICLR.cc/2018/Conference/Paper608/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper608/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper608/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper608/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper608/Reviewers", "ICLR.cc/2018/Conference/Paper608/Authors", "ICLR.cc/2018/Conference/Paper608/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730693}}}, {"tddate": null, "ddate": null, "tmdate": 1515186402668, "tcdate": 1515186402668, "number": 1, "cdate": 1515186402668, "id": "ByiJYDTXM", "invitation": "ICLR.cc/2018/Conference/-/Paper608/Official_Comment", "forum": "r17lFgZ0Z", "replyto": "rkEMEscxf", "signatures": ["ICLR.cc/2018/Conference/Paper608/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper608/Authors"], "content": {"title": "response from authors", "comment": "Thank you for the valuable feedback!\n\n1. Most of the research in task oriented dialog generation research uses dialog / speech acts and slots as they significantly help. It is expensive to collect these labels and there has been recent work on doing task-oriented dialog generation end-to-end without these labels. However we focus on NLG in a modular framework instead of end-to-end dialog generation and acts and slots are the dominant methodology in that area.\n2. We'll update this in the paper. They did not introduce the hld-lscLSTM model.\n3. We haven't looked at that because most of the points are actually within that cluster but this could be an interesting analysis to improve the system.\n4. This work was finished earlier than the conference submission period and at that time these datasets were very recent. We will be using these datasets in future work as they are much bigger and diverse than earlier datasets."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation", "abstract": "Automated metrics such as BLEU are widely used in the machine translation literature. They have also been used recently in the dialogue community for evaluating dialogue response generation. However, previous work in dialogue response generation has shown that these metrics do not correlate strongly with human judgment in the non task-oriented dialogue setting. Task-oriented dialogue responses are expressed on narrower domains and exhibit lower diversity. It is thus reasonable to think that these automated metrics would correlate well with human judgment in the task-oriented setting where the generation task consists of translating dialogue acts into a sentence. We conduct an empirical study to confirm whether this is the case. Our findings indicate that these automated metrics have stronger correlation with human judgments in the task-oriented setting compared to what has been observed in the non task-oriented setting. We also observe that these metrics correlate even better for datasets which provide multiple ground truth reference sentences. In addition, we show that some of the currently available corpora for task-oriented language generation can be solved with simple models and advocate for more challenging datasets.", "pdf": "/pdf/5bfad3c5eb18f4114f478aef529e4794872e076f.pdf", "paperhash": "sharma|relevance_of_unsupervised_metrics_in_taskoriented_dialogue_for_evaluating_natural_language_generation", "_bibtex": "@misc{\nsharma2018relevance,\ntitle={Relevance of Unsupervised Metrics in Task-Oriented Dialogue for Evaluating Natural Language Generation},\nauthor={Shikhar Sharma and Layla El Asri and Hannes Schulz and Jeremie Zumer},\nyear={2018},\nurl={https://openreview.net/forum?id=r17lFgZ0Z},\n}", "keywords": ["task-oriented dialog", "goal-oriented dialog", "nlg evaluation", "natural language generation", "automated metrics for nlg"], "authors": ["Shikhar Sharma", "Layla El Asri", "Hannes Schulz", "Jeremie Zumer"], "authorids": ["shikhar.sharma@microsoft.com", "layla.elasri@microsoft.com", "hannes.schulz@microsoft.com", "jeremie_zumer@hotmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730693, "id": "ICLR.cc/2018/Conference/-/Paper608/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r17lFgZ0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper608/Authors|ICLR.cc/2018/Conference/Paper608/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper608/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper608/Authors|ICLR.cc/2018/Conference/Paper608/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper608/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper608/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper608/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper608/Reviewers", "ICLR.cc/2018/Conference/Paper608/Authors", "ICLR.cc/2018/Conference/Paper608/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730693}}}], "count": 8}