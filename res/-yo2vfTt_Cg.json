{"notes": [{"id": "-yo2vfTt_Cg", "original": "KODhAATnYkK", "number": 785, "cdate": 1601308091395, "ddate": null, "tcdate": 1601308091395, "tmdate": 1614985626433, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "so1r_Wla-3K", "original": null, "number": 1, "cdate": 1610040533540, "ddate": null, "tcdate": 1610040533540, "tmdate": 1610474143374, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper considers adaptive stochastic optimization methods and shows that they can be re-interpreted as first order trust region methods with an ellipsoidal trust region, they consider a related second order method, and they show convergence properties and empirical results.\n\nThe results are of interest, but the significance of some of the results is not clear.  Part of this has to do with substance, and part of this has to do with presentation that can be improved.  Empirical results are weak, including appropriate baselines and details of the empirical results."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040533526, "tmdate": 1610474143357, "id": "ICLR.cc/2021/Conference/Paper785/-/Decision"}}}, {"id": "PMu5QYC63-1", "original": null, "number": 6, "cdate": 1605684110639, "ddate": null, "tcdate": 1605684110639, "tmdate": 1605684110639, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "CZel7TGzDC", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment", "content": {"title": "Baselines should be improved.", "comment": "Thanks to the authors for their comments.\n\nMy main concern is still lack of good baseline to compare against existing work, specifically because of changes in critical hyper-parameter:  batch size when comparing various methods.  Justification in the footnote 12, pg. 26. does not explain the discrepancy of why first order methods did not use 128/512 in the experiments. Rerunning these experiments with exact same batch size is recommended.\n\nAutoencoder problem on MNIST is a common baseline for comparison and does not require more than a single GPU to compare against KFAC or Shampoo (both are kronecker factored preconditioners).   For example, recent work has even reused them for comparisons: https://arxiv.org/abs/2006.08877 Code also seems to be available to run also. It would help us understand how well the method works against existing work in known settings.  There also seems to be several KFAC baselines availabe in torch, and tensorflow on github.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper785/AnonReviewer1"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper785/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-yo2vfTt_Cg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper785/Authors|ICLR.cc/2021/Conference/Paper785/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867201, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment"}}}, {"id": "cKhutvcKMzg", "original": null, "number": 5, "cdate": 1605176502500, "ddate": null, "tcdate": 1605176502500, "tmdate": 1605176527543, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "Z7_WLIzVt8N", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment", "content": {"title": "Sorry for the confusion..", "comment": "g and B are indeed the sub-sampled gradient and Hessian and they are computed on the same mini-batch! We will clarify this in Appendix C!\n\nRegarding generalization error, we repeat from above: We do not show any test results because this is not what the optimizers are tasked with. Generalization is a very nebulous task and most algorithms that are better at generalization are so because they are worse in training. In our mind, such a comparison is irrelevant unless one presents an algorithm specifically targeted at generalization.\n\nImpact: You are right, we do indeed consider the *currently possible* contribution of second order methods as marginal for the reasons given in the paper but as we also state in the outlook, we do foresee much more frequent use in the future when hardware has advanced. \n\nFinally, we want to thank you for you review and for sharing your concerns!"}, "signatures": ["ICLR.cc/2021/Conference/Paper785/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-yo2vfTt_Cg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper785/Authors|ICLR.cc/2021/Conference/Paper785/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867201, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment"}}}, {"id": "ccUr5Sy4WVt", "original": null, "number": 4, "cdate": 1605176267870, "ddate": null, "tcdate": 1605176267870, "tmdate": 1605176267870, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "jYiVkgDo6L", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment", "content": {"title": "Thank you, please see details below", "comment": "First off, thanks again for your detailed and valuable comments.\n\n3.2 measure of diagonal dominance. We agree that what matters is the closeness of  A^{-1}g and diag(A)^{-1}g. However, note that g is not known a-priori and changes during optimisation as well as from task to task. If A is, however, diagonal, this closeness is guaranteed for all possible vectors g in R^d. That's why!\n\nProposition 1. Right, we will do so! \n\nProposition 2 and Fig 6. Very good point Thanks, we will correct that!\n\nPage 6 after Lemma 2: We think that the Conn book only proves convergence and no rate (correct me if I'm wrong) but Bergou et al. is a great pointer. Thank you very much. We'll correct that too!\n\nAssumption 1: This is a very subtle point. Please note that we mention concentration inequalities so it should be clear from the context that we are talking about high probability statements here."}, "signatures": ["ICLR.cc/2021/Conference/Paper785/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-yo2vfTt_Cg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper785/Authors|ICLR.cc/2021/Conference/Paper785/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867201, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment"}}}, {"id": "CZel7TGzDC", "original": null, "number": 2, "cdate": 1605173264259, "ddate": null, "tcdate": 1605173264259, "tmdate": 1605175367157, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "0CkA4tDkUXb", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment", "content": {"title": "Epoch results in Appendix Fig 10", "comment": "Dear reviewer\n\nthank you very much for sharing your concerns. \n\nQuestion a) The reasons for picking a higher batch size for second orders methods are detailed in footnote 12 (p.26). This is very standard. Furthermore, we do report results over epochs in Appendix Fig. 10. Please take a look.\n\nQuestion d) We are comparing the MNIST autoencoder from that paper which is a common benchmark  (Hinton 2006, Xu 2017a, Martens 2010, Martens 2015). Honestly, the other settings seem outdated to us and so does pre-training. \n\nQuestion b) and c) Unfortunately (?), we are researchers at a public institution and hence we do neither have access to infinite compute power nor do we have a team of research engineers at hand ready to code up whatever experiment setting one can think of.  Note that, neither K-FAC nor Shampoo are available in torch.optim or tensorflow.keras.optimizers. \n\nBenchmarking K-FAC and/or using the K-FAC preconditioner within our framework is a very valuable suggestion though. Thank you! Out of curiosity, why would you consider Shampoo worth benchmarking?\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper785/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-yo2vfTt_Cg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper785/Authors|ICLR.cc/2021/Conference/Paper785/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867201, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment"}}}, {"id": "uovdEGVecOr", "original": null, "number": 3, "cdate": 1605175130589, "ddate": null, "tcdate": 1605175130589, "tmdate": 1605175146014, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "gm5br35wFIU", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment", "content": {"title": "Please find the details in Appendix B and C", "comment": "Dear Reviewer\n\nthank you for sharing your thoughts. As we say \"g and B are either \u2207L(w) and \u2207^2L(w) or suitable approximations\"\n\nThe convergence guarantees hold as long as Assumption 1 is satisfied (which is standard, see the references as well as the references therein).  As stated right after the assumption this can be done via sub-sampling due to concentration inequalities. The result constitutes a non-asymptotic bound on the deviation of the gradient/Hessian norms that holds with high probability.\n\nFor our experiments, however, w e use a fixed sample size for the sake of simplicity as stated in Appendix C. \n\nAssumption 2 is always satisfied. Please see Section B2 \"Subproblem solver\" where we explain that we solve the model using Steihaug-Toint CG, which usually converges in just a few steps but is guaranteed to satisfy A2 after at most d steps. Please see Carmon & Duchi: Analysis of Krylov Subspace Solutions of Regularized Nonconvex Quadratic Problems for more details on the convergence rate of krylov subspace methods on objectives like our model.\n\nGeneralization error: We do not show any test results because *this is not what the optimizers are tasked with*. Generalization is a very nebulous task and most algorithms that are better at generalization are so because they are worse in training. In our mind, such a comparison is irrelevant unless one presents an algorithm specifically targeted at generalization. \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper785/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-yo2vfTt_Cg", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper785/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper785/Authors|ICLR.cc/2021/Conference/Paper785/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867201, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Comment"}}}, {"id": "gm5br35wFIU", "original": null, "number": 3, "cdate": 1603606762082, "ddate": null, "tcdate": 1603606762082, "tmdate": 1605024605747, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Review", "content": {"title": "A bridge between adaptive optimization and second order trust region methods", "review": "This paper analyzes adaptive methods like Adam and AMSProp, and shows that they can be re-interpreted as first order trust region methods with an ellipsoidal trust region (Lemma 1).  The authors then propose a second order trust region method with similar ellipsoidal trust regions induced by the RMSProp matrices (Eq 7). Under some assumptions, they show that this algorithm will converge in a finite number of steps (depending upon the accuracy desired). They also show some experiments to demonstrate their algorithm.\n\nThe approach proposed in the paper is interesting, but the significance of the paper is not clear. The application of ellipsoidal trust region to Newton algorithms can certainly help with the development of new optimizers, but the presented algorithm was not very clear to me: I assume approximate g_t is the minibatch gradient, but how is the approximate B_t computed. How is m_t(s_t) computed approximately? Could you say a little more about Assumptions 1 and 2 - when do they hold? Finally, the experimental results were not very clear - it seems like ellipsoidal TR methods are often outperformed by first order methods. Also no generalization results on test sets were shown.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper785/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135042, "tmdate": 1606915809422, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper785/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Review"}}}, {"id": "jYiVkgDo6L", "original": null, "number": 2, "cdate": 1603576202859, "ddate": null, "tcdate": 1603576202859, "tmdate": 1605024605665, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Review", "content": {"title": "The paper investigates the use of scaled norms in the constraint of the trust region method.  ", "review": "-In section 3.2, the authors use the ratio of the diagonal to the overall mass of a matrix to \"measure\" the quality of diagonal vs full preconditioning. Why this a good measure? motivation and comments about this measure are needed. I think the most important thing to check here is the \"difference\" between A^{-1}g and diag(A)^{-1}g. I think an interesting question that one may investigate here is to link this ratio to the \u00a0\"difference\" between A^{-1}g and diag(A)^{-1}g....Or to link this ratio to the complexity...\n\n-In proposition 1, define sigma_1 & 2. I understand that they are defined in the appendix. Just move eq (42) to prop 1. The same about the relation between H & X in proposition 2.\n\n- Rewrite proposition 2, it is a bit misleading, the limit when n goes to infinite should be independent of n ! you may write proportional instead of limit... \n\n-Figure D.2 shows that sqrt(n)/(sqrt(n)+...) decreases to zero with n going to infinite. However, this quantity is non-decreasing as a function of n and it converges to 1 when n diverges!!\n\n-Page 6 after lemma 2, the authors stated that Thm 1 shows the first convergence rate for ellipsoidal TR methods. I disagree about this. The authors may check for instance \nthe work of Conn et al. (this work is cited in the paper) section 6.7 \u00a0and Bergou et al.: https://link.springer.com/article/10.1007/s10589-017-9929-2 \u00a0\n\n- \u00a0Assumption1: the authors stated that \"For finite-sum objectives such as Eq. (1), the above condition can be met by random sub-sampling due to classical concentration results for sums of random variables\" this is incorrect! take for instance the case where in the finite sum only one term is not zero and all the other are equal to zero. If the non-zero term is not in the sub-sampling for all t the bounds you mentioned may not be satisfied. You can have these bounds but only in a probabilistic manner as in the Blanchet et al. work (this work is cited in the paper)...\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper785/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135042, "tmdate": 1606915809422, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper785/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Review"}}}, {"id": "Z7_WLIzVt8N", "original": null, "number": 1, "cdate": 1603570274477, "ddate": null, "tcdate": 1603570274477, "tmdate": 1605024605605, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Review", "content": {"title": "first work studying systematically the practicality of stochatsic ellipsoidal TR method in deep learning", "review": "The paper proposes novel stochastic ellipsoidal trust-region methods  inspired by adaptive gradient methods and studies the application of them with adaptive diagonal preconditioners. Theoretical convergence analysis is provided for TR with RMSProp ellipsoid, and numerical results demonstrates the superiority of ellipsoided TR over uniform TR. Interestingly, the paper shows for the first time that, adaptive gradient methods can be view as first-order TR with ellipsoidal constraints. The negative comparative results with state-of-the-art adaptive methods are appreciated, showing that the TR-type methods may not be great choices for deep network training, since the Hessians are often diagonal-dominant in deep-learning practice, and hence the benefit of second-order methods are limited.\n\nAs said, the paper is mostly well-written and present an insightful investigation of stochastic ellipsoidal TR which could be potentially an alternative to state-of-the-art adaptive gradient methods for modern deep network training, and the paper gives a negative answer. On the other hand, the reviewer feels that the potential impact for the deep learning practitioners may be a bit limited and is not very sure about whether the contribution of this work is that significant.\n\nThe experimental details seem unclear. In the experiments, how were the approximate hessian B_t calculated? Are they computed on the same sampled minibatch of gradient or on a new/bigger minibatch? Meanwhile, only training accuracies are reported, which seems inadequate -- should also include the test accuracy plots.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper785/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135042, "tmdate": 1606915809422, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper785/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Review"}}}, {"id": "0CkA4tDkUXb", "original": null, "number": 4, "cdate": 1603854668841, "ddate": null, "tcdate": 1603854668841, "tmdate": 1605024605542, "tddate": null, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "invitation": "ICLR.cc/2021/Conference/Paper785/-/Official_Review", "content": {"title": "A different lens to look at second order methods", "review": "Authors propose a new perspective on adaptive gradient methods. Main contribution is a trust region based algorithm they call \"Stochastic Ellipsoidal Trust Region Method\" thats flexible to include both full, and diagonal matrix as the preconditioning matrix.  Authors also mention that the preconditioners are generally diagonally dominant in practice, and may only require diagonal matrix (leaves full matrix for future work).\n\nReason to score:\n\nWeak emperical results, small models, on small datasets without normalizing for batch sizes between experiments.\n\nI have listed my concerns below and hopefully authors can address my concern during the rebuttal period.\n\nI think the authors could substantially improve the emperical results in the paper by including commonly used adaptive methods as baseline (such as Adam), and providing results on stronger baselines, and break down on computational effeciency of the proposed approach in more details.\n\nQuestions/comments:\n\na) There is Appendix C that states that batch size used first order method is 32, vs for this method authors use 128/512 and then compare backprops. This extremely problematic when using # backprop as a way to measure efficiency, as this gives ~4-16x improvement from just larger batch sizes.  I would suggest redoing experiments with exact same batch sizes?\n\nb) Authors indicate using diagonal preconditioner; Could authors consider previous work on kronecker factored preconditioners such as KFAC, or Shampoo that is computationally cheaper in their experiments?\n\nc) Could authors also include walltime comparisons, to split time spent in forward, backward,  hessian-vector product, cg iterations (including details on these as layer sizes increases, for say upto 4k which are common in deep networks trained today?)\n\nd) Could you run a comparison against baselines and settings in: http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper785/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper785/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive norms for deep learning with regularized Newton methods", "authorids": ["~Jonas_K_Kohler1", "~Leonard_Adolphs1", "~Aurelien_Lucchi1"], "authors": ["Jonas K Kohler", "Leonard Adolphs", "Aurelien Lucchi"], "keywords": ["Stochastic Optimization", "Non-convex Optimization", "Deep Learning", "Adaptive methods", "Newton methods", "Second-order optimization"], "abstract": "We investigate the use of regularized Newton methods with adaptive norms for optimizing neural networks. This approach can be seen as a second-order counterpart of adaptive gradient methods, which we here show to be interpretable as first-order trust region methods with ellipsoidal constraints. In particular, we prove that the preconditioning matrix used in RMSProp and Adam satisfies the necessary conditions for provable convergence of second-order trust region methods with standard worst-case complexities on general non-convex objectives. Furthermore, we run experiments across different neural architectures and datasets to find that the ellipsoidal constraints constantly outperform their spherical counterpart both in terms of number of backpropagations and asymptotic loss value. Finally, we find comparable performance to state-of-the-art first-order methods in terms of backpropagations, but further advances in hardware are needed to render Newton methods competitive in terms of computational time.", "one-sentence_summary": "This paper proposes second-order variants of adaptive gradient methods such as RMSProp.", "pdf": "/pdf/ef68077229f18cbc8d7b8afee78e7fd5b5ed7e19.pdf", "supplementary_material": "/attachment/fc45c084dba8f1e6dc083a491f1687e0f37c863d.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kohler|adaptive_norms_for_deep_learning_with_regularized_newton_methods", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uxxexpvfvW", "_bibtex": "@misc{\nkohler2021adaptive,\ntitle={Adaptive norms for deep learning with regularized Newton methods},\nauthor={Jonas K Kohler and Leonard Adolphs and Aurelien Lucchi},\nyear={2021},\nurl={https://openreview.net/forum?id=-yo2vfTt_Cg}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-yo2vfTt_Cg", "replyto": "-yo2vfTt_Cg", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper785/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135042, "tmdate": 1606915809422, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper785/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper785/-/Official_Review"}}}], "count": 11}