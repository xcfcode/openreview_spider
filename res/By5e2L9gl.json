{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488554598007, "tcdate": 1478286322627, "number": 326, "id": "By5e2L9gl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "By5e2L9gl", "signatures": ["~Leonard_Berrada1"], "readers": ["everyone"], "content": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396512592, "tcdate": 1486396512592, "number": 1, "id": "S1YPnGU_g", "invitation": "ICLR.cc/2017/conference/-/paper326/acceptance", "forum": "By5e2L9gl", "replyto": "By5e2L9gl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The authors present a novel layer-wise optimization approach for learning convolutional neural networks with piecewise linear nonlinearities. The proposed approach trains piecewise linear ConvNets layer by layer, reduces the sub-problem into latent structured SVM. \n \n Reviewers mainly expressed concerns about the experimental results, which the authors have diligently addressed in their revised versions. While the reviewers haven't updated explicitly their reviews, I believe the changes made should have been sufficient for them to do so.\n \n Thus, I recommend this paper be accepted.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396514655, "id": "ICLR.cc/2017/conference/-/paper326/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "By5e2L9gl", "replyto": "By5e2L9gl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396514655}}}, {"tddate": null, "tmdate": 1484933933166, "tcdate": 1484400292212, "number": 5, "id": "Hk3s8owUe", "invitation": "ICLR.cc/2017/conference/-/paper326/public/comment", "forum": "By5e2L9gl", "replyto": "By5e2L9gl", "signatures": ["~Leonard_Berrada1"], "readers": ["everyone"], "writers": ["~Leonard_Berrada1"], "content": {"title": "Submission Revision", "comment": "tldr: New results on ImageNet, CIFAR-100, improved results on CIFAR-10.\n\nWe thank the reviewers for their helpful feedbacks. We list here the changes made in the revisions of the paper (version 1 being the original submission read by the reviewers).\n\nList of changes in version 2:\n\n1) New results with batch-normalization on CIFAR-10 (new subsection 5.2)\n2) Clarification of the objective of the paper and experiments (Methods paragraph in subsection 5.1)\n\nList of changes in version 3:\n\n1) New results on CIFAR-10: deeper architecture for a stronger baseline (subsection 5.2)\n2) New results on CIFAR-100 (subsection 5.2)\n3) Re-wording of the experiments section and removal of previous experiments on CIFAR-10 (with and without batch normalization) (section 5)\n4) New Appendix about the computation of the feature vectors and detailed example (Appendix B).\n5) Infeasibility of standard line-search in Introduction\n6) New references, including suggestions from the reviewers (section 2)\n7) More compact abstract\n8) Inclusion of batch normalization in Discussion (section 6)\n9) Minor rewording and typo fixes throughout the paper.\n\nList of changes in version 4:\n1) Added ImageNet results (subsection 5.3)\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287619938, "id": "ICLR.cc/2017/conference/-/paper326/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By5e2L9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper326/reviewers", "ICLR.cc/2017/conference/paper326/areachairs"], "cdate": 1485287619938}}}, {"tddate": null, "tmdate": 1482362158519, "tcdate": 1482362158519, "number": 4, "id": "SkI4atOEx", "invitation": "ICLR.cc/2017/conference/-/paper326/public/comment", "forum": "By5e2L9gl", "replyto": "H1OCNwe4g", "signatures": ["~Leonard_Berrada1"], "readers": ["everyone"], "writers": ["~Leonard_Berrada1"], "content": {"title": "Answer", "comment": "We thank the reviewer for the comments, and provide some clarifications below.\n\n\nComment: \u201clayer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs [...] It is not clear to me how the loss/gain balances each other\u201d\n\nResponse:  The performance impact of using coordinate descent is indeed an open question. For this work, we point out that: \n(i) LW-SVM improves over SGD solutions despite optimizing only one layer at a time,\n(ii) LW-SVM trained from scratch can also reach competitive results on CIFAR-10, \n(iii) The connection between PL-CNNs and latent SVMs opens an interesting research direction that may further improve on LW-SVM (for example,  by extending LW-SVM to a global optimization algorithm that updates all layers simultaneously).\n\n\nComment: \u201cThis paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc).\u201d\n\nResponse: In the revised version of the paper, we report results with batch-normalization and show similar or greater improvements of LW-SVM over the baselines.\n\n\nComment: \u201ca better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting\u201d\n\nResponse: The experiments of this work suggest that the improvement made by LW-SVM on the training data generalize well in our settings. In particular, results with batch-normalization show significant improvements on the testing accuracy. A better optimization algorithm may also lead to a better understanding of the deficiencies of the current learning objective, which could in itself be of interest to the representation learning community.\n\n\nComment: \u201cOnly CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet.\u201d\n\nResponse: In principle the architectures used on ImageNet (such as VGG-16, ResNets etc.) can be treated like the models used in our experiments, and we expect to obtain similar results on these. In addition to the current results, we are currently evaluating our method on CIFAR-100 and ImageNet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287619938, "id": "ICLR.cc/2017/conference/-/paper326/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By5e2L9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper326/reviewers", "ICLR.cc/2017/conference/paper326/areachairs"], "cdate": 1485287619938}}}, {"tddate": null, "tmdate": 1482362057048, "tcdate": 1482362057048, "number": 3, "id": "BJZR2tuEg", "invitation": "ICLR.cc/2017/conference/-/paper326/public/comment", "forum": "By5e2L9gl", "replyto": "BkrnA0UEg", "signatures": ["~Leonard_Berrada1"], "readers": ["everyone"], "writers": ["~Leonard_Berrada1"], "content": {"title": "Answer", "comment": "We thank the reviewer for his comments, which we discuss below:\n\nComment: \u201cAlthough the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task.\u201d\n\nResponse: In principle the architectures used on ImageNet (such as VGG-16, ResNets etc.) can be treated like the models used in our experiments, and we expect to obtain similar results on these. In addition to the current results, we are currently evaluating our method on CIFAR-100 and ImageNet, which are more challenging multi-class classification tasks. We do take note that it would be interesting to test the method on structured prediction tasks as well.\n\n\nComment: \u201cThe test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%.\u201d\n\nResponse: The best score of 70% is obtained with a network which does not use batch normalization, which makes an important difference. The results with batch-normalization in the revised version of the paper reach around 78% testing accuracy. Taking into account the reviewer\u2019s comments, we are currently running experiments with deeper architectures to improve this score.\n\n\nComment: \u201cIf I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. In practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm.\u201d\n\nResponse: The reviewer is correct that during the inner optimization, only the dual objective is guaranteed a monotonic improvement. In practice, we do run BCFW with a sufficient number of iterations to yield a very small duality gap. Therefore the method provides a monotonic decrease in practice. To illustrate this, we have plotted the training objective and the dual gap for an experiment presented in the paper (Adadelta with batch-norm + LW-SVM, Figure 3 in the revised paper): https://drive.google.com/file/d/0BxXMf_vDT8vCSFQwOWl0aldzeWs/view?usp=sharing (one data point at the end of each inner iteration of the CCCP). We will add a note about these points in the future revision of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287619938, "id": "ICLR.cc/2017/conference/-/paper326/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By5e2L9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper326/reviewers", "ICLR.cc/2017/conference/paper326/areachairs"], "cdate": 1485287619938}}}, {"tddate": null, "tmdate": 1482251949129, "tcdate": 1482251949129, "number": 3, "id": "BkrnA0UEg", "invitation": "ICLR.cc/2017/conference/-/paper326/official/review", "forum": "By5e2L9gl", "replyto": "By5e2L9gl", "signatures": ["ICLR.cc/2017/conference/paper326/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper326/AnonReviewer2"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents a novel layer-wise optimization approach for learning CNN with piecewise linear nonlinearities.  The proposed approach trains piecewise linear CNNs layer by layer and reduces the sub-problem into latent structured SVM, which has been well-studied in the literature. In addition, the paper presents improvements of the BCFW algorithm used in the inner procedure. Overall, this paper is interesting. However, unfortunately, the experiment is not convincing. \n\nPros:\n\n- To my best knowledge, the proposed approach is novel, and the authors provide nice theoretical analysis.\n- The paper is well-written and easy to follow. \n\nCons:\n\n- Although the proposed approach can be applied in general structured prediction problem, the experiments only conduct on a simple multi-class classification task. This makes this work less compelling. \n\t\n- The test accuracy performance on CIFAR-10 reported in the paper doesn't look right. The accuracy of the best model reported in this paper is 70.2% while existing work often reports 90+%. For example, https://arxiv.org/pdf/1412.6806.pdf showed an accuracy of 91% without data augmentation. Also, CIFAR-10 is a relatively small dataset, \n\nOther comments:\n\n- If I understand correctly, BCFW only guarantees monotonically increasing in the dual objective and does not have guarantees on the primal objective. Especially, in practice, the inner optimization process often stops pretty early (i.e., stops when duality gap is still large). Therefore, when putting them together, the CCCP procedure may not monotonically decrease as the inner procedure is only solved approximately. The authors should add this note when they discuss the properties of their algorithm.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512622774, "id": "ICLR.cc/2017/conference/-/paper326/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper326/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper326/AnonReviewer3", "ICLR.cc/2017/conference/paper326/AnonReviewer1", "ICLR.cc/2017/conference/paper326/AnonReviewer2"], "reply": {"forum": "By5e2L9gl", "replyto": "By5e2L9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper326/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper326/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512622774}}}, {"tddate": null, "tmdate": 1482235943183, "tcdate": 1482235943183, "number": 2, "id": "By1NljIVe", "invitation": "ICLR.cc/2017/conference/-/paper326/public/comment", "forum": "By5e2L9gl", "replyto": "HkmIo1QNe", "signatures": ["~Leonard_Berrada1"], "readers": ["everyone"], "writers": ["~Leonard_Berrada1"], "content": {"title": "Clarifications", "comment": "We thank the reviewer for the comments and provide some clarifications below.\n\nComment: \u201ccomparison between backprop and the discussed CCCP approach is not really appropriate\u201d\n\nResponse: We use the term \u2018backpropagation\u2019 to denote the overall subgradient descent (SGD) algorithm. This includes the forward pass and the backward pass (which computes the subgradient), as well as the state of the art variants of the step size computations, namely, Adagrad, Adadelta and Adam. As our main aim is to improve the optimization of the learning objective (3), a comparison with the existing optimization algorithms provides an appropriate baseline.\n\n\nComment: \u201cmini-batch optimization alleviates any form of monotonic decrease\u201d. \n\nResponse: Does the reviewer mean that by using mini-batch optimization we have lost our monotonic decrease guarantee? If so, this is not correct. There are two distinct monotonic improvements that we guarantee here, even when using mini-batches:\n(i) a monotonic decrease in the overall objective function at each outer iteration of the CCCP algorithm. This is because we solve a convex problem and therefore obtain the optimal solution, which is at least as good as the initial point.\n(ii) a monotonic increase in the dual of the convex problem at each step. Because the dual is smooth and quadratic, we perform a coordinate ascent on the dual variables with an optimal step-size. This feature of the BCFW algorithm (Lacoste-Julien et al., 2013) guarantees a monotonic increase despite the fact that the conditional gradient is computed using samples from a mini-batch / block of coordinates.\n\n\nComment: \u201cMore justification regarding the argument that search for the step-size is a disadvantage seems necessary\u201d, \u201cline-search would for example result in convergence guarantees\u201d.\n\nResponse: Another noteworthy advantage of our work is that we can analytically compute the optimal step-size for the conditional gradient obtained over a mini-batch. Indeed, this fact has been exploited successfully for structural SVM optimization with significant improvements over SGD methods.\n\nFor backpropagation style algorithms (that is, SGD on learning objective (3)), the computation of the step-size is not computationally feasible. This is due to the fact that the evaluation of the objective for each putative step-size would require an entire epoch. Furthermore, for PL-CNN, the objective (3) is only sub-differentiable. Hence, even an exhaustive line search cannot guarantee a monotonic improvement.\n\nIn other words, by establishing a connection between deep networks and structural SVMs, we have been able to successfully transfer the two advantages of BCFW to the deep learning domain (monotonic improvement of dual using mini-batches, efficient computation of the optimal step-size).\n\n\nComment: \u201c In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn) ICLR Workshop Track 2016\u201d. \n\nResponse: We thank the reviewer for the reference (we have found a full version at https://arxiv.org/pdf/1609.07152v2.pdf). While our method deals with convexity w.r.t. the parameters to improve the training task, the aforementioned work analyzes how convexity w.r.t input or the output of the network helps with the inference (for cases with complex outputs). We will include this reference in the future revision of the paper.\n\n\nComment: \u201cThe observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc.\u201d\n\nResponse: In our revised version of the paper, we have updated Observation 1 with a principled algorithm that is guaranteed to find the optimal solution of the convex problem. \n\n\nIn summary, all the theoretical guarantees presented in our work are applicable, despite the use of mini-batches and the use of a modified conditional gradient algorithm."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287619938, "id": "ICLR.cc/2017/conference/-/paper326/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By5e2L9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper326/reviewers", "ICLR.cc/2017/conference/paper326/areachairs"], "cdate": 1485287619938}}}, {"tddate": null, "tmdate": 1481993035083, "tcdate": 1481993035083, "number": 2, "id": "HkmIo1QNe", "invitation": "ICLR.cc/2017/conference/-/paper326/official/review", "forum": "By5e2L9gl", "replyto": "By5e2L9gl", "signatures": ["ICLR.cc/2017/conference/paper326/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper326/AnonReviewer1"], "content": {"title": "My thoughts", "rating": "4: Ok but not good enough - rejection", "review": "A layer wise optimization for CNNs with ReLU activations and max-pooling is proposed and shown to correspond to a series of latent structured SVM problems. Using CCCP style optimization a monotonic decrease of the overall objective function can be guaranteed.\n\nSummary:\n\u2014\u2014\u2014\nI think the discussed insights are very interesting but not presented convincingly. Firstly, claims are emphasized which are often violated in practice (e.g., no convergence guarantees due to mini-batches), statements could be validated more convincingly (e.g., is monotone convergence a curse or a blessing), the experimental evaluation should be extended. In summary, I think the paper requires some more attention to form a compelling story.\n\nQuality: I think some of the techniques could be described more carefully to better convey the intuition. At times apples are compared to oranges, e.g., back propagation is contrasted with CCCP.\nClarity: Some of the derivations and intuitions could be explained in more detail.\nOriginality: The suggested idea is reasonable albeit heuristics are required.\nSignificance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge at this point in time.\n\nDetails:\n\u2014\u2014\u2014\u2014\n1. I think the provided guarantees for the optimization procedure are certainly convenient theoretically but their practical relevance still needs to be demonstrated more convincingly, e.g., mini-batch optimization alleviates any form of monotonic decrease. Hence the emphasize in the paper is somewhat misguided according to my opinion and given he current experimental evaluation.\n\n2. In spirit similar is work by B. Amos and J. Kolter, Input-Convex Deep Networks (https://openreview.net/pdf?id=ROVmA279BsvnM0J1IpNn) ICLR Workshop Track 2016; which should probably be mentioned.\n\n3. I think the comparison between backprop and the discussed CCCP approach is not really appropriate. Note that backprop is a mechanism to compute gradients and is not at all related to any optimization technique/algorithm. This means that backprop combined with Armijo line-search would for example result in convergence guarantees. Hence a statement like `one of the main advantages of our approach compared to back propagation and its variants, which fail to provide similar guarantees on the value of the objective function from one iteration to the next\u2019 is according to my opinion superficial.\n\n4. More justification regarding the argument that search for the step-size is a disadvantage seems necessary. There is evidence that noise introduced by mini-batches and inaccurate/no line search is actually beneficial. In contrast the proposed CCCP procedure may converge pre-maturely to a local optimum close to the initialization. Since mini-batches are used no guarantees are available in any case. Hence I think additional evidence for the fact that such convergence guarantees don\u2019t result in premature stopping seems necessary.\n\n5. The observation 1 required to convert the layer wise procedure into an SVM seems rather ad-hoc. Note that you can easily construct examples where Eq. (9) and Eq. (7) differ significantly.\n\n6. I personally think the experimental evaluation is conducted on examples that are too small and some results are obvious. E.g., LW-SVM always improves over the solution of the SGD algorithm. If the authors were to use backtracking line search they could also improve upon LW-SVM.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512622774, "id": "ICLR.cc/2017/conference/-/paper326/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper326/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper326/AnonReviewer3", "ICLR.cc/2017/conference/paper326/AnonReviewer1", "ICLR.cc/2017/conference/paper326/AnonReviewer2"], "reply": {"forum": "By5e2L9gl", "replyto": "By5e2L9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper326/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper326/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512622774}}}, {"tddate": null, "tmdate": 1481827535671, "tcdate": 1481827535664, "number": 1, "id": "H1OCNwe4g", "invitation": "ICLR.cc/2017/conference/-/paper326/official/review", "forum": "By5e2L9gl", "replyto": "By5e2L9gl", "signatures": ["ICLR.cc/2017/conference/paper326/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper326/AnonReviewer3"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a new approaches for optimizing the objective of CNNs. The proposed method uses a lay-wise optimization, i.e. at each step, it optimizes the parameters in one layer of CNN while fixing the parameters in other layers. The key insight of this paper is that, for a large class of CNNs, the optimization problem at a particular can be formulated as optimizing a piecewise linear (PL) function. This PL function optimization happens to be the optimization problem commonly encountered in latent structural SVM. This connection allows this paper to borrows ideas from the latent structural SVM literature, in particular concave-convex procedure, to learn the parameters of CNNs.\n\nOverall, the paper is well-written. Traditional, CNNs and structural SVMs are almost two separate research communties. The connection of CNNs to latent structural SVM is interesting, and might bridge the gap and facilitate the transferring of ideas between these two camps.\n\nOf course, the proposed method also has some limitations. 1) It is limited to layer-wise optimization. Nowadays layer-wise optimization is essentially a coordinate descent algorithm and is not really a competitive strategy in learning CNNs. When you choose layer-wise optimization, you already lose something in terms of optimizing the objective (since you are using coordinate descent, instead of gradient descent). Of course, you also gain something since now you can guarantee that each coordinate descent step always improve the objective. It is not clear to me how the loss/gain balances each other. 2) This paper focues on improving the optimization of CNN objective. However, we all know that a better objective does not necessarily correspond to a good model (e.g. due to overfitting). Although the SGD with backprop in standard CNN learning does not always improve the solution of the objective (unlike the proposed method in this paper), but to me, this might be a good thing since it can prevent overfitting (the goal of learning is not to get better solution for the optimization problem in the first place -- the optimization problem is merely a proxy to learn a model with good generalization ability).\n\nThe experiment is a bit weak.\n1) Only CIFAR10 is used. This is a very small dataset by today's standard, while CNNs are typically used in large-scale datasets, such as ImageNet. It is not clear whether the conclusions of this paper still hold when applied on ImageNet.\n\n2) This paper only compares with a crippled variant of SGD (without batch normalization, dropout, etc). Although this paper mentions that the reason is that it wants to focus on optimization. But I mentioned earlier, SGD is not designed to purely obtain the best solution that optimizes the objective, the goal of SGD is to reasonably optimize the objective, while preventing overfitting. So the comparison to SGD purely in terms of the optimization is that meaningful in the first place. \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512622774, "id": "ICLR.cc/2017/conference/-/paper326/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper326/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper326/AnonReviewer3", "ICLR.cc/2017/conference/paper326/AnonReviewer1", "ICLR.cc/2017/conference/paper326/AnonReviewer2"], "reply": {"forum": "By5e2L9gl", "replyto": "By5e2L9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper326/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper326/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512622774}}}, {"tddate": null, "tmdate": 1480888605196, "tcdate": 1480888605191, "number": 1, "id": "B1SmZffQe", "invitation": "ICLR.cc/2017/conference/-/paper326/public/comment", "forum": "By5e2L9gl", "replyto": "SJl75t5fx", "signatures": ["~Leonard_Berrada1"], "readers": ["everyone"], "writers": ["~Leonard_Berrada1"], "content": {"title": "Clarification of the goal & new results with batch-normalization", "comment": "We thank the reviewer for the questions regarding the experiments section.\n\nOur main goal is to compare different algorithms that optimize the same objective function, namely problem (3), in terms of three measures: (i) training accuracy; (ii) the testing accuracy; and (iii) the learning objective value. Our layerwise optimization is complementary to backprop. To highlight this, we have incorporated new results that minimize the learning objective of a network that incorporates batch normalization (subsection 5.2). Similar to the original experiments, our algorithm leads to significant improvements on all three measures.\n\nIt is worth noting that our method can be extended to handle other forms of difference-of-convex non-linearities, such as batch normalization. However, in this work we focus on PL functions that have the added advantage of being amenable to Frank-Wolfe style optimization.\n\nIn the appendix of the revised paper, we also include new results that highlight the robustness of our method to hyperparameter settings compared to all the standard backprop variants. Please note that all the methods use the same learning objective. By obtaining lower objective values, our method is able to escape the bad local minima that are encountered by backprop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287619938, "id": "ICLR.cc/2017/conference/-/paper326/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "By5e2L9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper326/reviewers", "ICLR.cc/2017/conference/paper326/areachairs"], "cdate": 1485287619938}}}, {"tddate": null, "tmdate": 1480395288453, "tcdate": 1480395288449, "number": 1, "id": "SJl75t5fx", "invitation": "ICLR.cc/2017/conference/-/paper326/pre-review/question", "forum": "By5e2L9gl", "replyto": "By5e2L9gl", "signatures": ["ICLR.cc/2017/conference/paper326/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper326/AnonReviewer3"], "content": {"title": "comparison with backprop", "question": "1) The baseline in the experiment is a strawman variant of backprop (without batch norm, drop out, etc). I think this is unfair. Batch norm and dropout have been proved to be very useful for CNN models. If the proposed method cannot take advantage of batch norm/dropout, then this is an advantage of backprop. \n2) The dataset is too small for today's standard. Why not use something like ImageNet?\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusting SVM for Piecewise Linear CNNs", "abstract": "We present a novel layerwise optimization algorithm for the learning objective of Piecewise-Linear Convolutional Neural Networks (PL-CNNs), a large class of convolutional neural networks. Specifically, PL-CNNs employ piecewise linear non-linearities such as the commonly used ReLU and max-pool, and an SVM classifier as the final layer. The key observation of our approach is that the prob- lem corresponding to the parameter estimation of a layer can be formulated as a difference-of-convex (DC) program, which happens to be a latent structured SVM. We optimize the DC program using the concave-convex procedure, which requires us to iteratively solve a structured SVM problem. This allows to design an opti- mization algorithm with an optimal learning rate that does not require any tuning. Using the MNIST, CIFAR and ImageNet data sets, we show that our approach always improves over the state of the art variants of backpropagation and scales to large data and large network settings.", "pdf": "/pdf/8042d0d801234a9c0042401fa69b6b20a21ba77f.pdf", "TL;DR": "Formulating CNN layerwise optimization as an SVM problem", "paperhash": "berrada|trusting_svm_for_piecewise_linear_cnns", "conflicts": ["ox.ac.uk"], "keywords": [], "authors": ["Leonard Berrada", "Andrew Zisserman", "M. Pawan Kumar"], "authorids": ["lberrada@robots.ox.ac.uk", "az@robots.ox.ac.uk", "pawan@robots.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959338625, "id": "ICLR.cc/2017/conference/-/paper326/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper326/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper326/AnonReviewer3"], "reply": {"forum": "By5e2L9gl", "replyto": "By5e2L9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper326/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper326/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959338625}}}], "count": 11}