{"notes": [{"id": "RmcPm9m3tnk", "original": "qT6RYm5zCzg", "number": 2593, "cdate": 1601308287110, "ddate": null, "tcdate": 1601308287110, "tmdate": 1616081462006, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "B0aOa7TknCX", "original": null, "number": 1, "cdate": 1610040350419, "ddate": null, "tcdate": 1610040350419, "tmdate": 1610473939409, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This submission generated significant discussion between the reviewers; three of them ended up on the \"accept\" side, but one remained firmly in the \"reject\" camp.\n\nThe main strength of the paper is that it tackles a very hard problem: learning an unsupervised generative model (and accompanying inference model) of scene graph structures given only image data. As one reviewer mentioned, it is remarkable that the authors were able to get their system to work at all, given the seeming intractability of this problem. The work builds upon a clear line of prior work in this area, and the type of data on which it is evaluated (\"toy\" synthetic datasets a la CLEVR) is consistent with prior art.\n\nMultiple reviewers brought up the \"toy\" nature of the dataset as a drawback to the paper, but most agreed that this is not reason to reject the paper. Rather, the paper demonstrates a convincing proof of concept that this kind of model can be built, and improvements in the elements out of which the model is composed (generative and inference networks) should improve its applicability to real-world data.\n\nAnother question mark raised by multiple reviewers: could a simpler, handcrafted inference procedure work just as well or better? The authors included a new experiment against a hand-coded heuristic in their rebuttal, and their method outperforms it. One reviewer noted that more careful tuning might make a heuristic perform as well as the proposed method, but it is still clear that it is not trivial to get a hand-coded solution to perform well (even for this \"toy\" data). Another reviewer pointed out that this is one of the main attractions of variational inference methods: the ability to specific knowledge as simple generative priors rather than complex bottom-up inference procedures.\n\nOne reviewer, R1, remains negative about the paper. His (it is a he; I know this reviewer) main concern is that the scene graphs used are shallow and have a simple structure, and thus (a) it's not clear what value they add, (b) a simple postprocess could reconstruct them, assuming the individual object parts could be detected, and (c) it's not clear whether the method would generalize to deeper/more complex hierarchies. He believes this calls into question the validity of the entire method.\n\nI am sympathetic to this argument, but I think setting the bar this high may prevent progress in this field. For point (a), the authors included an image-manipulation application in their rebuttal--again, a proof of concept, not a directly useful tool. For point (b), the authors did compare against a hand-coded inference baseline and achieved better results, so while this may be possible, it is probably not as easy as the reviewer suggests. (c) remains an open question, to me. But even if this method as presented cannot generalize to more complex scene graphs, it likely paves the way for future work that can."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040350401, "tmdate": 1610473939390, "id": "ICLR.cc/2021/Conference/Paper2593/-/Decision"}}}, {"id": "zaYaEoYRhV", "original": null, "number": 2, "cdate": 1603893036139, "ddate": null, "tcdate": 1603893036139, "tmdate": 1606802557121, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Review", "content": {"title": "Not Sure If the Method is Significant Enough", "review": "=====Post-Rebuttal Comment=====\n\nI thank the authors for the detailed response and the updated results. While my overall opinion of the work is slightly more positive post-rebuttal, I still maintain that this is a clear reject, primarily for the following reason:\n\n- The technical contribution (adding a hierarchical layer to SPAIR and demonstrate the hierarchy can also be learned without supervision) is not significant enough to accept purely based on a \"proof-of-concept\" of a \"new\" direction.\n- For incremental contributions, I expect the experimental results to be more convincing to be acceptable, a few points that I still really expect to see:\n    - comparisons on harder datasets when one doesn't have to go into a specific metric to show an edge over a prior art that is targeting a different application\n    - results on decomposition with more significant overlaps (especially in 2D) and on objects where part boundaries are harder to infer (actual 3D objects is still preferable)\n    - object-level manipulation (row 3&4 in fig 4 did not match description) and latent space interpolation\n\nI would also strongly encourage the authors to highlight the similarity and different between SPAIR and SPACE when introducing the latent code formulation.\n\n=====Summary=====\n\nThis work proposes a method that can learn a three-level (scene, object, part) hierarchical representation of scenes in an unsupervised manner. A variational autoencoder is used here, where the encoder recursively infers the shape and pose of individual objects and parts, and the decoder recomposites the inferred parts are then recomposited to the inferred poses. The proposed method is evaluated on two datasets created for this paper. The results suggest that the proposed method can reliably breaks down scenes into meaningful objects and parts, and performs slightly better than another method designed for a slightly different task in terms of reconstruction quality, learned representation, and data efficiency for downstream tasks.\n\n=====Strengths=====\n\n- The motivation of the need for hierarchy is solid, and the solution proposed seems to me to be a reasonable way to impose some sort of hierarchy.\n- The model seems to be well-tuned, utilizing appropriate training tricks and architectures.\n- Good performance for the evaluations chosen in the paper.\n\n=====Weaknesses=====\n\n- *Very* inadequate attribution of ideas. I am not too familiar with the AIR line of work, but I think quite a few ideas can be traced back to prior works. It would be much better if the authors can, in addition to a brief one sentence mention in related works, add clear discussions for the inspirations of the main design choices.\n- Missing discussions of relevant works that do: 1. unsupervised part decomposition e.g. \u201cUCSG-NET - Unsupervised Discovering of Constructive Solid Geometry Tree\u201d, \"Bae-net: Branched autoencoder for shape co-segmentation\", \u201cCvxNet: Learnable Convex Decomposition\u201d; 2. Learning hierarchical representations e.g. \u201cStructureNet: Hierarchical Graph Networks for 3D Shape Generation\u201d.\n- I think the comparisons in this work are neither adequate nor fair. I am not convinced that the two toy dataset used here can prove the superiority of the method. The authors claim that \u201cother works can\u2019t work on our dataset\u201d, but I think the burden of the proof is on the authors to show that their method is superior, even under a more specific setting. In other words, if the method is indeed \u201cgeneral\u201d and can learn good decompositions, then I would expect it to perform better even under an slightly unfair setting i.e. comparing against metrics/datasets adopted in other works. Furthermore, the datasets used in this paper appears to be way too simple as compared to real world data. The authors argue that dataset with a single shape is easier, but I disagree: datasets like partnet contains much more complicated part structures, as well as joints between parts, than what is used here, even with only a single shape. (And it is pretty evident from the qualitative examples that the challenging part is decomposing objects into parts, not decomposing scenes into objects). Last but not least, I want to see more evidence that the proposed method is actually useful in real applications.\n- The learned representation does not seem to be of very good quality, as seen in Figure 4.\n- A lot of overclaims, to name a few: 1. \u201cGSGN is a general framework for representing and inferring scene graphs of arbitrary depth\u201d: I don\u2019t think a model being able to work a toy setting with three levels will mean that the same framework can be used for more complex settings of arbitrary depth (as an analogy: MLP works for MNIST but not on ImageNet). If the framework can handle more general cases, then show it. 2. \u201cClosely follow the rendering process in graphics engines\u201d: I don\u2019t think applying affine transformations and compositing alone is enough to warrant this claim, it is pretty clear that the learned representation lacks a good sense of \u201cobjectness\u201d, as textures, lighting and etc. are all entangled together (evident in Fig 3). There does not seem to be a straightforward way to extend the method to truly parallel the 3D rendering process, neither. 3. \u201cFirst deep generative model for unsupervised scene-graph discovery\u201d: there are a lot of works that infer structures in an unsupervised way, I don\u2019t think it\u2019s fair to give a very narrow definition of \u201cscene graph\u201d and claim \u201cthe first\u201d. 4. \u201cGSGN has capture many predefined object types in the dataset\u201d: I don\u2019t think one can make this claim when there are only three primitives and ten types of objects\u2026\n\n=====Reasons for Score=====\n\nOverall, I have the impression that this work is cherry picking a very specific setting where the proposed architecture works reasonably well. For a work making a quite big claim of being \u201cthe first deep generative model that learns \u2026\u201d, I would expect much more comprehensive evaluations than what is currently shown here. Furthermore, many ideas in this work are not attributed properly, making the novelty of the method quite unclear. From my limited knowledge of the direct predecessors of this work, I don\u2019t think there is too much novelty in this paper. I would be more than willing to change my score if the authors can 1. Provide more comprehensive evaluations 2. State the novelty / discuss prior works more clearly. But for now, I tend to give a pretty clear reject.\n\n=====Additional Comments & Questions=====\n\n- I am not sure if translation & rotation alone is a good way to handle 2D renders of 3D objects, since any translation & rotation will result in change of perspectives and illumination e.g. rotating the bronze sphere in Figure 3, row 1 will make the specular highlight inaccurate and translating the blue cube in row 3 will make the top surface less visible. Could the authors justify why predicting translation/rotation make sense, when the perspective/illumination of the object already provides a really strong cue?\n- Following previous point: would like to see examples of the same learned object being used in multiple scenes.\n- The quality of the learned primitives, as seen in Figure 4, seems to be pretty underwhelming. If the aim of the work is discovering those primitives, would it make more sense to impose a stronger prior on the properties of the primitives?\n- Table 1 & 2: why are all the ELBO terms the same? I would imagine them to be different, especially for SPACE-O/P, which, if I understand correctly, is a *completely different* model with different architecture and loss formulation? \n- Table 2 & 3: why are the metrics so close between SPACE-P & GSCN in table 2 but so different in Table 3? Does that suggest unbalanced dataset?\n- Still Table 2 & 3: why no comparison between SPACE-O & GSGN for object level occlusion?\n- The paper claims that being able to handle background is a unique advantage as compared to other works, but the background used in the toy dataset is quite simple. Would like to see more complex examples.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538092825, "tmdate": 1606915758473, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2593/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Review"}}}, {"id": "WS49JqZPDhf", "original": null, "number": 1, "cdate": 1603821917520, "ddate": null, "tcdate": 1603821917520, "tmdate": 1606797327448, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Review", "content": {"title": "Technically impressive, but is the complexity necessary?", "review": "**PAPER SUMMARY**\n\nThe paper presents a generative model for scenes that uses tree-structured latent variables to recursively decompose images into objects and parts, without any object or part supervision during training. The model is trained using variational inference. Experiments are performed on two new datasets (2D Shapes and Compositional CLEVR), demonstrating that the model is able to successfully uncover recursive scene/object/part decompositions in an unsupervised setting. The model is compared against prior work (SPACE) that performs non-hierarchical scene modeling.\n\n**STRENGTHS**\n\n- The paper presents a novel generative model that can infer tree-structured latent variables\n- The method is technically impressive, and a clear improvement over the non-hierarchical modeling used in prior work\n- The paper presents two new datasets (2D Shapes and Compositional CLEVR) for studying hierarchical scene decomposition. I hope these can be publicly released!\n\n**WEAKNESSES**\n- The method is quite complex, and though it is technically impressive I wish that it had been compared against very simple baselines\n- No experiments on real-world data\n- Unclear how the model will scale to wider and deeper trees\n- Many implementation details are unclear\n\n**SCALABILITY**\n\nA big selling point of the proposed model is that it can model the hierarchy of scenes into objects and parts, and the tree-based formulation of the latent space used to achieve this is technically impressive. However, how well would the method scale to larger scenes? All experiments used a relatively small three-level hierarchy where all nodes have a fixed out-degree of 4; thus in all experiments the full tree has just 21 nodes total. How well can the method scale to much larger trees, with deeper hierarchies or wider out-degrees?\n\n**PRIOR KNOWLEDGE**\n\nThe method requires choosing a tree depth and node out-degree as hyperparameters. To some extent, these hyperparameters encode very strong prior knowledge about the data being modeled (levels of hierarchy, and number of subparts within each part). In the experiments, these hyperparameters are perfectly matched to the synthetic datasets: you use a two-level tree with out-degree of four, and each image has between 1 and 4 objects, each of which is composed of between 1 and 3 parts. However in more complex real-world scenarios, you may not have such detailed knowledge of the world\u2019s compositional structure. For this reason, I\u2019m curious as to how the method would behave when the structural hyperparameters are mismatched to the underlying statistics of the dataset.\n\n**SIMPLE BASELINES**\n\nThe model is evaluated on two synthetic datasets -- 2D Shapes, and Compositional CLEVR. How difficult is the scene graph inference problem on these datasets? I wish that the authors had compared to very simple baselines in order to give a sense for how difficult the problem really is.\n\nFor example, I suspect that a simple \u201chandcrafted\u201d baseline that oversegmented the image into superpixels, then recursively merged superpixels based on proximity and simple appearance features (e.g. color histograms) could produce very plausible scene graphs for these two datasets.\n\nAt a high level, the promise of an end-to-end learning-based approach compared to a \u201chandcrafted\u201d approach like the above is that a learning-based approach should require less tuning, should be more adaptable to new datasets and tasks, and should scale better to complex real-world datasets where the assumptions of the model don\u2019t perfectly match the statistics of messy data from the real world. However in this case, I\u2019m not convinced that this complex variational method would be any simpler to implement or tune, or even give better results than, a very simple \u201chandcrafted\u201d baseline like the above on these synthetic datasets. There are also no experiments to demonstrate its scalability to real-world datasets.\n\nThis leads to a pointed question: If someone wanted to infer scene graphs from images, why should they prefer your approach over a very simple \u201chandcrafted\u201d approach?\n\n**MANY IMPLEMENTATION DETAILS UNCLEAR**\n\nThere are many implementation details that are unclear from the paper and supplementary material, without which reproducing the results are practically impossible. Even if some of these details do make sense as part of the main text, they should be specified more explicitly in the supplementary material. For example:\n\nWhat is the generative process for the presence variables $z_v^{pres}$? This is not clear from Equations 3 or 4, nor the surrounding discussion. From Equation 9, the encoder predicts presence variables conditioned on image patches; but how can you predict the presence variables when sampling an image from scratch?\n\nWhat is the dimension of the latent variables $z^{appr}_r?$\n\nYou say that the pose variables are Gaussian, but how is the pose parameterized in terms of relative location and scale?\n\nFrom the discussion after Equation 6, the pose variables also include a depth map -- how are these depth maps represented and parameterized? Are they per-pixel Gaussians like the other pose variables?\n\nHow exactly are the transparency maps $\\alpha_v$ computed from the depth maps? The text states that they are computed \u201cby the softmax over negative depth values\u201d, but this is hard to understand -- what are the sets of values being fed to softmax? If the depth map contains a per-pixel depth then a softmax over space wouldn\u2019t make sense, since this would cause the transparency of an object to depend on its spatial size (since it would have more pixels competing in the softmax).\n\nIn Equation 9, the distributions of the latents $z_v^{pres}$, $z_v^{pose}$ are conditioned only on data from the parent node $z_{pa(v)}^{appr}$ and $z_{pa(v)}$. Thus all children of a node will have the same distribution for their pose and appearance. Is this correct? If so, how do you encourage the child nodes to cover all parts of the parent, and not collapse to a single part of the parent?\n\nThere are no details about any of the neural network architectures used to implement the model, nor any details about any training hyperparameters (e.g learning rates, training schedule, regularization strengths etc).\n\n**SUMMARY**\n\nThe model is technically impressive, and a clear improvement over prior work. However on the whole I\u2019m not sure whether the complexity of the method is actually necessary to solve the problem at hand; I wish that the authors had done a better job demonstrating the benefits of the proposed method over very simple baselines. There are also many implementation details that are very unclear, for which reason I fear that the paper as written is utterly unreproducible.\n\nOn the whole I lean slightly toward acceptance, but I hope the authors can address my concerns in their rebuttal.\n\n\n**AFTER REBUTTAL**\n\nThe rebuttal largely addresses my concerns about implementation details.\n\nI am pleased to see the additional experimental results provided by the authors; I think that these do improve the paper. I still feel that some well-tuned handcrafted approach could likely perform on-par with the results of the proposed method, but the comparison with [Wei et al] show that achieving such results is at least not trivial, which does help to better ground the complexity of the task. The additional experiments with a three-level hierarchy show a bit more evidence for scalability than provided in the original paper. While these extra experiments do strengthen the paper, I feel that they don't really address the core issue with the paper, which is whether there is any hope for the proposed method to scale to more complex and realistic datasets.\n\nOverall I think that this is a reasonable paper and I still lean slightly toward acceptance, so I maintain my original rating of 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538092825, "tmdate": 1606915758473, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2593/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Review"}}}, {"id": "lygkHt1HkxM", "original": null, "number": 3, "cdate": 1603897139239, "ddate": null, "tcdate": 1603897139239, "tmdate": 1606689235095, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Review", "content": {"title": "auto-encoder for object-part scene graphs in simple environments", "review": "Generative Scene Graph Networks (GSGN) is a variational auto-encoder with the intermediate representation being tree-like scene graphs. The leaf nodes stand for primitive parts and edges stand for poses to compose parts into objects recursively. The experiments are done in two image datasets of single color, simple shape 2D/3D objects: Multi-dSprites and CLEVR, and the model is able to discover objects without supervision.\n\n**Strength**: I find the direction important, and the method well established in a variational inference framework with graphics-inspired designs. Experiment numbers look good in general.\n\n**Weakness**: Perhaps my biggest concern is that the current datasets are a bit weak. First, objects are too simple (single color, simple shape), so we are not sure if GSGN can work with more realistic visual domains where objects have more complex 3D structure or texture. Second, the object/primitive decomposition is slightly weird to me. I would expect hierarchal structure like an object being human body, and parts being legs, arms, head and so on. But in this work a \"part\" is a single-color object, and an \u201cobject\u201d is a bunch of single-color adjacent objects. Based on these two points, I believe the paper will be much stronger with experiments on more complex objects like humans or tables or \n\nAnother concern is the application of learned scene graph. The paper only shows unconditional sampling, but not really how to use the learned scene graph. For example, I'd expect scene graphs to be used for image manipulation, as one can change part of the object (shape, color, pose) without changing the rest. Showing the learned scene graph is useful for any downstream tasks can be a great plus for the current work.\n\nFinally, I wonder how variational the learned scene graphs can be, as the objects in the datasets are fairly simple and the learning might be easy. I'd be happy to see some analysis but this is not my main concern.\n\n---------\n\nAfter rebuttal: I'm glad they added some experiments and analysis I wanted to see, so I raise the score to 6. As the authors said, the paper is a proof-of-concept of unsupervised hierarchal scene graph learning, and the rebuttal to some degree reassured me. For example, modeling a cube on top of another top as two parts of an object (which was weird to me: why not each cube as an object?) helps edit tasks where the top cube is enlarged but the \"on top\" relation is maintained. The downstream representation transfer also makes sense. Of course experiments are still toy from computer vision perspective, but I'm now okay with acceptance.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538092825, "tmdate": 1606915758473, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2593/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Review"}}}, {"id": "2R26oAjJTID", "original": null, "number": 4, "cdate": 1603909690611, "ddate": null, "tcdate": 1603909690611, "tmdate": 1606688066839, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Review", "content": {"title": "A convincing proof-of-concept solution for a difficult new task", "review": "== Update ==\n\nThank you for your response and clarifications. I have left my score as is.\n\n== Original Review ==\n\nThe paper presents an unsupervised method for inferring scene graphs from images. Building upon\nscene-attention methods such as AIR and SPACE, it hierarchically decomposes a scene into objects and\nthose objects into parts, giving rise to a tree structure. It is shown that this model successfully\nrecovers the hierarchies underlying the data on two newly proposed hierarchical variants of the\nSprites and CLEVR datasets.\n\nStrengths:\n 1. The paper is well written, and despite the considerable complexity of the method, its\n    presentation is relatively easy to follow.\n 2. The task of interest is well-defined, and has clearly been effectivly solved on the datasets\n    considered. Both quantitative and qualitative evaluations make it very clear that the model has\n    learned to infer the correct scene graphs as desired. Its ability to infer the appearance of\n    occluded parts is especially impressive.\n 3. While there are no direct competitors on this newly defined task, the paper does a decent job of\n    comparing to the closest available baseline, showing how the additional structure can be\n    beneficial.\n\nWeaknesses:\n 1. One may argue that the datasets have been deliberately constructed to showcase the model. While\n    that is probably true, I think this is a valid approach given the novel nature of the task and\n    the lack of supervision. Despite the clearly helpful structure (limited number of objects and\n    parts), the datasets still appear sufficiently challenging.\n 2. In the experiments, scene graphs are limited to trees of height 2 and degree 4. This is a\n    significant constraint, however, each additional level of hierarchy introduces ambiguities and\n    makes it harder to learn the graph in an unsupervised manner. More complicated structures would\n    likely require supervision.\n 3. As far as I can tell, the object types were chosen once to generate the datasets, and then kept\n    fixed across the experiments. Reporting results for multiple different datasets with randomly\n    chosen object types would be somewhat more convincing.\n 4. As is common for unsupervised scene models, the proposed method likely only works on synthetic\n    images in its current state. However, due to the additional structural assumptions on the data,\n    it seems especially challenging to find suitable real-world use-cases.\n\nOverall, the paper presents an effective new method for the task it sets out to solve. While it is\nquestionable how it would work on real-world data, I believe the paper is of sufficient interest as\na proof of concept, and am therefore leaning towards acceptance.\n\nQuestions:\n 1. It is stated that auxiliary KL terms are added, with the sparseness constraint on $z^{pres}$\n    being one of them. But is not clear if there are others. This would be important to know in\n    order to evaluate how strong the model's inductive biases are.\n 2. The downstream task used for Fig. 5 is not clear to me. If the number of parts is computed for\n    each object, and these numbers are then summed, isn't the result equal to the total number of\n    parts in the scene, which SPACE-P can also infer? If only distinct parts are counted,\n    how is equality of parts defined on the dataset?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538092825, "tmdate": 1606915758473, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2593/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Review"}}}, {"id": "0PXW5ucde7B", "original": null, "number": 11, "cdate": 1606305416416, "ddate": null, "tcdate": 1606305416416, "tmdate": 1606307180950, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "RmcPm9m3tnk", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment", "content": {"title": "To All Reviewers", "comment": "We thank all reviewers for their time and insightful feedback. In this general response, we would like to address the main concern about the simplicity of our datasets.\n\nWe understand that some prior work on part decomposition has been applied to real-world datasets such as ShapeNet and PartNet. However, these works have three significant differences from ours. First, most of the works require some kind of supervision while ours is fully unsupervised. This usually includes 3D supervision (requiring voxels or point clouds as input) and part-level supervision (providing pre-segmented parts or decomposing into predefined parts like boxes and spheres). Second, they typically focus on geometry (e.g., part occupancy) without learning to represent appearance (e.g., color, material) simultaneously. Third, most of these methods can do either inference or generation, but not both. Lastly, most of these models do not support the controllability of the representation to generate novel out-of-distribution scenes.\n\nA recent line of work on unsupervised object-centric representation learning aims to eliminate the need for supervision in structured scene understanding. These methods learn **a holistic generative model capable of decomposing scenes into objects, learning appearance representations for each object, and generating novel scenes via controllable composition of object representations---all without supervision and in an end-to-end trainable way.** We believe such unsupervised and holistic models are more desirable, albeit more challenging to learn.\n\nAlthough it is a promising approach with great potential in the long run, the unsupervised object-centric representation learning approach has a more ambitious goal. The methods are in its very infancy, and thus currently have some difficulty dealing with real-world data; the most complex datasets used in state-of-the-art models are evaluated on CLEVR-like datasets. Some representatives include IODINE (ICML 2019), SPACE (ICLR 2020), and Slot Attention (NeurIPS 2020). However, we believe future advancements will likely generalize them to more complex data. Thus, **as pointed by Reviewer 1, we hope the reviewers to see our paper as a proof-of-concept paper about a challenging but promising direction.** Accordingly, we will make our claim about proof-of-concept clearer. \n\nIn this paper, we take a step forward in this specific line of research by further decomposing objects into parts. Our model is also unsupervised and holistic, in the sense that it can infer part-object structures, learn appearance representations, perform image manipulation (newly added Figure 4), and generate objects and scenes. **We are not aware of existing part decomposition methods that can do all of the above in a single model without supervision.** \n\nWhile we agree that it would have been better if we made our model work on complex natural images, we believe that our compositional CLEVR dataset, where an object is composed of several CLEVR-like shapes, is still **significantly more complex than the datasets used in previous works in the same line of research.** We showed that the severe occlusion among parts presents some difficulty to SPACE (Table 3). We believe that by introducing hierarchical structures and demonstrating effectiveness on this more challenging dataset, our model makes significant progress in the line of unsupervised object-centric representation learning.\n\nDuring the rebuttal period, we made a slightly more complex version of the Compositional CLEVR dataset, by introducing four new parts. We obtained similar results on this new dataset (Section E). We acknowledge that this is still simple compared to real-world data, and we expect that by upgrading the inference module, our model would benefit from future advancements that can decompose more complex scenes."}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RmcPm9m3tnk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2593/Authors|ICLR.cc/2021/Conference/Paper2593/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment"}}}, {"id": "kZp2G8ai4FW", "original": null, "number": 5, "cdate": 1606304335165, "ddate": null, "tcdate": 1606304335165, "tmdate": 1606306961855, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "WS49JqZPDhf", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (1/2)", "comment": "Thank you for your insightful and very detailed feedback. We are encouraged that you find our method technically impressive and a clear improvement over prior work. We would like to address your concerns and answer your questions in the following.\n\n**IS THE COMPLEXITY NECESSARY?**\n\nThe actual implementation is quite simple, although some complexity is needed for the mathematical formulation. To illustrate this, in the supplementary material we added Algorithm 1 and Algorithm 2 describing foreground inference (implementation of Equation 9), and Figure 9 depicting network architectures. From this, we expect that one will see how easy to implement this model. The main operations performed for decomposition at each level are just one feedforward pass through each of the three CNN submodules.\n\n> Compare to very simple baselines.\n\nThanks for your suggestion. We added qualitative results from a simple baseline called Superpixel Hierarchy (Wei et al., 2018) in Section D. This algorithm hierarchically merges pixels into superpixels based on connectivity and color histograms, until the full image is merged into one superpixel. It can then produce segmentation results when given the desired number of superpixels. We evaluate its ability to perform object and part grouping by providing the groundtruth number of components (the number of objects/parts plus one for background) as the desired number of superpixels. We find that our datasets present some difficulties to this simple baseline. The 2D Shapes dataset contains very tiny shapes, which tend to be merged into the background. Also, the parts within an object do not touch each other, making it hard to merge them into a single 4-connected superpixel. In the Compositional CLEVR dataset, the shadows in the background and the specular highlights on metal materials cause color dissimilarity within components, leading to incorrect decomposition.\n\n> Why should someone prefer your approach over a very simple \u201chandcrafted\u201d approach?\n\nFirst, we showed that the simple baseline we considered was incompetent to infer the scene graph even when given the groundtruth number of objects and parts. Second, our model does more than scene graph inference:\n\n- It infers the appearance of each part, which enables the model to inpaint occluded parts (many examples in Figure 3)\n- It is able to composite new objects by reconfiguring the pose and appearance variables in an inferred scene graph (newly added Figure 4B)\n- It can perform unconditioned scene generation, which even SPACE cannot do (Figure 5)\n- The learned hierarchical representations can improve data efficiency in downstream tasks (Figure 6)\n\nWe believe a model unifying the above capabilities is a meaningful contribution.\n\n**SCALABILITY & MISMATCHED STRUCTURAL HYPERPARAMETERS**\n\nWe implemented GSGN-9, a three-level GSGN where each node has an out-degree of 9, resulting in a tree with 91 nodes. We evaluated its performance on the Compositional CLEVR dataset, and updated the results in Table 2 and Table 3. We observed a slight drop in performance when compared to GSGN. However, GSGN-9 is still much better than SPACE-P at identifying parts that have severe occlusion. The slightly worse part F1 score is caused by slightly inaccurate prediction of center positions. If we increase the error tolerance from 5 pixels to 6 pixles, then the part F1 score of GSGN-9 becomes 99.07%, which is comparable to other models."}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RmcPm9m3tnk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2593/Authors|ICLR.cc/2021/Conference/Paper2593/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment"}}}, {"id": "HMOqj6VaGH-", "original": null, "number": 8, "cdate": 1606304541508, "ddate": null, "tcdate": 1606304541508, "tmdate": 1606306271407, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "zaYaEoYRhV", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (2/2)", "comment": "> A lot of overclaims, to name a few:\n\nThanks for pointing this. We agree that the tone of some of our claims requires some adjustment. We made the adjustment as follows:\n\n> 1. \u201cGSGN is a general framework for representing and inferring scene graphs of arbitrary depth\u201d: I don\u2019t think a model being able to work a toy setting with three levels will mean that the same framework can be used for more complex settings of arbitrary depth (as an analogy: MLP works for MNIST but not on ImageNet).\n\nWe changed it to \"While our formulation of GSGN allows representing and inferring scene graphs of arbitrary depth, in our experiments, we have only investigated the effectiveness of a three-level GSGN\".\n\n> 2. \u201cClosely follow the rendering process in graphics engines\u201d: I don\u2019t think applying affine transformations and compositing alone is enough to warrant this claim, it is pretty clear that the learned representation lacks a good sense of \u201cobjectness\u201d, as textures, lighting and etc. are all entangled together.\n\nWe changed it to \"follow the recursive compositing process in graphics engines\".\n\n> 3. \u201cFirst deep generative model for unsupervised scene-graph discovery\u201d: there are a lot of works that infer structures in an unsupervised way, I don\u2019t think it\u2019s fair to give a very narrow definition of \u201cscene graph\u201d and claim \u201cthe first\u201d.\n\nWe changed it to \"first deep generative model for unsupervised scene-graph discovery from multi-object scenes without knowledge of individual parts\".\n\n> 4. \u201cGSGN has capture many predefined object types in the dataset\u201d: I don\u2019t think one can make this claim when there are only three primitives and ten types of objects\n\nWe changed it to \"GSGN has captured many predefined object types in the two datasets considered\".\n\n**ADDITIONAL QUESTIONS**\n\n> Why predict translation/rotation, when the perspective/illumination of the object already provides a really strong cue?\n\nTranslation/rotation are inherent components of the scene graph. They identify an image region as an object/part, allowing structured understanding of the scene. Our current model learns the 2D appearance of objects/parts in a single rendered image. It does not learn 3D appearance, which would require a set of input images from multiple viewpoints. Hence, we cannot explicitly or accurately model perspective/illumination.\n\n> Examples of the same learned object being used in multiple scenes.\n\nPlease see row 3-4 of the newly added Figure 4A, where we replace an object of the current scene with an object from another scene. As we explained above, we cannot expect to model perspective/illumination changes from a single input image.\n\n> Table 1 & 2: why are all the ELBO terms the same?\n\nELBO is usually dominated by the reconstruction error. Here, we normalized the ELBO by the number of pixels, which is common practice in VAE literature. A similar ELBO in this case indicates similar reconstruction quality. The precise ELBO values are different.\n\n> Table 2 & 3: why are the metrics so close between SPACE-P & GSGN in Table 2 but so different in Table 3? Does that suggest unbalanced dataset?\n\nHere are the statistics over the 12800 test images. We did not have direct control over the minimum number of pixels per part.\n\n| Min Visible Pixels Per Part | <100 | 100~200 | >200 |\n|:---------------------------:|:----:|:-------:|:----:|\n|    Number of Test Images    |  49  |  3772   | 8979 |\n\n> Table 2 & 3: why no comparison between SPACE-O & GSGN for object level occlusion?\n\nThere is no significant difference in overall performance on object-level decomposition, so we thought it would be unnecessary."}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RmcPm9m3tnk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2593/Authors|ICLR.cc/2021/Conference/Paper2593/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment"}}}, {"id": "YY1qlh4Cohb", "original": null, "number": 10, "cdate": 1606304609751, "ddate": null, "tcdate": 1606304609751, "tmdate": 1606304609751, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "2R26oAjJTID", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your positive and thoughtful comments. We are encouraged that you find our paper well written and the inference of occluded parts impressive. We would like to address your concerns and answer your questions in the following.\n\n> Deliberately constructed datasets with limited number of objects and parts.\n\nWe appreciate that you think this approach is valid and the datasets are sufficiently challenging. While the number of objects and parts are limited, we designed each object to have a different structure in terms of the relative pose of its parts. This helps cover a wide range of scene graphs, of which we added an analysis in Section F.\n\n> Limited tree height and width.\n\nWe implemented GSGN-9, a three-level GSGN where each node has an out-degree of 9. We evaluated its performance on the Compositional CLEVR dataset, and updated the results in Table 2 and Table 3. GSGN-9 is still much better than SPACE-P at identifying parts that have severe occlusion. The slightly worse part F1 score is caused by slightly inaccurate prediction of center positions. If we increase the error tolerance from 5 pixels to 6 pixles, then the part F1 score of GSGN-9 becomes 99.07%, which is comparable to other models. While we did not evaluate GSGN-9 on more complex data, our results demonstrate the effectiveness when structural hyperparameters are mismatched to data (suggested by Reviewer 3), and show some potential for scalability.\n\n> Reporting results for multiple different datasets.\n\nThanks for the suggestion. We made a slightly more complex version of the Compositional CLEVR dataset with four new parts. Results are similar, and we added them in Section E. Although the object types were not randomly chosen, we believe this new result is still meaningful.\n\n> Real-world use cases.\n\nWe agree that our model currently cannot deal with real-world data. However, because we use scene decomposition models as an inference module, our model is likely to benefit from future advancements that generalize scene decomposition models to more complex scenarios.\n\n**ADDITIONAL QUESTIONS**\n\n> Auxiliary KL terms.\n\nWe added description of auxiliary KL terms in Section C.3. We note that the prior distributions in these terms were not chosen to match the actual distributions in the datasets. Also, we used mostly the same terms on both of our datasets.\n\n> How is the number of parts computed in the downstream task?\n\nWe clarified this in our description. Only distinct parts are counted within each object. Two parts are considered the same if they have the same shape, regardless of their pose, color, and material."}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RmcPm9m3tnk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2593/Authors|ICLR.cc/2021/Conference/Paper2593/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment"}}}, {"id": "btauxoKwG-", "original": null, "number": 9, "cdate": 1606304581055, "ddate": null, "tcdate": 1606304581055, "tmdate": 1606304581055, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "lygkHt1HkxM", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We appreciate your constructive comments. We would like to address your concerns below.\n\n> Objects are too simple (single color, simple shape).\n\nPlease see our general response regarding this concern. We also made a slightly more complex version of the Compositional CLEVR dataset with four new parts. We obtained similar results on this new dataset (Section E).\n\n> The object/primitive decomposition is slightly weird.\n\nWe agree that the compositional objects in our datasets do not look realistic. However, given that state-of-the-art unsupervised object-level decomposition models can only deal with simple shapes, we think it is reasonable to use these shapes as primitives. Also, some simple real-world objects like dumbbells and hair dryers have similar structures as those in our datasets, albeit with more complex shapes as primitives.\n\n> Application of learned scene graphs (e.g., image manipulation).\n\nThanks for your suggestion. We added image manipulation results in Figure 4. We showed that one can individually modify the position and scale of objects and parts. In addition, we demonstrated that by reconfiguring the appearance variables, one can add objects from other scenes, and generate novel objects by compositing parts from multiple scenes.\n\n> Analysis of how variational the learned scene graphs can be.\n\nWe added some analysis in Section F, regarding the number of possible scene graphs."}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RmcPm9m3tnk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2593/Authors|ICLR.cc/2021/Conference/Paper2593/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment"}}}, {"id": "Y7fgJw9fk3", "original": null, "number": 7, "cdate": 1606304488166, "ddate": null, "tcdate": 1606304488166, "tmdate": 1606304488166, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "zaYaEoYRhV", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (1/2)", "comment": "Thank you for your very detailed comments and suggestions for improvement. We would like to address your concerns and answer your questions in the following.\n\n> Unclear novelty and simple datasets.\n\nPlease see our general response regarding this concern.\n\n> Very inadequate attribution of ideas.\n\nWe improved the related work section as you suggested. We would like to mention that we did give credit to prior work when introducing our main design choices. For example, we cited relevant prior work when introducing parameter sharing and auxiliary KL loss.\n\n> Missing discussions of relevant works.\n\nThanks for pointing to relevant works. We have added them to the related work section. In general, these works solve different problems than ours:\n\n- The first three methods take 3D voxels as input, and predict part geometry (i.e., occupancy). They only work for single objects, and cannot model appearance (color, material, etc). In contrast, our model takes 2D images of multi-object scenes as input, and infers both the decomposition and appearance of each part.\n- The first two methods have some additional assumptions. UCSG-NET assumes predefined primitive parts, such as box and sphere, while our model can learn to discover the primitive parts purely from data. Bae-net assumes the objects come from the same category, thus having similar part structures. In particular, all objects have the same number of parts. Our model can infer the number of parts, and does not require that all objects have similar structures.\n- In StructureNet, the part hierarchy is provided as input to the model, rather than inferred from the input.\n\n> Comparing against metrics/datasets adopted in other works.\n\nWe have compared with the closest baselines, and we do not think it is fair or suitable to compare with other works. As we mentioned in the experiment section, \"Previous work on hierarchical scene representations assumes single-object scenes (Kosiorek et al., 2019) and requires predefined or pre-segmented parts (Li et al., 2017; Huang et al.,2020)\". We do not think it is fair to compare our model (without access to groundtruth parts) to (Li et al., 2017; Huang et al.,2020). Also, their datasets use voxels/point clouds as input, which is a different problem setting. In (Kosiorek et al., 2019), the main metric is unsupervised classification accuracy on MNIST, CIFAR10, and SVHN. These datasets do not seem suitable for evaluating interpretable part decomposition. And as we discussed above, the papers you mentioned solve different problems than ours, and also require different input formats.\n\n> The authors argue that dataset with a single shape is easier, but I disagree.\n\nWe did not say that dataset with a single shape is easier. What we intended to argue is that methods that work on single-object scenes circumvent the challenge of grouping parts into objects, because they assume that all parts belong to the same object. Clearly this assumption cannot hold when there are multiple objects. Our model directly addresses this challenge by taking a top-down inference approach. This is in itself a contribution. If we took a bottom-up approach, then when objects are close, it would be very hard to determine which part should belong to which object. Also, the fact that SPACE-P cannot separate occluded parts indicates that the bottom-up approach would likely be sub-optimal. We believe that solving this part grouping challenge is orthogonal to learning complicated part structures of a single object.\n\n> Real applications.\n\nAs suggested by Reviewer 4, we added Figure 4 showing an application of the learned scene graph in image manipulation. We demonstrated that by modifying the pose and appearance variables in a learned scene graph, one can individually manipulate the objects and parts in the scene. Also, by combining parts from multiple scenes, one can composite new objects that are never seen during training. While our model currently cannot deal with real-world data, it is likely that our inference module can be upgraded to incorporate future advancements that generalize scene decomposition models to more complex scenarios.\n\n> The learned representation does not seem to be of very good quality, as seen in Figure 4.\n\nWe respectfully disagree. Figure 3 would be more appropriate for investigating the quality of learned representations. Figure 4 (Figure 5 in updated version) shows unconditioned generations. The latent variables are not inferred from a given image, but directly sampled from the prior, i.e., the root node is sampled from $\\mathcal{N}(0,1)$, and others are sampled from conditional priors. Typically in VAEs, we cannot expect unconditioned generations to be of very good quality. However, it is clear from the figure that our model gives better object generations than SPACE. Also, our model can generate meaningful scenes while SPACE can only generate empty scenes."}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RmcPm9m3tnk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2593/Authors|ICLR.cc/2021/Conference/Paper2593/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment"}}}, {"id": "-rKTowOoQxE", "original": null, "number": 6, "cdate": 1606304372684, "ddate": null, "tcdate": 1606304372684, "tmdate": 1606304372684, "tddate": null, "forum": "RmcPm9m3tnk", "replyto": "WS49JqZPDhf", "invitation": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (2/2)", "comment": "**IMPLEMENTATION DETAILS**\n\nWe added implementation details to the supplementary material. We will also release the code and the Compositional CLEVR dataset upon publication.\n\n> What is the generative process for the presence variables $z_v^{pres}$?\n\nIn Equation 4, the term $p(z_v^{pres} \\!\\mid\\! z_{pa(v)}^{appr})$ suggests that during generation, the presence variable of node $v$ is conditioned on the appearance variable of its parent $pa(v)$. This makes sense because $z_{pa(v)}^{appr}$ is expected to summarize lower-level compositions. Note that this conditional distribution is a Bernoulli distribution, and as mentioned in the text right below Equation 4, its distribution parameter is learned by an MLP.\n\n> What is the dimension of the latent variables $z_r^{appr}$?\n\nOn both datasets, the part-, object-, and scene-level appearance variables have dimensions 32, 64, and 128, respectively.\n\n> How is the pose parameterized in terms of relative location and scale?\n\nWe clarified this in Section C.2. In short, the Gaussian variables are first converted to the desired range through nonlinear squashing functions, and then used to calculate the affine transformation matrix in the usual way required by the spatial transformer.\n\n> Depth maps and transparency maps.\n\nWe clarified this in Section C.2. The pose variables are not per-pixel, but per-node variables. Specifically, we use a 1-dimensional depth variable for each node $v$ to represent the relative depth of $v$ with respect to its siblings (e.g., if $v$ is a part of object $u$, then the siblings of $v$ are the other parts of the same object $u$). This depth value is broadcast to all pixels that would belong to node $v$ if there were no occlusion. In regions of occlusion, each pixel will maintain multiple depth values. The transparency map is then computed by a per-pixel softmax over the negative depth values, to select for each pixel the entity that contains it and has the smallest depth.\n\n> In Equation 9, the distributions of the latents $z_v^{pres}$, $z_v^{pose}$ are conditioned only on data from the parent node $z_{pa(v)}^{appr}$ and $x_{pa(v)}$. Thus all children of a node will have the same distribution for their pose and appearance. Is this correct? If so, how do you encourage the child nodes to cover all parts of the parent, and not collapse to a single part of the parent?\n\nGood question. First, the distributions are not the same, because the network parameters are not shared among the children nodes of the same parent. In fact, the children nodes are inferred in parallel through a CNN. Second, similar to SPAIR and SPACE, we divide $x_{pa(v)}$ into grid cells. Each child node is associated with one cell and is encouraged to identify a part that is close to that cell. We made the above clear in the supplementary material by showing network architectures in Figure 9 and describing the parameterization of positions in Equation 15.\n\n> Details about neural network architectures and training hyperparameters.\n\nThanks for the suggestion. We added Section C describing the implementation details."}, "signatures": ["ICLR.cc/2021/Conference/Paper2593/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Scene Graph Networks", "authorids": ["~Fei_Deng1", "~Zhuo_Zhi1", "donghun@etri.re.kr", "~Sungjin_Ahn1"], "authors": ["Fei Deng", "Zhuo Zhi", "Donghun Lee", "Sungjin Ahn"], "keywords": ["object-centric representations", "generative modeling", "scene generation", "variational autoencoders"], "abstract": "Human perception excels at building compositional hierarchies of parts and objects from unlabeled scenes that help systematic generalization. Yet most work on generative scene modeling either ignores the part-whole relationship or assumes access to predefined part labels. In this paper, we propose Generative Scene Graph Networks (GSGNs), the first deep generative model that learns to discover the primitive parts and infer the part-whole relationship jointly from multi-object scenes without supervision and in an end-to-end trainable way. We formulate GSGN as a variational autoencoder in which the latent representation is a tree-structured probabilistic scene graph. The leaf nodes in the latent tree correspond to primitive parts, and the edges represent the symbolic pose variables required for recursively composing the parts into whole objects and then the full scene. This allows novel objects and scenes to be generated both by sampling from the prior and by manual configuration of the pose variables, as we do with graphics engines. We evaluate GSGN on datasets of scenes containing multiple compositional objects, including a challenging Compositional CLEVR dataset that we have developed. We show that GSGN is able to infer the latent scene graph, generalize out of the training regime, and improve data efficiency in downstream tasks.", "one-sentence_summary": "We propose the first object-centric generative model capable of unsupervised scene graph discovery from multi-object scenes without access to predefined parts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "deng|generative_scene_graph_networks", "pdf": "/pdf/4972f3189bc1990cd88f0c12abbe7111acfe3c15.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ndeng2021generative,\ntitle={Generative Scene Graph Networks},\nauthor={Fei Deng and Zhuo Zhi and Donghun Lee and Sungjin Ahn},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RmcPm9m3tnk}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RmcPm9m3tnk", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2593/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2593/Authors|ICLR.cc/2021/Conference/Paper2593/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2593/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846544, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2593/-/Official_Comment"}}}], "count": 13}