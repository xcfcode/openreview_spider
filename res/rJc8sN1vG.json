{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124473983, "tcdate": 1518451538546, "number": 142, "cdate": 1518451538546, "id": "rJc8sN1vG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rJc8sN1vG", "signatures": ["~Yiming_Zhang1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Efficient Entropy For Policy Gradient with Multi-Dimensional Action Space", "abstract": "This paper considers entropy bonus, which is used to encourage exploration in policy gradient. In the case of high-dimensional action spaces, calculating the entropy and its gradient requires enumerating all the actions in the action space and running forward and backpropagation for each action, which may be computationally infeasible. We develop several novel unbiased estimators for the entropy bonus and its gradient. We apply these estimators to several models for the parameterized policies, including Independent Sampling, CommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM. Finally, we test our algorithms on a multi-hunter multi-rabbit grid environment. The results show that our entropy estimators substantially improve performance with marginal additional computational cost.", "paperhash": "zhang|efficient_entropy_for_policy_gradient_with_multidimensional_action_space", "keywords": ["deep reinforcement learning", "policy gradient", "multidimensional action space", "entropy bonus", "entropy regularization", "discrete action space"], "_bibtex": "@misc{\n  zhang2018efficient,\n  title={Efficient Entropy For Policy Gradient with Multi-Dimensional Action Space},\n  author={Yiming Zhang and Quan Ho Vuong and Kenny Song and Xiao-Yue Gong and Keith W. Ross},\n  year={2018},\n  url={https://openreview.net/forum?id=rJc8sN1vG}\n}", "authorids": ["yiming.zhang@nyu.edu", "quan.vuong@nyu.edu", "kenny.song@nyu.edu", "xygong@mit.edu", "keithwross@nyu.edu"], "authors": ["Yiming Zhang", "Quan Ho Vuong", "Kenny Song", "Xiao-Yue Gong", "Keith W. Ross"], "TL;DR": "Unbiased policy entropy estimators and policy parameterization for MDP with large multidimensional discrete action space", "pdf": "/pdf/a734ca9e8f0ac0c07060efe072c55a73a8d5b42f.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582925360, "tcdate": 1520361653674, "number": 1, "cdate": 1520361653674, "id": "rJ0nev3uM", "invitation": "ICLR.cc/2018/Workshop/-/Paper142/Official_Review", "forum": "rJc8sN1vG", "replyto": "rJc8sN1vG", "signatures": ["ICLR.cc/2018/Workshop/Paper142/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper142/AnonReviewer1"], "content": {"title": "Summary of paper that was on the edge for main conference", "rating": "6: Marginally above acceptance threshold", "review": "This paper is a short version of the paper \"Policy Gradient For Multidimensional Action Spaces: Action Sampling and Entropy Bonus\", which I was also a reviewer on. I gave the original paper a borderline accept rating and I'd be willing to give this paper the same rating.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Entropy For Policy Gradient with Multi-Dimensional Action Space", "abstract": "This paper considers entropy bonus, which is used to encourage exploration in policy gradient. In the case of high-dimensional action spaces, calculating the entropy and its gradient requires enumerating all the actions in the action space and running forward and backpropagation for each action, which may be computationally infeasible. We develop several novel unbiased estimators for the entropy bonus and its gradient. We apply these estimators to several models for the parameterized policies, including Independent Sampling, CommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM. Finally, we test our algorithms on a multi-hunter multi-rabbit grid environment. The results show that our entropy estimators substantially improve performance with marginal additional computational cost.", "paperhash": "zhang|efficient_entropy_for_policy_gradient_with_multidimensional_action_space", "keywords": ["deep reinforcement learning", "policy gradient", "multidimensional action space", "entropy bonus", "entropy regularization", "discrete action space"], "_bibtex": "@misc{\n  zhang2018efficient,\n  title={Efficient Entropy For Policy Gradient with Multi-Dimensional Action Space},\n  author={Yiming Zhang and Quan Ho Vuong and Kenny Song and Xiao-Yue Gong and Keith W. Ross},\n  year={2018},\n  url={https://openreview.net/forum?id=rJc8sN1vG}\n}", "authorids": ["yiming.zhang@nyu.edu", "quan.vuong@nyu.edu", "kenny.song@nyu.edu", "xygong@mit.edu", "keithwross@nyu.edu"], "authors": ["Yiming Zhang", "Quan Ho Vuong", "Kenny Song", "Xiao-Yue Gong", "Keith W. Ross"], "TL;DR": "Unbiased policy entropy estimators and policy parameterization for MDP with large multidimensional discrete action space", "pdf": "/pdf/a734ca9e8f0ac0c07060efe072c55a73a8d5b42f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582925172, "id": "ICLR.cc/2018/Workshop/-/Paper142/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper142/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper142/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper142/AnonReviewer3"], "reply": {"forum": "rJc8sN1vG", "replyto": "rJc8sN1vG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper142/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper142/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582925172}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582666651, "tcdate": 1520750180989, "number": 2, "cdate": 1520750180989, "id": "SkpwArztz", "invitation": "ICLR.cc/2018/Workshop/-/Paper142/Official_Review", "forum": "rJc8sN1vG", "replyto": "rJc8sN1vG", "signatures": ["ICLR.cc/2018/Workshop/Paper142/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper142/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes entropy estimates for autoregressive models for policy gradient. \nI think, the proposed estimates are novel and interesting. The paper is well written and motivated.\n\nThe experiments are done on toy task, though I appreciate that the authors validated the proposed method as compared to various other baselines. \n\nAlso, as a side point,  the objective function can be formulated in terms of score function estimator, that might be worth pointing out in the paper. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Entropy For Policy Gradient with Multi-Dimensional Action Space", "abstract": "This paper considers entropy bonus, which is used to encourage exploration in policy gradient. In the case of high-dimensional action spaces, calculating the entropy and its gradient requires enumerating all the actions in the action space and running forward and backpropagation for each action, which may be computationally infeasible. We develop several novel unbiased estimators for the entropy bonus and its gradient. We apply these estimators to several models for the parameterized policies, including Independent Sampling, CommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM. Finally, we test our algorithms on a multi-hunter multi-rabbit grid environment. The results show that our entropy estimators substantially improve performance with marginal additional computational cost.", "paperhash": "zhang|efficient_entropy_for_policy_gradient_with_multidimensional_action_space", "keywords": ["deep reinforcement learning", "policy gradient", "multidimensional action space", "entropy bonus", "entropy regularization", "discrete action space"], "_bibtex": "@misc{\n  zhang2018efficient,\n  title={Efficient Entropy For Policy Gradient with Multi-Dimensional Action Space},\n  author={Yiming Zhang and Quan Ho Vuong and Kenny Song and Xiao-Yue Gong and Keith W. Ross},\n  year={2018},\n  url={https://openreview.net/forum?id=rJc8sN1vG}\n}", "authorids": ["yiming.zhang@nyu.edu", "quan.vuong@nyu.edu", "kenny.song@nyu.edu", "xygong@mit.edu", "keithwross@nyu.edu"], "authors": ["Yiming Zhang", "Quan Ho Vuong", "Kenny Song", "Xiao-Yue Gong", "Keith W. Ross"], "TL;DR": "Unbiased policy entropy estimators and policy parameterization for MDP with large multidimensional discrete action space", "pdf": "/pdf/a734ca9e8f0ac0c07060efe072c55a73a8d5b42f.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582925172, "id": "ICLR.cc/2018/Workshop/-/Paper142/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper142/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper142/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper142/AnonReviewer3"], "reply": {"forum": "rJc8sN1vG", "replyto": "rJc8sN1vG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper142/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper142/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582925172}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573569457, "tcdate": 1521573569457, "number": 116, "cdate": 1521573569113, "id": "SkK60CAYM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rJc8sN1vG", "replyto": "rJc8sN1vG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Entropy For Policy Gradient with Multi-Dimensional Action Space", "abstract": "This paper considers entropy bonus, which is used to encourage exploration in policy gradient. In the case of high-dimensional action spaces, calculating the entropy and its gradient requires enumerating all the actions in the action space and running forward and backpropagation for each action, which may be computationally infeasible. We develop several novel unbiased estimators for the entropy bonus and its gradient. We apply these estimators to several models for the parameterized policies, including Independent Sampling, CommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM. Finally, we test our algorithms on a multi-hunter multi-rabbit grid environment. The results show that our entropy estimators substantially improve performance with marginal additional computational cost.", "paperhash": "zhang|efficient_entropy_for_policy_gradient_with_multidimensional_action_space", "keywords": ["deep reinforcement learning", "policy gradient", "multidimensional action space", "entropy bonus", "entropy regularization", "discrete action space"], "_bibtex": "@misc{\n  zhang2018efficient,\n  title={Efficient Entropy For Policy Gradient with Multi-Dimensional Action Space},\n  author={Yiming Zhang and Quan Ho Vuong and Kenny Song and Xiao-Yue Gong and Keith W. Ross},\n  year={2018},\n  url={https://openreview.net/forum?id=rJc8sN1vG}\n}", "authorids": ["yiming.zhang@nyu.edu", "quan.vuong@nyu.edu", "kenny.song@nyu.edu", "xygong@mit.edu", "keithwross@nyu.edu"], "authors": ["Yiming Zhang", "Quan Ho Vuong", "Kenny Song", "Xiao-Yue Gong", "Keith W. Ross"], "TL;DR": "Unbiased policy entropy estimators and policy parameterization for MDP with large multidimensional discrete action space", "pdf": "/pdf/a734ca9e8f0ac0c07060efe072c55a73a8d5b42f.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}