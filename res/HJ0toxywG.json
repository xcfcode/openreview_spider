{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124458667, "tcdate": 1518435205800, "number": 113, "cdate": 1518435205800, "id": "HJ0toxywG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HJ0toxywG", "signatures": ["~Yinpeng_Chen1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Share or Split : which is more efficient?", "abstract": "In this paper, we investigate two different feature representations in convolutional neural networks (CNN): (a) Share - all classes share the same feature space, and a fully connected layer is used to decode class activation, and (b) Split - each class has its own feature space and a class is activated if its corresponding feature vector has a large norm. This is inspired by Capsules (\\cite{Sabour17capsules}) which splits the feature space. We compare these two representations on a transformed MNIST dataset, which adds random scales and translations on the original digits. The experimental results show that Split has better performance when data is limited, while Share is better when data is big.\n", "paperhash": "chen|share_or_split_which_is_more_efficient", "_bibtex": "@misc{\n  chen2018share,\n  title={Share or Split : which is more efficient?},\n  author={Yinpeng Chen and Zicheng Liu and Lijuan Wang and Zhengyou Zhang},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ0toxywG}\n}", "authorids": ["yiche@microsoft.com", "zliu@microsoft.com", "lijuanw@microsoft.com", "zhang@microsoft.com"], "authors": ["Yinpeng Chen", "Zicheng Liu", "Lijuan Wang", "Zhengyou Zhang"], "keywords": [], "pdf": "/pdf/67a25aea4d868bec5d9c82cab81f9c0867547462.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582862309, "tcdate": 1520557969494, "number": 1, "cdate": 1520557969494, "id": "HkKcywJKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper113/Official_Review", "forum": "HJ0toxywG", "replyto": "HJ0toxywG", "signatures": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer1"], "content": {"title": "hard to draw many conclusions", "rating": "5: Marginally below acceptance threshold", "review": "This paper compares two approaches to final feature layer representation in a classifier:  \"Share\", which uses the same feature vector for all classes, and \"Split\", which has a different feature vector for each class, and chooses the \"winner\" based on which one has largest magnitude.  These two representations are used to classify transformed MNIST digits, as well as estimate digit location on a black background with a bounding box regressor.\n\nUnfortunately, it's hard for me to figure out exactly what to take away from this.  The conclusion at the end says \"our experimental results recommend using Split on classification task alone when data is limited and using Share when big data is available. When dealing with joint classification and detection, Share is recommended\".  But it seems a stretch to draw even this conclusion:  Share seems better in 3/4 settings, and in the one where it performs worse, it is by a small margin --- about 98.05% vs 98.25%.  So it's hard to know if this is indeed from the architecture structure, or from other optimization, loss or parameter differences.  I'm also not sure how many trials were performed or what is the difference in error between trials -- it appears it would be somewhere on the order of 0.1% since K_h=80 is about 0.1% worse than K_h=40 for 4.4M dataset, when one would expect it to be better with this much data.\n\nSince many of these classification results are close, I wonder whether it is possible to draw any equivalences between the two models?  For instance, if L1 norm were used instead of L2, this could be computed by adding an additional layer on top of the hidden representation with a block structure that has 1's in locations that correspond to the entries with sums in the L1.  In that case, the differences are that the top layer is fixed to block sums, versus learned, and the loss used.\n\nThe clearest conclusion appears to be that Split is worse for bounding box regression on transformed MNIST, since the task is roughly the same for every class, meaning that sharing the features between classes should have a significant benefit here --- which it does, as the authors point out.\n\nBeyond this, however, it seems hard to draw many conclusions from this experiment.  The difference in error is pretty small, and in a single experimental setting.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Share or Split : which is more efficient?", "abstract": "In this paper, we investigate two different feature representations in convolutional neural networks (CNN): (a) Share - all classes share the same feature space, and a fully connected layer is used to decode class activation, and (b) Split - each class has its own feature space and a class is activated if its corresponding feature vector has a large norm. This is inspired by Capsules (\\cite{Sabour17capsules}) which splits the feature space. We compare these two representations on a transformed MNIST dataset, which adds random scales and translations on the original digits. The experimental results show that Split has better performance when data is limited, while Share is better when data is big.\n", "paperhash": "chen|share_or_split_which_is_more_efficient", "_bibtex": "@misc{\n  chen2018share,\n  title={Share or Split : which is more efficient?},\n  author={Yinpeng Chen and Zicheng Liu and Lijuan Wang and Zhengyou Zhang},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ0toxywG}\n}", "authorids": ["yiche@microsoft.com", "zliu@microsoft.com", "lijuanw@microsoft.com", "zhang@microsoft.com"], "authors": ["Yinpeng Chen", "Zicheng Liu", "Lijuan Wang", "Zhengyou Zhang"], "keywords": [], "pdf": "/pdf/67a25aea4d868bec5d9c82cab81f9c0867547462.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582862090, "id": "ICLR.cc/2018/Workshop/-/Paper113/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper113/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper113/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper113/AnonReviewer3"], "reply": {"forum": "HJ0toxywG", "replyto": "HJ0toxywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper113/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper113/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582862090}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582708782, "tcdate": 1520692787322, "number": 2, "cdate": 1520692787322, "id": "SkiEAw-KG", "invitation": "ICLR.cc/2018/Workshop/-/Paper113/Official_Review", "forum": "HJ0toxywG", "replyto": "HJ0toxywG", "signatures": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer2"], "content": {"title": "-", "rating": "5: Marginally below acceptance threshold", "review": "This paper is a straightforward comparison of convnets trained using conventional multinomial logistic regression-based classification (\u201csharing\u201d) vs. the recently proposed capsules (\u201csplitting\u201d).  Both methods are evaluated on \u201ctransformed MNIST\u201d for classification and bounding box regression using different feature dimensionalities in the last hidden layer as well as different training set sizes.  Based on the results, the paper concludes that \u201csharing\u201d works better with sufficiently large training sets, but \u201csplitting\u201d is better with smaller training sets.\n\nGiven that the paper is essentially evaluating two existing methods, this is slightly below threshold for me as it seems a bit too limited to draw any robust conclusions from.  Detailed comments:\n\n- An L2 reconstruction loss is used to train both networks.  It would have been nice to know what the effect of this loss term on classification / bbox regression accuracy is, as well as the reconstruction error of each network.\n\n- The splitting network is reported as being significantly worse for bbox regression (with far larger performance differences than for classification), but I suspect this is due to a particular design choice: in the \u201csplitting\u201d network the bounding box regression and reconstruction networks take the \u201csplit\u201d features as input.  This is potentially detrimental as the split features must now be both class-specific (as their norm is used directly for classification) but also must contain class-generic information used for bbox regression and reconstruction.  An alternative that might make for a fairer comparison with the sharing network would be to use the \u201csplit\u201d features for classification alone and then have a separate output branch used for bbox regression and reconstruction.  (An equivalent design could be used for the \u201csharing\u201d network to keep the capacity comparable.)\n\n- The networks are only evaluated on an MNIST-derived dataset.  Given this limited evaluation and the quite small differences in classification accuracy between all the methods, I would have liked to see some measure of variance (e.g. confidence intervals) over difference dataset splits and random weight initializations in the results.  Furthermore, it\u2019s not clear that the conclusions would be similar on larger or more visually complex datasets like ImageNet or even CIFAR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Share or Split : which is more efficient?", "abstract": "In this paper, we investigate two different feature representations in convolutional neural networks (CNN): (a) Share - all classes share the same feature space, and a fully connected layer is used to decode class activation, and (b) Split - each class has its own feature space and a class is activated if its corresponding feature vector has a large norm. This is inspired by Capsules (\\cite{Sabour17capsules}) which splits the feature space. We compare these two representations on a transformed MNIST dataset, which adds random scales and translations on the original digits. The experimental results show that Split has better performance when data is limited, while Share is better when data is big.\n", "paperhash": "chen|share_or_split_which_is_more_efficient", "_bibtex": "@misc{\n  chen2018share,\n  title={Share or Split : which is more efficient?},\n  author={Yinpeng Chen and Zicheng Liu and Lijuan Wang and Zhengyou Zhang},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ0toxywG}\n}", "authorids": ["yiche@microsoft.com", "zliu@microsoft.com", "lijuanw@microsoft.com", "zhang@microsoft.com"], "authors": ["Yinpeng Chen", "Zicheng Liu", "Lijuan Wang", "Zhengyou Zhang"], "keywords": [], "pdf": "/pdf/67a25aea4d868bec5d9c82cab81f9c0867547462.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582862090, "id": "ICLR.cc/2018/Workshop/-/Paper113/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper113/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper113/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper113/AnonReviewer3"], "reply": {"forum": "HJ0toxywG", "replyto": "HJ0toxywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper113/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper113/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582862090}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582661530, "tcdate": 1520763541760, "number": 3, "cdate": 1520763541760, "id": "BJ0cftMtf", "invitation": "ICLR.cc/2018/Workshop/-/Paper113/Official_Review", "forum": "HJ0toxywG", "replyto": "HJ0toxywG", "signatures": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer3"], "content": {"title": "Lack of experiments", "rating": "5: Marginally below acceptance threshold", "review": "This paper investigates two different feature representations: share and split. Share is the conventional way for image classification, i.e. all classes share the same feature space and a fully connected layer determines the class activation. Split is another way by using separate feature space for each class and activation is determined by which class having a large norm. Experiments on transformed MNIST dataset show that Split is better for limited data and Share is better for enough data.\n\nStrength and weakness\n+ Explanation of pros and cons.\n- Results only on one dataset. \n- The backbone network is rather simple. It would be great to try deeper network to see if more powerful encoder would have different result.\n- For dataset sizes, there are only two options Train 55k and Train 4.4M. Also difference of number in Table 1 is very small, not sure if it is marginally large enough to convince the take away message. \n\nIn general, I think this is a very interesting topic. However, experimental results are only on one dataset and the numbers are not significantly enough for the statement.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Share or Split : which is more efficient?", "abstract": "In this paper, we investigate two different feature representations in convolutional neural networks (CNN): (a) Share - all classes share the same feature space, and a fully connected layer is used to decode class activation, and (b) Split - each class has its own feature space and a class is activated if its corresponding feature vector has a large norm. This is inspired by Capsules (\\cite{Sabour17capsules}) which splits the feature space. We compare these two representations on a transformed MNIST dataset, which adds random scales and translations on the original digits. The experimental results show that Split has better performance when data is limited, while Share is better when data is big.\n", "paperhash": "chen|share_or_split_which_is_more_efficient", "_bibtex": "@misc{\n  chen2018share,\n  title={Share or Split : which is more efficient?},\n  author={Yinpeng Chen and Zicheng Liu and Lijuan Wang and Zhengyou Zhang},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ0toxywG}\n}", "authorids": ["yiche@microsoft.com", "zliu@microsoft.com", "lijuanw@microsoft.com", "zhang@microsoft.com"], "authors": ["Yinpeng Chen", "Zicheng Liu", "Lijuan Wang", "Zhengyou Zhang"], "keywords": [], "pdf": "/pdf/67a25aea4d868bec5d9c82cab81f9c0867547462.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582862090, "id": "ICLR.cc/2018/Workshop/-/Paper113/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper113/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper113/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper113/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper113/AnonReviewer3"], "reply": {"forum": "HJ0toxywG", "replyto": "HJ0toxywG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper113/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper113/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582862090}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573584473, "tcdate": 1521573584473, "number": 177, "cdate": 1521573584129, "id": "BJd0ARAtG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HJ0toxywG", "replyto": "HJ0toxywG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Share or Split : which is more efficient?", "abstract": "In this paper, we investigate two different feature representations in convolutional neural networks (CNN): (a) Share - all classes share the same feature space, and a fully connected layer is used to decode class activation, and (b) Split - each class has its own feature space and a class is activated if its corresponding feature vector has a large norm. This is inspired by Capsules (\\cite{Sabour17capsules}) which splits the feature space. We compare these two representations on a transformed MNIST dataset, which adds random scales and translations on the original digits. The experimental results show that Split has better performance when data is limited, while Share is better when data is big.\n", "paperhash": "chen|share_or_split_which_is_more_efficient", "_bibtex": "@misc{\n  chen2018share,\n  title={Share or Split : which is more efficient?},\n  author={Yinpeng Chen and Zicheng Liu and Lijuan Wang and Zhengyou Zhang},\n  year={2018},\n  url={https://openreview.net/forum?id=HJ0toxywG}\n}", "authorids": ["yiche@microsoft.com", "zliu@microsoft.com", "lijuanw@microsoft.com", "zhang@microsoft.com"], "authors": ["Yinpeng Chen", "Zicheng Liu", "Lijuan Wang", "Zhengyou Zhang"], "keywords": [], "pdf": "/pdf/67a25aea4d868bec5d9c82cab81f9c0867547462.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}