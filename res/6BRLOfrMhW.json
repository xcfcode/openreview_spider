{"notes": [{"id": "6BRLOfrMhW", "original": "8ObDbp2iLHJ", "number": 696, "cdate": 1601308082278, "ddate": null, "tcdate": 1601308082278, "tmdate": 1615801250014, "tddate": null, "forum": "6BRLOfrMhW", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Partitioned Learned Bloom Filters", "authorids": ["~Kapil_Vaidya1", "eric_knorr@g.harvard.edu", "~Michael_Mitzenmacher1", "~Tim_Kraska1"], "authors": ["Kapil Vaidya", "Eric Knorr", "Michael Mitzenmacher", "Tim Kraska"], "keywords": ["optimization", "data structures", "algorithms", "theory", "learned algorithms"], "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vaidya|partitioned_learned_bloom_filters", "pdf": "/pdf/9d3c15624a3da1883d53dc7d7e286e835c51a105.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvaidya2021partitioned,\ntitle={Partitioned Learned Bloom Filters},\nauthor={Kapil Vaidya and Eric Knorr and Michael Mitzenmacher and Tim Kraska},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6BRLOfrMhW}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "stOwmVYw8f5", "original": null, "number": 1, "cdate": 1610040491148, "ddate": null, "tcdate": 1610040491148, "tmdate": 1610474097049, "tddate": null, "forum": "6BRLOfrMhW", "replyto": "6BRLOfrMhW", "invitation": "ICLR.cc/2021/Conference/Paper696/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "All of the reviewers thought that this paper addresses an interesting and important problem.  Several of the reviewers thought that the paper gave a creative approach for training bloom filters and this would be of interest to the community. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Partitioned Learned Bloom Filters", "authorids": ["~Kapil_Vaidya1", "eric_knorr@g.harvard.edu", "~Michael_Mitzenmacher1", "~Tim_Kraska1"], "authors": ["Kapil Vaidya", "Eric Knorr", "Michael Mitzenmacher", "Tim Kraska"], "keywords": ["optimization", "data structures", "algorithms", "theory", "learned algorithms"], "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vaidya|partitioned_learned_bloom_filters", "pdf": "/pdf/9d3c15624a3da1883d53dc7d7e286e835c51a105.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvaidya2021partitioned,\ntitle={Partitioned Learned Bloom Filters},\nauthor={Kapil Vaidya and Eric Knorr and Michael Mitzenmacher and Tim Kraska},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6BRLOfrMhW}\n}"}, "tags": [], "invitation": {"reply": {"forum": "6BRLOfrMhW", "replyto": "6BRLOfrMhW", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040491134, "tmdate": 1610474097033, "id": "ICLR.cc/2021/Conference/Paper696/-/Decision"}}}, {"id": "JZ9kl8xb-mr", "original": null, "number": 3, "cdate": 1604295378946, "ddate": null, "tcdate": 1604295378946, "tmdate": 1606757563067, "tddate": null, "forum": "6BRLOfrMhW", "replyto": "6BRLOfrMhW", "invitation": "ICLR.cc/2021/Conference/Paper696/-/Official_Review", "content": {"title": "A somewhat descriptive scheme for constructing memory-efficient partitioned bloom filters incorporating a learned model. ", "review": "A clear exposition of the problem and proposed solution, the paper key strength is in the formulation of the partitioned bloom filter as an optimization problem that generalizes previously proposed architectures, and prescribes an interpretable solution for the choice of the optimal partition-thresholds in terms of the properties of the given learned model (specifically, its false-negative and true-negative threshold dependent curves). Furthermore, the experimental section clearly illustrates a significant advantage of the proposed method over state of the art alternatives. \nThe theoretical treatment, however may be considered as a first attempt at combining essential properties of a learned model and bloom-filter design - the paper could be improved by considering and motivating the usage of a particular lower bound for the space cost of a bloom filter and incorporating in the the optimization formulation the relation between the size of the learned model and the qualities of the false positive and false negative curves. The latter, especially, makes a significant practical difference since the model may be part of the design and assuming out the size-quality tradeoff results in a sub-optimal scheme and renders the overall proposed solution as a heuristic still.\n\nUpdate (Nov 30th) In light of the author's responses and the other reviews I increase my score for this paper to 7: Good paper, accept.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper696/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Partitioned Learned Bloom Filters", "authorids": ["~Kapil_Vaidya1", "eric_knorr@g.harvard.edu", "~Michael_Mitzenmacher1", "~Tim_Kraska1"], "authors": ["Kapil Vaidya", "Eric Knorr", "Michael Mitzenmacher", "Tim Kraska"], "keywords": ["optimization", "data structures", "algorithms", "theory", "learned algorithms"], "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vaidya|partitioned_learned_bloom_filters", "pdf": "/pdf/9d3c15624a3da1883d53dc7d7e286e835c51a105.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvaidya2021partitioned,\ntitle={Partitioned Learned Bloom Filters},\nauthor={Kapil Vaidya and Eric Knorr and Michael Mitzenmacher and Tim Kraska},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6BRLOfrMhW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6BRLOfrMhW", "replyto": "6BRLOfrMhW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper696/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137173, "tmdate": 1606915797761, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper696/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper696/-/Official_Review"}}}, {"id": "SQDxMuUejI", "original": null, "number": 2, "cdate": 1603900594143, "ddate": null, "tcdate": 1603900594143, "tmdate": 1606312162379, "tddate": null, "forum": "6BRLOfrMhW", "replyto": "6BRLOfrMhW", "invitation": "ICLR.cc/2021/Conference/Paper696/-/Official_Review", "content": {"title": "Review", "review": "The paper proposes a generalization of the sandwiched Bloom filter model that maintais a set of score partitions instead of just two and an algorithm for optimizing parameters of the partition under the target false-positive rate. Authors evaluate partitioned Bloom filter on three datasets and demonstrate that delivers better false positive rates under for a given model size compared to the baselines.\n\nI find the paper quite innovative and the experimental results impressive. My main concern is regarding the paper clarity.\n1. How can a region's FPR $f_i$ be greater than 1? Perhaps, this is something very obvious, but I couldn't get this immediately and perhaps some other readers might  struggle here too.\n2. The learned model size appears in the optimization objective, but is considered given. I wonder what will change if we allow to trade-off the learned model power for a larger number of regions and larger backup Bloom filters in each region? Does it even make sense to ask such a question? Would the results change if a different model is used? I think it is possible that for a given variant of a learned Bloom filter, a different model may result into different values of optimal parameters and a difference performance, thus these should ideally be optimized independently for each of the baselines. I also think that size of the pickle file is arguably not the best estimate for the learned model size if indeed a different model is used for each of the filters, e.g. a neural network might admit a decent compression rate if a lower precision number format is used etc. Thus, it is important to separate the impact made by a learned model from an algorithmic improvement.\n3. Is there any variance caused by observing particular distributions G and H? Is it small enough to ignore or confidence interals for each of the curves might actually overlap? I would also be interested in understanding behaviour of all considered models as the sample size changes.\n\nI also feel like authors can cite *Meta-learning neural Bloom filters. Rae et al, 2019* as it considers a relevant (although a different as well) setting.\n\nNevertheless, I think the kind of analysis presented in the paper very useful for the community and for further development of learned data structures. I recommend acceptance and I will gladly raise my rather conservative score if authors could clarify the points mentioned above.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper696/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Partitioned Learned Bloom Filters", "authorids": ["~Kapil_Vaidya1", "eric_knorr@g.harvard.edu", "~Michael_Mitzenmacher1", "~Tim_Kraska1"], "authors": ["Kapil Vaidya", "Eric Knorr", "Michael Mitzenmacher", "Tim Kraska"], "keywords": ["optimization", "data structures", "algorithms", "theory", "learned algorithms"], "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vaidya|partitioned_learned_bloom_filters", "pdf": "/pdf/9d3c15624a3da1883d53dc7d7e286e835c51a105.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvaidya2021partitioned,\ntitle={Partitioned Learned Bloom Filters},\nauthor={Kapil Vaidya and Eric Knorr and Michael Mitzenmacher and Tim Kraska},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6BRLOfrMhW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6BRLOfrMhW", "replyto": "6BRLOfrMhW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper696/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137173, "tmdate": 1606915797761, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper696/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper696/-/Official_Review"}}}, {"id": "RfOeXKErb8r", "original": null, "number": 6, "cdate": 1605641275428, "ddate": null, "tcdate": 1605641275428, "tmdate": 1605641981493, "tddate": null, "forum": "6BRLOfrMhW", "replyto": "-e0h7pw3BP2", "invitation": "ICLR.cc/2021/Conference/Paper696/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for their comments.\n\n1) We first note that, if insertions are an issue, they sometimes can be dealt with without significant changes.  For example, if the number of insertions are small, one can insert new set items by adding to the corresponding Bloom filter while only slightly increasing the false positive rate.  Alternatively, there is another paper which deals with the issue of re-training models during insertions that provides additional insight: Adaptive Learned Bloom Filters under Incremental Workloads. If we keep the optimal parameters fixed, the backup Bloom filters may need to be resized as keys are inserted in them. When inserting keys, though the number of resizes of the backup Bloom filters would be more but the time would be similar to resizing a standard Bloom filter.  The reviewer may be correct that one might need to resize Bloom filters slightly more often than a single Bloom filter, because of variability in where insertions occur.  (Do note though that the insertions may be spread among the multiple backup Bloom filters, so it need not be dramatically different.)  However, if one resizes individual Bloom filters, then the time to resize will be smaller, as each individual Bloom filter is smaller than a single large Bloom filter.\n\n2) The construction time of PLBF is more than the standard Bloom filter owing to the extra steps of model construction and the optimal parameter computation. The model construction time is a common step for all the learned techniques but the parameter computation time varies across them.\n We have added experiments in Appendix E 3-4, that show the initial Bloom filter construction time for various baselines and the effect of the level of discretization(N) on the optimisation overhead and PLBF size.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper696/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Partitioned Learned Bloom Filters", "authorids": ["~Kapil_Vaidya1", "eric_knorr@g.harvard.edu", "~Michael_Mitzenmacher1", "~Tim_Kraska1"], "authors": ["Kapil Vaidya", "Eric Knorr", "Michael Mitzenmacher", "Tim Kraska"], "keywords": ["optimization", "data structures", "algorithms", "theory", "learned algorithms"], "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vaidya|partitioned_learned_bloom_filters", "pdf": "/pdf/9d3c15624a3da1883d53dc7d7e286e835c51a105.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvaidya2021partitioned,\ntitle={Partitioned Learned Bloom Filters},\nauthor={Kapil Vaidya and Eric Knorr and Michael Mitzenmacher and Tim Kraska},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6BRLOfrMhW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6BRLOfrMhW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper696/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper696/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper696/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper696/Authors|ICLR.cc/2021/Conference/Paper696/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868189, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper696/-/Official_Comment"}}}, {"id": "r58HeAu1fF-", "original": null, "number": 5, "cdate": 1605641020723, "ddate": null, "tcdate": 1605641020723, "tmdate": 1605641747869, "tddate": null, "forum": "6BRLOfrMhW", "replyto": "SQDxMuUejI", "invitation": "ICLR.cc/2021/Conference/Paper696/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We would like to thank the reviewer for their insightful comments.  In particular, we are confident that we can address all the issues raised.\n\n1) As noted by the reviewer, the false positive rate cannot be greater than 1 for any region, and this constraint is added in our optimization problem as shown in Eq(4). In order to solve the optimization problem, we relax the constraint to allow that the fpr variables can be larger than 1. This might lead to a solution to the mathematical optimization problem where fpr >1 for some regions, which then needs to be corrected, by setting the fpr for that region to 1 and re-optimizing, as we explain in Section 3.3.3. \n\n2) We solve the problem assuming we have been provided a model already trained on the data. As you correctly pointed out, there exists a trade-off between the size of the model and the algorithmic improvement it provides. The best performance of the learned model is proportional to KL divergence between G and H. With increasing model size, this quantity improves leading to smaller backup Bloom filters, which can create a trade off between the model size and the backup Bloom filter size.  As noted in another review comment, if we had some meta-model that could relate the model size and the model accuracy, we could include that into our optimization design, but we are unaware of such meta-models.  \nWe used the uncompressed pickle file size for the experiments as it represents the size of the random forest model accurately, which is what we used in our experiments. We agree for other models one would use a size metric that represented that model.  \n\n3) Since the key set is fixed, the number of keys in each region remains the same. We estimate the H distribution using a sample of non-keys leading to variation between real and estimated H. The estimation is robust for larger sample sizes.  (This is discussed in previous work, e.g., in Theorem 4 of [A Model for Learned Bloom Filters, and Optimizing by Sandwiching])  \n\nWe have added Meta-learning neural Bloom filters. Rae et al, 2019 as a related work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper696/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Partitioned Learned Bloom Filters", "authorids": ["~Kapil_Vaidya1", "eric_knorr@g.harvard.edu", "~Michael_Mitzenmacher1", "~Tim_Kraska1"], "authors": ["Kapil Vaidya", "Eric Knorr", "Michael Mitzenmacher", "Tim Kraska"], "keywords": ["optimization", "data structures", "algorithms", "theory", "learned algorithms"], "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vaidya|partitioned_learned_bloom_filters", "pdf": "/pdf/9d3c15624a3da1883d53dc7d7e286e835c51a105.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvaidya2021partitioned,\ntitle={Partitioned Learned Bloom Filters},\nauthor={Kapil Vaidya and Eric Knorr and Michael Mitzenmacher and Tim Kraska},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6BRLOfrMhW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6BRLOfrMhW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper696/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper696/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper696/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper696/Authors|ICLR.cc/2021/Conference/Paper696/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868189, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper696/-/Official_Comment"}}}, {"id": "7NZ4G1E-1dZ", "original": null, "number": 4, "cdate": 1605640866990, "ddate": null, "tcdate": 1605640866990, "tmdate": 1605640866990, "tddate": null, "forum": "6BRLOfrMhW", "replyto": "JZ9kl8xb-mr", "invitation": "ICLR.cc/2021/Conference/Paper696/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the comments.  The reviewer suggests that the current analysis takes the model as a given where it could be part of the design.  This would be the case if we had some meta-model that could relate the model size and the model accuracy;  if we had such a meta-model, we could include that into our optimization design.  However, we are not aware of any suitable model for model size vs. accuracy in these domains.  We therefore agree with the reviewer that this would be an interesting direction for future work.  [However, we expect in most situations this will be a lower order effect, in that the model must be sufficiently large to be reasonably accurate, but will at some point have diminishing returns.]  Regardless of whether this can be incorporated into the design phase, we believe the optimization framework we have presented shows clear payoffs over previous works, and in particular shows how to obtain near-optimal performance for any given model. "}, "signatures": ["ICLR.cc/2021/Conference/Paper696/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Partitioned Learned Bloom Filters", "authorids": ["~Kapil_Vaidya1", "eric_knorr@g.harvard.edu", "~Michael_Mitzenmacher1", "~Tim_Kraska1"], "authors": ["Kapil Vaidya", "Eric Knorr", "Michael Mitzenmacher", "Tim Kraska"], "keywords": ["optimization", "data structures", "algorithms", "theory", "learned algorithms"], "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vaidya|partitioned_learned_bloom_filters", "pdf": "/pdf/9d3c15624a3da1883d53dc7d7e286e835c51a105.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvaidya2021partitioned,\ntitle={Partitioned Learned Bloom Filters},\nauthor={Kapil Vaidya and Eric Knorr and Michael Mitzenmacher and Tim Kraska},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6BRLOfrMhW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "6BRLOfrMhW", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper696/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper696/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper696/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper696/Authors|ICLR.cc/2021/Conference/Paper696/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868189, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper696/-/Official_Comment"}}}, {"id": "-e0h7pw3BP2", "original": null, "number": 1, "cdate": 1603833189748, "ddate": null, "tcdate": 1603833189748, "tmdate": 1605024628044, "tddate": null, "forum": "6BRLOfrMhW", "replyto": "6BRLOfrMhW", "invitation": "ICLR.cc/2021/Conference/Paper696/-/Official_Review", "content": {"title": "Fine tuning partitioned learned Bloom filter", "review": "\nThis work proposed a technique to fine tune the partitioned learned Bloom filter to reduce the space consumption given a false positive rate threshold.\n\nThe idea is to formulate the problem into a two-part optimization problem: How to best partition the scores from the model into a given number of regions and how to choose thresholds for the regions to minimize the overall space consumption of Bloom filters. A relaxed version of the latter problem is addressed by using KKT conditions to obtain the optimal thresholds. The former problem is addressed by discretizing the region boundaries and dynamic programming.\n\nOverall, I like the idea of fine tuning the partitioned learned Bloom filter. It seems to me that it will also be possible to fine tune the number of partitions as well, e.g., a simple way is to use a binary search. The evaluation result is impressive compared with baselines in terms of space consumption vs. false positive rate. And the writing of the paper is clear and easy to follow.\n\nHaving said that, I have some concerns with this line of work.\n\nFirst, IMHO, the real challenge of putting these learned Bloom filters into work is how to maintain them under insertions. While it is OK that this is not the focus of this work, it seems to me that the proposed optimal learned Bloom filter can be brittle under insertions. It will be great to understand how the proposed technique degrades compared with the baselines. In addition, with partitioned Bloom filters, it seems to be more prone to resizing upon insertions compared with using a single Bloom filter. \n\nSecond, it is possible that the proposed technique needs to be reoptimized upon insertions. In this case, it will be important to understand the overhead of constructing the Bloom filters proposed in this technique compared with the baselines. However, the overhead of constructing the various variants of Bloom filters is missing in the evaluation. It might be possible to reduce the overhead by using a coarser grained discretization for the DP. The performance, however, can degrade with a coarser grained discretization.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper696/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper696/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Partitioned Learned Bloom Filters", "authorids": ["~Kapil_Vaidya1", "eric_knorr@g.harvard.edu", "~Michael_Mitzenmacher1", "~Tim_Kraska1"], "authors": ["Kapil Vaidya", "Eric Knorr", "Michael Mitzenmacher", "Tim Kraska"], "keywords": ["optimization", "data structures", "algorithms", "theory", "learned algorithms"], "abstract": "Bloom filters are space-efficient probabilistic data structures that are used to test whether an element is a member of a set, and may return false positives.  Recently, variations referred to as learned Bloom filters were developed that can provide improved performance in terms of the rate of false positives, by using a learned model for the represented set.  However, previous methods for learned Bloom filters do not take full advantage of the learned model.  Here we show how to frame the problem of optimal model utilization as an optimization problem, and using our framework derive algorithms that can achieve near-optimal performance in many cases.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vaidya|partitioned_learned_bloom_filters", "pdf": "/pdf/9d3c15624a3da1883d53dc7d7e286e835c51a105.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvaidya2021partitioned,\ntitle={Partitioned Learned Bloom Filters},\nauthor={Kapil Vaidya and Eric Knorr and Michael Mitzenmacher and Tim Kraska},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=6BRLOfrMhW}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "6BRLOfrMhW", "replyto": "6BRLOfrMhW", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper696/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137173, "tmdate": 1606915797761, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper696/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper696/-/Official_Review"}}}], "count": 8}