{"notes": [{"id": "01olnfLIbD", "original": "Xzvod_oeWVz", "number": 3659, "cdate": 1601308407164, "ddate": null, "tcdate": 1601308407164, "tmdate": 1614594357274, "tddate": null, "forum": "01olnfLIbD", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "3lGdK3neDyH", "original": null, "number": 1, "cdate": 1614593671480, "ddate": null, "tcdate": 1614593671480, "tmdate": 1614593671480, "tddate": null, "forum": "01olnfLIbD", "replyto": "JPxZUZn7-pK", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Comment", "content": {"title": "Final Version uploaded", "comment": "Dear Program Chairs,\n\nwe have uploaded a final version of this work, including additional experiments and discussion regarding multiple targets (See appendix F.8), a table showing clean validation accuracy (appendix F.9), and textual improvements to introduction and related work in direct response to many helpful suggestions from the reviewers - especially focusing on providing an improved understanding of the discussed attack within a wider taxonomy of data poisoning attacks.\n\nThe implementation provided as supplementary material is maintained at https://github.com/JonasGeiping/poisoning-gradient-matching."}, "signatures": ["~Jonas_Geiping1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Jonas_Geiping1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"forum": "01olnfLIbD", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649468596, "tmdate": 1610649468596, "id": "ICLR.cc/2021/Conference/Paper3659/-/Comment"}}}, {"id": "JPxZUZn7-pK", "original": null, "number": 1, "cdate": 1610040512746, "ddate": null, "tcdate": 1610040512746, "tmdate": 1610474120726, "tddate": null, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper presents a scalable data poisoning algorithm for targeted attacks, using the idea of designing poisoning patterns which \"align\" the gradients of the real objective and the adversarial objective. This intuition is supported by theoretical results, and the paper presents convincing experimental results about the effectiveness of the model.\n\nThe reviewers overall liked the paper. However, they requested a number of clarifications and some additional work, which should be incorporated in the final version (however, the authors are not required to use the wording as poison integrity/ poison availability). In particular, it would be great to see the experiment the authors suggested in their response to Reviewer 2 about the effectiveness of their method for multiple targets (this is important to better understand the limitations of the proposed approach)."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"forum": "01olnfLIbD", "replyto": "01olnfLIbD", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040512733, "tmdate": 1610474120710, "id": "ICLR.cc/2021/Conference/Paper3659/-/Decision"}}}, {"id": "vhPesayC7X9", "original": null, "number": 3, "cdate": 1603999085624, "ddate": null, "tcdate": 1603999085624, "tmdate": 1606772658103, "tddate": null, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Review", "content": {"title": "Blind review", "review": "## Summary\n- The paper proposes a novel data poisoning attack i.e., to perturb a small fraction of images in the victim's training dataset so as to cause targeted misclassification on certain examples at inference time.\n- The proposed approach works by perturbing the clean poison set to introduce a gradient direction which mimics the victim training their model on a targeted mislabeled set.\n- Experiments on CIFAR10 and ImageNet demonstrate that the model outperforms competing approaches.\n\n---\n\n## Strengths\n\n**1. Attack insight**\n- I appreciate the insight used to craft the perturbations for the poisoned instances. It seems reasonable to me to exploit the fact that the poisoned instances are used for training using a gradient-descent approach; using the gradient information as a yardstick to craft the perturbations is a nice insight that the paper leverages.\n\n**2. Thorough evaluation**\n- I am impressed by the thoroughness in the evaluation. The authors extensively evaluate numerous factors influencing the performances (e.g., size of ensembles, no. of restarts), compare with recent baselines, etc.\n- To add to it, the authors further evaluate on Imagenet and achieve strong results.\n\n**3. Writing**\n- I enjoyed the writing in the paper. I found the presentation clear and easy to follow.\n\n---\n\n## Concerns\n\n### Major Concerns\n\n**1. Attack assumes access to exact training data**\n- If I understand the approach correctly, it assumes access to the exact dataset used by the victim to train the model? Isn't this a really strong assumption?\n- Because if this is the case in the threat scenario, couldn't the attacker simply poison the entire dataset?\n- As a result, I wonder whether the attack also extends to the more interesting and practical case where the adversary has limited access to the victim's training set.\n\n**2. From scratch**\n- At many times in the paper, the authors remark that the attack works in spite of the targeted model being trained from scratch from an unknown initialization.\n- However, I would suspect that it is easier to tailor the poisoned instances with access to a strong gradient signal, such as early on during training. Are the authors aware whether the approach is robust to victim models that has been pretrained?\n\n### Minor Concerns\n\n**3. Test accuracies**\n- Could the authors comment on the difference in victim's test-set accuracy training with the clean and poisoned training set? I found this largely missing, since the focus primarily seems to be on the accuracy on the target set.\n- Because a minor concern I have is that the victim model might be overfitting to the poisoned instances by trading off test-set accuracy. It would be nice to know how severe this is.\n\n**4. \"single differentiation\"**\n- The authors claim that the attack requires only a \"single differentiation\". But doesn't the model have to be twice differentiable ($\\nabla_x \\nabla_\\theta \\mathcal{L}(\\cdot)$) to perform the updates?\n\n\n### Nitpicks\n\n**5. Strong focus on MetaPoison**\n- The paper makes many head-to-head comparisons with MetaPoison. I'm not sure why, since MetaPoison doesn't seem too closely-related. Especially in S5.2, it appears that it is singled out to demonstrate the computational overhead. This is understandable since it's relies on a meta-learning approach.\n\n**6. Writing**\n- Fig 4b is unreadable -- I recommend resizing the figure.\n- It seems surprising that the related work section claims that poisoning attacks, unlike backdoor attacks, do not require access to test data. As I'm aware, both require the same access to test-set -- specifically that a particular test instance is presented at inference time to cause misclassification. In fact I would think backdoors are more generalizable here since any test instance can be watermarked to introduce misclassification, unlike pre-specified instances in the case of poisoning.\n\n### Post-rebuttal update\nI thank the authors for their response -- this helps. Having read the other reviews, I am still leaning towards acceptance.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071910, "tmdate": 1606915803700, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3659/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Review"}}}, {"id": "5u-8sh4dQh3", "original": null, "number": 11, "cdate": 1606303175947, "ddate": null, "tcdate": 1606303175947, "tmdate": 1606303175947, "tddate": null, "forum": "01olnfLIbD", "replyto": "nHg4LNDJata", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "Final remarks", "comment": "Thanks for the prompt reply. Again, I think we substantially reached a good level of agreement. \n\n* I know that both integrity and availability poisoning attacks can be casted as a bilevel optimization. It only changes which points you select for the outer loss. However, this was not the main point of my review.\n\nI only think that the authors should work towards improving clarity. As they also recognize, the literature is quite confusing, and we have an opportunity for clarification. I suggest the authors to:\n\n* Clarify and narrow the scope of their attack immediately from the title and abstract. State that you are considering clean-label targeted/integrity attacks from the very beginning; and remove all the claims of large computational complexity which would better refer to availability attacks (e.g., in the abstract: \"Previous poisoning attacks ... being prohibitively expensive for large datasets\" - e.g., this is not the case for metapoisoning or poison frogs, which are your direct competitors).\n\n* Clarify your threat model in context. I think that the definition in 3.1 is fairly clear, even though some choices should be better motivated. In which practical cases the gray-box assumption that the architecture is known to the attacker makes sense? This should be motivated, and overall, it should be clarified how this attack makes different assumptions from other clean-label or backdoor poisoning attacks, if any, and why. \n\n* State that this attack may be also used in the context of poisoning availability attacks in future work (I fully agree with the last point of your answer).\n\nOverall, I really like to thank you for the fruitful discussions, and hope that my feedback can be useful to improve your work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "nHg4LNDJata", "original": null, "number": 10, "cdate": 1606302121958, "ddate": null, "tcdate": 1606302121958, "tmdate": 1606302121958, "tddate": null, "forum": "01olnfLIbD", "replyto": "8S8UNSdyvc3", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "Answer to Final Comments - Both availability and integrity attacks are bilevel optimization problems", "comment": "First off, we're very grateful for both your perspective and the substantial interest in clarifying this work - really.\nWe will use this discussion to clarify our writing and improve our presentation of the taxonomy of poisoning attacks, and better differentiate the subfield of clean-label targeted poisoning attacks from other attack scenarios.\n\n\n**Some additional comments regarding points raised in the final comments:**\n* While we like the distinction between backdoor attacks and clean-label attacks proposed in this review, the general literature is sadly much less precise. Other works (and also we in the current version) use \"backdoor\" as synonym of \"poison integrity\". Under this definition our attack is also a backdoor attack. Our related work section differentiates backdoor - \"poisoning attacks\" like ours from \"backdoor trigger attacks\" (such as Saha2019)  based on the criterion that \"trigger\" attacks are allowed to modify both training and testing images [but not labels], whereas \"poisoning attacks\" may only modify images from the training set [but not labels]. \n\nAn orthogonal direction to this would be whether the attacker is allowed to modify not only the training set, but also provide a pretrained, but backdoored model, which implies partial control over the (pre)-training phase or control over access to training data. So clean-label attacks are backdoor attacks, depending on the definition.\nOur threat model in Sec. 3.1. defines the clean-label targeted-poisoning setting that we consider unambigously.\n\n* The defender in the \"clean-label\" setting (as defined in your second paragraph) is indeed allowed control over training data, but the crucial challenge for the defender is that there is no additional \"clean training data\" available. This makes defenses difficult as the defender has to assume that any data used for comparison has also been poisoned. As such, many defenses that rely on classification of poisoned and non-poisoned data fail because they have no basis of comparison. Only defenses based on unsupervised anomaly detection can still work in this setting - however many of these defenses are based on anomaly detection in feature space. Yet, we show that the feature space is non-anomalous after our attack (in Fig. 4a).\n\n* Important: Both \"poison availability\" and \"poison integrity\"/\"targeted poisoning\" are formally bilevel optimization problems. In both cases, the lower-level problem is the training of a model parametrized by theta with respect to some data x, whereas the higher-level problem is either to maximize the loss over held-out data in the case of poison availability, or to minimize the loss of some specific held-out target image in the case of targeted poisoning. For both, the higher level problem depends on model parameters theta which themselves depend on x, so that the overall objective can be optimized w.r.t to x.\nHowever in practice this objective is infeasible to solve and all attack schemes have consider heuristic or approximative solutions in some shape or form.\n\n* Poison Frogs is also a clean-label targeted attack, the only difference between our threat model and the threat model in Shafahi2018 is that there the feature extractor is kept fixed (and known to be fixed by both the attacker and the defender). MetaPoison is also a clean-label targeted attack, and it considers the same threat model that we consider (where the feature extractor is allowed to change and unknown to the attacker).\n\n* We will work on clarifying our threat model textually. As far as we understand currently though, the definition in 3.1 is unambiguous - if this is not the case, we would be very grateful for information where ambiguity remains.\n\n* We provide a complexity/effectiveness plot in Fig.10 which also includes poison frogs. Poison frogs has a similar complexity to our attack - however the attack fails in the from-scratch scenario, possibly because it does not approximat the bilevel objective well enough. In contrast, MetaPoison manages to approximate the objective better and leads to stronger attacks, but this comes at a signififcant computational complexity. Our attack is the first to attack the difficult scenario of from-scratch training with a method that is roughly as complex as poison frogs, but even stronger than MetaPoison. We will add additional clarification regarding this. \n\n* Note that we never compare to poison availability attacks in this work and do not run experiments in this setting. It is conceivable that the proposed gradient matching is also able to work in the poison availability setting (this would require the replacement of the currently considered target gradients with a negative gradient sample from the validation set), but we think this is a significantly different scenario that is better suited for future work.\n\n\nAgain we're glad to have this discussion and will revise and clarify our work accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "Jyx88gL6pci", "original": null, "number": 9, "cdate": 1606296254510, "ddate": null, "tcdate": 1606296254510, "tmdate": 1606296254510, "tddate": null, "forum": "01olnfLIbD", "replyto": "cN9U40kfsM", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "Response to authors' rebuttal", "comment": "Please refer to the updated review. Comments can be found at the end."}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "8S8UNSdyvc3", "original": null, "number": 4, "cdate": 1604597206378, "ddate": null, "tcdate": 1604597206378, "tmdate": 1606296181839, "tddate": null, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Review", "content": {"title": "Review for Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "review": "Summary: This paper introduces a novel targeted clean-label poisoning attack, expected to be more efficient and scalable than current ones. The attack is formulated as a bilevel problem which is then solved with a (fast) heuristic approach based on aligning the gradients of the inner and outer objective functions. A theoretical analysis is also reported to show that this strategy consistently finds a descent direction for the outer objective, asymptotically converging to (a local) minimum. \n\nFirst of all, I like how the authors have derived their attack and its heuristic solution, and I'm wondering if this generalizes to other applications where bilevel problems are at the core, like meta learning or hyperparameter optimization. However, I have several concerns on the presentation and soundness of the reported results.\n\nFirst, I think that this paper makes confusion (at least in the reader's mind) when introducing the data poisoning problem. At the beginning, there is no clear distinction between the two main goals of data poisoning:\n(1) poisoning availability attacks, which aim to increase the test error causing a denial of service, and\n(2) poisoning integrity attacks (which are often referred to -in a misleading manner- as targeted attacks), which aim to allow specific intrusions/attacks at test time (backdoor attacks belong to this category). \nFor a clearer nomenclature/definition see: M. Barreno, B. Nelson, A. Joseph, and J. Tygar. The security of machine learning. Machine Learning, 81:121\u2013148, 2010. \nThere, indeed, targeted/untargeted is referred to the victim user, not to the goal/security violation caused by the attack.\n\nWhile this work seems to claim that, in general, poisoning attacks are computationally demanding, a distinction should be made. While poisoning availability attacks are typically much more computationally demanding (they do require solving the bilevel optimization problem to work well) and this heavily hinders their scalability to large datasets, poisoning integrity attacks can be quite efficient (there's no need to solve a bilevel problem for them to work well, as in the case of most of the considered competing attacks in this paper, or anyway the approach can be simplified - see, e.g., Koh et al., ICML 2017, where the whole network was frozen and the bilevel problem was solved only by assuming that the parameters of the last layer were updated).\nI believe that this aspect should be clarified from the beginning. First, this paper focuses on *targeted* (or integrity) poisoning attacks, and this should also be clear from the title. Second, sentences related to the overwhelming complexity of targeted/integrity poisoning attacks should be revised (e.g., Koh et al., ICML 2017 also worked on the DogFish data which should be a subset of ImageNet, if I'm not mistaken). \n\n\nAnother important issue which I do not completely understand is what the authors mean with the word \"from scratch\", from the viewpoint of previous attacks. I agree with them that previous attacks designed to work on pre-trained models with fine tuning may not work against models which are trained from scratch, but what prevents the attacker to train a model from scratch on the clean data and then design their attack samples with fine tuning? The attack samples can then be added to the initial training set to see if, when learning again from scratch on the poisoned data, the attack remains effective or not. Is this the setup that the authors have considered in their paper for such attacks, or they run them against an \"untrained\" (or not fully trained) model? If we are in the second scenario, I don't think the analysis reported should be considered fair enough.\n\n\nThere are parts in the paper where it is claimed that 'clean-label' attacks are in some sense better than label flips or attacks that do not preserve image semantics. Why? Are we expecting human labelers to check the quality of our data?\nOr are we expecting that clean-label attacks are harder to spot?\nBoth questions are unaddressed in this paper.\nFirst, I don't think that in many realistic scenarios humans are expected to cleanup the whole training set, especially when it contains a lot of samples. Second, it's also true that the level of noise used in this paper is not so small. By zooming in Figs 6-8, the perturbation becomes quite visible even to the human eye.\nHence, we cannot only instruct humans to detect these patterns, but we can probably train detectors to do that automatically. Accordingly, I don't see in which practical, relevant application scenarios \"clean-label\" can be retained useful as a requirement.\nFinally, even though the authors have analyzed the robustness of their attack against some defense mechanisms, the defenses considered aim to detect mostly poisoning availability attacks and NOT backdoor attacks or targeted/integrity poisoning.\nI am even skeptical that such methods can detect label flips or even other current attacks. Have the authors tested such defenses against the competing approaches (poisoning frogs, convex polytopes, etc.)? Do these attacks work or not against them?\nHow do detection methods for backdoor attacks work against the proposed attack? For a list of such detection methods, see, e.g., Table 1 in https://arxiv.org/abs/1910.03137 (note that some detection methods should work against clean-label attacks too, there's no need to put a trigger on the image).\n\n\nFinally, the experimental section is missing key information for reproducing the experiments. The parameters \\tau, R and M are given a value but not a definition. The figure with the average accuracy vs. time is missing a caption and a figure label. It is extremely unclear what this figure is showing as 1) the parameters are missing descriptions 2) the metric used for evaluating the figure is described nowhere in the paper. This problem also extends to tables 1, 2 and 3: a clear definition of the \"evaluation metric\" should be given. Are we interested more in preserving accuracy or in the attack success? How is poisoning success defined? (this might be explained in the supplementary material, but it is important for understanding the whole results). Why not including a plot with poisoning success vs. accuracy of the model?\n\nTo summarize, the paper is promising, but important details and clarifications are still needed. The experimental section and the way results are presented needs major improvement, as it is hard to tell if the attack is working and how efficient and effective it is from the data presented in this paper. \n\n\n\n** Minor comments: ** \n\n* The Poison Frogs attack is described in Section 2, marking as a drawback the fact that it only works with fine-tuning. It is not clear however why this is a limitation, as one could train the model with normal training and add the poisoned data in the last epochs. \n\n* In Algorithm 1, step 9: what is the update being performed? It seems to me that the pseudo-code does not capture the entire processing steps, hence making the whole work hard to reproduce.\n\n* Figure 2 shows gradient alignment along epochs (please report the axis labels), however it does not seem \"flat\" in the end, it is slightly decreasing. What happens if we increase the number of epochs? Will the alignment disappear?\n\n* Sometimes the reader's expertise is taken for granted (e.g. define \"unrolled gradient\"). This might make it difficult for the paper to reach a broader audience.\n\n* Eq. 1 shows no constraints on the data points staying in the feature space after perturbations. Is it considered during the experiments?\n\n* It is observed that VGG11 on CIFAR10 is less transferable, but it would be interesting to read a possible explanation for this phenomenon.\n\n* Equations should distinguish vectors from scalars to improve readability.\n\n* Figure 4 is unreadable as the text in labels and legends is too small.\n\n\n** Comments after reading the authors' rebuttal **\n\nI would like to thank the authors for their clarifications. The threat model is now clearer to me - and I think it deserves clarifications in the paper as well.\n\nFirst of all, as far as I understand now, there's a net distinction between backdoors and clean-label attacks. Backdoor attacks assume that the attacker controls the design phase and the training process, and releases a backdoored model (which then someone else re-uses possibly with fine tuning). Hence, defenses against backdoors aim to detect whether models have been backdoored or not, and it is reasonable to expect that the defender doesn't know the training data as well as other design choices (as the attacker released the model). In this setting, clean-label attacks do not make sense (as the attacker controls the training labels too).\n\nClean-label attacks assume a different setup. Here the attacker only injects poisoning samples into the training set but does neither control the training process nor the training labels. Hence, clean-label attacks make sense in this setting. However, it also makes sense that the defender knows the training data (as the defender is the one that trains the algorithm, and the purpose is to either detect and remove the poisoning points or reduce their influence over training) - and hence I'm expecting the authors to do consider previous defenses that assume knowledge of the training set in their work.\n\nTo summarize, I think that:\n\n(1) the authors should clarify in the title that they restrict themselves to clean-label integrity/targeted poisoning attacks.\n\n(2) the authors should clarify the threat model, and clearly distinguish poisoning availability attacks (bilevel data poisoning) vs poisoning integrity attacks. Furthermore, in the poisoning integrity/targeted family, backdoor and clean-label attacks should be distinguished and the threat models clarified (in particular, w.r.t assumptions on what the attacker/defender know and have access to).\n\n(3) the authors should revise their sentences on the complexity of data poisoning (previous clean-label targeted attacks like poison frogs are not as complex as bilevel data poisoning attacks). A fairer comparison in terms of complexity should also be considered - how faster is this new attack w.r.t. poison frogs and the other clean-label targeted attacks? (poisoning availability should not be considered here as the goal is different in that case).\n\n(4) In general, there is need to disambiguate clean-label targeted poisoning attacks from the rest, and better position this work in context. Reading the paper in its current form, it seems that the authors are also able to improve scalability of poisoning availability attacks whereas this is not the goal of this work. \n\nI'm willing to revise my score if the authors agree on making these clarifications in the paper, better highlighting the net contributions of their work and the proper context of competing approaches (which do not include backdoors and poisoning availability attacks).\n\n\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071910, "tmdate": 1606915803700, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3659/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Review"}}}, {"id": "qaG09SkNDSJ", "original": null, "number": 8, "cdate": 1605642901958, "ddate": null, "tcdate": 1605642901958, "tmdate": 1605642901958, "tddate": null, "forum": "01olnfLIbD", "replyto": "JcUtpsosZzr", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "My review comments are well addresed", "comment": "I thank the authors for providing additional numerical results and clarifying my questions. I have no further comments. Neat work!"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "cN9U40kfsM", "original": null, "number": 3, "cdate": 1605299541030, "ddate": null, "tcdate": 1605299541030, "tmdate": 1605303124580, "tddate": null, "forum": "01olnfLIbD", "replyto": "8S8UNSdyvc3", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "Answer to Reviewer 4", "comment": "**Different notions of data poisoning (Paragraph 3: \u201cFirst, \u2026 caused by the attack\u201d):** Thank you for your suggestions on nomenclature. We indeed refer to the definitions in Barreno et al, but follow the nomenclature used in recent  works of Shafahi et al and Huang et al.  We will make a point to clarify the notation and mention the poison integrity/ poison availability nomenclature..\n\n**Complexity in the setting of from-scratch victim training (Paragraph 4: \u201cWhile this work \u2026 not mistaken\u201d):** We will amend the sentence on the complexity of targeted attacks to specify the from-scratch setting. This is crucial because Koh et al. only consider a frozen feature extractor, simplifying the optimization problem significantly, compared to the \u201cfrom-scratch\u201d setting. Also, the DogFish dataset is simply 900 images of dogs and 900 of fish - this is not comparable to the >1,000,000 images in full ImageNet.\n\n**\u201cFrom-scratch\u201d setting (Paragraph 5: \u201cAnother important \u2026 fair enough\u201d):**\nThe attacker does not control the training routine of the victim. While the attacker can certainly use a pretrained network to craft poisons (as we do in our method), a victim will train a new network from a new random initialization - a setting where previous transfer based attacks fail - because they crucially rely on the feature extractor being fixed. These attacks break when the feature representation changes! We confirm the suspicion that these transfer based attacks do not succeed in the from-scratch setting in Table 2, where we replicate several previous methods, but train a victim from a new random initialization using the poisoned dataset.\n\n\n**\u201cClean-label\u201d description of attacks (Paragraph 6: \u201cThere are parts \u2026 trigger on the image\u201d):**\n* Clean-label attacks are more insidious, and realistic than label flipping attacks since clean-label attacks do not assume the attacker is also the labeler of the victim\u2019s data. Many industrial practitioners will simply collect unlabeled data from the internet, and label it themselves (or employ services like Amazon Turk). Therefore an attacker cannot rely upon being able to incorrectly label any specific image. Moreover, targeted attacks become trivial in the label-flipping regime since the attacker could simply introduce the mislabeled target image into the victim\u2019s training set. \n* Our attacks are still \u201cclean-label\u201d in the sense that these images are possibly noisy, but still undeniably images of e.g. dogs. Furthermore, the perturbations may be noticeable to a reader of the paper because we include the clean base images above, but an unwitting practitioner might not think twice about the poisoned images - the images pass a cursory glance from a human worker. Finally, we show our attack is successful for lower epsilon values, which are imperceptible perturbations. These can be found in fig. 3. \n* Designing a detector against adversarial attacks is actually surprisingly difficult, see (Carlini & Wagner, 2017 - cf. References section). That work discusses evasion attacks, however the same considerations hold in the targeted poisoning setting. We also test our attack against defenses that are meant to detect anomalies (see Fig 4).\n* We\u2019d also like to point out that the considered threat model (small epsilon, small budget, clean-label attacks) has been an active field of past research on poisoning attacks against deep neural networks. \n\n\n**Defensive strategies (Paragraph 6:)**\nThank you for pointing this out - yes, we only considered defenses that are known to work against targeted attacks and  we will clarify this in the revised version of our paper. For instance, the defense in Peri et al. successfully removes 100% of the poisons generated by poison frogs and convex polytopes. cf. Peri et al., and Hong et al. show that differential privacy reduces the effectiveness of Poison Frogs by 38.36%. Taking defenses that are specifically developed for targeted attacks on deep networks we compare to seems the most expressive numerical evaluation for our work. Furthermore, all but three of the defenses in Table 1 of https://arxiv.org/abs/1910.03137 require access to clean training data, an assumption that does not apply in our setting. These three remaining defenses (Tran et al., and Chen et al., Chen et al.) all rely upon the heuristic that poisons will be anomalous in feature space - an assumption we show does not apply for our attack. Furthermore, many of these defenses are in the setting of backdoor attacks, where a fixed, easily spotted patch is added to all poisons, not individually crafted perturbations as in our attack. \n\n**Hyperparameters and clean validation accuracy (Paragraph 7: \u201cFinally, the experimental \u2026 of the model\u201d)**\nNote that these parameters refer to variables introduced in Alg. 1. We will clarify this and backreference Alg. 1. Please see the above general comment regarding the definition of \u201cpoison success\u201d and the natural validation accuracy of the poisoned models. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "A389m8x3HYC", "original": null, "number": 7, "cdate": 1605302512686, "ddate": null, "tcdate": 1605302512686, "tmdate": 1605302512686, "tddate": null, "forum": "01olnfLIbD", "replyto": "Zy418x0vOS", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your support and sharing our enthusiasm about this work. We will gladly fix the typo in proof 1.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "JcUtpsosZzr", "original": null, "number": 6, "cdate": 1605302480538, "ddate": null, "tcdate": 1605302480538, "tmdate": 1605302480538, "tddate": null, "forum": "01olnfLIbD", "replyto": "38h-AaEjjBd", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "Answer to Reviewer 2", "comment": "**\u201cAlthough the attack model still requires knowing the network architecture (gray-box setting), the resulting poisoned datasets are more effective against different initializations, and some techniques (e.g. model ensemble, multiple restarts) are proposed to further boost the attack performance.\u201d**\nWhile the attack is most successful when the attacker has knowledge of the victim\u2019s architecture, we stress that this is not a requirement for a successful attack, as demonstrated in Table 3, with our Google Cloud AutoML results and Fig. 13. In Table 3 and Fig.13 we show that attacks directly transfer to other architectures. We also show that an ensemble of several architectures can attack any of the ensembled architectures and as such the attacker can ensemble common architectures. Lastly, for the google Cloud experiments the architecture is entirely unknown.\n\n**\u201cOne limitation from Appendix A.8 is that the proposal may not scale well to more than 1 target image, as indicated by the rapidly decreasing attack accuracy. It will be more meaningful to control the effective budget/target and check the resulting accuracy of different number of targets, in order to understand whether gradient matching is scalable to multiple-target setting.\u201d** \nWhile you are correct that the attack success decreases (in percentage) as we increase the number of targets, this is for a fixed budget. It would be interesting going forward to test the effect of scaling the budget with the number of targets. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "yu44fjLZro", "original": null, "number": 5, "cdate": 1605301921994, "ddate": null, "tcdate": 1605301921994, "tmdate": 1605301938817, "tddate": null, "forum": "01olnfLIbD", "replyto": "vhPesayC7X9", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "Answer to Reviewer 1", "comment": "**1. Attack assumes access to exact training data**\n\nThe assumption of knowledge of full training set is a white-box scenario in which the attack is most dangerous. However, we do conduct experiments where the poisons are created on a different dataset than the victim is trained on. We show these in appendix Table 7, where we find that poisons are still effective even if only a subset of CIFAR-10 is known to the attacker. Also note that the proposed attack only needs access to a pretrained model trained on a dataset similar to the victim dataset and access to only  the subset of data that is supposed to be poisoned - only these images are included in the gradient matching objective. Attacks where all data is modified are possible within our framework, this would correspond to a budget of 100%, and likely consider a smaller perturbation - but we did not focus on these attacks, because the attacker might only be able to modify a small subset of data. \n\n Attacks where all the data is modified are certainly possible, but we focus on the more strict threat model wherein an attacker might have knowledge about what training data a victim will use, but only be able to modify a small portion of this data.\n\n\n**2. From scratch**\nThe question is on what data would the victim pretrain? If the dataset is poisoned then even this pretraining would happen using poisoned data. Are you referring to a transfer learning setting like the one discussed in Shafahi et al? \nOr is the question whether the gradients should be aligned based on an early epoch? We include a comparison in Fig 12b, where we analyze the strength of the attack when using a pretrained model that is trained for fewer epochs - we find that using pretrained models from later epochs leads to more successful attacks. Note that although the gradient signal is small in magnitude, the magnitude is cancelled in the cosine similarity. We also analyzed the effects of the poisons at different stages of training, see Fig. 9.. We find that in general, the victim begins to misclassify the target image in the last 20 epochs, leading us to believe the poisoned gradient starts to take hold later in training. \n\n**3. Test accuracies**\nPlease refer to our general comment for information about clean test accuracies.\n\n**4. \"single differentiation\"**\nWe will clarify this statement - the statement is in the context of other bilevel methods, which need several evaluations of \u201cthe\u201d gradient $ \\nabla_x \\nabla_\\theta \\mathcal{L}(\\cdot)$ to take a single step - but indeed two backpropagations are necessary to compute an update to the poisoned data.\n\n**5. Strong Focus on Metapoison:** \nWe focus on MetaPoison because to our knowledge, this is the only other method that performs targeted, clean-label poisoning from scratch. We do agree though that the methods are quite different in their motivation/approaches. \n\n**6. Writing**\nThank you for the comments. We made some figures in-line/with double subfigures which may have made the legend hard to read without zooming in. We will expand these figures for the final version, and can post an enlarged version to the appendix in the meantime. \nAs for requiring access to the test set, there is a subtle, but important difference in the attacker assumptions. For poisoning attacks like ours, we only require the attacker to have picked out a specific target instance that they wish to have the victim misclassify. We do not assume the attacker can modify this target image, as is the case with backdoor attacks. One could imagine this distinction being important if the attacker wishes to poison a facial recognition system where the target is an unsuspecting third party, the attacker will not be able to add perturbations to the images of this party\u2019s face. Also, if the target is entirely unmodified, then there is no chance for a defender to sanitize the target image at test-time, as is possible for backdoor attacks (see e.g. \u201cWang et al. Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks\u201d - a test-time defense against trigger patches).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "SfrDchiFuvp", "original": null, "number": 4, "cdate": 1605301104611, "ddate": null, "tcdate": 1605301104611, "tmdate": 1605301104611, "tddate": null, "forum": "01olnfLIbD", "replyto": "8S8UNSdyvc3", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "Answer to Reviewer 4 - Minor Comments", "comment": "* \u201cThe Poison Frogs attack is described in Section 2, marking as a drawback the fact that it only works with fine-tuning. It is not clear however why this is a limitation, as one could train the model with normal training and add the poisoned data in the last epochs.\u201d\nThe attacker does not control how the victim trains their model. Poison Frogs works in the setting wherein the victim is using a transfer learning/fine-tuning training strategy, but not in the setting wherein a victim trains a randomly initialized model from scratch. Your proposed strategy is therefore not possible, firstly because it requires the attacker to gain knowledge of the new feature representation of the victim, and secondly because it requires the attacker to insert poisons in the middle of training. The attacker has no control over when poisons are inserted, they can only provide a modified dataset, as outlined in the threat model.\n* \u201cIn Algorithm 1, step 9: what is the update being performed? It seems to me that the pseudo-code does not capture the entire processing steps, hence making the whole work hard to reproduce.\u201d\nThe update being performed is one step of the first-order descent algorithm Adam, using the sign of the gradient, on the perturbation to the poisoned images where the objective is defined in eq. 3. We also include publicly available code in our submission to help reproduction of results. \n* \u201cFigure 2 shows gradient alignment along epochs (please report the axis labels), however it does not seem \"flat\" in the end, it is slightly decreasing. What happens if we increase the number of epochs? Will the alignment disappear?\u201d\nThe alignment slowly decreases, but stays positive when increasing the number of epochs. This is a consequence of training. It is actually the bound (right-hand side of Eq.(4)).  that remains stable over additional epochs, see Fig. 5. \n* \u201cEq. 1 shows no constraints on the data points staying in the feature space after perturbations. Is it considered during the experiments?\u201d\nWhile this could be a heuristic of the attacker, this is not a part of the targeted poisoning objective we consider. Note however that the feature representations of the poisons after training are not anomalous for their given class. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "vfl_sgowWkJ", "original": null, "number": 2, "cdate": 1605297552081, "ddate": null, "tcdate": 1605297552081, "tmdate": 1605297982950, "tddate": null, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment", "content": {"title": "General Comments", "comment": "We thank the reviewers for their constructive feedback. We will respond to specific points raised under their respective reviews. Here, we will respond to common concerns: \n\n1) On the readability of figures, specifically Fig. 4: in all figures, we will make it more clear in the main body what \u201cpoison success\u201d, or equivalently \u201cpoison accuracy\u201d means. For convenience, this value refers to the percentage of runs (averaged over randomly initialized networks) in which the target image is mis-classified as the poison class, see the appendix for more details - we will make this clearer in the main text. As for the size of the text in Fig. 4, if one zooms in on a computer screen, the text becomes readable. However, we recognize this is an inconvenience, and not possible on a printed version, so we will make each subfigure its own figure, and expand the legend size.  \n\n2) The question of  validation accuracy of poisoned datasets on clean images was also a common concern. However validation accuracy is unaffected. Due to the considered threat model (small epsilon, small budget of 1%), the attack, as alluded to in the introduction, does not noticeably degrade the clean validation accuracy. \nTo emphasize this with actual data, we have included below the validation accuracy for the baseline experiments in the inset figure (subsection 5.1). These are the values for validation accuracy on CIFAR-10, for the poisoned dataset and the clean dataset: \n* K=1,R=1: poisoned: 92.12, clean: 92.25\n* K=2,R=1: poisoned: 92.06, clean: 92.16\n* K=4,R=1: poisoned: 92.08, clean: 92.18\n* K=8,R=1: poisoned: 92.20, clean: 92.16\n* K=1,R=8: poisoned: 92.08, clean: 92.22\n* K=2,R=8: poisoned: 92.03, clean: 92.27\n* K=8,R=8: poisoned: 92.04, clean: 92.13\n\nAll values are averages over their respective runs. We will include a table with these natural accuracies in the updated appendix. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "01olnfLIbD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3659/Authors|ICLR.cc/2021/Conference/Paper3659/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835197, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Comment"}}}, {"id": "Zy418x0vOS", "original": null, "number": 1, "cdate": 1602966188336, "ddate": null, "tcdate": 1602966188336, "tmdate": 1605023960198, "tddate": null, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Review", "content": {"title": "Important empirical work demonstrating real threat of poisoning attack on large-scale CNNs.", "review": "This paper presents a scalable data poisoning attack algorithm focusing on targeted attacks. The technique is based on gradient matching, where the intuition is to design the poisoning patterns such that their effect on the gradient of the training loss mimics the gradient as if the targeted test image is included in the training data.\n\nThe paper presents both theoretical intuitions behind the algorithm, as well as empirical reduction and simplification to make the algorithm scalable to ImageNet and applicable to even a black-box attack against the Google Cloud AutoML toolkit.\n\nThe algorithm proposed in this paper is practical and general, making it a realistic poisoning threat to modern deep learning systems. The presentation is clear and the theoretical justification is intuitive and easy to understand.\n\nOverall, I think this paper is a good contribution to the study of the large-scale poisoning attack.\n\nMinor typo:\n In proof of Prop 1, you need the angle between the two gradients to be almost always smaller than 90 degrees, not 180 degrees.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071910, "tmdate": 1606915803700, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3659/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Review"}}}, {"id": "38h-AaEjjBd", "original": null, "number": 2, "cdate": 1603935944735, "ddate": null, "tcdate": 1603935944735, "tmdate": 1605023960133, "tddate": null, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "invitation": "ICLR.cc/2021/Conference/Paper3659/-/Official_Review", "content": {"title": "New and practical targeted poisoning attack", "review": "This paper proposed a simple yet effective approach for data poisoning attack targeting a few \"clean-label\" victim images, using the idea of gradient matching (cosine similarity maximization) between the gradients of adversarial and clean losses. Although the attack model still requires knowing the network architecture (gray-box setting), the resulting poisoned datasets are more effective against different initializations, and some techniques (e.g. model ensemble, multiple restarts) are proposed to further boost the attack performance. The attack results are significantly better than the compared poisoning attacks, and the authors show effective attacks on the ImageNet dataset as well as Google Cloud AutoML with the poisoned data. The authors also discussed the proposed attack on some defenses, showing that the poison has limited change to feature distribution, and differential privacy can mitigate the attack but at the cost of reduced utility (clean accuracy).\n\nOverall, this paper shows some new insights and sets new benchmarks for targeted data poisoning attacks, with practical threat assessment on ImageNet datasets and Google Cloud AutoML, which I deem as a significant contribution. The proposed gradient matching is simple, intuitive, yet very effective. One limitation from Appendix A.8 is that the proposal may not scale well to more than 1 target image, as indicated by the rapidly decreasing attack accuracy. It will be more meaningful to control the effective budget/target and check the resulting accuracy of different number of targets, in order to understand whether gradient matching is scalable to multiple-target setting.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3659/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3659/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching", "authorids": ["~Jonas_Geiping1", "~Liam_H_Fowl1", "~W._Ronny_Huang1", "~Wojciech_Czaja1", "~Gavin_Taylor1", "~Michael_Moeller1", "~Tom_Goldstein1"], "authors": ["Jonas Geiping", "Liam H Fowl", "W. Ronny Huang", "Wojciech Czaja", "Gavin Taylor", "Michael Moeller", "Tom Goldstein"], "keywords": ["Data Poisoning", "ImageNet", "Large-scale", "Gradient Alignment", "Security", "Backdoor Attacks", "from-scratch", "clean-label"], "abstract": "Data Poisoning attacks modify training data to maliciously control a model trained on such data.\nIn this work, we focus on targeted poisoning attacks which cause a reclassification of an unmodified test image and as such breach model integrity. We consider a\nparticularly malicious poisoning attack that is both ``from scratch\" and ``clean label\", meaning we analyze an attack that successfully works against new, randomly initialized models, and is nearly imperceptible to humans, all while perturbing only a small fraction of the training data. \nPrevious poisoning attacks against deep neural networks in this setting have been limited in scope and success, working only in simplified settings or being prohibitively expensive for large datasets.\nThe central mechanism of the new attack is matching the gradient direction of malicious examples. We analyze why this works, supplement with practical considerations. and show its threat to real-world practitioners, finding that it is the first poisoning method to cause targeted misclassification in modern deep networks trained from scratch on a full-sized, poisoned ImageNet dataset.\nFinally we demonstrate the limitations of existing defensive strategies against such an attack, concluding that data poisoning is a credible threat, even for large-scale deep learning systems.", "one-sentence_summary": "Data poisoning attacks that successfully poison neural networks trained from scratch, even on large-scale datasets like ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "geiping|witches_brew_industrial_scale_data_poisoning_via_gradient_matching", "supplementary_material": "/attachment/dda63631727de698b89e21cd475183d5b808b192.zip", "pdf": "/pdf/3a3c570da85848de52605f6669aae395d063027b.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ngeiping2021witches,\ntitle={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching},\nauthor={Jonas Geiping and Liam H Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=01olnfLIbD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "01olnfLIbD", "replyto": "01olnfLIbD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3659/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071910, "tmdate": 1606915803700, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3659/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3659/-/Official_Review"}}}], "count": 17}