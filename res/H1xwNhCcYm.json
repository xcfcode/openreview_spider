{"notes": [{"id": "H1xwNhCcYm", "original": "rylp1lR5tQ", "number": 1461, "cdate": 1538087983348, "ddate": null, "tcdate": 1538087983348, "tmdate": 1550844457505, "tddate": null, "forum": "H1xwNhCcYm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 25, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJxIWB4gl4", "original": null, "number": 1, "cdate": 1544729853678, "ddate": null, "tcdate": 1544729853678, "tmdate": 1545354511747, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Meta_Review", "content": {"metareview": "This paper makes the intriguing observation that a density model trained on CIFAR10 has higher likelihood on SVHN than CIFAR10, i.e., it assigns higher probability to inputs that are out of the training distribution. This phenomenon is also shown to occur for several other dataset pairs. This finding is surprising and interesting, and the exposition is generally clear. The authors provide empirical and theoretical analysis, although based on rather strong assumptions. Overall, there's consensus among the reviewers that the paper would make a valuable contribution to the proceedings, and should therefore be accepted for publication.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting empirical observation and analysis"}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1461/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352831144, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352831144}}}, {"id": "HJl0-fqLeV", "original": null, "number": 17, "cdate": 1545146885689, "ddate": null, "tcdate": 1545146885689, "tmdate": 1545146885689, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "BklXQj-R1V", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Re: on point 5 and asymmetric behaviour", "comment": "Thanks for your comment and question.\n\nPer the reviewers' requests for more evidence of the phenomenon on additional data sets, we wanted to bolster the 'motivating observations' section with experiments that better exhibit the curious out-of-distribution behavior.  We found that the FashionMNIST-vs-MNIST pair illustrated the phenomenon better (i.e. larger BPD gap) than the NotMNIST-vs-MNIST pair and hence we replaced those results in the main text.   We will also add the corresponding plots to Appendix B showing the asymmetric behavior for this pair as well (due to time constraints, we couldn't update all of the figures in the Appendix during the rebuttal period).  This was the only reason for the switch.  If you think the NotMNIST-vs-MNIST experiment is more interesting for some other reason, please do let us know your thoughts.     \n\nWe wouldn't claim the asymmetry \"solves\" the issue since (i) even for models trained on SVHN, there could be other datasets that lead to higher likelihood and (ii) it does not immediately reveal a procedure to correct the CIFAR10-vs-SVHN (or similar) issue.  The second-order analysis in Section 5 is still our best explanation for the asymmetric behavior.  That is, the interaction between the model curvature and the data set variance leads to the phenomenon, and when the sign of the difference in variances is switched (which occurs when the train and OOD sets are switched), then we expect the phenomenon behavior to flip as well.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "BklXQj-R1V", "original": null, "number": 5, "cdate": 1544588058843, "ddate": null, "tcdate": 1544588058843, "tmdate": 1544588058843, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "rygFbvpYRX", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Public_Comment", "content": {"comment": "could you please explain why the notmnist results were removed in the latest draft? I found these did illustrate well the issue this paper is trying to get across, albeit the asymmetric behaviour as reported in the appendix -- also, while on this, I'm also surprised that the official reviewers didn't ask more about this. Could you provide some thoughts on why reversing the train/test roles of data sets solves the pathological high test-likelihood issue? Thanks!", "title": "on point 5 and asymmetric behaviour"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311591948, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1xwNhCcYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311591948}}}, {"id": "rkxi_sW8kN", "original": null, "number": 16, "cdate": 1544063859031, "ddate": null, "tcdate": 1544063859031, "tmdate": 1544063859031, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "HJxL_ysjC7", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Re: Density Estimation Observation Appears Elsewhere ", "comment": "(Apologies for late response, we missed this earlier)\n\nThanks for pointing us to your work.  We will incorporate it into our discussion of related work.\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "B1eudmtWkN", "original": null, "number": 15, "cdate": 1543766895807, "ddate": null, "tcdate": 1543766895807, "tmdate": 1543766895807, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "HJlpx-we1E", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Re: Thank you for your feedback", "comment": "Thank you for these suggestions, Reviewer #3.  We probably won't be able to add them in the next week---as many of us authors are traveling to / attending NeurIPS---but we will add them to the next iteration of the draft."}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "HJlpx-we1E", "original": null, "number": 14, "cdate": 1543692533365, "ddate": null, "tcdate": 1543692533365, "tmdate": 1543693104112, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "rJxBdmxxyV", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Thank you for your feedback", "comment": "And thank you for revising the text. My main concerns are addressed, and the issue #5 is pretty minor given the other assumption made in the analysis.\n\nI am not a statistics expert, if one wants to test whether two univariate Gaussians have different means or not, a student-t test can be used. In this case of multivariate Gaussians, a brief search suggests using its generalization, \"Hotelling's two-sample t-squared statistics/test\". In the end, one wants to compare the distance (considering different dimensions have different correlations, the Mahalanobis distance is better) between the two means, and compare its scale to the covariance matrices of both Gaussians.\n\nA rougher test is see if one Gaussian's mean lies inside the confidence interval of the other Gaussian. See multivariate normal distribution's confidence interval.\n\nIn the case that the tests fail, one can see how much the test statistics are larger than e.g. the 95% quantile of the corresponding test distributions."}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "SkluvEgeJN", "original": null, "number": 13, "cdate": 1543664735967, "ddate": null, "tcdate": 1543664735967, "tmdate": 1543664735967, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "HJg_Y9yYRX", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Re: Related work", "comment": "Hello Kimin,\n\nThanks for pointing us to your work.  We will incorporate it into our discussion of related work."}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "rJxBdmxxyV", "original": null, "number": 12, "cdate": 1543664493254, "ddate": null, "tcdate": 1543664493254, "tmdate": 1543664493254, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "BJgniGai0m", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Re: Comments on the rebuttal", "comment": "Thank you for your responses and continuing the discussion, Reviewer #3.  Our replies are below.\n\n2.  \"All I am asking is that the paper warns its readers of this shortcoming at the beginning of the analysis.\":\n\nFair point.  We will add a sentence at the beginning of Section 5 to make explicit that these expressions are approximations. \n\n\n4.  We perfectly agree with your 'better description': \"one of the terms encourages the sensitivity....But we tried and it's not working.\"  This is exactly what we wanted to convey in the draft, and we thought we clarified this point in our rebuttal by saying \"Our point is made in the context of volume term which is only one of the terms in the change-of-variable objective.\"  We'll revise the draft to further emphasize our remarks pertain to the volume term only.\n\n\n5.  \"...making it 150 which is huge (actual value is probably smaller)\"\n\nThe difference is certainly much smaller.  It would be 150 only if the histograms were perfectly separated to each end of the x-axis in Figure 6 (a) of the original draft, which is not the case at all.  What metric / plot would convince you?  Some statistic of the dimension-wise means?    "}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "HJxL_ysjC7", "original": null, "number": 4, "cdate": 1543380846148, "ddate": null, "tcdate": 1543380846148, "tmdate": 1543432683594, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "r1eWc6qjnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Public_Comment", "content": {"comment": "> 1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled \u201cGenerative Ensembles for Robust Anomaly Detection\u201d makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10. \n\nThe parallel submission called Deep Anomaly Detection with Outlier Exposure also makes the observation that SVHN examples have higher likelihood than CIFAR-10 examples, and they also propose a way to correct this behavior. This is in Section 4.4 of https://openreview.net/pdf?id=HyxCxhRcY7\nThe results also suggest that SVHN results are one of the worst-cases for density estimators; density estimators are not as bad on many other datasets.", "title": "Density Estimation Observation Appears Elsewhere"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311591948, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1xwNhCcYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311591948}}}, {"id": "BJgniGai0m", "original": null, "number": 11, "cdate": 1543389860035, "ddate": null, "tcdate": 1543389860035, "tmdate": 1543390157128, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "Skgkfk6tRm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Comments on the rebuttal", "comment": "Thank you for your response. The extra results are promising, which makes the paper quite stronger. Other questions are addressed well. Now I am mainly focused on these three issues:\n\n2. Second order analysis, but only on the *sign* of the *difference* of two pdfs\n\nI would think that since x is an image, it would be hard to approximate a distribution with a mixture of a thousand Gaussians, let alone one Gaussian. Even if you are taking the difference of two pdfs, and taking the sign of the difference, a Gaussian would give you a hypersphere, not a large amounts of irregular-shaped blobs scattered through the image space.\n\nIt IS indeed inevitable that when theoretically analyzing deep networks, we have to start somewhere easy, and log-quadratic pdfs are a valid starting point. All I am asking is that the paper warns its readers of this shortcoming at the beginning of the analysis.\n\n4. Loss actively increasing volume term unlike prior work\n\nIt does seem that way, but by the same argument I can claim that any loss function function has a L2 component in it: if your loss is f(theta), then you just write f(theta) = g(theta) + |theta|_2^2, where g(theta) = f(theta) - L2. My bold claim only makes sense if in fact all terms in g(theta) collectively does not do much on the L2. Unfortunately this is not the case in this paper. \n\nSpecifically in this paper, the latent density term is the happiest if you make f nearly degenerate (everything maps to a tiny proximity of argmax_z{ p(z) }, for example), making the volume term nearly zero. And the volume term is needed to change this into something meaningful. The two terms strike a balance. So it is not right to claim f(x) encourages sensitivity if one term encourages it and another discourages it. -- Especially considering the experiment fixing the volume term did not make SVHN and CIFAR closer. A better way to describe this story is can be along the lines of \"one of the terms encourages the sensitivity (but the other discourages it), and that term makes SVHN likelihood pretty high, so one may think this is the issue. But we tried and it's not working\".\n\n5. Are SVHN and CIFAR centers close?\n\n*Individually*, each dimension of the means is quite close, but remember that two mean vectors are close only if *everything* is close.  These are I assume 32x32=1024 feature space, so you would amplify the estimated 0.15 by 1024, making it 150 which is huge (actual value is probably smaller). Since this is used for the difference of two distributions approximated by log-quadratics, one should see the drop of the approximated density function when you move as far as to the mean of the other distribution. I am not convinced that it is small.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "HylTaY-jAm", "original": null, "number": 10, "cdate": 1543342532941, "ddate": null, "tcdate": 1543342532941, "tmdate": 1543342532941, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "SyedvzM9Am", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Re: Figure 4 d) ", "comment": "No, the BPD never becomes lower for CIFAR-10 than for SVHN under any setting of the training time, optimization strategy, regularization type / strength, and model size that we tried.  It depends on what you mean by \u2019not complex enough.\u2019  We achieve sampling and BDP numbers on par with SOTA so we don\u2019t think that the explanation is simply to use a bigger model.  In fact, the Glow model trained by the authors of \u201cGenerative Models for Robust Anomaly Detection\u201d (https://openreview.net/forum?id=B1e8CsRctX) is as large as Kingma & Dhariwal's (2018), and they report the same phenomenon.  If by \u2019not complex enough\u2019 you mean that Glow could possibly be generally improved to better represent the training density, then sure, perhaps some innovation applied to Glow could make the model richer and fix the issue.  We do not believe such an innovation is trivial though, given how persistent the phenomenon is across hyperparameters and when ensembling (Appendix F)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "SyedvzM9Am", "original": null, "number": 9, "cdate": 1543279200101, "ddate": null, "tcdate": 1543279200101, "tmdate": 1543279200101, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "ByxeVRhY0m", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Figure 4 d)", "comment": "\nFrom Figure 4 d), we see that, due to the inductive bias of the model, SVHN has lower bpd. \nIf the model were trained further, would the bpd of the training set ever become lower than SVHN test? \n\nIf yes, then doesn't this indicate that, due to early stopping, the models are underfitting the CIFAR test set? In other words, generalizing density estimation from CIFAR training set to CIFAR test set is challenging and thus the models underfit the CIFAR test set, resulting in the simpler dataset (SVHN) having higher likelihood due to the inductive bias of the model. So possibly, given more data or a better inductive bias, this problem would go away? \nIf no, then it seems that the model is not complex enough since it is unable to obtain a lower bpd on CIFAR train compared to SVHN. \n\nHave you tested this? What are your thoughts? "}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "rygFbvpYRX", "original": null, "number": 8, "cdate": 1543259905164, "ddate": null, "tcdate": 1543259905164, "tmdate": 1543259905164, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Revised Draft", "comment": "We have uploaded a revised draft in which we have attempted to incorporate the reviewers\u2019 suggestions.  In particular, the new draft includes the following significant revisions:\n\n1.  Additional Data Sets:  In Section 3 we now report results for Glow trained and tested on the following data sets (in addition to CIFAR-10 vs SVHN): FashionMNIST (train) vs MNIST (test), CelebA (train) vs SVHN (test), ImageNet (train) vs CIFAR10/CIFAR100/SVHN (test).  The phenomenon of interest (i.e. higher likelihood on out-of-distribution test data) is observed for all of these new pairs.  Furthermore, we include the empirical means and variances of these data sets in the analysis in Section 5 and show that they agree with our original draft\u2019s conclusions.\n\n2.  Related Work:  We discuss the \u0160kv\u00e1ra et al. (2018) work (and other concurrent work) in Section 6, as suggested by Reviewer #3.  \n\n3.  Equation Spacing: We fix the spacing issue mentioned by Reviewer #1.\n\n4.  Revised Plot of Empirical Means: Reviewer #3 had doubts about to what degree the data set means overlap.  We believe this doubt was due to the range of the x-axis in what was formerly Figure 6 (a)---now Figure 5 (a).  We have revised the figure to have range 0-255 (normalized to 0-1) and added the additional data sets. \n\n5.  Removal of NotMNIST results: We have removed from the main text the NotMNIST vs MNIST experiment that was reported in the original draft.  However, the Appendix (most crucially Figures 8 and 13) still contains NotMNIST results and has not yet been updated with the new data sets.  We will fix this in the next draft.   "}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "Skgkfk6tRm", "original": null, "number": 7, "cdate": 1543257863134, "ddate": null, "tcdate": 1543257863134, "tmdate": 1543257863134, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "HJx8SCQEn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thanks again, Reviewer #3, for your thought-provoking critique.  We respond to your other comments below. \n\n1.  \u201cIn particular, Section 4 is a series of empirical analyses, based on one dataset pair\u2026.However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis.\u201d  \n\nSee general responses #1 and #3.\n\n\n2.  \u201cIt is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper\u2026.Section 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.\u201d\n\nSee general response #2.  We emphasize that we are not trying to approximate the density function, only approximate the difference and characterize its sign.  Moreover, the special structure of CV-Glow makes these derivative-based approximations better behaved and more tractable than an expansion of a generic deep neural network.\n\n\n3.  \u201cSome parts of the paper feel long-winded and aimless\u2026.In general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.  Section 2 background takes too much space.  Section 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.\u201d\n\nWe will attempt to make the writing more concise.  But we believe that most, if not all, of Section 2 is necessary in order to make the paper self-contained and accessible to someone who has never before seen invertible generative models.  While we are fastidious in our experimental description in Section 3, we think it is necessary since this is the foundational section of the paper.\n\n\n4.  \u201cI don't think Glow necessarily is encouraged to increase sensitivity to perturbations. The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.\u201d\n\nWe are not saying that the model will totally disregard the latent density and attempt to scale the input to very large or infinite values.  Our point is made in the context of volume term which is only one of the terms in the change-of-variable objective. The log volume term in the change-of-variable objective is maximizing the very quantity (the Jacobian\u2019s diagonal terms) that the cited work on derivative-based regularization penalties has sought to minimize.  The maximization of the derivatives in the objective directly implies increased sensitivity to perturbations.\n\n\n5.  \u201cFigure 6(a) [Figure 5(a) in revised draft] clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.\u201d\n\nWe are not sure how you are drawing this conclusion; perhaps from the scale of the x-axis? The histogram in Figure 6 (a) (original draft) has an x-axis covering the interval [0.4, 0.55], meaning the maximal difference between a mean in *any pair of dimensions* is 0.15.  Scaling back to pixel units, 0.15 * 255 = 38.25, meaning that 38.25 pixels is the maximum difference in means.  While this is not a difference of zero, we don\u2019t see how you could say this \u201cclearly suggests\u201d that the means are \u201cvery different.\u201d  In the latest draft, this figure---now Fig 5 (a)---has an x-axis that spans from 0-255.  Hopefully the overlap in the means in now conspicuous.  \n\n\n6.  \u201cHowever, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite: V\u00edt \u0160kv\u00e1ra et al. Are generative deep models for novelty detection truly better? at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.\u201d\n\nThank you for pointing us to this work.  We cite it in the revised draft.  It looks like they test on UCI data sets of dimensionality less than 200, and therefore their results speak to a much different data regime than the one we are studying.\n\n\n7.  \u201cA part of the paper's contribution (section 5 conclusion) seem to overlap with others' work. The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).\u201d\n\nWhile we do also analyze constant images, we believe that our results for multiple data set pairs (FashionMNIST-MNIST, CIFAR10-SVHN, CelebA-SVHN, ImageNet-CIFAR10/CIFAR100/SVHN) and for multiple deep generative models (flow-based models, VAE, PixelCNN) is novel. Our conclusions are arrived at through focused experimentation and a novel analytical expression applied to CV-Glow. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "ByxeVRhY0m", "original": null, "number": 6, "cdate": 1543257639836, "ddate": null, "tcdate": 1543257639836, "tmdate": 1543257639836, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "BJe__C5d3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thanks again, Reviewer #2, for your insightful feedback.  We respond to your other comments below. \n\n1.  \u201cWhy investigate a component specific to just flow-based models (the volume term)? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.\u201d \n\nSee general response #3.\n\n\n2.  \u201cFor instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood.\u201c\n\nWe do not believe our models are necessarily underfit.  In fact, we found that Glow had a tendency to *overfit,* and that one must carefully set Glow\u2019s l2 penalty and choose its scale parametrization (exp vs sigmoid, see Appendix D) in order to prevent it from doing so.  We thought this overfitting to the training data could be a reason for the phenomenon and therefore we tuned our implementations to have reasonable generalization.    \n\n\n3.  \u201cIt would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers.  For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting.\u201d\n\nSee general response #1 in regards to data sets and additional results.  Thank you for the suggestion of looking at data sets with similar statistics.  We do this, in a way, with our second order analysis and the \u2018gray-ing\u2019 experiment in Figure 5 (b) (formerly Figure 6 (b) in the original draft).  Gray CIFAR-10 (blue dotted line) nearly overlaps with original SVHN (red solid line) in terms of their log p(x) evaluations.  Figure 12 (formerly Figure 13) then shows the latent (empirical) distribution of the gray images, and we see that the gray CIFAR-10 latent variables nearly overlap with the SVHN latent variables.  This is to be expected though, given the overlapping p(x) histograms, since the probability assigned by CV-Glow (in comparison to other inputs) is fully determined by the position in latent space.\n\n4.  \u201cThe second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets.\u201d \n\nSee general response #2."}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "BygHqa2F0X", "original": null, "number": 5, "cdate": 1543257485414, "ddate": null, "tcdate": 1543257485414, "tmdate": 1543257485414, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "r1eWc6qjnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thanks again, Reviewer #1, for your thoughtful comments.  We respond to your other comments below.  \n\n1.  \u201cIt seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?\u201d\n\nThis is an interesting idea, but we are not sure it is applicable.  If one looks closely at Figure 2 (b), there are still blue and black histogram bars (denoting CIFAR-10 train and test instances) covering the entirety of SVHN\u2019s support (red bars).  \n\n\n2.  \u201c[The constant input]\u2019s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn\u2019t seem applicable.\u201d\n\nSee general response #2.\n\n\n3.  \u201cHow much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images.\u201d\n\nWe have not tested non-image data, since images are the primary focus of work on generative models, but this is an interesting area for future work. \n\n\n4.  \u201cSamples from a CIFAR model look nothing like SVHN. This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.\u201d\n\nThis is a very good point.  See our response to Shengyang Sun\u2019s comment below.  We see think this phenomenon has to do with concentration of measure and typical sets, but we do not yet have a rigorous explanation. \n\n\n5.  \u201cThere seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :)\u201d\n\nWe have fixed the spacing in the latest draft :)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "SygxWa3Y0m", "original": null, "number": 4, "cdate": 1543257336478, "ddate": null, "tcdate": 1543257336478, "tmdate": 1543257336478, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "ByloTnnFRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "General Rebuttal (2/2) ", "comment": "3.  Purpose / Direction of Section 4 [R2, R3]:  R2 asks \u201cWhy investigate a component specific to just flow-based models (the volume term)? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types.\u201d  While the phenomenon is common to multiple deep generative model classes, as Figure 3 shows, we found it very hard to analyze all three models simultaneously, on equal footing, due to their different structures and inference requirements.  For instance, how can we compare VAEs and PixelCNNs while controlling for the former\u2019s approximate inference requirements?  How do we know any problems with densities / outlier detection aren\u2019t due to a sub-optimal inference model or the variational approximation?  We thought we would make more headway by restricting the analysis to invertible models since they (i) admit exact likelihood calculations and (ii) have nice analytical properties stemming from the bijection constraint.  Having made this decision, we then thought the next natural step is to look at both terms in the change-of-variables objective---the density under p(z) and the volume term---to see if one of these in particular was the cause.  After seeing Figure 4 (c, d) (Figure 4 (a, b) in revised draft), we thought that the volume term is the culprit, which then lead to examination of constant-volume Glow (CV-Glow) (i.e. \u2018constant volume\u2019 across all inputs) as described on page 6.  While the volume term was a bit of a red herring, we thought the progression from {VAE, PixelCNN, NVP-Glow} \u2192 {NVP-Glow} \u2192 {CV-Glow} was a logical way to further examine the problem for an increasing tractable model class.          \n\nRelatedly, R3 writes of Section 4: \u201cSection 4 is a series of empirical analyses, based on one dataset pair\u2026.However, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis\u2026.Section 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.\u201d  The purpose of focusing on just CIFAR-10 vs SVHN in Section 4 is to drill-down and isolate why the phenomenon is happening in this one particular case.  We think this is an appropriate approach, as we didn\u2019t want to introduce too many experimental variables, as explained above.  Furthermore, the presence of this phenomenon for SVHN vs CIFAR-10 alone warrants investigation since those data sets are extremely popular in the ML community.  Yet, we have since added additional data sets (see general response #1) and hope the reviewer is now satisfied with this additional evidence of the phenomenon's prevalence."}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "ByloTnnFRQ", "original": null, "number": 3, "cdate": 1543257283284, "ddate": null, "tcdate": 1543257283284, "tmdate": 1543257283284, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "General Rebuttal (1/2)", "comment": "Thank you, reviewers, for your fair and helpful comments.  We\u2019ve provided a general response below that addresses concerns common to multiple reviewers.  We\u2019ll also respond to reviewers individually regarding issues particular to their review.\n\n1.  Limited Number of Data Sets [R2, R3]:  We have now added additional results to Section 3 (Figures 1 and 2) showing that the phenomenon (higher likelihood on non-train data) occurs for FashionMNIST (train) vs MNIST (test), CelebA (train) vs SVHN (test), ImageNet (train) vs CIFAR10/CIFAR100/SVHN (test).  Furthermore, we have included these data sets into our plot of the empirical means and variances (Section 5), showing that our second-order analysis and \u2018sitting inside of\u2019-conclusion agrees with these additional observations.   \n\n2.  Accuracy / Generality of Second-Order Analysis [R1, R2, R3]:  All reviewers bring up questions about the second-order analysis.  Starting with R1, they question how Equation 5 applies / can be interpreted for constant images.  To slightly correct R1\u2019s statement, the constant image with high likelihood under the SVHN-trained model is x=128.  Normalizing by the number of pixels, i.e. 128/265=0.5, places this constant image almost in the exact center of the means plot in Figure 6 (a)---thus, the second-order analysis does apply.  Then turning to Equation 5 and plugging in the variance Var[\\delta(128)]=0, we have:\n\nE_q [log p(x)] - E_p* [log p(x)] \\approx \u00bd * (negative number for CV-Glow) * (0 - Sigma_p*) >= 0.\n\nHence the second-order analysis still holds for the delta function located at 128 and agrees with the empirical result.  We will add this derivation to the appendix.  \n\nMoving on to R2, they state that the second-order analysis reduces to \u201cjust a measure of the empirical variances of the datasets.\u201d  This is true and was done so purposefully.  CV-Glow is the only generative model that we know of that (i) has high-capacity and (ii) is amenable to the second-order analysis.  For all other models mentioned (VAE, PixelCNN, NVP-Glow), the second-order equation depends on the second derivatives of the neural network w.r.t its input.  It\u2019s hard, if not impossible, to say anything general about how these second derivatives behave across the input space, let alone across re-fittings of the model.  CV-Glow uniquely has second derivatives that simplify to a function of (i) the log-convexity of the latent distribution and (ii) the square of the 1x1 convolutional kernel\u2019s parameters.  Since both of these terms have a constant sign, the interesting part of the equation does indeed boil down to \u201ca measure of the empirical variances of the datasets.\u201d  The complications introduced by the model have been taken out and what\u2019s left is a function of the data statistics, which does allow for some general conclusions.  We will try to clarify this reasoning / motivation in the paper, as space permits.  Furthermore, the fact that our second-order analysis lead us to and agrees with the additional experiments (see general response #1) and the gray-ing attack (Figure 5 (b), formerly Figure 6 (b) in the original draft), we see this as evidence of its validity.\n\nLastly, we address R3\u2019s comments that they \u201cfind it very disturbing to base [analysis] on a 2nd order approximation of a probability density function.\u201d  We agree that trying to approximate a neural-network-based density with only a second-order representation is a tall order.  But this is not precisely what we are doing.  Rather, we are approximating *the difference* in density functions, and therefore we only care about *the sign* of the expression.  We believe the second-order expression is a useful representation for this.  Moreover, if we assume the data distributions have no cross-moments, then from Equation 11 we notice that the diagonal derivatives are zero for second-order and beyond, thus making the second-order expansion exact.  For these two reasons, we don\u2019t believe our approximation is \u201cdisturbing.\u201d  And since we are working with deep generative models, any analytical statements will require rather strong assumptions. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "HkgLWfveT7", "original": null, "number": 2, "cdate": 1541595645882, "ddate": null, "tcdate": 1541595645882, "tmdate": 1541595645882, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "BJl6am8z37", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Re: Image Samples", "comment": "Thank you for your comment, Shengyang.  This is a good point and something we were a bit puzzled by as well.  Our current hypothesis is that the SVHN samples do not fall within the model\u2019s typical set.  To elaborate, in high dimensions samples at or very near to the mode are unlikely.  See the high-dimensional Gaussian example discussed here: https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/  While you are correct in that the variances in data space are not drastically different, the variances of each data set\u2019s latent variables (Figure 12, top column, middle) are well separated, with SVHN\u2019s variance being much smaller.  Thus the distribution in latent space may be a better way to characterize the model\u2019s typical set as samples are first drawn in latent space and then passed to the inverse function. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "r1eWc6qjnX", "original": null, "number": 3, "cdate": 1541283208880, "ddate": null, "tcdate": 1541283208880, "tmdate": 1541533115391, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Review", "content": {"title": "Interesting work and analysis", "review": "I really enjoyed reading the paper! The exposition is clear with interesting observations, and most importantly, the authors walk the extra mile in doing a theoretical analysis of the observed phenomena.\n\nQuestions for the authors:\n1. (Also AREA CHAIR NOTE): Another parallel submission to ICLR titled \u201cGenerative Ensembles for Robust Anomaly Detection\u201d makes similar observations and seemed to suggest that ensembling can help counter the observed CIFAR/SVHN phenomena unlike what we see in Figure 10. Their criteria also accounts for the variance in model log-likelihoods and is hence slightly different.\n2. Even though Figure 2b shows that SVHN test likelihoods are higher than CIFAR test likelihoods, the overlap in the histograms of CIFAR-train and CIFAR-test is much higher than the overlap in CIFAR-train and SVHN-test. If we define both maximum and minimum thresholds based on the CIFAR-train histogram, it seems like one could detect most SVHN samples just by the virtue that there likelihoods are much higher than even the max threshold determined by the CIFAR-train histogram?\n3. Why does the constant image (all zeros) in Figure 9 (appendix) have such a high likelihood? It\u2019s mean (=0 trivially) is clearly different from the means of the CIFAR-10 images (Figure 6a) so the second order analysis of Section 5 doesn\u2019t seem applicable.\n4. How much of this phenomena do you think is characteristic for images specifically? Would be interesting to test anomaly detection using deep generative models trained on modalities other than images.\n5. One of the anonymous comments on OpenReview is very interesting: samples from a CIFAR model look nothing like SVHN. This seems to call the validity of the anomalous into question. Curious what the authors have to say about this.\n\nMinor nitpick: There seems to be some space crunching going on via Latex margin and spacing hacks that the authors should ideally avoid :)", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Review", "cdate": 1542234224854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335951559, "tmdate": 1552335951559, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJe__C5d3Q", "original": null, "number": 2, "cdate": 1541086832444, "ddate": null, "tcdate": 1541086832444, "tmdate": 1541533115187, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Review", "content": {"title": "Interesting example of density modelling shortcoming", "review": "\nThis paper displays an occurrence of density models assigning higher likelihood to out-of-distribution inputs compared to the training distribution. Specifically, density models trained on CIFAR10 have higher likelihood on SVHN than CIFAR10. This is an interesting observation because the prevailing assumption is that density models can distinguish inliers from outliers. However, this phenomenon is not encountered when comparing MNIST and NotMNIST. The SVHN/CIFAR10 phenomenon has also been shown in concurrent work [1].\n\nGiven that you observed that SVHN has higher likelihood on all three model types (PixelCNN, VAE, Glow), why investigate a component specific to just flow-based models (the volume term)? It seems reasonable to suspect that the phenomenon may be due to a common cause in all three model types. For instance, the experiments seem to indicate that generalizing density estimation from CIFAR training set to CIFAR test set is likely challenging and thus the models underfit the true data distribution, resulting in the simpler dataset (SVHN) having higher likelihood. \n\nGiven the title of the paper, it would have been nice if this paper explored more than just MNIST vs NotMNIST and SVHN vs CIFAR10, so that the readers can gain a better feel for when generative models will be able to detect outliers. For instance, a scenario where the data statistics (pixel means and variances) are nearly equivalent for both datasets would be interesting. The second order analysis is good but it seems to come down to just a measure of the empirical variances of the datasets. \n\nThis paper is well written. I think the presentation of this density modelling shortcoming is a good contribution but leaves a bit to be desired. \n\n[1] Choi, H. and Jang, E. Generative Ensembles for Robust Anomaly Detection. https://arxiv.org/abs/1810.01392\n\n\nPros:\n- Interesting observation of density modelling shortcoming \n- Clear presentation\n\nCons:\n- Lack of a strong explanation for the results or a solution to the problem \n- Lack of an extensive exploration of datasets\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Review", "cdate": 1542234224854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335951559, "tmdate": 1552335951559, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJx8SCQEn7", "original": null, "number": 1, "cdate": 1540795965822, "ddate": null, "tcdate": 1540795965822, "tmdate": 1541533114975, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Review", "content": {"title": "Very interesting finding; insufficient empirical analysis, theory with approximations too bold", "review": "Pros:\n- The finding that SVHN has larger likelihood than CIFAR according to networks is interesting. \n- The empirical and theoretical analyses are clear, seem thorough, and make sense.\n- Section 5 can provide some insight when the model is too rigid and too log-concave (e.g. Gaussian).\nCons:\n- The premises of the analyses are not very convincing, limiting the significance of the paper.\n- In particular, Section 4 is a series of empirical analyses, based on one dataset pair. In 3/4 of the pairs the author tried, this phenomenon is not there. Whether the findings generalize to other situations where the phenomenon appears is uncertain. \n- It is good that Section 5 has some theoretical analysis. But I personally find it very disturbing to base it on a 2nd order approximation of a probability density function of images when modeling something as intricate as models that generate images. At least this limitation should be pointed out in the paper.\n- Some parts of the paper feel long-winded and aimless.\n\n[Quality]\nSee above pros and cons.\nA few less important disagreement I have with the paper:\n- I don't think Glow necessarily is encouraged to increase sensitivity to perturbations. The bijection needs to map training images to a high-density region of the Gaussian, and that aspect would make the model think twice before making the volume term too large.\n- Figure 6(a) clearly suggests that the data mean for SVHN and CIFAR are very different, instead of similar.\n\n[Clarity]\nIn general, the paper is clear and easy to understand given enough reading time, but feels at times long-winded.\nSection 2 background takes too much space.\nSection 3 too much redundancy -- it just explains that SVHN has a higher likelihood when trained on CIFAR, and a few variations of the same experiment.\nSection 4 seems to lack a high-level idea of what it want to prove -- the hypothesis around the volume term is dismissed shortly after, and it ultimately proves that we do not know what is the reason behind the high SVHN likelihood, making it look like a distracting side-experiment.\nA few editorial issues:\n- On page 4 footnote 2, as far as I know the paper did not define BPD.\n- There are two lines of text between Fig. 4 and Fig. 5, which is confusing.\n\n[Originality]\nI am not an expert in this specific field (analyzing generative models), but I believe this analysis is novel.\nHowever, there are papers empirically analyzing novelty detection using generative model -- should analyze or at least cite:\n    V\u00edt \u0160kv\u00e1ra et al. Are generative deep models for novelty detection truly better? \n    ^ at first glance, their AUROC is never under 0.5, indicating that this phenomenon did not appear in their experiments although a lot of inlier-novelty pairs are tried.\nA part of the paper's contribution (section 5 conclusion) seem to overlap with others' work. The section concludes that if the second dataset has small variances, it will get higher likelihood. But this is too similar to the cited findings on page 6 (models assign high likelihood to constant images).\n\n[Significance] \nThe paper has a very interesting finding; pointing out and in-depth analysis of negative results should benefit the community greatly.\nHowever, only 1 dataset pair is experimented -- there should be more to ensure the findings generalize, since Sections 3 and 4 rely completely on empirical analysis. According to the conclusions of the paper, such dataset pairs should be easy to find -- just find a dataset that \"lies within\" another. Did you try e.g. CIFAR-100 train and CIFAR-10 test?\nSection 5 is based on a 2nd order expansion on the $log p(x)$ given by a deep network -- I shouldn't be the judge of this, but from a realistic perspective this does not mean much.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Review", "cdate": 1542234224854, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335951559, "tmdate": 1552335951559, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgNd7oT3m", "original": null, "number": 1, "cdate": 1541415787948, "ddate": null, "tcdate": 1541415787948, "tmdate": 1541415787948, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "ByefSPJRsm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "content": {"title": "Re: Measurement and distribution", "comment": "Thanks for your questions, comments, and compliments.  As for considering other divergences / discrepancies, indeed using these for either parameter estimation or evaluation could lead to different results.  It is an area of future work.  Given the prevalence of fitting models via maximum likelihood (KLD[p_empirical || p_model]), we thought reporting the result for just this divergence a worthy contribution.  \n\nAs for your second question, we\u2019re not certain we completely understand your point.  Can you clarify a bit more, please?  A perceived mismatch between distance in pixel space vs semantic space may be due to natural images having a common global structure.  The models then extract mostly the shared structure and not the details that we visually cue upon."}, "signatures": ["ICLR.cc/2019/Conference/Paper1461/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621609841, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xwNhCcYm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1461/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1461/Authors|ICLR.cc/2019/Conference/Paper1461/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621609841}}}, {"id": "BJl6am8z37", "original": null, "number": 2, "cdate": 1540674500811, "ddate": null, "tcdate": 1540674500811, "tmdate": 1540674500811, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Public_Comment", "content": {"comment": "Thank you for this interesting work. \n\nIt is astonishing that a well-trained CIFAR10 model assigns larger log-likelihood to the SVHN datasets. \n\nWhat confuses me is that why the samples from such models won't generate SVHN-like images. According to your derivation, the SVHN variances is only marginally smaller than CIFAR10 variances, therefore it is probably not due to that SVHN-like figures live in a much smaller subspace that are unlikely to sample from. ", "title": "Image Samples"}, "signatures": ["~Shengyang_Sun4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["~Shengyang_Sun4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311591948, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1xwNhCcYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311591948}}}, {"id": "ByefSPJRsm", "original": null, "number": 1, "cdate": 1540384570027, "ddate": null, "tcdate": 1540384570027, "tmdate": 1540384570027, "tddate": null, "forum": "H1xwNhCcYm", "replyto": "H1xwNhCcYm", "invitation": "ICLR.cc/2019/Conference/-/Paper1461/Public_Comment", "content": {"comment": "Thanks very much for the excellent work. It is very interesting to see the distribution from this perspectives. I took a look on the paper Theis2016, it seems besides BPD, KLD, MMD, JSD are considered, is it possible that CIFAR10 and SVHN can be different based on these three measurement?\n\nThis also reminds me of domain shift problem, which aims to align p(x,y), can I understand in this way that although in data space, CIFAR and SVHN are similar (in term of the BPD number), however, in semantic level (y), they are still large gap between this two?\n\nThanks again for the excellent work~~", "title": "Measurement and distribution"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1461/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Do Deep Generative Models Know What They Don't Know? ", "abstract": "A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong.  Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel,  out-of-distribution inputs.  In this paper we challenge this assumption.  We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.  Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN.  To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood.  We find such behavior persists even when we restrict the flows to constant-volume transformations.  These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. \n Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.", "keywords": ["deep generative models", "out-of-distribution inputs", "flow-based models", "uncertainty", "density"], "authorids": ["e.nalisnick@eng.cam.ac.uk", "amatsukawa@google.com", "ywteh@google.com", "dilang@google.com", "balajiln@google.com"], "authors": ["Eric Nalisnick", "Akihiro Matsukawa", "Yee Whye Teh", "Dilan Gorur", "Balaji Lakshminarayanan"], "pdf": "/pdf/57565b8c1041cb92668ab326cc9c1d4bb671568a.pdf", "paperhash": "nalisnick|do_deep_generative_models_know_what_they_dont_know", "_bibtex": "@inproceedings{\nnalisnick2018do,\ntitle={Do Deep Generative Models Know What They Don't Know? },\nauthor={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xwNhCcYm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1461/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311591948, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1xwNhCcYm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1461/Authors", "ICLR.cc/2019/Conference/Paper1461/Reviewers", "ICLR.cc/2019/Conference/Paper1461/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311591948}}}], "count": 26}