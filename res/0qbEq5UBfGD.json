{"notes": [{"id": "0qbEq5UBfGD", "original": "-K4tQtgRb1N", "number": 2290, "cdate": 1601308252407, "ddate": null, "tcdate": 1601308252407, "tmdate": 1614985754056, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "dCgTffIf7Md", "original": null, "number": 1, "cdate": 1610040380890, "ddate": null, "tcdate": 1610040380890, "tmdate": 1610473973872, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper addresses an important problem of semi-supervised learning of time-series data.  Their approach is based on a convolution autoencoder for learning a time-series latent space.  To guide learning an appropriate embedding, they explore three alternative internal clustering metrics (prototype loss, Silhouette loss, and DB index loss) coupled with the autoencoder reconstruction loss. \n\nThe approach is reasonable and interesting, however as pointed out by the reviewers, the current submitted version needs major revision for it to be accepted.  Key weaknesses are:\n1.\tIt lacks an extensive literature review.  The reviewers have made several suggestions for improving this.\n2.\tThe experiments are weak.  First, state-of-the-art baselines are missing.  Second, additional alternative evaluation metrics will strengthen the evaluation of unsupervised methods (e.g., adding NMI, accuracy, cross-validation of internal metrics).  Providing evaluation of when which loss would work better on what types of data would provide insight to the various losses proposed.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040380876, "tmdate": 1610473973855, "id": "ICLR.cc/2021/Conference/Paper2290/-/Decision"}}}, {"id": "-jtD34iXp9r", "original": null, "number": 1, "cdate": 1603755742650, "ddate": null, "tcdate": 1603755742650, "tmdate": 1606743235192, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Review", "content": {"title": "Review - Some nice ideas but has weaknesses", "review": "Disclaimer: I am not an expert in the time-series domain, although I did some literature review while performing this review.\n\n#####################################\nSummary:\nThe work investigates semi-supervised (SS) clustering of time-series data (i.e., clustering with few labelled points, which can also be seen as semi-supervised classification here). It specifically investigates how to train a convolutional autoencoder (CAE), which is applied on the time-series data creating an embedding for each timeseries, so that CAE\u2019s embedding clusters samples appropriately. It investigates 3 semi-supervised (SS) losses for this purpose: 1 is the loss proposed in (Ren 2018), and 2 novel losses, the \u201cSilhouette loss\u201d and the \u201cDB index\u201d loss, inspired by the corresponding internal clustering metrics. The work evalutes on 3 databases whether these losses improve clustering of the CAE\u2019s embedding, over vanilla training of the CAE (just reconstruction loss). The work also performs a study of whether the size of the convolution filters affect results and whether performing the gradient updates by the SS loss every batch or every epoch makes a difference.\n\n######################################\nReasons for score:\nI am recommending a rejection, because of limited literature review, problems with writing/clarity, limited evaluation and analysis of the experimental setup (configuration etc), and the results are not too strong either. I would not mind much about the performance, if the other weaknesses were not there, as at least the Silhouette loss seems reasonable. But overall, the weaknesses overcame the good points.\n\n#########################################\n\nPros: \n+ Exploration of semi-supervision for clustering is interesting as it\u2019s of practical importance, and rather timely, as there is general interest in the community on SS.\n\n+ The work explores 3 different losses for inducing clustering in the embedding space, 2 of which seem novel. The 2 new losses are quite intuitive as they are inspired by well established metrics. They are generic (not Time-series specific) and could potentially be interesting to parts of the community that look into how to create more discriminative models by clustering the embedding space of deep networks better, such as in semi-supervised learning.\n\n+ Performance of Silhouette loss seems to be interesting, as in all three databases it performs ok. (although I am not 100% convinced due to some unclear points about experimental setup and not too extensive evaluation, see below).\n\n#########################################\nCons:  \n\nCon.1:\nThe work is not well positioned within the literature. The review is limited both from the scope of unsupervised clustering, as well as from the point of view of semi-supervised learning (both in general and specifically to time-series). Details on literature review:\n\nOne one hand, the work discusses only \u201cshallow\u201d unsupervised clustering methods, but misses deep models for unsupervised clustering. As a result, it considers as the *only* baseline a vanilla Auto-Encoder (AE). However, for unsupervised clustering, AE has been long surpassed by more appropriate models for clustering (AE has by itself no incentive to cluster the latent space). Examples include Contrastive AEs, extensions of VAE and GANs, etc. I below give a citation to a recent work that extends VAEs for clustering [1], which also contains references to other models, which the authors can consult. Also, some old and recent works that apply such models to time-series are [2, 3, 4]. Hopefully references therein will be of interest to authors. I wouldn\u2019t expect all of them to be evaluated, but at least some discussed, to give a good picture of where the field is on time-series clustering. (A suggestion: Perhaps the authors could improve the work by arguing that they chose AE for its simplicity, to focus specifically on the effect of the loss. But at least this discussion should be made.)\n\n[1] Dilokthanakul et al, Deep unsupervised clustering with gaussian mixture variational autoencoders, 2016.\n\n[2] Rifai et al, Contractive Auto-Encoders: Explicit Invariance During Feature Extraction, 2011.\n\n[3] Fortuin et al, SOM-VAE: Interpretable Discrete Representation Learning on Time Series, ICLR 2019\n\n[4] McConville et al, N2D: (Not Too) Deep Clustering via Clustering the Local Manifold of an Autoencoded Embedding, ICPR 2020 (arxiv Aug 2019)\n\nWithin the scope of the time-series application, authors mention deep approaches based approaches (via LSTM and CNN) but they don\u2019t cite any. I would expect some important works to have been cited.\n\nFrom the point of view of semi-supervised learning (SSL) in deep-learning, the literature review is also limited, constrained to Ren et al (2018). This should be improved. I below provide a reference to a work that proposed an SSL loss for improved clustering of latent-space via label propagation, which is in the same spirit as the losses discussed in this paper and hence should be discussed [5]. I also provide a reference to a more recent work with cluster-inducing properties, in case the authors find it useful to discuss [6]. ([6] is not SSL, but it may be of interest, according to your judgement).\n\n[5] Kamnitsas et al, Semi-Supervised Learning via Compact Latent Space Clustering, ICML 2018.\n\n[6] Kenyon-Dean et al, Clustering-Oriented Representation Learning with Attractive-Repulsive Loss, AAAI 2019 workshop.\n\nFinally, I below provide a reference to a recent work on SSL in time-series, where the references therein can also help identify important related work [7].\n\n[7]  Jawed et al, Self-supervised Learning for Semi-supervised Time Series Classification, PAKDD 2020.\n\n\nCon.2: \nEvaluation shows that no method is consistently better than the baseline AE. Which diminishes a lot the value in the technical contribution (e.g. the interest in the 2 novel losses). Even more, central claim of the paper about performance of the methods does not hold.\n\nSpecificially, the claim from the abstract \u201cour methods can consistently improve k-Means clustering\u201d is not True. In Fig 2 and 4 from evaluation, we clearly see that no method \u201cconsistently\u201d improves over baseline AE across all settings. For example, DB index is clearly worse than all on ECH5000. Prototype loss also falls short, especially in UWave and Fig 4 ECG500. Silhouete loss also falls below AE on UWave in the little-labels regime in Fig2.a and across all settings in Fig.4.b. This actually shows that no model \u201cconsistently\u201d improves AE.\nSimilarly, these claims about performance are accompanied in Abstract & Sec 1 with comments about the \u201cmaximum increase\u201d in performance. But the maximum increase does not really convey the actual picture, as these are only the extrema. Instead, the authors should try to derive some statistic that represents the overall performance better.\n\nCon.3: \nEvaluation is limited:\n\nMethod is only against unsupervised CAE and Ren 2018. This is a result of limited literature review I guess (weak point 1). The authors could expand the evaluation by comparing to:\n* other \u201cshallow\u201d approaches that are already applied in time-series. The authors mention some of these in the related literature, arguing they are slow. The authors could at least try some of them, and report performance and time-for-running. This is very important, as the time-to-run of these approaches should be compared with the time-to-train of the proposed approach, which is never mentioned. (to my understanding the authors train and evaluate on the same samples, ala transductive SSL, hence training time plays the role of time-to-run of the traditional approaches)\n* other related losses for SSL in deep models (e.g. [5], or other similar easy to apply ones like VAT, see references in [5]), or other baseline generative models (e.g. VAE/GAN variants). \n\nCon.4: \nThe hyperparameter study in Sec 4.2.2 is not really useful. The authors explore whether different number of layers in the AE affect performance, and whether updating the weights of the AE with the gradients of the SSL loss at every batch step or only every epoch makes a difference. This is not insightful about the proposed losses themselves.\n\nCon.5: \nThere is currently no mention of an attempt to configure the model and training configuration for each loss best. E.g., learning rate, number of steps till convergence, etc. So, I am guessing, same learning rate etc is used in all settings, which is unclear at which settings they have been configured. But, I think that the losses may have gradients at different scale (I m not sure, but seems so). For example, L_proto in Eq.3 has the form ylogp and may have gradients that are at a different scale than Eq 5 and Eq 6 that directly take the gradient of the distances. This means that for optimal convergence, and hence fair comparison, gradients of each loss should be scaled differently. E.g. using different learning rate. Or, better, using a different weight multiplier in front of L_supp, so that the same learning rate can be used for the reconstruction loss across all settings and only scale the L_supp gradients independently. What I would normally expect is a study of the performance\u2019s sensitivity on such as hyperparameter.\n\nCon.6: \nReproducibility is not high. There is no code, and various parameters for training (learning rates, number of sgd steps, etc) are not included.\n\nCon.7: \nClarity of the paper, especially Section 3 (method) is not good. It is largely due to not good mathematical notation, which could help define certain things well, coupled with a problem in Fig 1:\n\nSec 3.1 describes the CAE architecture, which is a very small model (2 layers of conv+pool) but still I don\u2019t think it does it adequately. I am particularly confused about the description of the 1D conv filters, their width, the dimension along which the conv is applied etc. This description is done in natural language and I think it\u2019s currently ambiguous. The authors could introduce some more formal definitions of the input tensor (X \\in ??), weight tensors, the output tensor of a convolutions, define dimensionality of these tensors, which would clarify better the model\u2019s operation.\n\nMath notation should be improved. E.g. in Eq.1, C_j is not formally defined. Eq.2, p_j undefined (I think it\u2019s the prototype for class j, where in Eq.1 is C_j, while p has been previously defined as a probality).\n\nEq 3, L_proto, is a semi-supervised loss (L + U). \n\nFig 1, shows only a loss that is called \u201csemi-supervised\u201d and symbolised as L_sup, while the figure shows that this loss is only dependent on embeddings of labelled data, not those of the unlabelled data. Hence it confuses, as it looks like a supervised loss.\n\nj is sometimes a class (Sec. 3.2.1), sometimes a sample (Eq4), and vice versa for C_j. Also, C_j in 4.b. represents a set of samples, and not just a class number, as in Eq.1. Please, repass math notation and define carefully each term. The current state makes for a difficult reading experience.\n\n\\bar{C} in Eq.6 undefined.\n\n##############################################\nQuestions for rebuttal period:  \n\nPlease address the weak points I raised above, as well as the following (some are related to above anyway):\n\nCan you please clarify whether all the time-series within a database (e.g. FacesUCR) are of the same length? I guess so, but I would like a clarification. Also, please clarify this in text, e.g. in 4.1.3, as it is important for the reader to understand whether all samples have same dimension.\n\nCan you state explicitly in the text whether you are using any weighting of the SSL loss VS the reconstruction loss? I think not, but please state it explicitly. If not, please add a short discussion of whether you think the gradients of the different losses are at comparable scale, and whether such weight is needed or not.\n\nCan you explain how were configuration parameters for the methods been performed? Since there is no weighting for each loss, I am particularly interested in things like the learning rate. Was it configured on the CAE? On the CAE+SSL loss? On each one separately? On which set, since there is no validation fold? Please add to text.\n\nSec. 4.2.2 \u201c Since autoencoder updates\u2026 for comparison\u201d: Can you please clarify/rephrase what this means? Why at each epoch only? Do you mean specifically due to the SSL loss? Because in previous sentences you say that the autoencoder *still* gets updated at each batch via the reconstruction loss.\n\n#############################################\n\nMinors, or additional feedback for improving the work in the future (not subject to rebuttal):\n\nSec. 4.2.1. \u201cThis also contributes to the poor KNN performance on this dataset.\u201d. I have not noticed any mention of KNN here (at least not until later in Fig.4). Where is this comment based on? If it refers to experiments later on (Fig 4), please clarify/refer to them in text.\n\nSec. 4.2.2. \u201cThe findings here also support \u2026 Prototype loss model.\u201d: I think these results do not \u201csupport\u201d previous results, as the experiments are made on the database the same database that showed the trend in the first place. If they were made on ECG5000, they would \u201csupport\u201d another trend. I would remove this argument. What these results support is that the filter-size is not important on this database.\n\nThe authors could try to derive metrics that summarize performance across multiple settings. (e.g. \u201caverage\u201d performance across all settings in each database, or average performance across databases on a particular setting (e.g. 4 labels)). Summarized in a table? It could help define clearly which method does better.\n\nIt would be interesting to explore how the amount of unlabelled data affects results of training.\n\nIt would be interesting to have 2 sets of unlabelled data, one for training and one for evaluation. To test whether the embedding generalizes and provides good clustering on unseen data, not just the ones it was trained on, and how the 2 cases (seen unlabelled vs on unseen unlabelled) compare.\n\nSec. 4.2.1: The authors provide a description of how training evolves (\u201cDuring training, the clusters\u2026 ground-truth data\u201d) in this case of sparse labels. Perhaps in future work it would be interesting to actually provide an empirical analysis of this behaviour (e.g. showing how clustering changes between epochs? Perhaps even on a toy-dataset, in a low-dimensional embedding, to observe how the losses behave).\n\nIt would be nice to have statistical significance tests between model results in Fig 2, especially about the main claims. (E.g. to prove that Silhouete is significantly better than others)\n\nTypos and other minors:\n\nSec.1: \u201cand is not always segmented\u201d: Unclear to me what this means, perhaps rephrase a bit.\n\nSome typos: \n\nSec. 1 \u201chave been show\u201d => shown\n\nSec.1 \u201can semi-supervised\u201d => a\n\nSec. 1 \u201cmodel semi-supervised model\u201d\n\nSec. 1 \u201ca unsupervised\u201d => an\n\nSec 3.2.2 \u201cthe a partitioning\u201d\n\n\n**=========== Update to Review, after Updates to Manuscript during Rebuttal period ==========**\n\nSummary of improvements during rebuttal and remaining concerns:\n\n- Extended the literature review, with references to an unsupervised clustering method (k-shapes),  and methods that use CAE, LSTM and VAE. This is a significant improvement. The literature review still does not discuss previous losses that lead to clustering within the latent space, which is what the proposed methods here perform, and are hence closely related (see my initial comment).\n\n- Rephrased/corrected claims about performance of the method. The corresponding claims about performance is now that the methods \u201ccan usually improve k-Means clustering performance\u201d, improving the baseline CAE. This is now accurate. Yet, I do not think the performance is a sufficiently strong argument unfortunately (especially given the limited evaluation, only against few and basic baselines).\n\n- Evaluation has been expanded by employing unsupervised clustering via k-Means, k-Means+PCA, and k-Shape in Table 1. However, there is still no comparison with any other more advanced method, such as SSL based, DL based, etc (as per my initial comment). I think there is a lot of improvement here, before the paper can display improvement over the current state of the art.\n\n- No improvement on the ablation study, which is currently not particularly useful, e.g. by investigating aspects of the losses and their behaviour to provide more insights into the method, or performing empirical sensitivity-analysis to meta-parameter values.\n\n- Added clarifications about some values hyper-parameters used (Adam optim with default TF2.1 learning rate for all). There seems there was no attempt to find optimal hyper-parameter configuration for each method, in order to perform fair comparisons (e.g. each method may require different weighting/learningrate). The authors seem to acknowledge the issue and defer it to future work. However, I think that without such investigation (e.g. via an empirical study and sensitivity to these hyper-parameters), we cannot be strongly confident about conclusions on the relative performance of methods, as they can be sensitive to configuration.\n\n- Provided the code of the work as supplementary material, hence reproducibility is greatly increased. Thank you.\n\n- Improved clarity of the paper by improving Fig 1 and some of the math notation problems. \n\n\nOverall, the work has been improved during the revision, hence I will raise my rating (from 3 to 4). However, I believe the remaining problems of limited literature review, evaluation, no sensitivity analysis to hyper-parameters (or effort thereafter to configure them for each method) or other sort of empirical analysis that would give insights to the behaviour of the losses, still keep my score relatively low.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099722, "tmdate": 1606915767072, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2290/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Review"}}}, {"id": "dLrJYLAaIrc", "original": null, "number": 6, "cdate": 1606272191914, "ddate": null, "tcdate": 1606272191914, "tmdate": 1606272191914, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "j_N840cf9bn", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment", "content": {"title": "To address the specific questions raised at the end of the review:", "comment": "1. Yes, all time series within a dataset are the same length. This is a requirement of the CAE architecture. However, we note that datasets with varying lengths may still be used, but some pre-processing step would be required to normalize the lengths of the samples first.\n2. We do not use any weighting of the losses. We found that in practice the model did not have issues with either loss dominating the gradients. However, as mentioned above we are planning to explore this in extensive details using experimentation.\n3. We used the Adam optimizer as implemented in Tensorflow 2.1 for our experiments. All experiments use the default learning rate of 0.001. We add text to clarify this at Section 4.1.1\n4. Thanks for your comment. This was an error on our part. The text has been updated to reflect that the autoencoder updates at the end of each batch. The experiment is testing the gradient update strategy for the semi-supervised losses only, as the autoencoder gradient is calculated and parameters updated at each batch. In the experiment where no semi-supervised loss is in use, the performance between the two update strategies will be identical."}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0qbEq5UBfGD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2290/Authors|ICLR.cc/2021/Conference/Paper2290/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850117, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment"}}}, {"id": "j_N840cf9bn", "original": null, "number": 5, "cdate": 1606272167343, "ddate": null, "tcdate": 1606272167343, "tmdate": 1606272167343, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "-jtD34iXp9r", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment", "content": {"title": "Thank you for your time, and for your comprehensive review and constructive criticism. We are much obliged.", "comment": "* \u201cCon.1: The work is not well positioned within the literature.\u201d\n    * Thank you for your comprehensive review of these deep architectures. We have extended the Related Work Section accordingly. In particular, we add some discussion of VAE models to Section 2,  but as you mentioned our focus for this paper was primarily on how to improve the clustering ability of the CAE architecture. We have also added a discussion for examples of other uses of autoencoder architectures on time series data.\n* \u201cCon.2: Evaluation shows that no method is consistently better than the baseline AE.\u201d\n    * You are absolutely right. Our claim was not accurately worded in the original version of the paper. We have modified the wording of our claim to reflect the concerns about \u201cconsistently improving K-means performance\u201d. Also we noted that the tables in the Appendix show that Silhouette Loss does on average always outperform the CAE, with the exception of the 4-examples case on UWave data. \n* \u201cCon.3: Evaluation is limited:\u201d\n    * We agree that the experimental evaluation could be more extended. Accordingly, we have performed new experiments to compare our proposed work versus related unsupervised approaches for time series clustering. The results of these experiments are added as a new table (Table 1) which shows the performance of some existing unsupervised time series clustering approaches versus our proposed solution. We have also added some discussion of these results to the paper in Section 4.2. Given that the focus of our paper is not on autoencoding part of the proposed model, we did not feel we need to compare with a VAE based solution.\nOn the other hand regarding the existing semi-supervised solution, in the cases of the two methods discussed in the related work, we felt that Dau et al. (2016) was not scalable to large datasets or longer time series because of its reliance on DTW. Also, for He et al. (2019), the problem formulation is slightly different than ours, since the semi-supervised elements are implemented as constraints (must-link and cannot-link) between pairs of samples, rather than a set of labels. Therefore, these approaches do not seem to be comparable with our proposed work. Instead we felt that the Prototype Loss was the most comparable approach with our proposed solution, and hence, we used it as a reference for comparison in our experiments which uses traditional class labels.\n\n\n* \u201cCon.4: The hyperparameter study in Sec 4.2.2 is not really useful.\u201d\n    * We agree that the selected hyperparameters might not be most useful for all readers. Nevertheless we felt some readers might benefit from these experimental results. \n* \u201cCon.5: There is currently no mention of an attempt to configure the model and training configuration...\u201d\n    * Thank you very much for pointing out these important issues with lack of characterization of our experiments. We have updated the text in Section 4.1.1 to give more details about model training as follows: \u201cFor optimization, we use the Adam optimizer as implemented in Tensorflow 2.1.0. We use the default learning rate of lr=0.001 for all experiments, and train for 200 epochs...\u201d. We also agree that it is important to explore the weighting of the losses. However, we would like to defer this extension of the study to future work. We have included this in the list of items we have referenced in the \u201cConclusion\u201d Section of the revised paper. In particular we will explore weighting the losses and compare the scale of the gradients then.\n* \u201cCon.6: Reproducibility is not high. There is no code, and various parameters for training (learning rates, number of sgd steps, etc) are not included.\u201d\n    * Thank you for your note. As mentioned above, we have updated the text in Section 4.1.1 to give details about model training. Also we have made our code available as supplementary material, along with the three datasets from UCR Archive.\n* \u201cCon.7: Clarity of the paper, especially Section 3 (method) is not good\u201d\n    * Thank you for your note. We feel Figure 1 in the original version of the paper has caused confusion. We have updated Figure 1 as well as some of the referenced mathematical notations for clarity accordingly. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0qbEq5UBfGD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2290/Authors|ICLR.cc/2021/Conference/Paper2290/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850117, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment"}}}, {"id": "2gaoJnkOW73", "original": null, "number": 4, "cdate": 1606268977215, "ddate": null, "tcdate": 1606268977215, "tmdate": 1606268977215, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "dm5jdr3tlOU", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment", "content": {"title": "Thank you very much for your valuable review and for your time. To address your concerns:", "comment": "* \u201cIn Equation 2b, shouldn\u2019t be C_j and C'_j instead of p_j and p'_j?\u201d\n    * Yes, we have corrected the error in Equation 2b. The equation should use the prototypes C_j instead of the probabilities p_j. The number of clusters used in the optimization is identical to the number of classes in the dataset. We assume that all labeled samples will have a single label, and that labels are mutually exclusive (i.e. a sample does not belong to more than one class).\n\n* \u201cIn \u201cBoth LSTM and convolutional autoencoders have been shown to be successful at learning latent representations of time series data.\u201d Please, provide at least one reference for each type of model.\u201d\n    * Thanks for your comment. We have added references for well-known prior work utilizing these architectures in Section 2.\n* \u201cEvaluating clustering quality is a tricky task. Using only ARI might be misleading...\u201d\n    * Thank you for your comment. In earlier experiments, we evaluated both NMI and ARI, and found that the values were similar for both across a variety of datasets, so we only reported ARI for our experiments. \n* \u201cA comparison with previous methods is lacking. Although I\u2019m not aware...\u201d\n    * Thank you for your comment. We have extensively revised the Related Work Section of the paper to discuss other existing methods and compare them with our proposed work. We have also performed new experiments to compare our proposed solutions with existing solutions (please see Table 1 and Section 4.2 for more details).\n* \u201cA few typos and bad formatting...\u201d\n    * Thank you, we have corrected these issues."}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0qbEq5UBfGD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2290/Authors|ICLR.cc/2021/Conference/Paper2290/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850117, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment"}}}, {"id": "C9XFegpubmq", "original": null, "number": 3, "cdate": 1606268784806, "ddate": null, "tcdate": 1606268784806, "tmdate": 1606268784806, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "PMEYgqXAIEK", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment", "content": {"title": "Thank you very much for your valuable review. To address your concerns:", "comment": "* \u201cAs there is a labeled dataset, the reason to solve this problem as two steps is not clear to me...\u201d\n    * We would like to explain that although the datasets we used for evaluation have labels for all samples, the intended real-world use of our model is to cluster data where not all of the samples are labeled. In this use case, a domain-expert could choose a small subset of the samples to provide labels for, and the rest of the samples would remain unlabeled. We completely labeled datasets for our evaluation so that we can measure how the performance of the model changes with different amounts of labeled data. Since this may have not been explained properly in the original version of the paper, we have added some clarifying text to the beginning of Section 4.1.3 elaborating on the same: \u201cAll UCR Archive datasets are labeled, which is useful for our evaluation since we may experiment with differing amounts of labeled data. In a real-world scenario with unlabeled data, domain experts provide label information for a small subset of the data.\u201d\n* \u201cThe effect of the clustering method looks unclear. In other words, will the conclusion be held on different clustering methods, rather than K-means.\u201d\n    * Yes, the choice for the final clustering algorithm applied is left up to the user. Any existing clustering algorithm may be applied to the trained latent space, and the clusters obtained should be similar to those obtained through K-Means, since the latent space is optimized on its ability to partition the data. We noticed that this clarification was not included in the original version of the paper; thanks for your note. We have added clarifying text to Section 4.1.2: \u201cWe use the K-Means algorithm because the centroid-based nature of K-Means is a natural fit for the proposed losses. Notably, the Prototype Loss corresponds almost exactly to a K-Means objective, and both Silhouette and DB Index loss also rely on notions of cluster density around a centroid. However, any other general clustering method may be applied.\u201d\n* \u201cDifferent losses have different performance rankings in different numbers of labels and datasets...\u201d\n    * We agree that it would be ideal for us to find a universal solution. However, as we show in our experiments, it seems different datasets require different loss function solutions with different logic. That\u2019s why we feel both proposed loss functions are applicable and dominating under relevant circumstances dictated by the data requirements. In particular, in the case of our two proposed methods, we find the Silhouette loss works best generally. Silhouette loss optimizes a per-sample measure cluster label and data agreement, while DB Index loss optimizes separability between pairs of clusters. We think that DB Index may perform better in cases with smaller numbers of clusters, while Silhouette may work better with larger numbers of clusters. \n\n* \u201cMinor concerns\u201d\n    * Thank you for your note. We have addressed your concerns. In particular, we have performed new experiments to compare our proposed solutions with existing solutions that can serve as baseline solutions (please see Table 1 and Section 4.2 for more details)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0qbEq5UBfGD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2290/Authors|ICLR.cc/2021/Conference/Paper2290/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850117, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment"}}}, {"id": "nfKqtmgALF", "original": null, "number": 2, "cdate": 1606268499382, "ddate": null, "tcdate": 1606268499382, "tmdate": 1606268499382, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "PW6vdbzUuiC", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment", "content": {"title": "Thank you very much for your time and valuable review.", "comment": "To address your concerns:\n* \u201cMissing related work\u201d\n    * We have extensively revised the Related Work Section to include the missing related work, including the recommended citation.\n* \u201cNo comparison against unsupervised time-series clustering models\u201d\n    * We have performed new experiments to compare our proposed work versus related unsupervised approaches for time series clustering. The results of these experiments are added as a new table (Table 1) which shows the performance of both K-Shape and K-Means on the raw data. We have also discussed these results in Section 4.2.\n* \u201cComparison against semi-supervised methods reviewed in related work is needed.\u201d\n    * In the cases of the two methods discussed in the related work, we felt that Dau et al. (2016) was not scalable to large datasets or longer time series because of its reliance on DTW. For He et al. (2019), the problem formulation is slightly different than ours, since the semi-supervised elements are implemented as constraints (must-link and cannot-link) between pairs of samples, rather than a set of labels. We felt that the Prototype Loss was the most comparable approach with our proposed solution, and used it as a reference for comparison in our experiments.\n* \u201cEvaluation on all 100+ datasets of the UCR archive is necessary. To advance the state of the art, the method needs to show improvement in the entire benchmark and not just on three datasets, which we are not sure how exactly they were picked and why.\u201d\n    * We chose the three datasets because they have a relatively large number of samples compared to the other datasets in the UCR Archive. Other datasets in the archive would not lend themselves well as test datasets to challenge our proposed solution, therefore, not included. We have added clarifying text to Section 4.1.3 explaining the same. \u201cIn the case of trainable architecture like our proposed model, large datasets are advantageous, as larger numbers of samples will increase the quality of the latent featurization, and help to improve generalization of the features for unseen samples.\u201d"}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0qbEq5UBfGD", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2290/Authors|ICLR.cc/2021/Conference/Paper2290/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850117, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Comment"}}}, {"id": "dm5jdr3tlOU", "original": null, "number": 2, "cdate": 1603912900221, "ddate": null, "tcdate": 1603912900221, "tmdate": 1605024245766, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Review", "content": {"title": "Interesting work towards clustering time-series data", "review": "Summary:\n\nThis paper proposes a semi-supervised architecture for clustering time-series data based on Convolutional Autoencoders (AEs). The model combines the regular reconstruction loss usually employed in AEs with two new losses based on the intrinsic clustering evaluation metrics, Silhouette and DBIndex. The experiments show that this setup can achieve good clustering results (ARI) even when very few labeled examples of each class (4-28) are provided.\n\nPositive points:\n\nOverall, the presentation of the paper is good, it is well organized and easy to understand.\nThe experiments are plenty and well thought to evaluate the proposed model in three different datasets.\n\nQuestions:\n\nIn Equation 2b, shouldn\u2019t be C_j and C'_j instead of p_j and p'_j? Otherwise, what p_j means?\nIs the number of clusters defined by the number of classes? What happens if a class is originally split into multiple clusters?\n\nPoints to improve:\n\nIn \u201cBoth LSTM and convolutional autoencoders have been show to be successful at learning latent representations of time series data.\u201d Please, provide at least one reference for each type of model.\n\nEvaluating clustering quality is a tricky task. Using only ARI might be misleading, as ARI and most clustering metrics, has its problems. Therefore, I would suggest adding other metrics such as Normalized Mutual Information (NMI), Purity, and Clustering Accuracy.\n\nA comparison with previous methods is lacking. Although I\u2019m not aware of other methods for semi-supervised clustering of time-series data. There several options to compare with in the unsupervised clustering case. The closes one might be Fortuin et. al. (2019) \u201cSOM-VAE: Interpretable Discrete Representation Learning on Time Series\u201d.\n\nA few typos and bad formating to fix: \n\u201chave been show\u201c-> \"have been show\" (section 1)\n\u201can semi\u201d -> \u201ca semi\u201d (section 1)\n\u201csqunces\u201d -> \u201csequences\u201d\nfix reversed quotes as the first quote in \u201creversed\" (section 3.1)\n\u201cequation (5)\u201d -> \u201cEquation (5)\u201d (section 3.3.2)\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099722, "tmdate": 1606915767072, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2290/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Review"}}}, {"id": "PMEYgqXAIEK", "original": null, "number": 3, "cdate": 1603920485043, "ddate": null, "tcdate": 1603920485043, "tmdate": 1605024245707, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Review", "content": {"title": "Benchmarking different clustering losses in semi-supervised time series data clustering", "review": "This paper benchmarked three different existing losses, DB/Propotype/Sihouette, on time series clustering. Although there is no technique innovation, this is an interesting topic and the results look good to me. However, there are some concerns: \n\n1. I am not an expert in time series clustering, especially in the semi-supervised clustering case. As there is a labeled dataset, the reason to solve this problem as two steps is not clear to me: 1. learning the semi-supervised representation and 2. do the clustering. Why not just benchmarking by semi-supervised learning metric, like accuracy? \n\n2. The effect of the clustering method looks unclear. In other words, will the conclusion be held on different clustering methods, rather than K-means. \n\n3. Different losses have different performance rankings in different numbers of labels and datasets (Figure 2). Picking up the best and claiming the advantage does not form a fair comparison. It will be more interesting if the author can propose a method, which can consistently win other methods. \n\nMinor concerns: \n\n1. the bar plot should have space between each category.\n2. it will be more convincing if baselines from other papers are considered. \n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099722, "tmdate": 1606915767072, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2290/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Review"}}}, {"id": "PW6vdbzUuiC", "original": null, "number": 4, "cdate": 1603997506094, "ddate": null, "tcdate": 1603997506094, "tmdate": 1605024245648, "tddate": null, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "invitation": "ICLR.cc/2021/Conference/Paper2290/-/Official_Review", "content": {"title": "A simple semi-supervised framework for time-series clustering. Evaluation is not adequate to get conclusive results of advancement of the state of the art in any way", "review": "The paper presents an autoencoder-based approach for semi-supervised time-series clustering. Specifically, the paper exploits convolutional autoencoder (CAE) and integrates internal validity indexes to evaluate clustering quality in addition to small number of labeled data. Results on three datasets show improvement over the unsupervised case.\n\nPros\n\n- Well-written paper that tackles an important problem\n- Study of validity clustering indexes and their integration to current CAE architectures\n- Results on three datasets showing the potential of this methodology\n\nCons\n\n- Missing related work\n- No comparison against unsupervised time-series clustering methods\n- No comparison against semi-supervised time-series clustering methods\n- Only three datasets used despite 100+ available\n\nDetails:\n\n- The paper is missing a decade of progress in the area of unsupervised time-series clustering. \n\n[a] Paparrizos, John, and Luis Gravano. \"Fast and accurate time-series clustering.\" ACM Transactions on Database Systems (TODS) 42.2 (2017): 1-49.\n\n- Comparison against unsupervised methods such as k-means and k-shape [a] is necessary to understand the effectiveness of CAE at the first place.\n\n- Comparison against semi-supervised methods reviewed in related work is needed. \n\n- Evaluation on all 100+ datasets of the UCR archive is necessary. To advance the state of the art, the method needs to show improvement in the entire benchmark and not just on three datasets, which we are not sure how exactly they were picked and why.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2290/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2290/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Space Semi-Supervised Time Series Data Clustering", "authorids": ["~Andrew_Hill2", "katerina.kechris@cuanschutz.edu", "bowlerr@njhealth.org", "~Farnoush_Kashani1"], "authors": ["Andrew Hill", "Katerina Kechris", "Russell Bowler", "Farnoush Kashani"], "keywords": ["Semi-supervised clustering", "clustering", "deep learning", "autoencoder"], "abstract": "Time series data is abundantly available in the real world, but there is a distinct lack of large, labeled datasets available for many types of learning tasks. Semi-supervised models, which can leverage small amounts of expert-labeled data along with a larger unlabeled dataset, have been shown to improve performance over unsupervised learning models. Existing semi-supervised time series clustering algorithms suffer from lack of scalability as they are limited to perform learning operations within the original data space. We propose an autoencoder-based semi-supervised learning model along with multiple semi-supervised objective functions which can be used to improve the quality of the autoencoder\u2019s learned latent space via the addition of a small number of labeled examples. Experiments on a variety of datasets show that our methods can usually improve k-Means clustering performance. Our methods achieve a maximum average ARI of 0.897, a 140% increase over an unsupervised CAE model. Our methods also achieve a maximum improvement of 44% over a semi-supervised model.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hill|latent_space_semisupervised_time_series_data_clustering", "pdf": "/pdf/15e8258ac8dc9171b858f21c0fda82d83c3047dd.pdf", "supplementary_material": "/attachment/781ba678ddb9841276270ba94a18e1a57e290021.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=I_0LhOcRzn", "_bibtex": "@misc{\nhill2021latent,\ntitle={Latent Space Semi-Supervised Time Series Data Clustering},\nauthor={Andrew Hill and Katerina Kechris and Russell Bowler and Farnoush Kashani},\nyear={2021},\nurl={https://openreview.net/forum?id=0qbEq5UBfGD}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0qbEq5UBfGD", "replyto": "0qbEq5UBfGD", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2290/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099722, "tmdate": 1606915767072, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2290/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2290/-/Official_Review"}}}], "count": 11}