{"notes": [{"id": "BylBns0qtX", "original": "Bklq1Oh5KQ", "number": 706, "cdate": 1538087852763, "ddate": null, "tcdate": 1538087852763, "tmdate": 1545355423790, "tddate": null, "forum": "BylBns0qtX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BklsDJHXgE", "original": null, "number": 1, "cdate": 1544929123075, "ddate": null, "tcdate": 1544929123075, "tmdate": 1545354491749, "tddate": null, "forum": "BylBns0qtX", "replyto": "BylBns0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Meta_Review", "content": {"metareview": "This paper shows experiments in favor of learning and using heteroscedastic noise models for differentiable Bayes filter. Reviewers agree that this is interesting and also very useful for the community. However, they have also found plenty of issues with the presentation, execution and evaluations shown in the paper. Post rebuttal, one of the reviewer increased their score, but the other has reduced the score. Overall, the reviewers are in agreement that more work is required before this work can be accepted.\n\nSome of existing work on variational inference has not been included which, I agree, is problematic. Simple methods have been compared but then why these methods were chosen and not the other ones, is not completely clear. The paper definitely can improve on this aspect, clearly discussing relationships to many existing methods and then picking important methods to clearly bring some useful insights about learning heteroscedastic noise. Such insights are currently missing in the paper.\n\nReviewers have given many useful feedback in their review, and I believe this can be helpful for the authors to improve their work. In its current form, the paper is not ready to be accepted and I recommend rejection. I encourage the authors to resubmit this work.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Interesting but not good enough."}, "signatures": ["ICLR.cc/2019/Conference/Paper706/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper706/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353116758, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBns0qtX", "replyto": "BylBns0qtX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper706/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper706/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper706/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353116758}}}, {"id": "Bkxqg9aD27", "original": null, "number": 1, "cdate": 1541032433903, "ddate": null, "tcdate": 1541032433903, "tmdate": 1544446215382, "tddate": null, "forum": "BylBns0qtX", "replyto": "BylBns0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Review", "content": {"title": "Good exploration of optimizing Bayesian filter noise variance through back propagation, but with incomplete results", "review": "This is a well written paper which proposes to learn heteroscedastic noise models from data by optimizing the prediction likelihood end-to-end through differentiable Bayesian Filters. In addition to existing Bayesian filters, the paper also proposes two different versions of the [differentiable] Unscented Kalman Filter. Performance of the different filters and noise models is evaluated on two real-world robotic problems: Visual Odometry and visual tracking of an object pushed by the robot.\nWhile the general idea of learning the noise variances through backpropagation are straightforward extensions of existing work on differential Bayesian filters, the questions that the paper explores are important to make end-to-end learning of Bayesian filter more common. The results will help future research select the correct differential filter for their use case, and insight in potential benefits (or lack thereof) by learning heteroscedastic or homoscedastic process noise, and/or observation noise.\nA downside is that the paper does not further explore how to weigh different loss terms which are apparently important to successfully train such models. Also unfortunate is the footnote which states that the current results are incomplete and will be updated, hence as a reviewer I am not sure which results and conclusions are valid right now.\n\n\nPros:\n+ clearly written\n+ useful experiments for those seeking to select a differential Bayesian filter, and learning (heteroscedastic) noise from data.\n+ experiments on real-world use cases rather than toy problems\n\nCons:\n- Incomplete experiments according to footnote, thus results and conclusions might change after this review.\n- Unclear what the effect of the selected process / observation model is on the learned noise\n\n\nBelow are more detailed comments and questions:\n* p6. Footnote: \"due to time constraints, ..., results will be updated\" Is this acceptable? I have never seen such a notice when reviewing. So, are the current results on a single fold? Will the numbers in the tables, or the conclusions change after this review?\n* If I understand correctly, the paper 'only' focuses on learning the heteroscedastic noise variance, but assumes that the deterministic non-linear parts of the process and observation models are fixed. I did not find this very clearly stated in the paper, though at least the Appendix explicitly states the used functions for the process models.\n* I would have liked to see in the paper more explanation on how the process and observations models were selected and validated  in the experiments, since I expect that the validity of these functions affects the learned noise variances. Since the noise needs to account for the inaccuracies in the deterministic models, would the choice for these functions not impact your conclusions? And, would it or would it not be possible to learn both these deterministic models and the noise jointly from the training data?\n* Is it possible to add priors on Q and R parameters for Bayesian treatment of learning model parameters? I can imagine that priors can guide the optimization to either adjust more of the Q or more of the R variance to improve the likelihood.\n\n* Section 1:\n\t* \"Our experiments show that ... \" This may be a matter of taste, but I did not expect to see the main conclusions already in the introduction. They should appear in the abstract to help out the quick reader. In the introduction, it appears as if you are talking about some separate preliminary experiments, and which you base some conclusions that will be used in the remainder of this paper.\n\n* Section 3:\n\t* So, mostly empirical study, since heteroscedastic noise models were already used?\n\t* \"Previous work evaluated ... \" please add citations\n\n* Section 4.1:\n\t* \"train a discriminative neural network o with parameters wo to preprocess the raw sensory data D and thus create a more compact representation of the observations z = o(D;wo).\" At this point in the paper, I don't understand this. How is z learned, via supervised learning (what is the target value for z)? Or is z some latent representation that is jointly optimized with the filters? This only became somewhat clearer in Sec. 5.2 on p.8 where it states that \"We ... train a neural network to extract the position of the object, the contact point and normal as well as ...\". So if I understand correctly, the function o for z = o(D) is thus learned offline w.r.t. some designed observation variables for which GT is available (from manual annotations?).\n\n* Section 4.2:\n\t* \"we predict a separate Qi for every sigma point and then compute Q as the weighted mean\" \u2192 So, separate parameters w_g for each sigma point i, or is a single learned non-linear function applied to all points?\n\n* Section 4.3:\n\t* Equation 14: inconsistent use of boldface script: should use bold sigma_t, and bold l_t ?\n\t* \"In practice, we found that during learning ... by only increasing the predicted variance\" \u2192  This is an interesting observation, which I would have liked to see explored more. I understand that term (ii) is needed to guide the learning processes, but in the end wouldn't we want to optimize the actual likelihood? So, could you (after the loss with (ii) converged) reduce \\lambda_2 to zero to properly optimize only the log likelihood without guidance from a good initial state? Or is it not possible to reliably optimize the likelihood via back-propagation at all from some reason?\n\n* Section 5.1.1\n\t* \"... of varying length (from 270 to over 4500 steps) ...\" it would be good to mention the fps, to get understand to what real-world time horizons 50 / 100 frames correspond.\n\n* Section 5.1.2:\n\t* Table 1: How are the parameters of the filters in the \"no learning\" column obtained? Are these tuned in some other way, or taken form existing implementations? Also, can you clarify if the 'no learning' parameters served as the initial condition for the learning approaches?\n\t* Table 1, first row column Q+R: \"0.2\" \u2192 Is there a missing zero here, i.e. \"0.20\"? Otherwise, the precision of reported results in this table is not consistent. Hard to say: is the mean of R+Q 0.2, and slightly lower than R+Qh, or could it be as high as 0.24 ?\n\t* \"learning a heteroscedastic process noise model leads to big improvements and makes the filters competitive with the EKF\". Results for EKF still appear significantly better than the novel UKF, and even the PF (especially rotational error).\n\n* Section 6: \n\t* \"Large outliers in the prediction of the preprocessing networks were not associated with higher observation noise.\" I don't see on what presented results these conclusions were drawn, as this is the first time the word \"outlier\" is mentioned in the paper. Outliers seem indeed important, as they contradict the typical assumptions e.g. of Gaussian noise, so it would be useful to clarify how the proposed techniques handle such outliers.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper706/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Review", "cdate": 1542234398350, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylBns0qtX", "replyto": "BylBns0qtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335785014, "tmdate": 1552335785014, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1gBt34n3X", "original": null, "number": 3, "cdate": 1541323901116, "ddate": null, "tcdate": 1541323901116, "tmdate": 1543388346447, "tddate": null, "forum": "BylBns0qtX", "replyto": "BylBns0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Review", "content": {"title": "Nice study which (sadly) ignores large parts of the related work", "review": "# Review for \"On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters\"\n\nThe method revisits Bayes filters. It evaluates the benefit of training the observation and process noise models, while keeping all other models fixed. Experimentally, a clear performance boost is verified if heteroscedastic noise is used.\n\nFirst, I want to applaud the effort done to do the study. I think it is very beneficial for the community to revisit classic algorithms and evaluate them in a broader and more recent context. I certainly will revisit this article and point colleagues to it. \n\nThe paper is well-written and the experiments seems to be well done. The review of the relevant models is adequate, although space filling, since the methodology  is not at the core of ICLR. I however consider it highly relevant for the future of the field.\n\nHowever, there is a major flaw: the variational state-space model literature is completely ignored. I consider this blank spot unacceptable. Especially, the models proposed have already explored heteroskedastic noise models in contexts where state-space models and posterior approximations were fully trained. It is just that an ablation study was never done.\n\nI am very torn, as I like the paper in general but think that the recognition of the variational SSM literature needs to be added, and not having it in here would foster a separation of two \"micro communities\".\n\nHere is an incomplete list of articles, which can serve as starting points for a more thorough literature review.\n\n- Archer, E., Park, I. M., Buesing, L., Cunningham, J., & Paninski, L.\n(2015). Black box variational inference for state space models. arXiv preprint arXiv:1511.07367.\n- Fraccaro, M., S\u00f8nderby, S. K., Paquet, U., & Winther, O. (2016). Sequential neural models with stochastic layers. In Advances in neural information processing systems (pp. 2199-2207).\n- Karl, M., Soelch, M., Bayer, J., & van der Smagt, P. (2016). Deep  variational bayes filters: Unsupervised learning of state space models from rawdata. ICLR 2017.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper706/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Review", "cdate": 1542234398350, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylBns0qtX", "replyto": "BylBns0qtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335785014, "tmdate": 1552335785014, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1x3H8msRX", "original": null, "number": 6, "cdate": 1543349827690, "ddate": null, "tcdate": 1543349827690, "tmdate": 1543349827690, "tddate": null, "forum": "BylBns0qtX", "replyto": "BylBns0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "content": {"title": "Comment on the revised version", "comment": "We want to again thank our reviewers for their helpful and kind feedback. This comment provides a short summary of our contributions and the changes we made in the revised version of the paper.\n\nContributions:\nIn this work, we analyzed the advantages of learning heteroscedastic models of observation and especially process noise through differentiable bayesian filters. \nFor this, we evaluated training the noise models through 4 different filtering algorithms: Differentiable versions of Extended Kalman Filter and Particle Filter had already been proposed in related work (Haarnoja et al., 2016, Jonschkowski et al., 2018, Karkus et al., 2018), but we also added  differentiable versions of two Unscented Kalman Filter variants. In addition to comparing within the different filters, we also evaluated learning the noise models on two different tasks: The Kitti Visual Odometry task has a low-dimensional state and smooth dynamics. In contrast, the planar pushing task has discontinuous dynamics and is known to follow a heteroscedastic noise model. It also has a much higher-dimensional state than any other task evaluated in related work.\n\nSummary of revisions:\n- Minor changes following the questions of our reviewers to clarify things or add information\n- Following the suggestion of Reviewer 1, we added a section about variational methods for learning in state space models to the related works section. They offer an alternative approach to learning as compared to our method of backpropagation through differentiable filtering algorithms. Experiments that compare both approaches would be very interesting, but could not be carried out within the rebuttal period.\n- We updated the results for the Kitti experiments to use the full testset available. To improve the comparability of the obtained results with related work (Haarnoja et al., 2016, Jonschkowski et al., 2018), we changed the experimental setting to allow for finetuning of the perception network during training. This improved the overall results on the full testset, but did not change the conclusions drawn from the experiment. We also rewrote the discussion of the results to better explain the differences in performance between the different filtering algorithms.\n\nReferences:\nTuomas Haarnoja, Anurag Ajay, Sergey Levine, and Pieter Abbeel. Backprop kf: Learning discriminative deterministic state estimators. In Advances in Neural Information Processing Systems. 2016\n\nRico Jonschkowski, Divyam Rastogi, and Oliver Brock. Differentiable particle filters: End-to-end learning with algorithmic priors. In Proceedings of Robotics: Science and Systems, Pittsburgh, USA, 2018.\n\nPeter Karkus, David Hsu, and Wee Sun Lee. Particle filter networks: End-to-end probabilistic localization from visual observations. 2018"}, "signatures": ["ICLR.cc/2019/Conference/Paper706/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614061, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBns0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper706/Authors|ICLR.cc/2019/Conference/Paper706/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614061}}}, {"id": "B1enjzWvCm", "original": null, "number": 5, "cdate": 1543078564451, "ddate": null, "tcdate": 1543078564451, "tmdate": 1543078564451, "tddate": null, "forum": "BylBns0qtX", "replyto": "Bkxqg9aD27", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "content": {"title": "Response (3 /3)", "comment": "* Section 6: \n\t* \"Large outliers in the prediction of the preprocessing networks were not associated with higher observation noise.\" I don't see on what presented results these conclusions were drawn, as this is the first time the word \"outlier\" is mentioned in the paper. Outliers seem indeed important, as they contradict the typical assumptions e.g. of Gaussian noise, so it would be useful to clarify how the proposed techniques handle such outliers.\n\nReply: \nAgreed, we tried to make it clearer what we meant by outliers.\nIn this case, outliers were mostly meant to mean \"unusually bad predictions\" especially of the object position in the pushing task. An important point here is that on the pushing task there is no structural explanation for the bad predictions (such as for example occlusions). Therefore we do not think that they actually violate a gaussian assumption about the observation noise.\n \nOn the question of how the method handles outliers: \nThe idea behind using a heteroscedastic noise model is that it allows to assign different levels of noise to different inputs. For example, if the object is occluded in the image, a high observation noise can be predicted. This flags the observations in this timestep as unreliable, such that the filters rely more on the process model prediction.\nSuch outliers would indeed violate a global gaussian assumption about the observation noise. To alleviate this, our method instead learns input-dependent \"local\" noise distributions. This allows it to capture e.g. the noise in the unoccluded case with one distribution and the prediction errors in the case of occlusion with another one. \n\n\nReferences:\n\nRico Jonschkowski, Divyam Rastogi, and Oliver Brock. Differentiable particle filters: End-to-end learning with algorithmic priors. In Proceedings of Robotics: Science and Systems, Pittsburgh, USA, 2018.\n\nAlina Kloss, Stefan Schaal, and Jeannette Bohg. Combining learned and analytical models for\npredicting action effects. arXiv preprint arXiv:1710.04102, 2017.\n\nSubham Sahoo, Christoph Lampert and Georg Martius. Learning Equations for Extrapolation and Control. In Proceedings of the 35th International Conference on Machine Learning, PMLR 80:4442-4450, 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper706/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614061, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBns0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper706/Authors|ICLR.cc/2019/Conference/Paper706/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614061}}}, {"id": "SyxX8G-w0m", "original": null, "number": 4, "cdate": 1543078474840, "ddate": null, "tcdate": 1543078474840, "tmdate": 1543078537498, "tddate": null, "forum": "BylBns0qtX", "replyto": "Bkxqg9aD27", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "content": {"title": "Response (2/3)", "comment": "* Section 4.1: [...] How is z learned, via supervised learning (what is the target value for z)? \nOr is z some latent representation that is jointly optimized with the filters? [..] \nSo if I understand correctly, the function o for z = o(D) is thus learned offline w.r.t. some designed observation variables for which GT is available (from manual annotations?).\n\nReply: \nYes, z is a subset of the quantities in the state x, all those which can be observed from the input images. This means that we have ground truth values for training z in a supervised way, without providing any additional annotations beyond the ground truth states. We tried to make this more clear in the updated version.\n\n* Section 4.2: \"we predict a separate Qi for every sigma point and then compute Q as the weighted mean\"\nSo, separate parameters w_g for each sigma point i, or is a single learned non-linear function applied to all points?\n\nReply:\nThe network for predicting the Qi is the same for every sigma point (shared weights w_g). However, in the UKF the sigma points have associated weights that are used for computing mean and covariance of the approximate posterior. We use these \"per sigma point\" weights to compute one Q from the different Qi. (In the MCUKF, those weights are uniform)\n\n* Section 4.3: [\u2026] \nThis is an interesting observation, which I would have liked to see explored more. I understand that term (ii) is needed to guide the learning processes, but in the end wouldn't we want to optimize the actual likelihood? So, could you (after the loss with (ii) converged) reduce \\lambda_2 to zero to properly optimize only the log likelihood without guidance from a good initial state? Or is it not possible to reliably optimize the likelihood via back-propagation at all from some reason?\n\nReply: \nAs said above, we agree that more experiments on this would be very interesting. Our intuition was that once the training is in a \"good\" state, term (ii) would be small, such that the likelihood would be the main driver for the training even without changing the weighting of the loss terms.\n\n* Section 5.1.1 [...] it would be good to mention the fps, to get understand to what real-world time horizons 50 / 100 frames correspond.\n\nReply: \nFor the kitti-dataset, images were recorded with a frequency of 10Hz, so 100 frames correspond to 10 seconds of driving. We included this information in the updated version.\n\n* Section 5.1.2:\n\t* Table 1: How are the parameters of the filters in the \"no learning\" column obtained? Are these tuned in some other way, or taken form existing implementations? Also, can you clarify if the 'no learning' parameters served as the initial condition for the learning approaches?\n\nReply: \nThe noise values for linear and angular velocity (the 2 last components) were chosen based on the ground-truth standard deviation of one set of the kitti data. \nFor the position and heading, we could think of no obvious way of choosing the values based on data and thus set them to identity, which is a common choice for the first values to try out. \nAnd yes, we used these values as initial conditions in the learning settings, by adding a bias term with a strong weight decay.\n\n\t* Table 1, first row column Q+R: \"0.2\" \u2192 Is there a missing zero here, i.e. \"0.20\"? Otherwise, the precision of reported results in this table is not consistent. \n\nReply: \nSorry for the confusion. Yes, the notation was meant to convey 0.20, we changed it. \n\n\t* \"learning a heteroscedastic process noise model leads to big improvements and makes the filters competitive with the EKF\". Results for EKF still appear significantly better than the novel UKF, and even the PF (especially rotational error).\n\nReply: \nTrue, there is also no reason why the (MC)UKF or PF should have any advantage on the kitti task: For once, the process model is not \"heavily\" nonlinear, such that the EKF is a good choice. \nThe main problem here is that the sampling and the sigma points generate additional uncertainty about the state-estimate. Usually, this would be resolved by observations, such that more weight can be given to the particles/sigma points that are closer to the true state. But in the visual odometry task, there are no observations of heading and position. Varying those parts of the state thus only adds uncertainty, but does not help.  We updated the results section to better explain this.\n\nHowever, (Jonschkowski et al., 2018) obtained much better results with their version of the particle filter. The main difference to our version is that their perception model directly predicts likelihoods from the observations and the particles. This approach seems to allow for a better weighting of the particles. (We chose to use the same perception model as for the other filters to allow for better comparability between them)"}, "signatures": ["ICLR.cc/2019/Conference/Paper706/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614061, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBns0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper706/Authors|ICLR.cc/2019/Conference/Paper706/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614061}}}, {"id": "rkgsGzWwRX", "original": null, "number": 3, "cdate": 1543078419075, "ddate": null, "tcdate": 1543078419075, "tmdate": 1543078419075, "tddate": null, "forum": "BylBns0qtX", "replyto": "Bkxqg9aD27", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "content": {"title": "Response (1/3)", "comment": "Thanks a lot for the positive and detailed review. The suggested experiments about weighting \nthe different loss terms are a very interesting direction which we would like to explore further. However, due to the rather long training time of such models, we can not include such experiments within the rebuttal period.\n\n\nReplies to your questions:\n\n* p6. Footnote [...] So, are the current results on a single fold? Will the numbers in the tables, or the conclusions change after this review?\n\nReply: \nThe footnote was poorly worded, sorry for this. The results were from a 4-fold cross-validation instead of the full 9-fold cross validation that can be done when using all sequences. \n\nWe updated the results. The reason for the long delay is that we decided to allow for finetuning of the perception model for the kitti experiments to improve the comparability to related work, which required rerunning the experiments. This does not affect the conclusion, but improves the overall results.\n\n* If I understand correctly, the paper 'only' focuses on learning the heteroscedastic noise variance, but assumes that the deterministic non-linear parts of the process and observation models are fixed. \nI did not find this very clearly stated in the paper, though at least the Appendix explicitly states the used functions for the process models.\n\nReply: \nThis is correct. We tried to make this clearer in the introduction and the experimental section.\n\n* I would have liked to see in the paper more explanation on how the process and observations models were selected and validated [...] Since the noise needs to account for the inaccuracies in the deterministic models, would the choice for these functions not impact your conclusions? And, would it or would it not be possible to learn both these deterministic models and the noise jointly from the training data?\n\nReply: \nThe perception networks were pretrained on ground truth data and selected to have a small\nprediction error. For the kitti-task, we followed the implementation of (Jonschkowski et al., 2018) for the prediction network architecture as well as the process model for comparability.\nOn the pushing task, we selected an architecture similar to (Kloss et al., 2017) for the perception network. The analytical model we used as a process model was also evaluated in this work. \n\nWe agree that the choice of those models also impacts the absolute results, as does the specific task at hand (as can be seen from the difference between the kitti and the pushing results). We however believe that the chosen models capture the real process close enough to not influence the conclusion. How the method behaves with worse models would be an interesting question for future work.\n\nConcerning the question if all could be learned jointly: For the kitti task, (Jonschkowski et al., 2018) demonstrated that it is possible to learn everything jointly from scratch. However, they reported better results when pretraining the models first and then finetuning through the filter. \nWe did not try it on the pushing task, which we expect to be more difficult to learn from scratch, as the state and observations have more dimensions. In general, the results found in e.g.\n(Kloss et al., 2017) and (Sahoo et al., 2018)  also suggest that learning process models might not be desirable if a suitable analytical model is available, due to limited generalization abilities of neural networks. \n\n* Is it possible to add priors on Q and R parameters for Bayesian treatment of learning model parameters? I can imagine that priors can guide the optimization to either adjust more of the Q or more of the R variance to improve the likelihood.\n\nReply:\nAdding priors for a bayesian treatment of the noise models is as far as we know not trivially \npossible within the framework of filtering. For this, a variational approach might be better suited.\nIn our approach, we do however supply a form of prior on the value of the covariance matrices in the form of an initialization bias.  \n\n\n* Section 3: So, mostly empirical study, since heteroscedastic noise models were already used?\n\nReply: \nTrue, it is mostly an ablation study of concepts that were already proposed in other work but not evaluated in isolation. However, in contrast to previous work, we actually evaluate the heteroscedastic noise model on a task that is known to follow heteroscedastic process noise (pushing). In addition, we proposed the two differentiable UKF variants and did a comparison of the different filtering techniques on two different tasks."}, "signatures": ["ICLR.cc/2019/Conference/Paper706/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614061, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBns0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper706/Authors|ICLR.cc/2019/Conference/Paper706/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614061}}}, {"id": "rJl-GybDAQ", "original": null, "number": 2, "cdate": 1543077641076, "ddate": null, "tcdate": 1543077641076, "tmdate": 1543077641076, "tddate": null, "forum": "BylBns0qtX", "replyto": "SkxvJDW937", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "content": {"title": "Response", "comment": "Thanks a lot for the helpful review. We agree that the suggested baseline experiment would be very useful and will try to add it in future versions. We can unfortunately not include it within the rebuttal period.\n\nReplies to your questions:\n\n* In the UKF the Julier paper of 1997 also notes a heuristic solution for ensuring positive \ndefiniteness of the estimated covariance matrix is lambda is negative. Was this tried?\n\nReply: \nNo, we did not try this. However, another main problem that we found with negative lambda (while lambda + n is positive) is that the sigma point for the mean is given negative weight while the other points receive positive weights. This can make the mean estimate diverge on its own and also does not make a lot of sense intuitively. We added this in the updated version.\nIn general we would therefore recommend using positive lambda to avoid both problems.\n\n\n* How was the number of particles selected for the PF at test time?  In particular, how did the computation time between the methods compare?\n\nReply:\nWe chose the number of test particles to be the same as was used in (Jonschkowski et al., 2018) to ensure comparability on the kitti task.  \nAlthough we did not attempt timing experiments, the test-time did not seem to increase much\nbetween 100 and 1000 particles, as long as the computations for each particle can still be run in parallel. This is of course dependent on the GPU in use. Due to this parallelism, there also seems to be no big difference between the computation times for the different filtering methods.\n\n\nReferences:\n\nRico Jonschkowski, Divyam Rastogi, and Oliver Brock. Differentiable particle filters: End-to-end learning with algorithmic priors. In Proceedings of Robotics: Science and Systems, Pittsburgh, USA, 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper706/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614061, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBns0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper706/Authors|ICLR.cc/2019/Conference/Paper706/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614061}}}, {"id": "H1xwa0lvC7", "original": null, "number": 1, "cdate": 1543077566557, "ddate": null, "tcdate": 1543077566557, "tmdate": 1543077566557, "tddate": null, "forum": "BylBns0qtX", "replyto": "B1gBt34n3X", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "content": {"title": "Response", "comment": "Thanks a lot for the positive and encouraging feedback on our work! We highly appreciate the pointer \nto the variational bayes methods and agree that not mentioning them was an oversight on our part. We have updated the related works section accordingly. \nIn future work, it would be very interesting to see how our approach compares to a variational method. Running such experiments is however not possible during the rebuttal period."}, "signatures": ["ICLR.cc/2019/Conference/Paper706/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614061, "tddate": null, "super": null, "final": null, "reply": {"forum": "BylBns0qtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper706/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper706/Authors|ICLR.cc/2019/Conference/Paper706/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers", "ICLR.cc/2019/Conference/Paper706/Authors", "ICLR.cc/2019/Conference/Paper706/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614061}}}, {"id": "SkxvJDW937", "original": null, "number": 2, "cdate": 1541179103272, "ddate": null, "tcdate": 1541179103272, "tmdate": 1541533756584, "tddate": null, "forum": "BylBns0qtX", "replyto": "BylBns0qtX", "invitation": "ICLR.cc/2019/Conference/-/Paper706/Official_Review", "content": {"title": "Small novelty with insufficient novelty", "review": "This paper presents a method to learn and use state and observation dependent noise in traditional Bayesian filtering algorithms.  For observation noise, the approach consists of constructing a neural network model which takes as input the raw observation data and produces a compact representation and an associated (diagonal) covariance.  Similarly for state process noise, a network predicts the (diagonal) covariance of the temporal process given the current state.\n\nThe paper notes that these noise models can be trained end-to-end by instantiating an (approximate) Bayesian filter.  In particular, they explore the use of a Kalman Filteer, Extended Kalman Filter, (Monte Carlo and regular) Unscented Kalman Filter and a Particle Filter.\n\nThe technique is applied to two different tasks, visual odometry on the KITTI dataset and a \"planar pushing\" task.  The results show that the addition of a learned noise model made no significant difference on the KITTI dataset, with the EKF without learning performing as well as any of the other variations.  The planar pushing task has a higher dimensional state space and more challenging noise dynamics.  In that case some gains are seen with learning.\n\nOverall the contribution of this paper seems small and the experimenal results insufficient.  The observation that gradient based training can be done through a Bayesian filter, as the paper pointed out, was developed elsewhere.  Extending that to a more complex noise model seems like a rather small contribution.  Indeed, the observational noise component was not found to have a significant or consistent impact and hence only the process noise is particularly notable.  Further, at least one obvious and important baseline was missing.  Specifically, process noise models could be trained independently by simply maximizing the likelihood of the next predicted state.  It's not clear that there's a significant benefit to training the model end-to-end in this case.  There may well be, but that is something that should be demonstrated.\n\nA number of other, smaller issues:\n - Eq (4) should be written as a matrix inverse, not a fraction.\n - In the UKF the Julier paper of 1997 also notes a heuristic solution for ensuring positive definiteness of the estimated covarance matrix is lambda is negative.  Was this tried?\n - How was the number of particles selected for the PF at test time?  In particular, how did the computation time between the methods compare?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper706/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters", "abstract": "In many robotic applications, it is crucial to maintain a belief about the state of \na system, like the location of a robot or the pose of an object.\nThese state estimates serve as input for planning and decision making and \nprovide feedback during task execution. \nRecursive Bayesian Filtering algorithms address the state estimation problem,\nbut they require a model of the process dynamics and the sensory observations as well as \nnoise estimates that quantify the accuracy of these models. \nRecently, multiple works have demonstrated that the process and sensor models can be \nlearned by end-to-end training through differentiable versions of Recursive Filtering methods.\nHowever, even if the predictive models are known, finding suitable noise models \nremains challenging. Therefore, many practical applications rely on very simplistic noise \nmodels. \nOur hypothesis is that end-to-end training through differentiable Bayesian \nFilters enables us to learn more complex heteroscedastic noise models for\nthe system dynamics. We evaluate learning such models with different types of \nfiltering algorithms and on two different robotic tasks. Our experiments show that especially \nfor sampling-based filters like the Particle Filter, learning heteroscedastic noise \nmodels can drastically improve the tracking performance in comparison to using \nconstant noise models.", "keywords": ["bayesian filtering", "heteroscedastic noise", "deep learning"], "authorids": ["alina.kloss@tuebingen.mpg.de", "bohg@stanford.edu"], "authors": ["Alina Kloss", "Jeannette Bohg"], "TL;DR": "We evaluate learning heteroscedastic noise models within different Differentiable Bayes Filters", "pdf": "/pdf/aa4931d30bb6e63253fa62ac019e7630dea53cfa.pdf", "paperhash": "kloss|on_learning_heteroscedastic_noise_models_within_differentiable_bayes_filters", "_bibtex": "@misc{\nkloss2019on,\ntitle={On Learning Heteroscedastic Noise Models within Differentiable Bayes Filters},\nauthor={Alina Kloss and Jeannette Bohg},\nyear={2019},\nurl={https://openreview.net/forum?id=BylBns0qtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper706/Official_Review", "cdate": 1542234398350, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BylBns0qtX", "replyto": "BylBns0qtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper706/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335785014, "tmdate": 1552335785014, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper706/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}