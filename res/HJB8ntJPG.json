{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124435948, "tcdate": 1518472269430, "number": 314, "cdate": 1518472269430, "id": "HJB8ntJPG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HJB8ntJPG", "signatures": ["~Frederik_Pahde1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Multi-Modal Few-Shot Learning: A Benchmark", "abstract": "The state-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance. To this end, we propose a multi-modal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multi-modal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multi-modal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose an framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strategy for sample selection.  Experiments on our proposed benchmark demonstrate that learning generative models in a cross-modal fashion facilitates few-shot learning by compensating the lack of data in novel categories.", "paperhash": "pahde|multimodal_fewshot_learning_a_benchmark", "keywords": ["Few-Shot Learning", "Multi-Modal", "Fine-grained Recognition", "Meta-Learning"], "_bibtex": "@misc{\n  pahde2018multi-modal,\n  title={Multi-Modal Few-Shot Learning: A Benchmark},\n  author={Frederik Pahde and Moin Nabi and Tassilo Klein},\n  year={2018},\n  url={https://openreview.net/forum?id=HJB8ntJPG}\n}", "authorids": ["frederik.pahde@sap.com", "m.nabi@sap.com", "tassilo.klein@sap.com"], "authors": ["Frederik Pahde", "Moin Nabi", "Tassilo Klein"], "TL;DR": "We propose a benchmark for few-shot learning in multi-modal scenarios in conjunction with an approach including a discriminative text-conditional GAN for cross-modal sample generation with a simple self-paced strategy for sample selection.", "pdf": "/pdf/0fbb6e567be07c116da5b87159b723aa1ec34160.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582862867, "tcdate": 1520557672648, "number": 1, "cdate": 1520557672648, "id": "ByZO0IJFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper314/Official_Review", "forum": "HJB8ntJPG", "replyto": "HJB8ntJPG", "signatures": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer3"], "content": {"title": "interesting idea", "rating": "6: Marginally above acceptance threshold", "review": "Though the idea of this paper has been explored in some degree in previous works, I still think the idea of this paper is interesting and novel enough", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Modal Few-Shot Learning: A Benchmark", "abstract": "The state-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance. To this end, we propose a multi-modal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multi-modal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multi-modal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose an framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strategy for sample selection.  Experiments on our proposed benchmark demonstrate that learning generative models in a cross-modal fashion facilitates few-shot learning by compensating the lack of data in novel categories.", "paperhash": "pahde|multimodal_fewshot_learning_a_benchmark", "keywords": ["Few-Shot Learning", "Multi-Modal", "Fine-grained Recognition", "Meta-Learning"], "_bibtex": "@misc{\n  pahde2018multi-modal,\n  title={Multi-Modal Few-Shot Learning: A Benchmark},\n  author={Frederik Pahde and Moin Nabi and Tassilo Klein},\n  year={2018},\n  url={https://openreview.net/forum?id=HJB8ntJPG}\n}", "authorids": ["frederik.pahde@sap.com", "m.nabi@sap.com", "tassilo.klein@sap.com"], "authors": ["Frederik Pahde", "Moin Nabi", "Tassilo Klein"], "TL;DR": "We propose a benchmark for few-shot learning in multi-modal scenarios in conjunction with an approach including a discriminative text-conditional GAN for cross-modal sample generation with a simple self-paced strategy for sample selection.", "pdf": "/pdf/0fbb6e567be07c116da5b87159b723aa1ec34160.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582862677, "id": "ICLR.cc/2018/Workshop/-/Paper314/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper314/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper314/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper314/AnonReviewer1"], "reply": {"forum": "HJB8ntJPG", "replyto": "HJB8ntJPG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper314/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper314/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582862677}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582795675, "tcdate": 1520626165297, "number": 2, "cdate": 1520626165297, "id": "Bk6lcPlKf", "invitation": "ICLR.cc/2018/Workshop/-/Paper314/Official_Review", "forum": "HJB8ntJPG", "replyto": "HJB8ntJPG", "signatures": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer2"], "content": {"title": "ICLR Workshop Official Review", "rating": "5: Marginally below acceptance threshold", "review": "Summary: This paper paper calls itself \u201cmulti-modal few-shot learning\u201d, but I think is more clearly summarised as addressing the task of few+zero-shot learning. By this I mean it addresses the scenario where recognisers for new categories are to be induced, and the available data is twofold: (i) a small number of training examples (few-shot assumption), and (ii) some metadata about the category in the form of a text descriptions of images of that category (text metadata is typical zero-shot assumption). The general idea is to train a text-conditional GAN to generate synthetic images of the new category, and these are used to augment the few-shot images, and thus train a more robust recognizer, and authors propose a couple improvements (class-discriminator, simple curriculum) to improve this. The results show it is better than naive few-shot learning. \n\nNovelty: Borderline. The idea of combining few+zero shot settings has been done in some prior work ([A], among others), so the problem setting is not quite novel. The approach of data augmentation by GAN has been done in vision [B,C,D]. \nClarity: Personally, I find the  spin of \u201cmulti-modal few shot\u201d misleading. Then you expect the final model to be one that works on multi-modal data, but the final model only works on vision data. So I think the few+zero-shot explanation would be clearer.\nSignificance: Hard to know the empirical significance as comparison is very light. Existing state of the art few-shot and state of the art zero-shot methods are not compared. But it looks like the proposed few+zero shot method is much worse than prior methods that only use one of these cues, e.g., [E].\nQuality: Would be better with some more controls like text-only baseline to complement image-only baseline.\n\n[A] https://arxiv.org/abs/1801.09086\n[B] https://arxiv.org/abs/1611.01331\n[C] https://arxiv.org/abs/1711.00648\n[D] https://arxiv.org/abs/1701.07717\n[E] https://arxiv.org/abs/1711.06025\n\nPros/Cons:\n+ Overall the paper is a reasonable idea which works to some extent.\n- But it\u2019s not very surprising. The novelty is borderline, and the empirical results are not great.\n- Also the spin of the presentation obfuscates the problem setting really addressed.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Modal Few-Shot Learning: A Benchmark", "abstract": "The state-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance. To this end, we propose a multi-modal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multi-modal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multi-modal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose an framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strategy for sample selection.  Experiments on our proposed benchmark demonstrate that learning generative models in a cross-modal fashion facilitates few-shot learning by compensating the lack of data in novel categories.", "paperhash": "pahde|multimodal_fewshot_learning_a_benchmark", "keywords": ["Few-Shot Learning", "Multi-Modal", "Fine-grained Recognition", "Meta-Learning"], "_bibtex": "@misc{\n  pahde2018multi-modal,\n  title={Multi-Modal Few-Shot Learning: A Benchmark},\n  author={Frederik Pahde and Moin Nabi and Tassilo Klein},\n  year={2018},\n  url={https://openreview.net/forum?id=HJB8ntJPG}\n}", "authorids": ["frederik.pahde@sap.com", "m.nabi@sap.com", "tassilo.klein@sap.com"], "authors": ["Frederik Pahde", "Moin Nabi", "Tassilo Klein"], "TL;DR": "We propose a benchmark for few-shot learning in multi-modal scenarios in conjunction with an approach including a discriminative text-conditional GAN for cross-modal sample generation with a simple self-paced strategy for sample selection.", "pdf": "/pdf/0fbb6e567be07c116da5b87159b723aa1ec34160.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582862677, "id": "ICLR.cc/2018/Workshop/-/Paper314/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper314/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper314/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper314/AnonReviewer1"], "reply": {"forum": "HJB8ntJPG", "replyto": "HJB8ntJPG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper314/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper314/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582862677}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582744149, "tcdate": 1520660018471, "number": 3, "cdate": 1520660018471, "id": "r1c4AkWKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper314/Official_Review", "forum": "HJB8ntJPG", "replyto": "HJB8ntJPG", "signatures": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer1"], "content": {"title": "Proposes a novel split for existing dataset and an incremental method for muli-modal few-shot learning", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes to split existing dataset (CUB with text descriptions), into a few shots multi-modal learning setting. It also proposes to use a cross model generator based on GANs (which is also heavily inspired from previous works) to aid in training by compensating for the few shot setting.\n\nThe methodological contribution of the paper is highly incremental. The benchmark is a novel split on top of existing datasets. And hence, overall the contributions of the paper are not convincing enough. One suggestion could be that comprehensive benchmarking of many different methods be done on the task;  cross modal learning has been studied in many different contexts computer vision (eg. with sketches and real images for objects and faces).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Modal Few-Shot Learning: A Benchmark", "abstract": "The state-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance. To this end, we propose a multi-modal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multi-modal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multi-modal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose an framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strategy for sample selection.  Experiments on our proposed benchmark demonstrate that learning generative models in a cross-modal fashion facilitates few-shot learning by compensating the lack of data in novel categories.", "paperhash": "pahde|multimodal_fewshot_learning_a_benchmark", "keywords": ["Few-Shot Learning", "Multi-Modal", "Fine-grained Recognition", "Meta-Learning"], "_bibtex": "@misc{\n  pahde2018multi-modal,\n  title={Multi-Modal Few-Shot Learning: A Benchmark},\n  author={Frederik Pahde and Moin Nabi and Tassilo Klein},\n  year={2018},\n  url={https://openreview.net/forum?id=HJB8ntJPG}\n}", "authorids": ["frederik.pahde@sap.com", "m.nabi@sap.com", "tassilo.klein@sap.com"], "authors": ["Frederik Pahde", "Moin Nabi", "Tassilo Klein"], "TL;DR": "We propose a benchmark for few-shot learning in multi-modal scenarios in conjunction with an approach including a discriminative text-conditional GAN for cross-modal sample generation with a simple self-paced strategy for sample selection.", "pdf": "/pdf/0fbb6e567be07c116da5b87159b723aa1ec34160.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582862677, "id": "ICLR.cc/2018/Workshop/-/Paper314/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper314/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper314/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper314/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper314/AnonReviewer1"], "reply": {"forum": "HJB8ntJPG", "replyto": "HJB8ntJPG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper314/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper314/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582862677}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573588122, "tcdate": 1521573588122, "number": 193, "cdate": 1521573587777, "id": "rk30AAAFf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HJB8ntJPG", "replyto": "HJB8ntJPG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-Modal Few-Shot Learning: A Benchmark", "abstract": "The state-of-the-art deep learning algorithms generally require large amounts of data for model training. Lack thereof can severely deteriorate the performance. To this end, we propose a multi-modal approach that facilitates bridging the information gap by means of meaningful joint embeddings. Specifically, we present a benchmark that is multi-modal during training (i.e. images and texts) and single-modal in testing time (i.e. images), with the associated task to utilize multi-modal data in base classes (with many samples), to learn explicit visual classifiers for novel classes (with few samples). Next, we propose an framework built upon the idea of cross-modal data hallucination. In this regard, we introduce a discriminative text-conditional GAN for sample generation with a simple self-paced strategy for sample selection.  Experiments on our proposed benchmark demonstrate that learning generative models in a cross-modal fashion facilitates few-shot learning by compensating the lack of data in novel categories.", "paperhash": "pahde|multimodal_fewshot_learning_a_benchmark", "keywords": ["Few-Shot Learning", "Multi-Modal", "Fine-grained Recognition", "Meta-Learning"], "_bibtex": "@misc{\n  pahde2018multi-modal,\n  title={Multi-Modal Few-Shot Learning: A Benchmark},\n  author={Frederik Pahde and Moin Nabi and Tassilo Klein},\n  year={2018},\n  url={https://openreview.net/forum?id=HJB8ntJPG}\n}", "authorids": ["frederik.pahde@sap.com", "m.nabi@sap.com", "tassilo.klein@sap.com"], "authors": ["Frederik Pahde", "Moin Nabi", "Tassilo Klein"], "TL;DR": "We propose a benchmark for few-shot learning in multi-modal scenarios in conjunction with an approach including a discriminative text-conditional GAN for cross-modal sample generation with a simple self-paced strategy for sample selection.", "pdf": "/pdf/0fbb6e567be07c116da5b87159b723aa1ec34160.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}