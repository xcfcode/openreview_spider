{"notes": [{"id": "-kigPjfTIGd", "original": "CXY78YpSSX", "number": 2277, "cdate": 1601308250938, "ddate": null, "tcdate": 1601308250938, "tmdate": 1614985750474, "tddate": null, "forum": "-kigPjfTIGd", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "kTDpMBiNQS", "original": null, "number": 1, "cdate": 1610040385214, "ddate": null, "tcdate": 1610040385214, "tmdate": 1610473978664, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a GAN for video generation based on stagewise training over different resolutions, addressing scalability issues with previous approaches. Reviewers noted that the paper is clearly written, proposes a method that improves upon the DVD-GAN architecture by reducing training time and memory consumption, and has competitive quantitative results.\n\nOn the other hand, the more negative reviewers are concerned that the empirical improvements demonstrated are somewhat incremental, and that there is not much novelty as the proposed approach is similar to other methods that decompose the generation process into multiple stages at different temporal window lengths and/or spatial resolutions. The authors argue that these criticisms are subjective and non-actionable. I sympathize with their frustration, but an acceptance decision for a competitive conference like ICLR does involve some subjective judgment as to whether the method and/or results meet a high bar beyond mere correctness. For this submission that's a close call, but between the novelty/incrementality concerns and the other more minor issues raised by reviewers (e.g., missing frame-conditional evaluation) I believe this paper could benefit from another round of revisions and improvements and recommend rejection.\n\nI hope the authors will consider improving the submission based on the reviewers' feedback and resubmitting to a future venue, as the paper certainly has merit. To this end I have a few concrete recommendations for the authors which could have flipped my recommendation to an accept if implemented:\n\n* Report results in the frame-conditional setting for comparison with DVD-GAN and other methods that operate in this setting.\n* Proofread the paper more thoroughly. I noticed several typos while skimming the paper, e.g. in the theory section, the second term of eq. 6 confusingly uses $\\rho$ instead of $\\log$. (Relatedly, given that appendix B.1 reports that the hinge loss is used, I'm not sure whether $\\log$ is correct in the first place -- this probably deserves further explanation or correction.)\n* Demonstrate/argue more convincingly (in one way or another) that SSW-GAN's improved efficiency really expands the frontier of what was possible before. It is true that the 128x128/100 video samples contain 2x as many total pixels as DVD-GAN's 256x256/12 samples, but this isn't a *strict* improvement as the spatial resolution is smaller, and a 2x difference leaves space for reviewers to reasonably wonder whether previous methods really couldn't have matched this if pushed. Some possible examples of this: show that SSW-GAN can generate longer 256x256 videos (a strict improvement over what was possible with DVD-GAN), or orders of magnitude longer (e.g., 1 minute) but still temporally coherent videos at 128x128, or videos with substantially improved subjective sample quality at the same (or higher) resolution.\n* The paper notes that \"DVD-GAN models do not unroll well and tend to produce samples that become motionless past its training horizon\". If this were quantified, e.g. by additionally reporting IS/FID/FVD separately for different timestep ranges, it could make a more compelling argument in favor of SSW-GAN."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040385199, "tmdate": 1610473978646, "id": "ICLR.cc/2021/Conference/Paper2277/-/Decision"}}}, {"id": "oI6oCp8aam6", "original": null, "number": 1, "cdate": 1603740708661, "ddate": null, "tcdate": 1603740708661, "tmdate": 1606708174344, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review", "content": {"title": "Generated samples not temporally consistent", "review": "The paper proposes a GAN-based model which generates videos in multiple stages. The main idea is the upsampling of the spatio-temporal resolution upon addition of a stage. This is the key feature of the proposed model allowing the model to generate videos of higher temporal resolution while using significantly less computational resources. \n\n**Strengths**\n+ The paper is clearly written\n+ The model performs competitively with relevant baselines with respect to quantitative metrics\n+ The evaluation of the model has been conducted on real world datasets\n+ Implementation details have been mentioned clearly\n\n**Weaknesses**\n- There have been earlier attempts for multi-stage video generation  [1,2]. However, the paper misses citations in this direction. Also, apart from condition for the generation, how is the proposed model different from the existing multi-stage ones?\n- The generated samples for Kinetics dataset are not temporally consistent and misses several details especially for smaller entities in video. To list a few: in Figure 2 row 4, the baby's face looks distorted and different in every frame; in Figure 3 row 2 and in Figure 2 row 1, the face of the person is completely incomprehensible.\n- The generated samples in the paper do not have a lot of perceived motion in them. How does the model perform when the input class is supposed to possess huge temporal variations?\n\nOverall, the paper presents a scalable way to generate video with higher temporal resolution. However, the generated results do not look realistic and lot of important details are missing in the generated samples. Therefore, my initial rating for this paper is 4. \n\n\n*References used in the review:*\n\n[1] Learning to generate time-lapse videos using multi-stage dynamic generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018\n\n[2] Zhao, L., Peng, X., Tian, Y., Kapadia, M. and Metaxas, D.N., Towards Image-to-Video Translation: A Structure-Aware Approach via Multi-stage Generative Adversarial Networks. International Journal of Computer Vision, 2020\n\n\n\n============================================**Post-Rebuttal Comments**==================================\n\nI appreciate the revisions and additional results presented by the authors. The authors have addressed my concerns as well as improved the clarity of the model description in the revised version of the paper. While that results are not perfect, I acknowledge that the problem of video generation is difficult and I believe such multi-stage model can motivate future methods in this direction of scalable video generation. Therefore, I would like to improve my score to 6 and would recommend acceptance of this paper.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099959, "tmdate": 1606915768206, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2277/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review"}}}, {"id": "mnnZKAle0kG", "original": null, "number": 15, "cdate": 1606265222902, "ddate": null, "tcdate": 1606265222902, "tmdate": 1606265222902, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "YZeNtTYsDOx", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Paper Update #2", "comment": "We have now uploaded a second revision of our paper. \n\n* **[R1]** We have now added FVD scores for our models for the Kinetics and BDD experiments. We hope this will ease future comparisons to our method.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "eHpNf7uVWH", "original": null, "number": 14, "cdate": 1606265115731, "ddate": null, "tcdate": 1606265115731, "tmdate": 1606265115731, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "Tgvacj42HcS", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Follow-up", "comment": "We have now updated the paper to include FVD scores for our models for the Kinetics and BDD experiments. \n\nThank you again for your review. We hope we have addressed your concerns and confirmed your positive review of our submission with the updated revision. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "EdpQSmZXPAU", "original": null, "number": 13, "cdate": 1606219908078, "ddate": null, "tcdate": 1606219908078, "tmdate": 1606219908078, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "ewD_R35cSYa", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Re: Follow-up", "comment": "The reviewer  checked the improvements on analysis, and the reviewer maintains the initial recommendation for acceptance. The reviewer thinks that their comments, as well as the comments of the other reviewers, have been addressed accordingly. \n\nThe following are the reasons for the given score and recommending acceptance.\n\nStrengths: \n-The paper is well written and produces an important new GAN-based baseline for the open research question of video prediction, addresses the problem of video prediction for up to 100 frames which could not be solved using the previous methods, and, what is also important, addresses the existing drawback of excessive time consumption for training the model. \n- The analysis and ablation studies have been significantly improved\n\nWeaknesses:\n- It is focused on improvements of the existing architecture (but the reviewer thinks that this is still a valid contribution as it addresses multiple limitations of the baseline architecture such as the duration of the predicted video and training time)\n- Unlike the original DVD-GAN paper, this paper does not produce any results for the conditional setting which limits the comparison with the alternative methods such as DVD-GAN, but the reviewer accepts that it may not have been possible due to the time limits of the rebuttal ; however, this point was mitigated by improvement in presented additional experiments including those which haven't been presented in DVD-GAN paper (PSD)"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "ewD_R35cSYa", "original": null, "number": 11, "cdate": 1605746785518, "ddate": null, "tcdate": 1605746785518, "tmdate": 1605746785518, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "RpjOZtyK7q", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Follow-up on comment by AnonReviewer #1", "comment": "Thank you for your additional suggestions and helpful comments.\n\n1. As requested, we have added a figure (Fig. 10 in the appendix F) that shows the second stage output along with the corresponding first stage output for some random samples from our model. The figure shows that the second stage refines the details of the first stage generation but preserves the overall scene structure. Please refer to appendix F for more details.\n\n2. Thanks for your clarification. We have added plots that show the average PSD at different timesteps for multiple samples from the ground-truth data as well as our predictions. The plots show that our predictions have similar PSD to the ground truth, and that this is true even for the last frame of the generations. This aligns with our observation that our samples, while not necessarily accurate, do not significantly blur over time. Please refer to section E in the appendix for further details. Thank you for suggesting this experiment!\n\n3. We have updated the paper following our previous comment with a discussion on the effect of class motion on the sample quality. We have included the table as well as a visualization of some of the samples generated for the high motion and low motion categories. This discussion can be found in section G of the appendix.\n\nThank you again for your constructive suggestions. We hope that this discussion can help you confirm your favorable assessment of our submission. Please let us know if you have additional comments or questions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "YZeNtTYsDOx", "original": null, "number": 10, "cdate": 1605746682224, "ddate": null, "tcdate": 1605746682224, "tmdate": 1605746682224, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Paper Update", "comment": "Dear reviewers and area chair,\n\nFirst we would like to thank you for your thoughtful reviews. We were happy to hear that the paper shows competitive results on real world datasets [R1, R4, R5], that it is exploring a reasonable research direction for adversarial video generation [R5] and that it would constitute a good contribution to the venue [R1, R5].\n\nWe want to point out the following changes that we did to the paper thanks to your feedback:\n\n* **[R1]** We added a comparison looking at the Power Spectrum Density (PSD) between the original data and our generations in Appendix E. We look at the PSD for different timesteps: 1, 10, 25, 50. We observe that our generations have a similar PSD to that of the original data, even at the end of the generation, indicating that the generations do not blur significantly and remain stable over time. We also added a citation to (Ayzel et al., 2020). Thank you for suggesting this experiment! \n\n* **[R1, R4]** We added an additional analysis looking at the difference between classes with high motion versus classes with low motion in Appendix G.  We randomly selected 5 classes with high motion/temporal variations and 5 classes with low motion and computed per-class scores. For each category we generated 1000 random samples and used all the available ground truth videos in that category (usually between 500 to 1000 examples). We compute FID/IS scores and observe high variability in FID and IS across classes, but we do not observe significant trends within high motion or low motion classes. We also added additional samples from those classes. Looking at the samples, we also do not observe any significant distinctions between the different groups of classes. \n\n* **[R1]** We provide an additional upsampling visualization in Appendix F showing that the upsampling stage is well grounded in the low-resolution videos as expected.\n\n* **[R5]** We updated the text to clarify the architecture details and experimental setting. Please let us know if this addresses your concerns. \n\n* **[R5]** We included initial results on an experiment where we train a two-stage model end-to-end. We train this model without a first stage discriminator nor a matching discriminator, instead directly discriminating on the second stage output. We observe that the output of the first stage is no longer a valid low resolution video, but the second stage generates valid snippets nonetheless. We also empirically observe that samples generated by the single pass end-to-end model do not show a lot of motion. Please refer to Appendix H for more details.\n\n* **[R4]** We added the provided citations. Thank you for pointing them out!\n \nThank you again for taking the time to review our paper. We hope that this will address your concerns, please let us know if you have further questions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "RpjOZtyK7q", "original": null, "number": 9, "cdate": 1605604801005, "ddate": null, "tcdate": 1605604801005, "tmdate": 1605604801005, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "yD7phq65eLO", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Re: Answer to AnonReviewer #1 ", "comment": "1. The main difference between the provided theoretical analysis and the implementation, as the reviewer can see, would be  that it uses gradient descent (local) optimisation and therefore there could be some ways for the model to 'cheat' and get into a local minimum. Therefore, is it possible to visualise the examples of the original and upsampled version to show that it actually upsamples in the expected way? \n2. *We looked at the reference for the PSD metric, but we are unsure how to use it to assess our model, as in the reference there is a ground-truth video for a given prediction. Could you clarify how to use PSD to assess potential blurring in our generation setup? Nevertheless, we agree that the visual quality of the generation could be further improved. While we match state-of-art performance, generative modeling of videos is not a solved problem.*\nIndeed, in Figure 4 of Ayzel et al (2020) they present PSD for the ground truth video for a given prediction. I was talking more about Figure 5 instead, which shows 'PSD averaged over all verification events and nowcasts'. It is my understanding that this should be possible to do without having directly corresponding ground truth for the predicted videos. This could give the idea of blurriness of the proposed methods' predictions vs real videos and DVD-GAN's. The motivation behind this comment was that the FID and IS metrics to assess the quality of predictions are very limited in their scope[1, 2], and it makes sense as evaluation of generative models is unsolved problem too. The question to the authors is whether they could see the way to mitigate the limitation of the analysis that it only outputs IS, FID and FVD scores. The reviewer knows it is a common place to use just these three scores, but thinks that it would be a nice thing to do as it would give some additional insight into the high-level characteristics of predictions. \nAnother thing which could improve the analysis would be to replicate the setup from the original DVD-GAN paper (BAIR Robot Pushing Dataset) on conditional frame prediction, but the reviewer also acknowledges that this setup could take more time than is available for the rebuttal. The benefit of evaluating on conditional frame prediction is that it enables a wide range of metrics (LPIPS, for example). \n3. Sounds interesting, looking forward to seeing the updated paper!\n[1]Barratt, Shane, and Rishi Sharma. \"A note on the inception score.\" arXiv preprint arXiv:1801.01973 (2018).\n[2]Borji A. Pros and cons of gan evaluation measures. Computer Vision and Image Understanding. 2019 Feb 1;179:41-65."}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "gx7x9kW-1T", "original": null, "number": 7, "cdate": 1605380287548, "ddate": null, "tcdate": 1605380287548, "tmdate": 1605380287548, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "oI6oCp8aam6", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Answer to AnonReviewer #4 (part 2) ", "comment": "In summary, we show that our results are in line with current SOTA video GAN models on a hard dataset of videos in the wild such as Kinetics. Current SOTA models have huge computational requirements -  our model matches their performance while requiring 2x less resources and at the same time we are able to generate up to 100 frames at 128x128 pixels, more than 2x the amount of frames that could be generated by DVD-GAN.\n\nWe thank you again for your review. We hope that we addressed your concerns. Let us know if you have further questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "vrBEyU7v0Y", "original": null, "number": 6, "cdate": 1605380257294, "ddate": null, "tcdate": 1605380257294, "tmdate": 1605380257294, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "oI6oCp8aam6", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Answer to AnonReviewer #4 (part 1)", "comment": "We thank you for your effort in reviewing our paper. \n\n**Earlier attempts for multi-stage video generation** \nWe will include both citations in the paper. We note however that both references have a different motivation to our work and are significantly different. We summarize them below:\n\nReference [1] proposes a multi-stage video prediction model. The first stage aims at generating realistic images, while the second stage models dynamics. This is similar in spirit to other approaches in which motion and content are disentangled (for example MoCoGAN or the dual discriminator of DVD-GAN). In contrast, the goal of our multi-stage generator is to have a coarse-to-fine approach to video generation that allows for higher resolution long videos. Our model arguably matches the SOTA in video generation (DVD-GAN) while requiring less resources, and we are able to generate videos for 100 frames which are significantly longer temporal horizons than both [1] (32 frames) and DVD-GAN (up to 48 frames). \n\nReference [2] considers the problem of image-to-video translation. Their method works by first producing a motion structure, and then the second stage refines that generation to produce frames. Similarly to [1], the goal of this work is not to propose a scalable alternative to training video GANs, and instead their stages focus on modeling different aspects of a video. \n\nIn summary, both approaches aim at decomposing the generative process into different semantic stages, while the goal of our model is to provide a scalable alternative to SOTA video GAN models that uses a coarse-to-fine decomposition to achieve this goal.\n\n**Temporal consistency on Kinetics ... generations missed several details** \n\nWe would like to point out that video generation on complex datasets is a hard task which is not solved. The quality of video generative models is far from being at the level of their image counterparts. \n\nWe agree that the consistency or fine-grained details in some of our generations could be improved. Nevertheless, the quantitative evaluations indicate that the samples generated by our model are of similar or better quality than those from DVD-GAN, as you noticed in your review, and DVD-GAN is the current state-of-art in video generation as far as we know. Our samples also are of similar qualitative quality than DVD-GAN.  For reference, these are some 128x128 generations from DVD-GAN using the truncation trick, provided by the DVD-GAN authors. They also show inconsistencies or lack of details similar to the ones in our results (bottom row, second column for instance): https://drive.google.com/file/d/1P8SsWEGP6tEGPPNPH-iVycOlN6vpIgE8/view.\n\nThe goal of our paper is to provide a more scalable alternative to current video generation GANs to allow us to generate longer videos, while maintaining the sample quality.\n\n**Missing motion**\nWe randomly selected 5 classes with high motion/temporal variations and 5 classes with low motion and computed per-class scores. For each category we generated 1000 random samples and used all the available videos in that category (usually between 500 to 1000 examples) to compute the scores. We observe high variability in FID and IS across classes, but we do not observe significant trends within high motion or low motion classes. For example, there are categories in the non-motion category with FID score > 110, and the IS scores are on average slightly better for the motion category. Looking at the samples, we also do not observe any significant distinctions between the different groups of classes. \n\nInstead we noticed differences in the generations across all classes in the dataset. We believe the class variability might be due to other factors such as amount of data available, having more semantically related classes or the amount of structure present in a particular class - generating simple objects is easier than generating faces.\n\nWe will include this discussion with some visualizations in an updated manuscript. Below we provide the per-class scores we obtained:\n\n**Low Motion group**\n\nCategory | FID | IS | # examples\n--------------------------\nCooking_egg | 111.63 | 7.60 | 441\n\nCrying | 70.74 | 8.92 | 627\n\nDoing_nails | 91.67 | 13.32 | 537\n\nReading_book | 64.97 | 11.13 | 793\n\nYawning | 79.08 | 9.71 | 530\n\n**High Motion group**\n\nCategory | FID | IS | # examples\n--------------------------\nBungee_jumping | 82.21 | 11.66 | 799\n\nCapoeira | 84.57 | 9.48 | 816\n\nCheerleading | 116.84 | 12.10 | 982\n\nKitesurfing | 108.81 | 11.36 | 648\n\nSkydiving | 90.25 | 5.99 | 983"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "QFSTgE_38aF", "original": null, "number": 3, "cdate": 1605379708654, "ddate": null, "tcdate": 1605379708654, "tmdate": 1605380099100, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "EBNqdQHe1k2", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Answer to AnonReviewer #3", "comment": "We thank you for your time and effort in reviewing our paper.\n\n1. To the best of our knowledge, we are the first to propose a stage-wise generative model for video where **each stage is trained independently**. We show that our approach is competitive with DVD-GAN, which is the current state-of-the-art for video generation. Furthermore, we demonstrate that our approach can generate videos of up to 100 frames at 128x128 resolution. Our method is the first one to produce such generations. Finally, we show that approach is theoretically grounded.  We believe that these contributions are novel and significant. We also note that the fact that we have multiple stages is a byproduct of our motivation and formulation rather than the novelty of our paper. We would happily discuss the novelty in relation with some related works if you provide us with some references.\n\n2. A comparison to previous video generation models in terms of output dimensionality should take into consideration the length of the videos. While the spatial resolution of our results is the same as previous approaches, to the best of our knowledge we are the first ones to generate 128x128 videos of up to 100 frames. Note that the dimensionality of 128x128x100 is considerably higher than the 256x256x12 results found in DVD-GAN. \nFurthermore, we are able to generate such videos while requiring 2x less computational resources, i.e. we could further increase the output dimensionality if using the same amount of computational resources as DVD-GAN. While the hardware requirement is still high, it is nonetheless 2x less than current state-of-art approaches.\n\n3. We would like to point out that video generation is a hard task which is not solved. Video generative models are far from being at the quality level of their images counterpart.  It is therefore inappropriate to compare the quality of image-based generative models with video generative models.\nOur results are of similar or better quality to DVD-GAN according to the IS and FID metrics, which is the state-of-the-art in video generation. Our samples also are of similar qualitative quality than DVD-GAN.  For reference, these are some 128x128 generations from DVD-GAN, provided by the DVD-GAN authors: https://drive.google.com/file/d/1P8SsWEGP6tEGPPNPH-iVycOlN6vpIgE8/view.\nWe would be happy to compare to methods that produce more realistic results on Kinetics if you  could provide us with a reference and a metric for the comparison.\n\nAgain, thank you for your review. We hope that our answers clarify the significance and novelty of our contributions. We hope that you will reconsider our score, which we believe is severe with our work. Let us know if you have any further questions or concerns.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "qv2Srjm9JV2", "original": null, "number": 5, "cdate": 1605380074376, "ddate": null, "tcdate": 1605380074376, "tmdate": 1605380074376, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "S_P6fy7E5qA", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Answer to AnonReviewer #2", "comment": "Thank you for taking the time to review our paper. We respond to your different concerns below.\n\n1. To the best of our knowledge, we are the first to propose a stage-wise generative model for video where **each stage is trained independently**. We show that our approach is competitive with DVD-GAN, which is the current state-of-the-art for video generation as far as we know. Furthermore, we demonstrate that our approach can generate videos of up to 100 frames at 128x128 resolution. Our method is the first one to produce generations at such scale. Finally, we show that approach is theoretically grounded.  We believe that these contributions are novel and significant. \nWe will happily discuss the novelty in relation with other related work if you could provide us with some references.\n\n2. We empirically validate our approach on Kinetics-600 and BDD100K, two large-scale datasets with complex videos in real-world scenarios. In addition to baselines based on our approach, we compare to DVD-GAN which is arguably the current state-of-art for video generation. Our approach matches or outperforms state-of-art approaches while requiring significantly less computational resources. We are not aware of a peer-reviewed generative model that outperforms DVD-GAN for video generation. \n\n3. Even though the computational cost is still important, our approach is nonetheless a 2x improvement over current SOTA models which is significant. It is a step forward in having more tractable video models. Furthermore, thanks to the computational savings, we are able to generate videos for longer temporal horizons (100 frames) than previous work on video generation. \n\nThank you for taking the time to review our paper. We hope that our answers clarify the significance and value of our contributions. We hope that you will reconsider your score, which we believe is severe with our work. Let us know if you have any further questions or concerns.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "yD7phq65eLO", "original": null, "number": 4, "cdate": 1605379873627, "ddate": null, "tcdate": 1605379873627, "tmdate": 1605379987743, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "zpYaN8vCLe", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Answer to AnonReviewer #1 ", "comment": "Thank you for your review!. We address your questions below:\n\n1. While the video discriminator in stage 1 and stage 2 only discriminate videos in isolation (x^w_l and x^w, respectively), the matching discriminator scores pairs (x^w_l, x^w). If the low-resolution sample x^w_l is good (for example, it is ground truth data) but the high resolution sample does not match, then the discriminator should be able to tell that fake pair apart from real pairs, pushing the second stage generator to produce a matching upscaling. \nIn our theoretical analysis we show that, when discriminating pairs of (x^w_l, x_w), there exists a global minimum when the model and data joint distributions match. \n\n2. We looked at the reference for the PSD metric, but we are unsure how to use it to assess our model, as in the reference there is a ground-truth video for a given prediction. Could you clarify how to use PSD to assess potential blurring in our generation setup? Nevertheless, we agree that the visual quality of the generation could be further improved. While we match state-of-art performance, generative modeling of videos is not a solved problem.\n\n3. We randomly selected 5 classes with high motion/temporal variations and 5 classes with low motion and computed per-class scores. For each category we generated 1000 random samples and used all the available ground truth videos in that category (usually between 500 to 1000 examples) to compute the scores. We observe high variability in FID and IS across classes, but we do not observe significant trends within high motion or low motion classes. For example, there are categories in the non-motion category with FID score > 110 while some motion classes achieve a FID score of ~80 , and the IS scores are on average slightly better for the motion category. Looking at the samples, we also do not observe any significant distinctions between the different groups of classes.\nInstead we noticed differences in the generations across all classes in the dataset. We believe the class variability might be due to other factors such as amount of data available, having more semantically related classes or the amount of structure present in a particular class - generating simple objects is easier than generating faces.\nWe will include this discussion with some visualizations in an updated manuscript. \n\nBelow we provide the per-class scores we obtained:\n\n**Low Motion group**\n\nCategory | FID | IS | # examples\n--------------------------\nCooking_egg | 111.63 | 7.60 | 441\n\nCrying | 70.74 | 8.92 | 627\n\nDoing_nails | 91.67 | 13.32 | 537\n\nReading_book | 64.97 | 11.13 | 793\n\nYawning | 79.08 | 9.71 | 530\n\n**High Motion group**\n\n\nCategory | FID | IS | # examples\n--------------------------\nBungee_jumping | 82.21 | 11.66 | 799\n\nCapoeira | 84.57 | 9.48 | 816\n\nCheerleading | 116.84 | 12.10 | 982\n\nKitesurfing | 108.81 | 11.36 | 648\n\nSkydiving | 90.25 | 5.99 | 983\n\n\nThanks again for your time. Aside from the PSD metric, we hope we have answered your remarks. Please let us know if you have further questions.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "Tgvacj42HcS", "original": null, "number": 2, "cdate": 1605379576599, "ddate": null, "tcdate": 1605379576599, "tmdate": 1605379576599, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "sa9JPWHZ-M", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment", "content": {"title": "Answer to AnonReviewer #5", "comment": "Thank you for your time in writing the review for our paper. We really appreciate your very thoughtful and detailed review. We found your feedback very helpful, thanks!\n\nBelow we address your questions:\n\n1. We agree that the joint training of multiple stages is an exciting direction to explore. The main difficulty in training stages jointly is that we have to keep the activations of all previous stages in memory for backpropagation, increasing the overall memory requirements of the model compared to training stages independently. For the particular model we propose in this submission, this brings the memory requirements close to DVD-GAN. \nWe are currently performing an initial experiment on a two-stage model on Kinetics in which all stages are trained jointly. The second stage takes a random window out of the first stage generation as input, and we directly discriminate on the output of the second stage without a matching discriminator. We observe two main things: first, not using a matching discriminator causes the output of stage 1 to not be a proper low resolution video. While it is still used by the second stage, there is nothing that encourages the output of stage 1 to be a real video. Second, this model seems to be more inconsistent than the equivalent SSW-GAN when unrolled, as the low resolution but complete first stage output is not grounded. Some of these issues could be addressed by still using a matching discriminator or a discriminator for the first stage output.\nThank you for suggesting this experiment. We will add those results in the next revision of our paper once the training is fully complete.\n\n2. The model is trained in stages, all components in a given stage are trained at the same time and independently of other stages. The first stage is trained separately of the upsampling stage, with the latter one including the conditional generator and the matching discriminator. We will update Section 4 to clarify this.\n\n3. Upsampling is done differently for the temporal and spatial dimensions. Spatial upsampling is achieved through progressively increasing the resolution of the intermediate activations of the upsampling generator. On the other hand, temporal upsampling is done at the input of the upsampling generator, where the noise vector already has full temporal dimension, and we temporally upsample the low resolution video accordingly. This is similar to how DVD-GAN operates - the noise has full temporal size, while the spatial resolution is increased progressively in the generator. We will clarify this in the paper\n\n4. All our first stages in the paper are trained to generate 32x32/25 videos. Then, as you mention, we train the second stage on 32x32/6 windows to generate 128x128/12 outputs. During evaluation, the scores obtained for 128x128/12 are obtained by upsampling 6 randomly selected frames from the 32x32/25 low-resolution videos. It is true that the first stage always generates 25 frames and we will clarify it in the paper.\nStage 2 models can be unrolled over the full first stage output to generate 128x128/50 videos. We also include an ablation where we train on smaller 32x32/3 windows to generate 128x128/6 frames, but still unroll the model to generate 128x128/50 videos. \n\n5. The explanation for how we compute the metrics is correct. We corresponded with the authors of TriVD-GAN[1], which overlap with authors of DVD-GAN, to make sure we used the same evaluation setup as DVD-GAN. We included this explanation because at first we were unable to reproduce their metrics. As you mention, all numbers in our experimental section for DVD-GAN are taken from their reported results.\n\n6. We will compute and report FVD metrics. Note however that TriVD-GAN focuses on video prediction, and that DVD-GAN only reports FVD metrics for the BAIR dataset and their video prediction results on Kinetics, while we focus on video generation.\n\nAgain, we thank you for your insightful comments and suggestions. We will soon update the paper to clarify some of the points you raised including details about the experiments, the architectural details and the overall training scheme, as well as to more clearly state the main idea. Please let us know if you have any other questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "-kigPjfTIGd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2277/Authors|ICLR.cc/2021/Conference/Paper2277/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923850236, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Comment"}}}, {"id": "S_P6fy7E5qA", "original": null, "number": 2, "cdate": 1603780927664, "ddate": null, "tcdate": 1603780927664, "tmdate": 1605024248137, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review", "content": {"title": "This paper proposes a stage-wise strategy to train Generative Adversarial Networks for videos. The contribution of this paper is very limited and the experiments are not convincing.", "review": "Pros:\n1. A stage-wise approach to train GANs for video is defined to reduce the computational costs needed to generate long high resolution videos.\n2. The authors provide some quality results of the proposed approach.\n\nCons:\n1. The contribution of this paper is very limited. The authors just do some incremental improvement based on current GAN models, and the theoretical analysis for the stage-wise training approach is not enough. \n2. The experiments are not convincing. The authors only compared the baseline methods in the experiments. Besides, the proposed training strategy should be applied in different generation models based on GAN to show the effectiveness in different cases. \n3. This paper aims to reduce the computation cost of the model training, but do not achieve significant effect, which takes 23 days for model training.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099959, "tmdate": 1606915768206, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2277/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review"}}}, {"id": "EBNqdQHe1k2", "original": null, "number": 4, "cdate": 1603866601408, "ddate": null, "tcdate": 1603866601408, "tmdate": 1605024248062, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "Summary:\nThe paper proposes a stage-wise training pipeline for training 128x128 resolution videos of up to 100 frames. It starts by generating low resolution and temporally downsampled videos, and upsample the results in a stage-wise manner. Experimental results on Kinetics-600 and BDD100K demonstrate that the network is effective in generating higher resolution videos.\n\nStrengths:\nThe idea is easy to understand. The paper is well written and easy to follow. Quantitative results show that the proposed method is superior than existing methods under some circumstances.\n\nWeaknesses:\n1.\tThe novelty is very low. Stage-wise and progressive training have been proposed for such a long time, they have been used everywhere. The way the authors use them don\u2019t really exhibit anything novel to me.\n2.\tThe resolution of the outputs (128x128) is lower than prior works (e.g. DVD-GAN has 256x256 outputs). Since the paper claims the computation cost is lower, one would expect the model can generate higher resolution and much longer duration videos, but in fact it\u2019s quite the opposite. To prove the effectiveness, I feel the authors need to show something higher than 256x256, say 512 or 1024 resolution. On the other hand, the hardware requirement is still high (128 GPUs) instead of some normal equipment that everyone can have, so I really don\u2019t see any benefit of the model. If the authors can train DVD-GAN using only a handful of GPUs, that might also be a contribution, but it\u2019s not the case now.\n3.\tOutput quality is reasonable, but still far from realistic. Recent GAN works have shown amazing quality in synthesized results, and the bar has become much higher than a few years ago. In that aspect, I feel there\u2019s still much room for improvement for the result quality.\n\nOverall, given the limited novelty, low resolution output and still high hardware requirement, I\u2019m inclined to reject the paper.\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099959, "tmdate": 1606915768206, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2277/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review"}}}, {"id": "zpYaN8vCLe", "original": null, "number": 3, "cdate": 1603804165643, "ddate": null, "tcdate": 1603804165643, "tmdate": 1605024247994, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review", "content": {"title": "Good contribution in improvement of DVD-GAN", "review": "The method shows promising results on on generating high duration (up to 100 frames) class conditional videos with convincing Inception scores, indicating quality similar to DVD-GAN, while consuming less memory and with better coherence. \nWhile the contribution of the paper is mainly to improve the DVD-GAN architecture to reduce training time and memory consumption, the reviewer believes that the paper would be a good contribution to the venue.\nBelow there are some questions on the methodology:\n1. Is there any way to tell does the matching discriminator actually only estimates the ability to upsample $\\hat{x}_w$ from the previous low resolution sample $x_w^l$? From the architecture it is not evident whether or not it only does this or it is also entangled with assessment of how good the low-resolution sample $x_w^l$  was. In other words, if the low resolution sample scores good (e.g. because it's the real-world data) but the upsampling does not match,  would the objective of the matching descriptor training still score it as a good upscaling?  Or is there any reason preventing from this type of behaviour? \n2. Although, as mentioned in the introduction, it may not be as big problem as for VAE-based models, the problem of blurring might exist for DVD-GAN-like models. It is written in the caption of Figure 5 that 'Despite the two stages of local upsampling, the frame quality does not degrade noticeably through time.\u2019 Although the reviewer appreciates that previous work reported only IS/FID/FVD metrics and that defining proper evaluation metrics for generative models is an open question, it might be a good idea to show some other quantitative metrics such as power spectral density (PSD) plots similar to figure 5 from [1]. This would help get an idea how it compares to the real-world video in terms of blurring of the results. \n3. Given that the generation of videos is class-conditional, is it possible to show the metrics per class? Are the scores per class similar or does the method score better for larger classes or the classes with specific motion dynamics? \n\n[1] Ayzel et al (2020) RainNet v1.0: a convolutional neural network for radar-based precipitation nowcasting", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099959, "tmdate": 1606915768206, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2277/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review"}}}, {"id": "sa9JPWHZ-M", "original": null, "number": 5, "cdate": 1604600480639, "ddate": null, "tcdate": 1604600480639, "tmdate": 1605024247919, "tddate": null, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "invitation": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review", "content": {"title": "Official Review of SSW-GAN: Scalable Stage-wise Training of Video GANs", "review": "**Paper Contributions**\n\nThe paper proposes SSW-GAN, an adversarial generative model of video which proposes a new generator architecture along with splitting the training into multiple stages.\n\n**Strong points of the paper**\n\n* The results are very strong.\n* Prior work has focused on efficient decomposition of the discriminator, this work focuses on decomposition of the generator (effectively), and this is an extremely reasonable direction to take adversarial video research in.\n* The claim that this requires substantially less computational cost is grounded.\n\n**Weak points of the paper**\n\n* A major departure from prior work is training stage 1 of SSW-GAN separate of stage 2, but this is independent of the computational benefits present from the generator architecture innovations. It is missing an important ablation showing the results of training the stages jointly.\n* The generator architectures changes from prior work are quite concrete, but the high level description doesn't clearly reflect that.\n* Some of the comparisons and model descriptions are lacking details, which make it difficult to understand experiments.\n\n**Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.**\n\nI believe this paper is an accept (7), however I think there are a number of places where the description of the architecture and experiments needs more detail, and for my final rating I would like to see these addressed in rebuttal. \n\nFurthermore, I think there is a key experiment missing. Were that to be added I would strongly consider moving to clear accept (8).\n\n**Supporting arguments for your recommendation.**\n\nThere are two key innovations in the SSW-GAN model:\n* A generator architecture which provides output to discriminators that does not require an entire high-resolution full-length sample to be generated (because between stage 1 and 2 you select only a window of frames to run stage 2 on).\n* Splitting the training into two phases, where at first you only train stage 1 and then you train stage 2.\n\nThese are very interesting ideas, since either one substantially reduces the training-cost of the video model (something aptly described in the paper), and not something strongly touched on by previous work. In addition, the metric scores of the SSW-GAN model are very nice, state of the art in almost all cases. For that reason I think the work in this paper is worthy of acceptance.\n\nHowever there are a couple points of clarification and increased description which I think are needed, and I would like to see the following points addressed in the rebuttal:\n\n* The core idea of judging fixed-length output upscaling as a generative problem seems like a very clear and reasonable idea. Most of the high-level description of the most omits this in place of a general \u201cmulti stage definition\u201d of the SSW-GAN model. I think the introduction and abstract would benefit about being more clear with regards to this change.\n\n* In section 4, it is unclear if all four bold sections are trained independently or not. I think it would be good to be clear in the paper-structure which components are trained together.\n\n* In general, the paper is not super clear where upsampling (both in time and space) occurs. I think it would be good to describe this in the paper text and in the architecture figures.\n\n* I am not super clear on the comparisons between SSW-GAN and DVD-GAN in \u201ccomparison with prior work\u201d. When you say \"our model trained to generate 128x128/12 videos \u201c is this the model which had a first stage trained on 32s32/25 and then you trained the second stage on input windows of 6 frames, generating 128x128/12, and took just single samples from that to compare?\n\n* Similarly, when you say \u201cHowever, our model is only trained on 128x128/12 outputs, as it is unrolled and applied convolutionally over the first stage output to generate 48 frames.\u201d, isn\u2019t it the case that the first stage is trained on longer sequences?\n\n*  In Table 1, the numbers for DVD-GAN seem lifted from the paper, which I believe is using a Kinetics-600 trained I3D for metric calculation. This means the FID numbers are comparable, but in section 5 when describing IS you say you are using a Kinetics-400 trained I3D (like in FVD). Is this correct (in which case the numbers are not quite comparable) or is this is just mis-explained, and all numbers in Table 1 come from Kinetics-600 trained I3Ds?\n\n* I think the IS/FID metrics you pick in Table 1 are the better metrics, but could you also add FVD numbers? That would allow you to compare against TriVD-GAN [1], which outperforms DVD-GAN. I think this is important because that paper also discusses modifications which reduce the memory requirement of DVD-GAN.\n\nFinally, there is a major question this paper does not address: **is the two-stage training of SSW-GAN necessary, or can it be trained in a single pass** (but with the generator decomposition as described)? Doing so would be simpler, and also might potentially reduce the need for the Matching Discriminator, which the paper contains an ablation for, but I think needs a further ablation when the model is not trained in two stages.\n\nI believe a key experiment would be training an SSW-GAN architecture but in a single pass, the results of that experiment would mean quite a lot for interpreting the changes described in this paper. In particular, the paper's title and abstract focus on the multi-stage aspect of the model, but skim over the substantial change of making the generator architecture more modular and scalable. I think it would be very beneficial to understand how each of these changes independently effect the performance of the model.\n\nI do not think this ablation is required to maintain an accept rating, but including it would push my rating closer to clear accept, and be quite a strong addition to the paper's content.\n \n[1] https://arxiv.org/abs/2003.04035\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2277/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2277/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SSW-GAN: Scalable Stage-wise Training of Video GANs", "authorids": ["~Lluis_Castrejon1", "~Nicolas_Ballas1", "~Aaron_Courville3"], "authors": ["Lluis Castrejon", "Nicolas Ballas", "Aaron Courville"], "keywords": ["video generation", "GANs", "scalable methods"], "abstract": "Current state-of-the-art generative models for videos have high computational requirements that impede high resolution generations beyond a few frames. In this work we propose a stage-wise strategy to train Generative Adversarial Networks (GANs) for videos.  We decompose the generative process to first produce a downsampled video that is then spatially upscaled and temporally interpolated by subsequent stages. Upsampling stages are applied locally on temporal chunks of previous outputs to manage the computational complexity. Stages are defined as Generative Adversarial Networks, which are trained sequentially and independently. We validate our approach on Kinetics-600 and BDD100K, for which we train a three stage model capable of generating 128x128 videos with 100 frames.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "castrejon|sswgan_scalable_stagewise_training_of_video_gans", "one-sentence_summary": "We propose a scalable methods to generate videos by training a GAN to produce a low resolution and temporally subsampled version of a video, which is then upsampled by one or more local upsampling stages.", "supplementary_material": "/attachment/2445a28c935bb0cfe8ce6d2cb88aaafea748da61.zip", "pdf": "/pdf/d48909e8a9644b69d2eb51496566c2c2fafc153e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=v4KsmLKzCj", "_bibtex": "@misc{\ncastrejon2021sswgan,\ntitle={{\\{}SSW{\\}}-{\\{}GAN{\\}}: Scalable Stage-wise Training of Video {\\{}GAN{\\}}s},\nauthor={Lluis Castrejon and Nicolas Ballas and Aaron Courville},\nyear={2021},\nurl={https://openreview.net/forum?id=-kigPjfTIGd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "-kigPjfTIGd", "replyto": "-kigPjfTIGd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2277/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538099959, "tmdate": 1606915768206, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2277/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2277/-/Official_Review"}}}], "count": 19}