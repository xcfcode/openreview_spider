{"notes": [{"id": "OOsR8BzCnl5", "original": "38rTshOgITr", "number": 591, "cdate": 1601308071544, "ddate": null, "tcdate": 1601308071544, "tmdate": 1614920461423, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "N1BQ4uK8fyo", "original": null, "number": 1, "cdate": 1610040452951, "ddate": null, "tcdate": 1610040452951, "tmdate": 1610474055297, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper introduces a new idea for multi-view classification: using a Dirichlet distribution over the views to model uncertainty.\n\nThe paper appears to be clear, well written and sound.\nAlso, the experimental comparison is thorough.\n\nThe authors have given pertinent responses to the reviewers' questions, including w.r.t comparing against Bayesian/deep CCA in terms of accuracy.\n\nOverall, this is a good paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040452938, "tmdate": 1610474055280, "id": "ICLR.cc/2021/Conference/Paper591/-/Decision"}}}, {"id": "pSfB3nhjBpj", "original": null, "number": 2, "cdate": 1603900174562, "ddate": null, "tcdate": 1603900174562, "tmdate": 1606763279875, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Review", "content": {"title": "evidence level based multi-view classification", "review": "The authors propose using Dempster-Shafer evidence theory to build a more trusted multi-view classification method.\n\nThe article is fairly well structured, apart from the literature review, which somewhat does not discuss some of the related works properly, for example, the probabilistic CCA based methods and the optimal transport based papers [Dixin Luo, 2020]. \u00a0The article has the unfortunate tendency to contains a few unproven (or wrong) claims. For instance, \"[...] accurate classification results, they are usually vulnerable to yielding incorrect predictions, particularly when presented with views that are not well-represented [...]\", which would deserve a citation or some empirical evidence. Specifically, I think some of the Bayesian versions of CCA methods are able to handle such a situation by providing uncertainty, and the authors did not include them as the baselines in the experimental results as well. It is also claimed that the proposed method is \"optimal\" sample-adaptive multi-view integration, which I could not see any proof in a theoretical manner. \n\nMy main concern with this paper is the experimental results. First of all, the paper is a kind of Bayesian multi-view learning method and should compare with Bayesian and deep CCA-based method as well. Second, the paper used a Dirichlet distribution for each view. Why HDP cannot be included as another baseline. Regarding the results, the authors emphasize that the performance has been improved by around 15%. This is not surprising as those methods are just single-view based methods. \n\nConcerns/questions/comments:\n\n1- The authors added Gaussian noise to half of the views. It is not clear to me that some views are noisy and some of them are not noisy. All of the features in one view are noisy, or all of the features are noisy. \n\n2- Dirichlet distribution is somehow more suitable for the situation that one sample can belong to multiple classes. Did the authors consider that? \n\n3- The integration methods, such as those proposed based on CCA and optimal transport (OT), try to combine different views somehow to achieve the best performance. However, the proposed method seems to investigate each view separately and then combine them. My concern is that when the results are noisy in all views, how the method performs. Is it helpful to combine them in this way? Adding some empirical experimental results would be great.\n\n4- It is not clear to me that if the authors assume different views are well aligned or not. If not, can the proposed method handle such a situation?\n\n5- The experiments if Figure 4 aim to show that the proposed method is more robust to the noisy environment. However, adding $\\sigma = 10^9$ does not mean\u00a0anything, and I think the\u00a0method's performance\u00a0is because it is not considering the view with the noise. But in real practice, when we talk about multi-view, both views are reasonably noisy. I prefer to see such a comparison. At least, a comparison in that way is also needed. \n\n6- A comparison of the methods in semi-supervised classification would be great. \n\n7- How the results change if you remove one of the views from a multiple-view (more than 2 views) dataset.\u00a0\n\n8- Last but not least, Table 1 seems to be a comparison with one-view based methods. This is not enough to show the proposed methods are good enough. \n\n\n----------- UPDATE I ----------\n\nI thank the authors for their response to my concerns/comments. It seems like I have to defend my position for suggesting a rejection of the paper. While the response of the authors has clarified some aspects, some comments have not been adequately addressed.\n\nStill, the experiments and related works are my main concern with this paper. \n\nR1: Based on this response, It seems the paper is trying to tackle the negative transferability problem. While this is an important problem, the authors need to discuss that in the related works, for example [1]. However, in a multi-view learning problem, the main issue is that each view includes some features which can help the classification goal. In other words, part of the features in all views are noisy. At least, you need to add this experiment in my point of view.\n\nR10: [2,3,4] are a few examples that used DP for classification purposes. When one of the main arguments of the paper is adding uncertainty estimation using DP, discussing other related papers are important. In some cases, you can also compare with them.  Besides that, the authors can compare with HDP similar to what they did for the CCA-based methods.\n\nIt would be great if the authors could report the classification results based on only using one view for Figure 4 as a baseline. The authors might also want to check the paper [5]. \n\n[1] Bayesian multi-domain learning for cancer subtype discovery, NeurIPS 2018.\n\n[2] Multi-Task Learning for Classification with Dirichlet Process Priors, Journal of Machine Learning Research 2007.\n\n[3] Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification, NeurIPS 2018.\n\n[4] Factorial Multi-Task Learning : A Bayesian Nonparametric Approach, ICML 2013.\n\n[5] Hierarchical Optimal Transport for Robust Multi-View Learning, NeurIPS 2020.\n\n----------- UPDATE II ----------\nI have read this paper again and went through the author's responses. While I appreciate the authors for their responses and believe there is some novelty in including uncertainty quantification, I'm still not confident with their experimental designs and literature review.\n\nBased on my understanding, this method is not a multi-view learning in a classical way. It is better to say that the proposed method is somehow an ensemble approach. Going through the model, the paper deals with each view separately and then tries to combine the classification results of each view by weighting them. There is some novelty here, where the weighting is adapting based on the uncertainty. However, in a classical multi-view, the problem is that you have different views, where features of each view are randomly noisy. So, the goal is how one can combine information from different views to improve the classification results. Still, I cannot see that. \n\nIn the experiments, the authors make N views to be noisy, and the other N(+1) views are clean. Then they are trying to classify each view (somehow) separately and because they are reducing the weight of noisy views, their performance (slightly) will be improved. For example, please check table 1. For instance, the Handwritten dataset, the performance of DE, the only ensemble model as the baselines, performed almost similar 99.79 vs 99.97 ( 98.30 vs 98.51). This improvement can be due to tuning the other methods, and the authors did discuss this in neither the main text nor the supplement.\n\nIn Figure 4, when the noise is small, the different models' performances are almost the same. Also, \\sigma = 10^9 is not meaningful and with some sort of variance comparisons, one can find that that view is corrupted.\n\nThe authors did not address if the method can work in real/harder situations when all features and views are noisy. The authors responded they do not have any restriction on that; however, they did not show either empirically or theoretically that the proposed method can handle this situation more suitable. I agree that the authors have tried on the real dataset, however, to see the performance comparison, you somehow need to randomly corrupt different features on different views.\n\nSince the method does seem to be a multi-view ensemble learning, I would have rather expected to try a more common approach for achieving the same goal first, for example to use one of the available Bayesian single-view methods for each view and they combine the results and see how the results look like.\n\nThe authors claim that the negative transfer effect is not related here, which I can't entirely agree with. This term has been used in multi-task learning as well. The reason that no one uses this term in multi-view learning is that this is not a real scenario in multi-view learning. One view is degrading the classification performance because some views are noisy. If we do not use those very noisy views, the performance should be improved.\n\nI also want to point out that their authors show that removing each view can degrade their model's performance. And this is desire, especially when the number of views is small, as having three views means three models in their ensemble architecture. It is not clear that this improvement is due to better information sharing or more complex methods.\n\nRegarding the CCA-based method, one can use (Optimal) Bayesian Classification on learned space to get the uncertainty as well. Although, I agree that the proposed method is somehow an end-to-end learning method. \n\nI would suggest the authors somehow re-organize/write the paper as a multi-view ensemble learning method and compare it with those multi-view ensemble methods as the main focus of the paper. You also need to discuss [5] as it is very related to your work. Although, it is a Bayesian but not an ensemble method. ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper591/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139685, "tmdate": 1606915787529, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper591/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Review"}}}, {"id": "l3tQlBnCFXM", "original": null, "number": 9, "cdate": 1606290631961, "ddate": null, "tcdate": 1606290631961, "tmdate": 1606291509355, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "pSfB3nhjBpj", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment", "content": {"title": "Response to Reviewer 1 after update", "comment": "We thank the reviewer for the update for the review. We now provide a point-to-point response to the left two concerns/comments as follows.\n\nQ1: \u201cIt seems the paper is trying to tackle the negative transferability problem\u2026\u201d\n\nR1: This is NOT a transfer learning or domain adaption model at all, therefore, we do not focus on tackling the negative transferability problem. Our work is a multi-view classification method that focuses on jointly using all views for trusted classification instead of transferring knowledge from one domain to another. \n\nThere is no assumption about how many views (or how many features) are noisy. All views (no matter partial views or all views containing noise) can be adaptively integrated at the evidence level and this has been validated by sufficient experiments (including naturally noisy data in Table 4 and synthetic noisy data in Figure 4, 5). \n\nBTW: Negative transfer (NT), i.e., the source domain data/knowledge causes reduced learning performance in the target domain, which has been a long-standing and challenging problem in TL [1][2]. \n\n[1] Overcoming Negative Transfer: A Survey, IEEE TKDE 2020\n\n[2] Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning, NeurIPS 2019\n\n\n\nQ10: \u201c[2,3,4] are a few examples that used DP for classification purposes. When one of the main arguments of the paper is adding uncertainty estimation using DP, discussing other related papers are important\u2026\u201d\n\nR10:  We would like to discuss the difference between the methods mentioned by the reviewer and ours. \n(1) We do not use DP (Dirichlet Process) at all in our model, we only use the Dirichlet distribution. So, there is no close relation between these models and ours. \n(2) All these models mentioned by the reviewer are single-view models and most of them are designed for other tasks (e.g., multi-task learning), so it is not proper to compare our method with these models. \n(3) We have compared ours with plenty of methods including the most related ones (e.g., EDL/UA/DE/MCDO and others). \n\nWe will clarify these points in our revised version.\n\nMinor: \u201cIt would be great if the authors could report the classification results based on only using one view for Figure 4 as a baseline...\u201d\n\nR\uff1aFig. 4 is used to validate the ability to handle data with noisy view(s) under multi-view setting, so we do not add single-view baselines. To respond to the comment, we have conduct an experiment on two example datasets and the results are as follows:\n\nHandwritten\n\n|  method/noise ($\\sigma$)   | 0  |  1 | 10 |\n|  ----  | ----  | ----  | ----  |\n| Ours  | 98.51\u00b10.15 | 80.75\u00b12.17 |78.75\u00b13.35\n| EDL (multi-view)  | 98.00\u00b10.13 |81.28\u00b13.62 | 40.10\u00b15.47\n| EDL (single-view)  | 97.67\u00b10.32 | 16.46\u00b11.81 | 13.14\u00b10.57\n\nCUB\n\n| method/noise ($\\sigma$)    | 0  |  1 | 10 |\n|  ----  | ----  | ----  | ----  |\n| Ours  | 91.00\u00b10.42 | 62.50\u00b11.27 |60.83\u00b10.93\n| EDL (multi-view)  | 90.83\u00b11.13 |35.83\u00b17.06 | 30.00\u00b12.44\n| EDL (single-view)  | 89.50\u00b11.17 | 30.14\u00b12.43 | 16.23\u00b11.43\n\nIt can be clearly observed that the results using multiple views are much better than those of using single view data. \nDue to time limitation, we will add baselines for the other datasets and update Fig. 4 in the final version.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OOsR8BzCnl5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper591/Authors|ICLR.cc/2021/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869308, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment"}}}, {"id": "m7KHoAepwg", "original": null, "number": 7, "cdate": 1606006468084, "ddate": null, "tcdate": 1606006468084, "tmdate": 1606007716958, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment", "content": {"title": "Thanks and updated manuscript", "comment": "We would like to thank all the reviewers for their positive feedback on the novelty of our work and the significance in experiments.  We have updated the manuscript according to the reviewers\u2019 suggestions. The new added experiments and implementation details are in the appendix. "}, "signatures": ["ICLR.cc/2021/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OOsR8BzCnl5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper591/Authors|ICLR.cc/2021/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869308, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment"}}}, {"id": "mXWfCsHE6NY", "original": null, "number": 5, "cdate": 1605404929958, "ddate": null, "tcdate": 1605404929958, "tmdate": 1606007644237, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "JCHdIdUuUQ9", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We appreciate for the identification of our novelty and the positive comments. \n\nQ1: It will be better if intuitive comparison or discussion between using u (overall uncertainty) and softmax scores in multi-view learning could be provided.\n\nR1: Thanks for the suggestion. More discussion and clarification of using u and softmax scores will be added. Specifically, the classification probability from the softmax function is usually too high for the misclassified samples [1]. When subjective uncertainty is introduced, the model will provide the degree of overall uncertainty, which is important for trusted classification.\n\n[1] Sensoy M, Kaplan L, Kandemir M. Evidential deep learning to quantify classification uncertainty[C]//Advances in Neural Information Processing Systems. 2018: 3179-3189.\n\nQ2: The authors could present failure cases which may be associated with high uncertainties (ideally).\n\nR2: In Table 3\uff08Table 5 in the updated version\uff09we provide several samples that associate with high uncertainty. We will follow the reviewers\u2019 suggestion to add instructions on whether they are classified correctly in the end. \n\nQ3: For the results (Table 2) of the end-to-end experiments, are the data used original or being corrupted manually?\n\nR3: In the experiment in Table 2 (Table 4 in the updated version), we used the original data with noise. To avoid confusion, we have emphasized this in the revision. "}, "signatures": ["ICLR.cc/2021/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OOsR8BzCnl5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper591/Authors|ICLR.cc/2021/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869308, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment"}}}, {"id": "qMGzKKk5900", "original": null, "number": 4, "cdate": 1605404844718, "ddate": null, "tcdate": 1605404844718, "tmdate": 1606007420279, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "pSfB3nhjBpj", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (1/2)", "comment": "We appreciate the detailed comments. We believe the following point-to-point response can address all the concerns:\n\nQ1: The authors added Gaussian noise to half of the views. It is not clear to me that some views are noisy and some of them are not noisy. All of the features in one view are noisy, or all of the features are noisy.\n\nR1: In our experiments, there are datasets with two or more views. For a 2N-view dataset, all features in N views are corrupted with noise, while for (2N+1)-view datasets, N views are corrupted. We also conducted experiments on the naturally noisy data in the supplement (end-to-end experiment), where some of the samples are associated with noise on their all views.\n\nQ2: Dirichlet distribution is somehow more suitable for the situation that one sample can belong to multiple classes. Did the authors consider that?\n\nR2: We agree with the reviewer that the Dirichlet distribution may be applicable for the multi-label task which mainly focuses on how to effectively explore the correlation among different labels. In this work, we focus on the *multi-class* classification task, where each instance has only one label. The multi-label problem is a good direction for future work, but it out of the scope of the current work.\n\nQ3: The integration methods, such as those proposed based on CCA and optimal transport (OT), try to combine different views somehow to achieve the best performance. However, the proposed method seems to investigate each view separately and then combine them. My concern is that when the results are noisy in all views, how the method performs. Is it helpful to combine them in this way? Adding some empirical experimental results would be great.\n\nR3: When all views are noisy, traditional methods (e.g., CCA-based methods) may fail, while fortunately, it is one main advantage of our model for this case due to incorporating each view at evidence level. As shown in Fig. 5, our method can provide promising uncertainty estimation and thus can dynamically balance these noisy views. We also provide empirical results in Table 3 (Table 5 in the updated version). It is observed that when two views are all (heavily) noisy, our method produces high uncertainties for these samples. This is quite helpful for trusted classification.\n\nQ4: It is not clear to me that if the authors assume different views are well aligned or not. If not, can the proposed method handle such a situation?\n\nR4: In our work, different views are aligned which is the same as the setting in most existing multi-view learning methods. Our model does not pay attention to the task of view alignment which is another topic and out of the scope of this work.\n\nQ5: The experiments if Figure 4 aim to show that the proposed method is more robust to the noisy environment. However, adding \\sigma=10^9 does not mean anything, and I think the method's performance is because it is not considering the view with the noise. But in real practice, when we talk about multi-view, both views are reasonably noisy. I prefer to see such a comparison. At least, a comparison in that way is also needed.\n\nR5: Fig. 4 provides the results when partial views are noisy, while note that the end-to-end experiments (in supplement) actually validate the effectiveness of our model in real applications. Specifically, Table 3 (Table 5 in the updated version) illustrates the examples that both views are noisy. It is observed that when two views are all (heavily) noisy, our method produces high uncertainties for these samples.\n\nQ6:  A comparison of the methods in semi-supervised classification would be great.\n\nR6: Our proposed method is a supervised classification algorithm, and currently we do not extend our method to semi-supervised learning. Thus, it is difficult to conduct a fair comparison with semi-supervised methods. Moreover, there are few multi-view semi-supervised classification models for estimating uncertainty.\n\nQ7: How the results change if you remove one of the views from a multiple-view (more than 2 views) dataset.\n\nR7: We will conduct experiments by removing a view manually.\n\nQ8: Table 1 seems to be a comparison with one-view based methods. This is not enough to show the proposed methods are good enough.\n\nR8: Table 1 is only a part of the experiment to show the advantage of multi-view learning. Actually, we also compared ours with methods using multiple views and the results are shown in Fig. 4. For clarification, we conducted experiments in a step-by-step manner in section 4.4: (1) Comparison with uncertainty-based algorithms using the best view (Table .1). (2) Comparison with uncertainty-based algorithms using multiple views (Fig. 4). (3) Uncertainty estimation (Fig. 3 and Fig. 5). Moreover, we also conducted experiments on real-world noisy datasets (see the end-to-end experiment in supplement). \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OOsR8BzCnl5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper591/Authors|ICLR.cc/2021/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869308, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment"}}}, {"id": "lFkiB0qKutu", "original": null, "number": 3, "cdate": 1605404719556, "ddate": null, "tcdate": 1605404719556, "tmdate": 1605407122064, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "pSfB3nhjBpj", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (2/2)", "comment": "The concerns we sorted out in the comments:\n\nQ9: Concerns related to Bayesian CCA.\n\nR9: The CCA-based methods are quite different from ours. (1) Ours is a classification model while CCA-based methods are unsupervised representation learning models. (2) To the best of our knowledge, existing CCA-based methods are unable to provide trusted decisions. Therefore, we can compare ours with Bayesian and deep CCA-based methods only in terms of accuracy and we will do it and provide related discussion. \n\nQ10: Why HDP cannot be included as another baseline.\n\nR10: Hierarchical Dirichlet process (HDP) is a nonparametric Bayesian approach to clustering grouped data instead of classification method. Specifically, the HDP is quite different from the uncertainty-based classification tasks. (1) HDP is usually used for clustering while our method is used for multi-view classification. (2) HDP is usually used in grouped data, while our model is used for multi-view data. Therefore, it is difficult to conduct a fair comparison in experiments.\n\nQ11: Some concerns about writing.\n\nR11: (1) Empirical evidence about our claim \"[...] accurate classification results, they are usually vulnerable to yielding incorrect predictions, particularly when presented with views that are not well-represented [...]\": As shown in Figure 4, when some views are not displayed well, the prediction results are likely to be wrong. (2) Misunderstandings caused by the use of the word \u201coptimal\u201d: The meaning of \u201coptimal\u201d here means that our model can be optimized and ideally will obtain an optimal integration solution. To avoid confusion, we will change \u201coptimal\u201d to \u201cpromising\u201d or \u201cnovel\u201d. Thanks for the reviewer's suggestion."}, "signatures": ["ICLR.cc/2021/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OOsR8BzCnl5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper591/Authors|ICLR.cc/2021/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869308, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment"}}}, {"id": "UlzpcAEnuVE", "original": null, "number": 6, "cdate": 1605405124253, "ddate": null, "tcdate": 1605405124253, "tmdate": 1605406998137, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "vuzQhhGvpRA", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment", "content": {"title": "Response to Question about Table 1 ", "comment": "We appreciate for the identification of our novelty and the positive comments. \n\nSorry for the confusion of Table. 1. Our model is a multi-view learning method, so through all this paper we use multiple views in our model. In Table 1, our model (using all views) outperforms all compared ones that only using the best single view.  Actually, we also compared ours with methods using multiple views and the results are shown in Fig. 4. For clarification, we conducted experiments in a step-by-step manner in section 4.4: (1) Comparison with uncertainty-based algorithms using the best view (Table .1). (2) Comparison with uncertainty-based algorithms using multiple views (Fig. 4). (3) Uncertainty estimation (Fig. 3 and Fig. 5). Moreover, we also conducted experiments on real-world noisy datasets (see the end-to-end experiment in supplement). \n\nWe will clarify this in the revision. Thanks.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OOsR8BzCnl5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper591/Authors|ICLR.cc/2021/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869308, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment"}}}, {"id": "_pyjcRpwKaU", "original": null, "number": 2, "cdate": 1605404561470, "ddate": null, "tcdate": 1605404561470, "tmdate": 1605405342683, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "dSAJJLl9Zaj", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We appreciate for the identification of our contribution.\n\nOur model is a multi-view learning method, so through all this paper we use multiple views in our model. In Table 1, our model outperforms all compared ones that only using the best single view. Furthermore, in Figure 4, our algorithm is also competitive compared with other methods using multiple views especially under noisy cases. We also conducted experiments on the naturally noisy data in the supplement (end-to-end experiment).\n\nWe will add more details of the experimental setup in the supplement, and release the code after acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Paper591/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OOsR8BzCnl5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper591/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper591/Authors|ICLR.cc/2021/Conference/Paper591/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869308, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Comment"}}}, {"id": "vuzQhhGvpRA", "original": null, "number": 1, "cdate": 1605239591210, "ddate": null, "tcdate": 1605239591210, "tmdate": 1605239591210, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Public_Comment", "content": {"title": "Question about Table 1", "comment": "Hi, \n\nThis paper is very well motivated and easy to follow. I have one question about Table 1, as Section 4.2 mentioned that \"report the results of each method with the best-performing view \", does the proposed method also use the best-performing view? If yes, the proposed method (using only one view) would be the same as EDL, then how it outperforms EDL model in table 1?"}, "signatures": ["~Xujiang_Zhao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Xujiang_Zhao1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OOsR8BzCnl5", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/Authors", "ICLR.cc/2021/Conference/Paper591/Reviewers", "ICLR.cc/2021/Conference/Paper591/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024980762, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper591/-/Public_Comment"}}}, {"id": "JCHdIdUuUQ9", "original": null, "number": 1, "cdate": 1603802720911, "ddate": null, "tcdate": 1603802720911, "tmdate": 1605024652293, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Review", "content": {"title": " A reliable multi-view classification mechanism equipped with uncertainty", "review": "This paper proposes a reliable multi-view classification mechanism equipped with uncertainty, called Trusted Multi-View Classification. The goal is to dynamically assess the quality of different views for different samples to provide reliable uncertainty estimation. The idea is clear and well-motivated. The authors perform empirical studies on diverse datasets to conclude that the proposed algorithm is effective, robust and reliable. \n\nStrengths: \n+ It is interesting to conduct multi-view classification by dynamically integrating different views at an evidence level, which provides a novel and flexible way in multi-view classification.\n+ The way of using Dempster-Shafer theory for integrating evidences in a unified and learnable framework is quite neat. \n+ The paper is well-written and clearly presented. \n+ Strong and sufficient empirical results are provided.\n\nMinor comments: \n+ It is reasonable to use the subjective logic theory to directly model uncertainty, however, beyond the advantages mentioned, it will be better if intuitive comparison or discussion between using u (overall uncertainty) and softmax scores in multi-view learning could be provided. \n+ The authors could present failure cases which may be associated with high uncertainties (ideally).\n+ For the results (Table 2) of the end-to-end experiments, are the data used original or being corrupted manually? \n\n\nOverall, the paper is very well motivated and easy to follow. The assumptions and decisions are well supported. The stepwise experiments are helpful and provide good insights to evaluate the proposed algorithm. The method seems to be of great potential in real-world (cost-sensitive) applications. \n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper591/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139685, "tmdate": 1606915787529, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper591/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Review"}}}, {"id": "dSAJJLl9Zaj", "original": null, "number": 3, "cdate": 1603970810240, "ddate": null, "tcdate": 1603970810240, "tmdate": 1605024652144, "tddate": null, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "invitation": "ICLR.cc/2021/Conference/Paper591/-/Official_Review", "content": {"title": "Interesting use of Dirichlet distribution for bringing uncertainty in output in multi-view multiclass classification", "review": "This paper has proposed a novel trust-based multi-view classifier. The idea of transferring classification output to the parameter of Dirichlet distribution to give uncertainty in output is novel and interesting. The model has interestingly used Dirichlet\u2019s strength to define the weight of a view. \n \nPaper is well written and mathematically sound. To my knowledge using Dirichlet distribution for bringing uncertainty in output in multi-view multiclass classification is new.\n\nThe proposed method has been also compared with recent existing works in the task of a single view and multiple view classification. \n\nThe proposed model has been motivated as classification with uncertainty in multi-view case. But in the Experiment section, it shows that even for single view setup the method is outperforming the existing models. It will be good to discuss the reason behind that. Why it is better than other methods in a single view case too?\n\nDetails of the experimental setup are important for the reproducibility of experimental results. It would have been better to have in the main paper.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper591/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper591/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trusted Multi-View Classification", "authorids": ["~Zongbo_Han1", "~Changqing_Zhang1", "~Huazhu_Fu4", "~Joey_Tianyi_Zhou1"], "authors": ["Zongbo Han", "Changqing Zhang", "Huazhu Fu", "Joey Tianyi Zhou"], "keywords": [], "abstract": "Multi-view classification (MVC) generally focuses on improving classification accuracy by using information from different views, typically integrating them into a unified comprehensive representation for downstream tasks. However, it is also crucial to dynamically assess the quality of a view for different samples in order to provide reliable uncertainty estimations, which indicate whether predictions can be trusted. To this end, we propose a novel multi-view classification method, termed trusted multi-view classification, which provides a new paradigm for multi-view learning by dynamically integrating different views at an evidence level. The algorithm jointly utilizes multiple views to promote both classification reliability (uncertainty estimation during testing) and robustness (out-of-distribution-awareness during training) by integrating evidence from each view. To achieve this, the Dirichlet distribution is used to model the distribution of the class probabilities, parameterized with evidence from different views and integrated with the Dempster-Shafer theory. The unified learning framework induces accurate uncertainty and accordingly endows the model with both reliability and robustness for out-of-distribution samples. Extensive experimental results validate the effectiveness of the proposed model in accuracy, reliability and robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|trusted_multiview_classification", "pdf": "/pdf/4ae336db914c13c1db09afbb3dea3d948ad4aa37.pdf", "supplementary_material": "/attachment/340df893388838e9a70e995b0d7067166c7641a0.zip", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nhan2021trusted,\ntitle={Trusted Multi-View Classification},\nauthor={Zongbo Han and Changqing Zhang and Huazhu Fu and Joey Tianyi Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=OOsR8BzCnl5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OOsR8BzCnl5", "replyto": "OOsR8BzCnl5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper591/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139685, "tmdate": 1606915787529, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper591/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper591/-/Official_Review"}}}], "count": 13}