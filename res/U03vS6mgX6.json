{"notes": [{"id": "U03vS6mgX6", "original": "8kuJnycVsl", "number": 11, "cdate": 1582750152034, "ddate": null, "tcdate": 1582750152034, "tmdate": 1587925108412, "tddate": null, "forum": "U03vS6mgX6", "replyto": null, "invitation": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Blind_Submission", "content": {"title": "A Mean-field Analysis of Deep ResNet and Beyond:Towards Provable Optimization Via Overparameterization From Depth", "authorids": ["yplu@stanford.edu", "chaom@princeton.edu", "yulonglu@math.duke.edu", "jianfeng@math.duke.edu", "lexing@stanford.edu"], "keywords": ["Optimization", "Mean-field Analysis"], "abstract": "Training deep neural networks with stochastic gradient descent (SGD) can often achieve zero training loss on real-world tasks although the optimization landscape is known to be highly non-convex. To understand the success of SGD for training deep neural networks, this work presents a mean-field analysis of deep residual networks, based on a line of works that interpret the continuum limit of the deep residual network as an ordinary differential equation when the network capacity tends to infinity. Specifically, we propose a \\textbf{new continuum limit} of deep residual networks, which enjoys a good landscape in the sense that \\textbf{every local minimizer is global}. \nThis characterization enables us to derive the first global convergence result for multilayer neural networks in the mean-field regime. Furthermore, without assuming the convexity of the loss landscape, our proof relies on a zero-loss assumption at the global minimizer that can be achieved when the model shares a universal approximation property. Key to our result is the observation that a deep residual network resembles a shallow network ensemble~\\cite{veit2016residual}, \\emph{i.e.} a two-layer network. We bound the difference between the shallow network and our ResNet model via the adjoint sensitivity method, which enables us to apply existing mean-field analyses of two-layer networks to deep networks. Furthermore, we propose several novel training schemes based on the new continuous model, including one training procedure that switches the order of the residual blocks and results in strong empirical performance on the benchmark datasets. ", "pdf": "/pdf/f0d1781326578e49413fac30b6d9c577eea5f126.pdf", "TL;DR": "We derive a new continuous depth limit of deep ResNets where a local minimum is a global one.", "authors": ["Yiping Lu", "Chao Ma", "Yulong Lu", "Jianfeng Lu", "Lexing Ying"], "paperhash": "lu|a_meanfield_analysis_of_deep_resnet_and_beyondtowards_provable_optimization_via_overparameterization_from_depth", "_bibtex": "@inproceedings{\nlu2020a,\ntitle={A Mean-field Analysis of Deep ResNet and Beyond:Towards Provable Optimization Via Overparameterization From Depth},\nauthor={Yiping Lu and Chao Ma and Yulong Lu and Jianfeng Lu and Lexing Ying},\nbooktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},\nyear={2020},\nurl={https://openreview.net/forum?id=U03vS6mgX6}\n}"}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq"]}, "signatures": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "readers": ["everyone"], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "invitees": ["~"], "tcdate": 1582750147213, "tmdate": 1587924718420, "id": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "htUj9NiVoI", "original": null, "number": 1, "cdate": 1582774857093, "ddate": null, "tcdate": 1582774857093, "tmdate": 1582774857093, "tddate": null, "forum": "U03vS6mgX6", "replyto": "U03vS6mgX6", "invitation": "ICLR.cc/2020/Workshop/DeepDiffEq/Paper11/-/Decision", "content": {"decision": "Accept (Poster)", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Mean-field Analysis of Deep ResNet and Beyond:Towards Provable Optimization Via Overparameterization From Depth", "authorids": ["yplu@stanford.edu", "chaom@princeton.edu", "yulonglu@math.duke.edu", "jianfeng@math.duke.edu", "lexing@stanford.edu"], "keywords": ["Optimization", "Mean-field Analysis"], "abstract": "Training deep neural networks with stochastic gradient descent (SGD) can often achieve zero training loss on real-world tasks although the optimization landscape is known to be highly non-convex. To understand the success of SGD for training deep neural networks, this work presents a mean-field analysis of deep residual networks, based on a line of works that interpret the continuum limit of the deep residual network as an ordinary differential equation when the network capacity tends to infinity. Specifically, we propose a \\textbf{new continuum limit} of deep residual networks, which enjoys a good landscape in the sense that \\textbf{every local minimizer is global}. \nThis characterization enables us to derive the first global convergence result for multilayer neural networks in the mean-field regime. Furthermore, without assuming the convexity of the loss landscape, our proof relies on a zero-loss assumption at the global minimizer that can be achieved when the model shares a universal approximation property. Key to our result is the observation that a deep residual network resembles a shallow network ensemble~\\cite{veit2016residual}, \\emph{i.e.} a two-layer network. We bound the difference between the shallow network and our ResNet model via the adjoint sensitivity method, which enables us to apply existing mean-field analyses of two-layer networks to deep networks. Furthermore, we propose several novel training schemes based on the new continuous model, including one training procedure that switches the order of the residual blocks and results in strong empirical performance on the benchmark datasets. ", "pdf": "/pdf/f0d1781326578e49413fac30b6d9c577eea5f126.pdf", "TL;DR": "We derive a new continuous depth limit of deep ResNets where a local minimum is a global one.", "authors": ["Yiping Lu", "Chao Ma", "Yulong Lu", "Jianfeng Lu", "Lexing Ying"], "paperhash": "lu|a_meanfield_analysis_of_deep_resnet_and_beyondtowards_provable_optimization_via_overparameterization_from_depth", "_bibtex": "@inproceedings{\nlu2020a,\ntitle={A Mean-field Analysis of Deep ResNet and Beyond:Towards Provable Optimization Via Overparameterization From Depth},\nauthor={Yiping Lu and Chao Ma and Yulong Lu and Jianfeng Lu and Lexing Ying},\nbooktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},\nyear={2020},\nurl={https://openreview.net/forum?id=U03vS6mgX6}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Paper Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject"], "description": "Decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}, "forum": "U03vS6mgX6", "replyto": "U03vS6mgX6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}}, "cdate": 1582156800000, "expdate": 1589155200000, "duedate": 1588291200000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "tcdate": 1582771074681, "tmdate": 1587925014911, "super": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Decision", "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "id": "ICLR.cc/2020/Workshop/DeepDiffEq/Paper11/-/Decision"}}}], "count": 2}