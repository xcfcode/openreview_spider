{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396423086, "tcdate": 1486396423086, "number": 1, "id": "r1JfhfI_g", "invitation": "ICLR.cc/2017/conference/-/paper193/acceptance", "forum": "HkIQH7qel", "replyto": "HkIQH7qel", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, most reviewers are not leaning sufficiently towards acceptance. In particular, it is unfortunate that authors can not evaluate their model on the leaderboard due to copyright issues. The role of standard datasets and benchmarks is to allow for meaningful comparisons. Evaluation on non-standard splits defeats this purpose. Fortunately, sounds like authors are working on getting their model evaluated on the leaderboard. Resolving that and incorporating reviewers' feedback will help make the paper stronger."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "pdf": "/pdf/fbd210da689f374d13a294133f9b9cc1fa9f671c.pdf", "TL;DR": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "paperhash": "lee|learning_recurrent_span_representations_for_extractive_question_answering", "keywords": ["Natural language processing"], "conflicts": ["cs.washington.edu", "google.com"], "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396423660, "id": "ICLR.cc/2017/conference/-/paper193/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkIQH7qel", "replyto": "HkIQH7qel", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396423660}}}, {"tddate": null, "tmdate": 1484273124307, "tcdate": 1484273124307, "number": 2, "id": "S1nJIhrIl", "invitation": "ICLR.cc/2017/conference/-/paper193/public/comment", "forum": "HkIQH7qel", "replyto": "HkIQH7qel", "signatures": ["~Kenton_Lee1"], "readers": ["everyone"], "writers": ["~Kenton_Lee1"], "content": {"title": "Review response", "comment": "We thank all three reviewers for the valuable comments and suggestions. \n\nWe agree with reviewer 1 that the lack of test results is not ideal and sadly we do not yet have a manner in which we can run on the hidden test set, as not all of our code is open-sourced. However, we hope that the significant dev set size of 10k items, along with the cross-validation results add some reassurance that our hyperparameter tuning scheme has not overfit the data.\n\nReviewer 3 points out that the results in this paper are no longer state of the art. It is true that there are other papers on the leaderboard that have now surpassed our results, largely through ensembling. However, we believe that our paper is the only work to specifically study the impact of different span representations and we agree with Reviewer 3 that our findings should be complementary to other recent work on this dataset. We have added some extra quantitative and qualitative analysis of the differences between the span classifier and the endpoints predictor to illustrate the manner in which the quality of endpoint predictions degrade for longer sentences, in particular showing the tendency of endpoint models to pick out endpoints from separate answer candidates.\n\nReviewer 2 points out that the difference in performance between our model and the Match-LSTM cannot be accounted for by the difference in label type alone, and asks for the other most salient differences between the two approaches. While there are many small differences between the two implementations, the ablations in Table 2.a. suggest that most of this gap is accounted for by the passage independent question representation that is missing in the Match-LSTM. We have added an analysis of this representation in a new Table 3 and we have updated our discussion of the Match LSTM to clarify the basis of our comparison."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "pdf": "/pdf/fbd210da689f374d13a294133f9b9cc1fa9f671c.pdf", "TL;DR": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "paperhash": "lee|learning_recurrent_span_representations_for_extractive_question_answering", "keywords": ["Natural language processing"], "conflicts": ["cs.washington.edu", "google.com"], "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287692286, "id": "ICLR.cc/2017/conference/-/paper193/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkIQH7qel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper193/reviewers", "ICLR.cc/2017/conference/paper193/areachairs"], "cdate": 1485287692286}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484268385009, "tcdate": 1478272286123, "number": 193, "id": "HkIQH7qel", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HkIQH7qel", "signatures": ["~Tom_Kwiatkowski1"], "readers": ["everyone"], "content": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "pdf": "/pdf/fbd210da689f374d13a294133f9b9cc1fa9f671c.pdf", "TL;DR": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "paperhash": "lee|learning_recurrent_span_representations_for_extractive_question_answering", "keywords": ["Natural language processing"], "conflicts": ["cs.washington.edu", "google.com"], "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481962394853, "tcdate": 1481962394853, "number": 3, "id": "HkXjQ_zVg", "invitation": "ICLR.cc/2017/conference/-/paper193/official/review", "forum": "HkIQH7qel", "replyto": "HkIQH7qel", "signatures": ["ICLR.cc/2017/conference/paper193/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper193/AnonReviewer3"], "content": {"title": "review", "rating": "7: Good paper, accept", "review": "This paper proposes RaSoR, a method to efficiently representing and scoring all possible spans in an extractive QA task. While the test set results on SQuAD have not been released, it looks likely that they are not going to be state-of-the-art; with that said, the idea of enumerating all possible spans proposed in this paper could potentially improve many architectures. The paper is very well-written and the analysis/ablations in the final sections are mostly interesting (especially Figure 2, which confirms what we would intuitively believe). Based on its potential to positively impact other researchers working on SQuAD, I recommend that the paper is accepted. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "pdf": "/pdf/fbd210da689f374d13a294133f9b9cc1fa9f671c.pdf", "TL;DR": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "paperhash": "lee|learning_recurrent_span_representations_for_extractive_question_answering", "keywords": ["Natural language processing"], "conflicts": ["cs.washington.edu", "google.com"], "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512669057, "id": "ICLR.cc/2017/conference/-/paper193/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper193/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper193/AnonReviewer2", "ICLR.cc/2017/conference/paper193/AnonReviewer1", "ICLR.cc/2017/conference/paper193/AnonReviewer3"], "reply": {"forum": "HkIQH7qel", "replyto": "HkIQH7qel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper193/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper193/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512669057}}}, {"tddate": null, "tmdate": 1481898421771, "tcdate": 1481898421771, "number": 2, "id": "Hk0nYO-El", "invitation": "ICLR.cc/2017/conference/-/paper193/official/review", "forum": "HkIQH7qel", "replyto": "HkIQH7qel", "signatures": ["ICLR.cc/2017/conference/paper193/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper193/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents an architecture for answer extraction task and evaluates on the SQUAD dataset. The proposed model builds fixed length representations of all spans in the answer document based on recurrent neural network. It outperforms a few baselines in exact match and F1 on SQUAD.\n\nIt is unfortunate that the blind test results are not obtained yet due to the copyright issue. There are quite a few other systems/submissions on the SQUAD leader board that were available for comparison.\n\nGiven that there's no result on the test set reported, the grid search for hyperparameters on the dev set directly is also a concern, even though the authors did cross validation experiments.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "pdf": "/pdf/fbd210da689f374d13a294133f9b9cc1fa9f671c.pdf", "TL;DR": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "paperhash": "lee|learning_recurrent_span_representations_for_extractive_question_answering", "keywords": ["Natural language processing"], "conflicts": ["cs.washington.edu", "google.com"], "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512669057, "id": "ICLR.cc/2017/conference/-/paper193/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper193/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper193/AnonReviewer2", "ICLR.cc/2017/conference/paper193/AnonReviewer1", "ICLR.cc/2017/conference/paper193/AnonReviewer3"], "reply": {"forum": "HkIQH7qel", "replyto": "HkIQH7qel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper193/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper193/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512669057}}}, {"tddate": null, "tmdate": 1481894036440, "tcdate": 1481894036440, "number": 1, "id": "B1hc_Pb4x", "invitation": "ICLR.cc/2017/conference/-/paper193/official/review", "forum": "HkIQH7qel", "replyto": "HkIQH7qel", "signatures": ["ICLR.cc/2017/conference/paper193/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper193/AnonReviewer2"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The authors proposed RASOR to address the problem of finding the best answer span according to a given question. The focus of the paper is mainly on how to model the relationship between question and the answer spans. The idea proposed by this paper is reasonable, but not ground breaking. The analysis is interesting and potentially useful. I would hope the authors can go extra miles to analyze different choices of boundary prediction models and make a more convincing case for the necessity of modeling the score of the span globally.\n\nThe main idea behind RASOR is to globally normalize and rank the scores of the possible answer spans. RASOR is able to achieve this by first modeling the hidden vectors of all words with LSTMs. Then, the representation of a text span is formed by concatenating the corresponding hidden vectors of the start and the end word of the corresponding chunk. The approach is reasonable, but not earth shattering. Also, the table 6 shows that the improvement over end-prediction point is not very large.\n\nI appreciate the fact that authors conduct several analysis experiments as some of them are quite interesting. For example, it seems that question independent representation is also very import to the performance. In addition to the current analysis, I also want to get a clear idea on what makes the current model be better than the Match-LSTM. Is it hyper-parameter tuning? Or it is due to the use of the question independent representation?\n\nAnother good thing about the proposed model is that it is relatively simple, so there is a chance that the proposed techniques can be combined with other newly proposed ones. \n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "pdf": "/pdf/fbd210da689f374d13a294133f9b9cc1fa9f671c.pdf", "TL;DR": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "paperhash": "lee|learning_recurrent_span_representations_for_extractive_question_answering", "keywords": ["Natural language processing"], "conflicts": ["cs.washington.edu", "google.com"], "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512669057, "id": "ICLR.cc/2017/conference/-/paper193/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper193/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper193/AnonReviewer2", "ICLR.cc/2017/conference/paper193/AnonReviewer1", "ICLR.cc/2017/conference/paper193/AnonReviewer3"], "reply": {"forum": "HkIQH7qel", "replyto": "HkIQH7qel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper193/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper193/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512669057}}}, {"tddate": null, "tmdate": 1480972792311, "tcdate": 1480972792305, "number": 1, "id": "rkl-5LXXg", "invitation": "ICLR.cc/2017/conference/-/paper193/public/comment", "forum": "HkIQH7qel", "replyto": "SJtjsYAMg", "signatures": ["~Kenton_Lee1"], "readers": ["everyone"], "writers": ["~Kenton_Lee1"], "content": {"title": "Model clarifications", "comment": "1. Assuming that you are referring to w_q in equation 10, this is a learned weighted vector that is used to score the output of the feed-forward neural network from the same equation. We have updated the submission to clarify this. The attention mechanism in the passage-independent representation is simply one method of aggregating the sequence of LSTM outputs into a single embedding. We explored alternatives, such as using the first and last LSTM outputs, or averaging, but these performed significantly worse.\n\n2. We mean greedy to be the following: (1) During training, the end index prediction is always conditioned on the gold start index, and the model is not exposed the scenario where the model chooses a non-gold start index. (2) During decoding, greedily choosing the highest scoring start index could lead to an overall sub-optimal pair of endpoints. \n\nOur endpoints predictor chooses start and end indices independently, meaning that the choice of the end index does not depend on the start index. Making these choices independently is not as good as making a global decision about the best answer span, but it avoids relying on a potentially suboptimal start index which a greedy model would not have been exposed to.\n\nDue to significant differences between the PtrNet architecture and RaSoR, we cannot attribute all of the increase in performance to the non-greedy decoding strategy. However, we believe that it is a significant factor in the model's better performance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "pdf": "/pdf/fbd210da689f374d13a294133f9b9cc1fa9f671c.pdf", "TL;DR": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "paperhash": "lee|learning_recurrent_span_representations_for_extractive_question_answering", "keywords": ["Natural language processing"], "conflicts": ["cs.washington.edu", "google.com"], "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287692286, "id": "ICLR.cc/2017/conference/-/paper193/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkIQH7qel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper193/reviewers", "ICLR.cc/2017/conference/paper193/areachairs"], "cdate": 1485287692286}}}, {"tddate": null, "tmdate": 1480657824952, "tcdate": 1480657824947, "number": 1, "id": "SJtjsYAMg", "invitation": "ICLR.cc/2017/conference/-/paper193/pre-review/question", "forum": "HkIQH7qel", "replyto": "HkIQH7qel", "signatures": ["ICLR.cc/2017/conference/paper193/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper193/AnonReviewer2"], "content": {"title": "Pre-review questions", "question": "1.\tI found the descriptions of passage independent representation a bit confusing. What is word_q in equation 10? Why do we need to use attention structure here?\n2.\tIn page 6, the authors mention that the reason why Wang & Jiang\u2019 model does not perform so well is due to \u201cboth training and evaluation are greedy, making their system susceptible to search errors when decoding.\u201d However, the endpoints version of RASOR seems also to be greedy, but it seems to perform pretty well, as it outperformed Wang & Jiang (2016) already. Could the authors clarify this point for me?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Recurrent Span Representations for Extractive Question Answering", "abstract": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "pdf": "/pdf/fbd210da689f374d13a294133f9b9cc1fa9f671c.pdf", "TL;DR": "We present a globally normalized architecture for extractive question answering that contains explicit representations of all possible answer spans.", "paperhash": "lee|learning_recurrent_span_representations_for_extractive_question_answering", "keywords": ["Natural language processing"], "conflicts": ["cs.washington.edu", "google.com"], "authors": ["Kenton Lee", "Tom Kwiatkowksi", "Ankur Parikh", "Dipanjan Das"], "authorids": ["kentonl@cs.washington.edu", "tomkwiat@google.com", "aparikh@google.com", "dipanjand@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959414697, "id": "ICLR.cc/2017/conference/-/paper193/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper193/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper193/AnonReviewer2"], "reply": {"forum": "HkIQH7qel", "replyto": "HkIQH7qel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper193/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper193/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959414697}}}], "count": 8}