{"notes": [{"tddate": null, "ddate": null, "tmdate": 1528370703910, "tcdate": 1509134005583, "number": 745, "cdate": 1518730166526, "id": "ryRh0bb0Z", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "ryRh0bb0Z", "original": "Hkp3AWbRZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Multi-View Data Generation Without View Supervision", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize. ", "pdf": "/pdf/a9118b028383e33ddd307f714ca1ecb1ec094415.pdf", "TL;DR": "We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.", "paperhash": "chen|multiview_data_generation_without_view_supervision", "authors": ["Mickael Chen", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "keywords": ["multi-view", "adversarial learning", "generative model"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lif.univ-mrs.fr"], "_bibtex": "@inproceedings{\nchen2018multiview,\ntitle={Multi-View Data Generation Without View Supervision},\nauthor={Mickael Chen and Ludovic Denoyer and Thierry Arti\u00e8res},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryRh0bb0Z},\n}"}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260100663, "tcdate": 1517249216063, "number": 33, "cdate": 1517249216041, "id": "HkuazyaBf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "ryRh0bb0Z", "replyto": "ryRh0bb0Z", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper presents an unsupervised GAN-based model for disentagling the multiple views of the data and their content.\n\nOverall it seems that this paper was well received by the reviewers, who find it novel and significant . The consensus is that the results are promising.\n\nThere are some concerns, but the major ones listed below have been addressed in the rebuttal. Specifically:\n-\tR3 had a concern about the experimental evaluation, which has been addressed in the rebuttal.\n-\tR2 had a concern about a problem inherent in this setting (what is treated as \u201ccontent\u201d), and the authors have clarified in the discussion the assumptions under which such methods operate.\n-\tR1 had concerns related to how the proposed model fits in the literature. Again, the authors have addressed this concern adequately.\n", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Data Generation Without View Supervision", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize. ", "pdf": "/pdf/a9118b028383e33ddd307f714ca1ecb1ec094415.pdf", "TL;DR": "We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.", "paperhash": "chen|multiview_data_generation_without_view_supervision", "authors": ["Mickael Chen", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "keywords": ["multi-view", "adversarial learning", "generative model"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lif.univ-mrs.fr"], "_bibtex": "@inproceedings{\nchen2018multiview,\ntitle={Multi-View Data Generation Without View Supervision},\nauthor={Mickael Chen and Ludovic Denoyer and Thierry Arti\u00e8res},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryRh0bb0Z},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515741608305, "tcdate": 1511428255800, "number": 1, "cdate": 1511428255800, "id": "r1Ojef4gf", "invitation": "ICLR.cc/2018/Conference/-/Paper745/Official_Review", "forum": "ryRh0bb0Z", "replyto": "ryRh0bb0Z", "signatures": ["ICLR.cc/2018/Conference/Paper745/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "The authors propose a GAN formulation for multi-view learning trained to disentangle content from other aspects (\"view\") influencing the image, presented from a slightly narrow perspective.", "rating": "7: Good paper, accept", "review": "The paper proposes a GAN-based method for image generation that attempts to separate latent variables describing fixed \"content\" of objects from latent variables describing properties of \"view\" (all dynamic properties such as lighting, viewpoint, accessories, etc). The model is further extended for conditional generation and demonstrated on a range of image benchmark data sets.\n\nThe core idea is to train the model on pairs of images corresponding to the same content but varying in views, using adversarial training to discriminate such examples from generated pairs. This is a reasonable procedure and it seems to work well, but also conceptually quite straightforward -- this is quite likely how most people working in the field would solve this problem, standard GAN techniques are used for training the generator and discriminator, and the network architecture is directly borrowed from Radford et al. (2015) and not even explained at all in the paper. The conditional variant is less obvious, requiring two kinds of negative images, and again the proposed approach seems technically sound.\n\nGiven the simplicity of the algorithmic choices, the potential novelty of the paper lies more in the problem formulation itself, which considers the question of separating two sets of latent variables from each other in setups where one of them (the \"view\") can vary from pair to pair in arbitrary manner and no attributes characterising the view are provided. This is an interesting problem setup, but not novel as such and unfortunately the paper does not do a very good job in putting it into the right context. The work is contrasted only against recent GAN-based image generation literature (where covariates for the views are often included) and the aspects related to multi-view learning are described only at the level of general intuition, instead of relating to the existing literature on the topic. The only relevant work cited from this angle is Mathieu et al. (2016), but even that is dismissed lightly by saying it is worse in generative tasks. How about the differences (theoretical and empirical) between the proposed approach and theirs in disentangling the latent variables? One would expect to see more discussion on this, given the importance of this property as motivation for the method.\n\nThe generative story using three sets of latent variables, one shared, to describe a pair of objects corresponds to inter-battery factor analysis (IBFA) and is hence very closely related to canonical correlation analysis as well (Tucker \"An inter-battery method of factor analysis\", Psychometrika, 1958; Klami et al. \"Bayesian canonical correlation analysis\", JMLR, 2013). Linear CCA naturally would not be sufficient for generative modeling and its non-linear variants (e.g. Wang et al. \"Deep variational canonical correlation analysis\", arXiv:1610.03454, 2016; Damianou et al. \"Manifold relevance determination\", ICML, 2012) would not produce visually pleasing generative samples either, but the relationship is so close that these models have even been used for analysing setups identical to yours (e.g. Li et al. \"Cross-pose face recognition by canonical correlation analysis\", arXiv:1507.08076, 2015) but with goals other than generation. Consequently, the reader would expect to learn something about the relationship between the proposed method and the earlier literature building on the same latent variable formulation. A particularly interesting question would be whether the proposed model actually is a direct GAN-based extension of IBFA, and if not then how does it differ. Use of adversarial training to encourage separation of latent variables is clearly a reasonable idea and quite likely does better job than the earlier solutions (typically based on some sort of group-sparsity assumption in shared-private factorisation) with the possible or even likely exception of Mathieu at al. (2016), and aspects like this should be explicitly discussed to extend the contribution from pure image generation to multi-view literature in general.\n\nThe empirical experiments are somewhat non-informative, relying heavily on visual comparisons and only satisfying the minimum requirement of demonstrating that the method does its job. The results look aesthetically more pleasing than the baselines, but the reader does not learn much about how the method actually behaves in practice; when does it break down, how sensitive it is to various choices (network structure, learning algorithm, amount of data,  how well the content and view can be disentangled from each other, etc.). In other words, the evaluation is a bit lazy somewhat in the same sense as the writing and treatment of related work; the authors implemented the model and ran it on a collection of public data sets, but did not venture further into scientific reporting of the merits and limitations of the approach.\n\nFinally, Table 1 seems to have some min/max values the wrong way around.\n\n\nRevision of the review in light of the author response:\nThe authors have adequately addressed my main remarks, and while doing so have improved both the positioning of the paper amongst relevant literature and the somewhat limited empirical comparisons. In particular, the authors now discuss alternative multi-view generative models not based on GANs and the revised paper includes considerably extended set of numerical comparisons that better illustrate the advantage over earlier techniques. I have increased my preliminary rating to account for these improvements.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Multi-View Data Generation Without View Supervision", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize. ", "pdf": "/pdf/a9118b028383e33ddd307f714ca1ecb1ec094415.pdf", "TL;DR": "We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.", "paperhash": "chen|multiview_data_generation_without_view_supervision", "authors": ["Mickael Chen", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "keywords": ["multi-view", "adversarial learning", "generative model"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lif.univ-mrs.fr"], "_bibtex": "@inproceedings{\nchen2018multiview,\ntitle={Multi-View Data Generation Without View Supervision},\nauthor={Mickael Chen and Ludovic Denoyer and Thierry Arti\u00e8res},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryRh0bb0Z},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642502188, "id": "ICLR.cc/2018/Conference/-/Paper745/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper745/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper745/AnonReviewer1", "ICLR.cc/2018/Conference/Paper745/AnonReviewer2", "ICLR.cc/2018/Conference/Paper745/AnonReviewer3"], "reply": {"forum": "ryRh0bb0Z", "replyto": "ryRh0bb0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642502188}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642502243, "tcdate": 1511679414185, "number": 2, "cdate": 1511679414185, "id": "SyAnSJdxf", "invitation": "ICLR.cc/2018/Conference/-/Paper745/Official_Review", "forum": "ryRh0bb0Z", "replyto": "ryRh0bb0Z", "signatures": ["ICLR.cc/2018/Conference/Paper745/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Interesting approach on separating content and views for a specific distribution, but lacks interpretability for diverse datasets", "rating": "5: Marginally below acceptance threshold", "review": "This paper firstly proposes a GAN architecture that aim at decomposing the underlying distribution of a particular class into \"content\" and \"view\". The content can be seen as an intrinsic instantiation of the class that is independent of certain types of variation (eg viewpoint), and a view is the observation of the object under a particular variation. The authors additionally propose a second conditional GAN that learns to generate different views given a specific content. \n\nI find the idea of separating content and view interesting and I like the GMV and CGMV architectures. Not relying on manual attribute/class annotation for the views is also positive. The approach seems to work well for a relatively clean setup such as the chair dataset, but for the other datasets the separation is not so apparent. For example, in figure 5, what does each column represent in terms of view? It seems that it depends heavily on the content. That raises the question of how useful it is to have such a separation between content and views; for some datasets their diversity can be a bottleneck for this partition, making the interpretation of views difficult. \n\nA missing (supervised) reference that considers also the separation of content and views.\n[A] Learning to generate chairs with convolutional neural networks, Alexey Dosovitskiy, Jost Tobias Springenberg, Thomas Brox, CVPR 15\n\nQ:Figure 5, you mean \"all images in a column were generated with the same view vector\"\nQ: Why on Figure 7 you use different examples for CGAN?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Data Generation Without View Supervision", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize. ", "pdf": "/pdf/a9118b028383e33ddd307f714ca1ecb1ec094415.pdf", "TL;DR": "We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.", "paperhash": "chen|multiview_data_generation_without_view_supervision", "authors": ["Mickael Chen", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "keywords": ["multi-view", "adversarial learning", "generative model"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lif.univ-mrs.fr"], "_bibtex": "@inproceedings{\nchen2018multiview,\ntitle={Multi-View Data Generation Without View Supervision},\nauthor={Mickael Chen and Ludovic Denoyer and Thierry Arti\u00e8res},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryRh0bb0Z},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642502188, "id": "ICLR.cc/2018/Conference/-/Paper745/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper745/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper745/AnonReviewer1", "ICLR.cc/2018/Conference/Paper745/AnonReviewer2", "ICLR.cc/2018/Conference/Paper745/AnonReviewer3"], "reply": {"forum": "ryRh0bb0Z", "replyto": "ryRh0bb0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642502188}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642502204, "tcdate": 1512006868932, "number": 3, "cdate": 1512006868932, "id": "r1aAVyagf", "invitation": "ICLR.cc/2018/Conference/-/Paper745/Official_Review", "forum": "ryRh0bb0Z", "replyto": "ryRh0bb0Z", "signatures": ["ICLR.cc/2018/Conference/Paper745/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "A good paper using GANs for multi-view image generation", "rating": "7: Good paper, accept", "review": "The paper proposes a new generative model based on the Generative Adversarial Network (GAN). The method disentangles the content and the view of objects without view supervision. The proposed Generative Multi-View (GMV) model can be considered to be an extension of the traditional GAN, where the GMV takes the content latent  vector and the view latent vector as input. In addition, the GMV is trained to generate a pair of objects that share the content but with different views. In this way, the GMV successfully models the content and the view of the objects without using view labels. The paper also extends GMV into a conditional generative model that takes an input image and generates different views of the object in the input image. Experiments are conducted on four different datasets to show the generative ability of the proposed method.\n\nPositives:\n- The proposed method is novel in disentangling the content and the view of objects in a GAN and training the GAN with pairs of objects. By using pairs that share the content but with different views, the model can be trained successfully without using view labels.\n\n- The experimental results on the four datasets show that the proposed network is able to model the context and the view of objects when generating images of these objects.\n\nNegatives:\n- The paper only shows comparison between the proposed method and several baselines: DCGAN and CGAN. There is no comparison with methods that also disentangle the content from the view such as Mathieu et al. 2016.\n\n- For the comparison with CGAN in Figure 7, it would be better to show the results of C-GMV and CGAN on the same input images. Then it is easier for the readers to see the differences in the results from the two methods. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Data Generation Without View Supervision", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize. ", "pdf": "/pdf/a9118b028383e33ddd307f714ca1ecb1ec094415.pdf", "TL;DR": "We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.", "paperhash": "chen|multiview_data_generation_without_view_supervision", "authors": ["Mickael Chen", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "keywords": ["multi-view", "adversarial learning", "generative model"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lif.univ-mrs.fr"], "_bibtex": "@inproceedings{\nchen2018multiview,\ntitle={Multi-View Data Generation Without View Supervision},\nauthor={Mickael Chen and Ludovic Denoyer and Thierry Arti\u00e8res},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryRh0bb0Z},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642502188, "id": "ICLR.cc/2018/Conference/-/Paper745/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper745/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper745/AnonReviewer1", "ICLR.cc/2018/Conference/Paper745/AnonReviewer2", "ICLR.cc/2018/Conference/Paper745/AnonReviewer3"], "reply": {"forum": "ryRh0bb0Z", "replyto": "ryRh0bb0Z", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642502188}}}, {"tddate": null, "ddate": null, "tmdate": 1514740357076, "tcdate": 1514740357076, "number": 3, "cdate": 1514740357076, "id": "SkaF99UXf", "invitation": "ICLR.cc/2018/Conference/-/Paper745/Official_Comment", "forum": "ryRh0bb0Z", "replyto": "r1Ojef4gf", "signatures": ["ICLR.cc/2018/Conference/Paper745/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper745/Authors"], "content": {"title": "Re: The authors propose a GAN formulation for multi-view learning trained to disentangle content from other aspects (\"view\") influencing the image, presented from a slightly narrow perspective.", "comment": "We thank the reviewer for the comments and feedback. We apologize for the late reply due to the large number of experiments that have been made to improve the quality of the paper.\n\nConcerning the fact that our generative model is \u201cconceptually quite straightforward\u201d, we would like to emphasis that the proposed paper is as far as we know the first paper to evaluate this idea of using a discriminator on pairs of outputs for the multiview problem, this discriminator being in charge of telling is the two outputs correspond to the same object. \n\nWe acknowledge the reviewer for pointing us this extensive literature on IBFA and on similar ideas in CCA and non linear variants of CCA. Of course our method is clearly related to this literature and we added this related work on the state if the art section. As suggested by the reviewer the assumption made by our method is very similar to the one made with IBFA models. The main difference being in the way the models are learned: by using \u2018strong\u2018 regularization and particular factorization functions in the IBFA literature, or by using a discriminator in our case.  Note also that most experiments in the IBFA literature are based on datasets where a limited finite number of possible views is provided while our model is evaluated on complex datasets with multiple possible views, without any available view supervision. A detailed discussion on this point has been added in Section 6. \n\nAbout Radford architecture. Yes we do reuse the architecture in [Radford et al., 2015] for the DCGAN architecture because the core idea of the paper is elsewhere, as it was the case for [Mathieu et al., 2016]. Actually the main features of our method, its ability to learn from data whose views are not aligned between objects and which are unlabeled comes from our particular learning scheme and the way we build pairs of examples. This is why we focus the presentation of our method on this particular way of constructing training examples for our models.\n\nPlease consider  that we have added a large additional experimental section that objectively evaluates the quality of the generated samples of the different models (GMV, CMGV, GANx, CGAN and Mathieu et al.) in terms of quality of the outputs, and in terms of diversity of the generated samples, showing the superiority of our model w.r.t these baselines (new section 5.3, pages 11 to 13 of the new version)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Data Generation Without View Supervision", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize. ", "pdf": "/pdf/a9118b028383e33ddd307f714ca1ecb1ec094415.pdf", "TL;DR": "We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.", "paperhash": "chen|multiview_data_generation_without_view_supervision", "authors": ["Mickael Chen", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "keywords": ["multi-view", "adversarial learning", "generative model"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lif.univ-mrs.fr"], "_bibtex": "@inproceedings{\nchen2018multiview,\ntitle={Multi-View Data Generation Without View Supervision},\nauthor={Mickael Chen and Ludovic Denoyer and Thierry Arti\u00e8res},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryRh0bb0Z},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728445, "id": "ICLR.cc/2018/Conference/-/Paper745/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryRh0bb0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper745/Authors|ICLR.cc/2018/Conference/Paper745/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper745/Authors|ICLR.cc/2018/Conference/Paper745/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper745/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper745/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper745/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper745/Reviewers", "ICLR.cc/2018/Conference/Paper745/Authors", "ICLR.cc/2018/Conference/Paper745/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728445}}}, {"tddate": null, "ddate": null, "tmdate": 1514740309277, "tcdate": 1514740309277, "number": 2, "cdate": 1514740309277, "id": "HJpLqqIXf", "invitation": "ICLR.cc/2018/Conference/-/Paper745/Official_Comment", "forum": "ryRh0bb0Z", "replyto": "SyAnSJdxf", "signatures": ["ICLR.cc/2018/Conference/Paper745/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper745/Authors"], "content": {"title": "Re: Interesting approach on separating content and views for a specific distribution, but lacks interpretability for diverse datasets", "comment": "We thank the reviewer for the comments and feedback. We apologize for the late reply due to the large number of experiments that have been made to improve the quality of the paper.\n\nAs far as we understand, the main concern is about the fact that the interpretation of the notion of view can be difficult depending on the nature of the dataset. We agree on that point. Indeed, what we call \u2018content\u2019 in this paper corresponds to the invariant factors contained in a set of images representing a same object, the view corresponding to the remaining \u2018changing\u2019 factors. This is the assumption also made for the IBFA and CCA based approaches (see next review). We have added a discussion on this point in the paper in the literature review section. Note also  that the more difficult interpretation of views in our work is the counterpart of  the increased ability of the method to deal with various datasets.\n\nConcerning the suggested reference, our related work is focused on  models that are not based on view supervision. \n\nNote that we have added a large additional experimental section that objectively evaluates the quality of the generated samples of the different models (GMV, CMGV, GANx, CGAN and Mathieu et al.) in terms of quality of the outputs, and in terms of diversity of the generated samples, showing the superiority of our model w.r.t these baselines (new section 5.3, pages 11 to 13 of the new version)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Data Generation Without View Supervision", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize. ", "pdf": "/pdf/a9118b028383e33ddd307f714ca1ecb1ec094415.pdf", "TL;DR": "We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.", "paperhash": "chen|multiview_data_generation_without_view_supervision", "authors": ["Mickael Chen", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "keywords": ["multi-view", "adversarial learning", "generative model"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lif.univ-mrs.fr"], "_bibtex": "@inproceedings{\nchen2018multiview,\ntitle={Multi-View Data Generation Without View Supervision},\nauthor={Mickael Chen and Ludovic Denoyer and Thierry Arti\u00e8res},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryRh0bb0Z},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728445, "id": "ICLR.cc/2018/Conference/-/Paper745/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryRh0bb0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper745/Authors|ICLR.cc/2018/Conference/Paper745/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper745/Authors|ICLR.cc/2018/Conference/Paper745/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper745/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper745/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper745/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper745/Reviewers", "ICLR.cc/2018/Conference/Paper745/Authors", "ICLR.cc/2018/Conference/Paper745/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728445}}}, {"tddate": null, "ddate": null, "tmdate": 1514740200271, "tcdate": 1514740200271, "number": 1, "cdate": 1514740200271, "id": "r1gec9IQG", "invitation": "ICLR.cc/2018/Conference/-/Paper745/Official_Comment", "forum": "ryRh0bb0Z", "replyto": "r1aAVyagf", "signatures": ["ICLR.cc/2018/Conference/Paper745/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper745/Authors"], "content": {"title": "Re: A good paper using GANs for multi-view image generation", "comment": "We thank the reviewer for the comments and feedback. We apologize for the late reply due to the number of additional experiments that have been made to improve the quality of the paper.\n\nThe first concern of the reviewer is about the lack of comparisons with other techniques. We updated the paper with results obtained on the same tasks with the approach by Mathieu et al. 2016 which is the closest to ours. Note that we were able to obtain comparable quality of outputs using the Mathieu et al. model by carefully testing many different neural networks architectures, the ones being provided in the open-source implementation, provided by the authors being inefficient on our problems. The quality of the generated samples of the different models (GMV, CMGV, GANx, CGAN and Mathieu et al.) have been evaluated in terms of quality of the outputs, and in terms of diversity of the generated samples, showing the superiority of our model w.r.t these baselines (new section 5.3, pages 11 to 13 of the new version)\n\nWe have also taken care to illustrate samples of the different models based on the same input images to allow for a better qualitative comparison  (Figure 8)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-View Data Generation Without View Supervision", "abstract": "The development of high-dimensional generative models has recently gained a great surge of interest with the introduction of variational auto-encoders and generative adversarial neural networks. Different variants have been proposed where the underlying latent space is structured, for example, based on attributes describing the data to generate. We focus on a particular problem where one aims at generating samples corresponding to a number of objects under various views. We assume that the distribution of the data is driven by two independent latent factors: the content, which represents the intrinsic features of an object, and the view, which stands for the settings of a particular observation of that object. Therefore, we propose a generative model and a conditional variant built on such a disentangled latent space. This approach allows us to generate realistic samples corresponding to various objects in a high variety of views. Unlike many multi-view approaches, our model doesn't need any supervision on the views but only on the content. Compared to other conditional generation approaches that are mostly based on binary or categorical attributes, we make no such assumption about the factors of variations. Our model can be used on problems with a huge, potentially infinite, number of categories. We experiment it on four images datasets on which we demonstrate the effectiveness of the model and its ability to generalize. ", "pdf": "/pdf/a9118b028383e33ddd307f714ca1ecb1ec094415.pdf", "TL;DR": "We describe a novel multi-view generative model that can generate multiple views of the same object, or multiple objects in the same view with no need of label on views.", "paperhash": "chen|multiview_data_generation_without_view_supervision", "authors": ["Mickael Chen", "Ludovic Denoyer", "Thierry Arti\u00e8res"], "keywords": ["multi-view", "adversarial learning", "generative model"], "authorids": ["mickael.chen@lip6.fr", "ludovic.denoyer@lip6.fr", "thierry.artieres@lif.univ-mrs.fr"], "_bibtex": "@inproceedings{\nchen2018multiview,\ntitle={Multi-View Data Generation Without View Supervision},\nauthor={Mickael Chen and Ludovic Denoyer and Thierry Arti\u00e8res},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=ryRh0bb0Z},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728445, "id": "ICLR.cc/2018/Conference/-/Paper745/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "ryRh0bb0Z", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper745/Authors|ICLR.cc/2018/Conference/Paper745/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper745/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper745/Authors|ICLR.cc/2018/Conference/Paper745/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper745/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper745/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper745/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper745/Reviewers", "ICLR.cc/2018/Conference/Paper745/Authors", "ICLR.cc/2018/Conference/Paper745/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728445}}}], "count": 8}