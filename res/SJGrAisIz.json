{"notes": [{"ddate": null, "tddate": 1521738198177, "tmdate": 1522300564761, "tcdate": 1521682992353, "number": 2, "cdate": 1521682992353, "id": "rydE9Yx5z", "invitation": "ICLR.cc/2018/Workshop/-/Paper50/Official_Comment", "forum": "SJGrAisIz", "replyto": "r1Sf4AnOz", "signatures": ["ICLR.cc/2018/Workshop/Paper50/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper50/Authors"], "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your valuable comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Robustness of Pruned Neural Networks", "abstract": "Deep neural network pruning forms a compressed network by discarding \u201cunimportant\u201d weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial attacks. We discover that although pruned models maintain the original accuracy, they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.", "pdf": "/pdf/24deba40b17253ac322e05d9209c2a0f8d8b88c7.pdf", "TL;DR": "We study the robustness of pruned neural networks under adversarial attacks and perform adversarial training on pruned models.", "paperhash": "wang|adversarial_robustness_of_pruned_neural_networks", "keywords": ["model compression", "adversarial attacks", "adversarial training", "neural network pruning", "fast gradient descent method (FGSM)", "projected gradient descent (PGD)"], "authors": ["Luyu Wang", "Gavin Weiguang Ding", "Ruitong Huang", "Yanshuai Cao", "Yik Chau Lui"], "authorids": ["luyu.wang@borealisai.com", "gavin.ding@borealisai.com", "ruitong.huang@borealisai.com", "yanshuai.cao@borealisai.com", "yikchau.y.lui@borealisai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450061, "id": "ICLR.cc/2018/Workshop/-/Paper50/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SJGrAisIz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper50/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper50/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper50/Reviewers", "ICLR.cc/2018/Workshop/Paper50/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450061}}, "tauthor": "wanglouis49@gmail.com"}, {"ddate": null, "tddate": 1521738189606, "tmdate": 1522300543527, "tcdate": 1521651954007, "number": 1, "cdate": 1521651954007, "id": "ry9l-Mlqz", "invitation": "ICLR.cc/2018/Workshop/-/Paper50/Official_Comment", "forum": "SJGrAisIz", "replyto": "BykU64ltG", "signatures": ["ICLR.cc/2018/Workshop/Paper50/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper50/Authors"], "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your valuable comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Robustness of Pruned Neural Networks", "abstract": "Deep neural network pruning forms a compressed network by discarding \u201cunimportant\u201d weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial attacks. We discover that although pruned models maintain the original accuracy, they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.", "pdf": "/pdf/24deba40b17253ac322e05d9209c2a0f8d8b88c7.pdf", "TL;DR": "We study the robustness of pruned neural networks under adversarial attacks and perform adversarial training on pruned models.", "paperhash": "wang|adversarial_robustness_of_pruned_neural_networks", "keywords": ["model compression", "adversarial attacks", "adversarial training", "neural network pruning", "fast gradient descent method (FGSM)", "projected gradient descent (PGD)"], "authors": ["Luyu Wang", "Gavin Weiguang Ding", "Ruitong Huang", "Yanshuai Cao", "Yik Chau Lui"], "authorids": ["luyu.wang@borealisai.com", "gavin.ding@borealisai.com", "ruitong.huang@borealisai.com", "yanshuai.cao@borealisai.com", "yikchau.y.lui@borealisai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450061, "id": "ICLR.cc/2018/Workshop/-/Paper50/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SJGrAisIz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper50/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper50/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper50/Reviewers", "ICLR.cc/2018/Workshop/Paper50/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450061}}, "tauthor": "wanglouis49@gmail.com"}, {"tddate": null, "ddate": null, "tmdate": 1521684447448, "tcdate": 1521684447448, "number": 3, "cdate": 1521684447448, "id": "B1PJecgcz", "invitation": "ICLR.cc/2018/Workshop/-/Paper50/Official_Comment", "forum": "SJGrAisIz", "replyto": "ryYtergYM", "signatures": ["ICLR.cc/2018/Workshop/Paper50/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper50/Authors"], "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your comments and suggestions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Robustness of Pruned Neural Networks", "abstract": "Deep neural network pruning forms a compressed network by discarding \u201cunimportant\u201d weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial attacks. We discover that although pruned models maintain the original accuracy, they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.", "pdf": "/pdf/24deba40b17253ac322e05d9209c2a0f8d8b88c7.pdf", "TL;DR": "We study the robustness of pruned neural networks under adversarial attacks and perform adversarial training on pruned models.", "paperhash": "wang|adversarial_robustness_of_pruned_neural_networks", "keywords": ["model compression", "adversarial attacks", "adversarial training", "neural network pruning", "fast gradient descent method (FGSM)", "projected gradient descent (PGD)"], "authors": ["Luyu Wang", "Gavin Weiguang Ding", "Ruitong Huang", "Yanshuai Cao", "Yik Chau Lui"], "authorids": ["luyu.wang@borealisai.com", "gavin.ding@borealisai.com", "ruitong.huang@borealisai.com", "yanshuai.cao@borealisai.com", "yikchau.y.lui@borealisai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450061, "id": "ICLR.cc/2018/Workshop/-/Paper50/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "SJGrAisIz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper50/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper50/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper50/Reviewers", "ICLR.cc/2018/Workshop/Paper50/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450061}}, "tauthor": "wanglouis49@gmail.com"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582917234, "tcdate": 1520391181470, "number": 1, "cdate": 1520391181470, "id": "r1Sf4AnOz", "invitation": "ICLR.cc/2018/Workshop/-/Paper50/Official_Review", "forum": "SJGrAisIz", "replyto": "SJGrAisIz", "signatures": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer3"], "content": {"title": "Tradeoff between model pruning and adversarial robustness", "rating": "5: Marginally below acceptance threshold", "review": "In this work, the authors propose to investigate adversarial robustness of pruned models. Here are some comments:\n\n1 The problem definition seems to be interesting. However, it is not quite novel, since it is a kind of problem with common sense, i.e., when the model is pruned, the capacity is reduced and consequently the robustness is decreased.\n\n2 In Table1, pruning seems to be more sensitive to FGSM. For PGD and Papernot's Black box, the robustness is almost equally bad, w.r.t., models before and after pruning. Does pruning tend to be more sensitive to one particular type of adversarial attacks?\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Robustness of Pruned Neural Networks", "abstract": "Deep neural network pruning forms a compressed network by discarding \u201cunimportant\u201d weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial attacks. We discover that although pruned models maintain the original accuracy, they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.", "pdf": "/pdf/24deba40b17253ac322e05d9209c2a0f8d8b88c7.pdf", "TL;DR": "We study the robustness of pruned neural networks under adversarial attacks and perform adversarial training on pruned models.", "paperhash": "wang|adversarial_robustness_of_pruned_neural_networks", "keywords": ["model compression", "adversarial attacks", "adversarial training", "neural network pruning", "fast gradient descent method (FGSM)", "projected gradient descent (PGD)"], "authors": ["Luyu Wang", "Gavin Weiguang Ding", "Ruitong Huang", "Yanshuai Cao", "Yik Chau Lui"], "authorids": ["luyu.wang@borealisai.com", "gavin.ding@borealisai.com", "ruitong.huang@borealisai.com", "yanshuai.cao@borealisai.com", "yikchau.y.lui@borealisai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582917051, "id": "ICLR.cc/2018/Workshop/-/Paper50/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper50/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper50/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper50/AnonReviewer1"], "reply": {"forum": "SJGrAisIz", "replyto": "SJGrAisIz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582917051}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582817639, "tcdate": 1520614727155, "number": 2, "cdate": 1520614727155, "id": "BykU64ltG", "invitation": "ICLR.cc/2018/Workshop/-/Paper50/Official_Review", "forum": "SJGrAisIz", "replyto": "SJGrAisIz", "signatures": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer2"], "content": {"title": "Investigation of adversarial robustness of pruned networks", "rating": "5: Marginally below acceptance threshold", "review": "The paper performs an empirical investigation of robustness of pruned networks to adversarial samples. They show that pruned networks more susceptible to adversarial perturbations and this can be improved by training with adversarial samples.\nSome of the aspects are not covered in this paper :\n1) The paper does not give any connection to Stochastic Activation Pruning (SAP) for Robust Adversarial Defense to https://openreview.net/forum?id=H1uR4GZRZ, even though it seems the proposed method is already given in the more elaborate study of various pruning schemes in SAP and its impact on robustness. \n\n2) It is not clear if the model was re-trained after pruning as is suggested in the original paper by Han etal. 2015b. It is possible that the weights need to re-adjusted after pruning, and perhaps the robustness level is maintained compared to the original network.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Robustness of Pruned Neural Networks", "abstract": "Deep neural network pruning forms a compressed network by discarding \u201cunimportant\u201d weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial attacks. We discover that although pruned models maintain the original accuracy, they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.", "pdf": "/pdf/24deba40b17253ac322e05d9209c2a0f8d8b88c7.pdf", "TL;DR": "We study the robustness of pruned neural networks under adversarial attacks and perform adversarial training on pruned models.", "paperhash": "wang|adversarial_robustness_of_pruned_neural_networks", "keywords": ["model compression", "adversarial attacks", "adversarial training", "neural network pruning", "fast gradient descent method (FGSM)", "projected gradient descent (PGD)"], "authors": ["Luyu Wang", "Gavin Weiguang Ding", "Ruitong Huang", "Yanshuai Cao", "Yik Chau Lui"], "authorids": ["luyu.wang@borealisai.com", "gavin.ding@borealisai.com", "ruitong.huang@borealisai.com", "yanshuai.cao@borealisai.com", "yikchau.y.lui@borealisai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582917051, "id": "ICLR.cc/2018/Workshop/-/Paper50/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper50/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper50/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper50/AnonReviewer1"], "reply": {"forum": "SJGrAisIz", "replyto": "SJGrAisIz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582917051}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582815898, "tcdate": 1520615553512, "number": 3, "cdate": 1520615553512, "id": "ryYtergYM", "invitation": "ICLR.cc/2018/Workshop/-/Paper50/Official_Review", "forum": "SJGrAisIz", "replyto": "SJGrAisIz", "signatures": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer1"], "content": {"title": "Adversarial Robustness of Pruned Neural Networks", "rating": "6: Marginally above acceptance threshold", "review": "In this paper, the authors present a preliminary empirical study of the relationship between neural network pruning and robustness against adversarial examples.\n\nThe main conclusion of the study, supported by experiments with small architectures in MNIST and CIFAR 10, is that models which have been pruned according to magnitude-based criteria -- either at the individual weight level or at the filter level -- might be considerably more sensitive to adversarial examples than their unpruned counterparts. Most remarkably, this occurs even if the pruned model retains the same accuracy as the unpruned model on the original, unperturbed data. Therefore, their experiments suggest that overparameterization might be useful in achieving resilience against adversarial examples even if it does not yield a noticeable improvement in accuracy. When adversarial training is used, the results remain consistent with this observation: the higher the compression (i.e. the less overparameterized the model is), the more sensitive the model becomes to adversarial examples.\n\nWhile the initial findings of this study should be corroborated by a more comprehensive set of experiments, including a wider variety of architectures, larger models, more challenging datasets and more complex pruning algorithms, these preliminary findings might be nevertheless worth discussing as part of the ICLR 2018 Workshop.\n\nMINOR POINTS\n\nThe manuscript has many typos and would benefit from additional proof-reading. For example, \u201cpruing\u201d, \u201csuspet\u201d in page 2 or \u201cnatual\u201d in Figure 1.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Robustness of Pruned Neural Networks", "abstract": "Deep neural network pruning forms a compressed network by discarding \u201cunimportant\u201d weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial attacks. We discover that although pruned models maintain the original accuracy, they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.", "pdf": "/pdf/24deba40b17253ac322e05d9209c2a0f8d8b88c7.pdf", "TL;DR": "We study the robustness of pruned neural networks under adversarial attacks and perform adversarial training on pruned models.", "paperhash": "wang|adversarial_robustness_of_pruned_neural_networks", "keywords": ["model compression", "adversarial attacks", "adversarial training", "neural network pruning", "fast gradient descent method (FGSM)", "projected gradient descent (PGD)"], "authors": ["Luyu Wang", "Gavin Weiguang Ding", "Ruitong Huang", "Yanshuai Cao", "Yik Chau Lui"], "authorids": ["luyu.wang@borealisai.com", "gavin.ding@borealisai.com", "ruitong.huang@borealisai.com", "yanshuai.cao@borealisai.com", "yikchau.y.lui@borealisai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582917051, "id": "ICLR.cc/2018/Workshop/-/Paper50/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper50/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper50/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper50/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper50/AnonReviewer1"], "reply": {"forum": "SJGrAisIz", "replyto": "SJGrAisIz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper50/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582917051}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573581985, "tcdate": 1521573581985, "number": 166, "cdate": 1521573581643, "id": "HyUAC00YG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "SJGrAisIz", "replyto": "SJGrAisIz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Robustness of Pruned Neural Networks", "abstract": "Deep neural network pruning forms a compressed network by discarding \u201cunimportant\u201d weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial attacks. We discover that although pruned models maintain the original accuracy, they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.", "pdf": "/pdf/24deba40b17253ac322e05d9209c2a0f8d8b88c7.pdf", "TL;DR": "We study the robustness of pruned neural networks under adversarial attacks and perform adversarial training on pruned models.", "paperhash": "wang|adversarial_robustness_of_pruned_neural_networks", "keywords": ["model compression", "adversarial attacks", "adversarial training", "neural network pruning", "fast gradient descent method (FGSM)", "projected gradient descent (PGD)"], "authors": ["Luyu Wang", "Gavin Weiguang Ding", "Ruitong Huang", "Yanshuai Cao", "Yik Chau Lui"], "authorids": ["luyu.wang@borealisai.com", "gavin.ding@borealisai.com", "ruitong.huang@borealisai.com", "yanshuai.cao@borealisai.com", "yikchau.y.lui@borealisai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518331008277, "tcdate": 1518218809566, "number": 50, "cdate": 1518218809566, "id": "SJGrAisIz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "SJGrAisIz", "signatures": ["~Luyu_Wang2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Adversarial Robustness of Pruned Neural Networks", "abstract": "Deep neural network pruning forms a compressed network by discarding \u201cunimportant\u201d weights or filters. Standard evaluation metrics have shown their remarkable speedup and prediction accuracy in test time, but their adversarial robustness remains unexplored even though it is an important security feature in deployment. We study the robustness of pruned neural networks under adversarial attacks. We discover that although pruned models maintain the original accuracy, they are more vulnerable to such attacks. We further show that adversarial training improves the robustness of pruned networks. However, it is observed there exist trade-offs among compression rate, accuracy and robustness in adversarially trained pruned neural networks. Our analysis suggests that we should pay additional attention to robustness in neural network pruning rather than just maintaining the classification accuracy.", "pdf": "/pdf/24deba40b17253ac322e05d9209c2a0f8d8b88c7.pdf", "TL;DR": "We study the robustness of pruned neural networks under adversarial attacks and perform adversarial training on pruned models.", "paperhash": "wang|adversarial_robustness_of_pruned_neural_networks", "keywords": ["model compression", "adversarial attacks", "adversarial training", "neural network pruning", "fast gradient descent method (FGSM)", "projected gradient descent (PGD)"], "authors": ["Luyu Wang", "Gavin Weiguang Ding", "Ruitong Huang", "Yanshuai Cao", "Yik Chau Lui"], "authorids": ["luyu.wang@borealisai.com", "gavin.ding@borealisai.com", "ruitong.huang@borealisai.com", "yanshuai.cao@borealisai.com", "yikchau.y.lui@borealisai.com"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 8}