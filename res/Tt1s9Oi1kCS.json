{"notes": [{"id": "Tt1s9Oi1kCS", "original": "WMX6Y4Iyq4q", "number": 1863, "cdate": 1601308205379, "ddate": null, "tcdate": 1601308205379, "tmdate": 1614985677527, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "AEkI9TmD7A", "original": null, "number": 1, "cdate": 1610040476965, "ddate": null, "tcdate": 1610040476965, "tmdate": 1610474081586, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "invitation": "ICLR.cc/2021/Conference/Paper1863/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper presents an interesting idea for task-free incremental learning on the data stream. The reviewers have extensive discussions after reading all the reviews and the author's rebuttal. There are concerns raised about the presentation of the method and the justification for some parts of the model design choices. The reviewers believe that after addressing these weaknesses the work can be made stronger and may be accepted in a competitive venue.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040476952, "tmdate": 1610474081571, "id": "ICLR.cc/2021/Conference/Paper1863/-/Decision"}}}, {"id": "2ldLkvD34Ax", "original": null, "number": 6, "cdate": 1605776597760, "ddate": null, "tcdate": 1605776597760, "tmdate": 1605776597760, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "invitation": "ICLR.cc/2021/Conference/Paper1863/-/Official_Comment", "content": {"title": "CoPE: A Rebuttal Revision", "comment": "Dear area chair,\nDear reviewer,\n\nThank you for your valuable reviews which we have incorporated into the revised submission, with an extra page for the rebuttal version. We included additional elaboration on our framework with an overviewing figure (Figure 1, page 2), and the additional page allowed us to significantly elaborate on our main findings in the conclusion as suggested by the reviewers. \nWe made our notion of \u2018high momentum\u2019 explicit in Section 4.1 by indication of high alpha (AnonymousReviewer4).\nFurthermore, the discussion in Section 6.2 on the imbalanced benchmarks is extended by means of confusion matrices (AnonymousReviewer3) of CoPE compared to the CoPE-CE baseline for both Split-MNIST and Split-CIFAR10, showing the efficacy of CoPE in alleviating catastrophic forgetting.\n\nAny further elaboration is given in individually addressed comments below, where references to figures and tables refer to the original submission.\n\nWe would gladly help you out with any further questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper1863/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tt1s9Oi1kCS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1863/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1863/Authors|ICLR.cc/2021/Conference/Paper1863/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1863/-/Official_Comment"}}}, {"id": "mvRncRWt90O", "original": null, "number": 5, "cdate": 1605204877869, "ddate": null, "tcdate": 1605204877869, "tmdate": 1605204877869, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": "pwCkCTYjcG8", "invitation": "ICLR.cc/2021/Conference/Paper1863/-/Official_Comment", "content": {"title": "Re: An interesting framework and model for learning on non-stationary data streams", "comment": "Dear reviewer, we express our gratitude for your valuable evaluation of our work. We answer the points of discussion (A) below.\n \n**A1:** We prefer the usage of the term \u2018momentum\u2019 as the learning rate could cause ambiguity for the reader w.r.t. the learning rate in the optimization process. Nonetheless, we will explicitly elaborate in the discussion of our method on the semantics of \u2018high\u2019 momentum to avoid confusion. \n\n**A2:**  It is an interesting discussion about whether a single prototype per class suffices or not. We believe it does, as the prototypes are learned (i.e., they do not correspond to a particular input image) and the feature space is learned simultaneously.\nSuppose you have a class with two distinctive subcategories (say male and female individuals of a given bird species).  At first, one might think you need a prototype for each, as the two subcategories may be mapped to different areas in feature space. However, one can always add one (or a few) extra projection layers that map these distinct areas onto the same point in a higher-level representation. So if the network is deep enough, a single prototype is likely to be sufficient.\nThis assumption is also supported in meta-learning (e.g. prototypical networks) and nearest mean classification (e.g. in iCaRL). However, if we would consider using multiple prototypes, our PPP-loss is designed to rely both on prototypes and pseudo-prototypes, so we could readily extend the repellor and attractor sets (see Section 4.3)  by adding the additional prototypes. \n\n**A3:**  Further, as suggested, we will extend the conclusion using the additional rebuttal page to elaborate on our main findings and we will include an overviewing figure of the framework.\n\nPlease let us know if you have any more questions we can help with."}, "signatures": ["ICLR.cc/2021/Conference/Paper1863/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tt1s9Oi1kCS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1863/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1863/Authors|ICLR.cc/2021/Conference/Paper1863/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1863/-/Official_Comment"}}}, {"id": "fBUsjmNf6zA", "original": null, "number": 4, "cdate": 1605202743119, "ddate": null, "tcdate": 1605202743119, "tmdate": 1605202743119, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": "u5qnAVszzt5", "invitation": "ICLR.cc/2021/Conference/Paper1863/-/Official_Comment", "content": {"title": "Re: A new heuristic for online learning from non-IID data", "comment": "Dear Reviewer, thank you for your valuable feedback. We address the raised concerns in the following answers (A).\n\n**A1:**  We will use the extra page to improve readability, mostly along the lines of suggestions made by AnonymousReviewer3. \n\n**A2:**  We are surprised by your comment that the assumptions about the data stream are not clearly defined, as this is exactly why we introduced the learner-evaluator framework in Section 2. In Continual Learning (CL) literature, these assumptions often remain implicit which makes the comparison of methods cumbersome. With this problem in mind, we propose in Section 2 a new framework that subdivides the setups within CL based on their assumptions on the data stream by disentangling information available to the learner and evaluator.\nAlternatively, maybe the reviewer refers to more general basic assumptions, such as smoothness or P(Y|X) being task-independent or stationary? These are rather standard in the field of CL, and therefore we did not write them down explicitly. To be clear, we specify our setting below:\ni) We work in the data incremental setting under our general learner-evaluator framework.\nii) We assume the class distributions P(Y|X) to be stationary and identical for learner and evaluator, while P(X) is non-stationary for the learner and stationary for the evaluator.\\\n\\\nNote that the above assumptions apply to our specific setup proposed and evaluated in sections 4 and 6, while the learner-evaluator framework of Section 2 is more generic, including also the field of concept drift with non-stationary data streams for evaluation.\n\n**A3:**  We recognize theoretical justification as a major problem in deep learning, and continual learning specifically. Nonetheless, CoPE is based on well established and empirically validated methods within continual learning (experience replay), representation learning (nearest mean classifiers), and unsupervised representation learning (instance-based contrastive losses). We provide extensive ablation studies to justify each of the components constituting CoPE, such as \n     -  the benefit of using a balanced replay buffer (CoPE-CE), \n     -  additionally using a prototypical approach (CoPE vs CoPE-CE in all experiments), \n     -  the effects of including the pseudo-prototypes in the PPP-loss (Table 2), \n     -  the advantages in low batch size regimes (Table 2), \n     -  the effects of the replay-buffer size (Figure 3).\n\nFurther in Appendix D, we provide further ablation studies,  \n     -  the effects of the low and high momentum regimes,\n     -  isolating the repellor and attractor loss terms in the PPP-loss, \n     -  the ratio of the repellor and attractor loss terms during training.\n\n**A4:**  *\u201cthe experiments were performed for one particular synthetic setup on one benchmark data set and the proposed algorithm is compared to a very limited class of baseline\u201d*\\\nWe are not sure if we understand the concern of the reviewer. The experiments were conducted for 8 benchmarks based on Split-MNIST, Split-CIFAR10, and Split-CIFAR100, and  CoPE is compared to 11 baselines. The balanced data streams are standard benchmarks in the field, separating tasks in the data stream, which enables us to compare w.r.t. state-of-the-art requiring task-boundaries (e.g. iCaRL and GEM). On top of that, we provide 3 highly imbalanced benchmarks, resembling a more real-world setup, where one task has significantly more (e.g. factor 10) data compared to other tasks. This setup additionally shows robustness of our method in class-imbalanced regimes, where we attempt to push continual learning towards more realistic (and highly imbalanced) data streams.\n\n**A5:**  *\u201cthere is no discussion about the computational cost\u201d*\\\nThis discussion has been established in Appendix C  due to lack of space, but provides a detailed discussion of the computational cost compared to the current state-of-the-art replay methods.\n\nWe hope this could provide some clarifications. Please reach out for any further concerns.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1863/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tt1s9Oi1kCS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1863/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1863/Authors|ICLR.cc/2021/Conference/Paper1863/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1863/-/Official_Comment"}}}, {"id": "UsvVyFOfZDt", "original": null, "number": 3, "cdate": 1605200338421, "ddate": null, "tcdate": 1605200338421, "tmdate": 1605200338421, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": "VhGLunpaCVS", "invitation": "ICLR.cc/2021/Conference/Paper1863/-/Official_Comment", "content": {"title": "Re: Recommendation to accept", "comment": "Dear reviewer, thank you for your fruitful evaluation of our work. \nThe extra page will be used to provide additional explanation as suggested, with an overview figure of the framework, and we will extend the conclusion as suggested by AnonymousReviewer4. The code will be publicly released upon paper acceptance (to comply with the double-blind policy) as we also believe it to be of key importance to support the reproducibility of our results. In addition, we hope that the comprehensive overview of the setup in Appendix B and the algorithm details in Appendix A make it easier to reproduce our work. Following your suggestion, we will additionally provide the confusion matrices in the updated paper version to get further insight into the results. \nPlease reach out if you have any further questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper1863/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Tt1s9Oi1kCS", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1863/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1863/Authors|ICLR.cc/2021/Conference/Paper1863/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1863/-/Official_Comment"}}}, {"id": "pwCkCTYjcG8", "original": null, "number": 1, "cdate": 1603924889312, "ddate": null, "tcdate": 1603924889312, "tmdate": 1605024341143, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "invitation": "ICLR.cc/2021/Conference/Paper1863/-/Official_Review", "content": {"title": "An interesting framework and model for learning on non-stationary data streams", "review": "Summary:\n\nThis article introduces a learner-evaluator framework that incorporates the different variations of problems related to incremental learning. It also proposes a method called Continual Prototype Evolution for dealing with the most general version of the problem, incremental learning on data streams, in which the learning task is not specified. The paper presents an extensive amount of experiments indicating that in this scenario, the proposed method improves significantly on existing approaches in terms of accuracy and memory efficiency. The article is well organized, easy to read and understand.\n\nPositive aspects:\n\n- It presents an adequate coverage of the literature on continual learning in section 3 and an interesting framework organizing the area in section 2.\n- The method does not need information about the task being learned, being more applicable to real-world scenarios.\n- An extensive experimental evaluation of the proposed method and comparisons with related methods is provided.\n- Ablation studies indicate the most important aspects of the proposed model. \n\nPoints to discuss/improve:\n\n- Momentum parameter: It took me a while to understand why the authors use a \u201chigh momentum\u201d. While in gradient descent the momentum creates a tendency to keep the parameter changing in the previous directions of motion, here the momentum is supposed to make it change more slowly. There is another way of formulating Eq. 1, in which alpha works as a learning rate. Defining alpha = (1-alpha), a slow learning rate instead of high momentum, we can have Eq. 1 as: p^c = p^c + alpha(p^c - \\bar{p}^c). This is easier to understand in my opinion, but authors can choose to disregard this if they prefer the current form.\n\n- The method assumes that one prototype for each class is enough. However, in certain problems, a class might need to be represented by different prototypes, indicating the different ways of being from the same class. How the model would handle these situations?\n\n- The conclusion does not discuss adequately the main findings of the article, probably due to lack of space.\n\n- Page 15 is completely black in most PDF viewers I tried. Only Chrome was able to display it correctly.\n\nConclusion:\n\nI believe this article should be accepted as a see it presents interesting findings in the area of incremental learning on non-stationary data streams.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1863/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1863/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109101, "tmdate": 1606915793913, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1863/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1863/-/Official_Review"}}}, {"id": "VhGLunpaCVS", "original": null, "number": 2, "cdate": 1603932321708, "ddate": null, "tcdate": 1603932321708, "tmdate": 1605024341073, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "invitation": "ICLR.cc/2021/Conference/Paper1863/-/Official_Review", "content": {"title": "Recommendation to accept ", "review": "This paper covers an interesting topic of continual learning of the stream of data. One limitation of the existing classification algorithms is their close-set assumption. In close-set methods, a predefined set of classes are considered and a model is trained on the available data from these classes, based on the assumption that test data will be driven from a similar distribution as the training data. However, most of the real-world problems are open-set problems. Open-set models should be able to learn continuously in an online manner with minimum or zero supervision. In other words, they should be able to learn new classes or update the existing classes based on the received new data on-the-fly, without forgetting the previously learned knowledge.  \nPros: \nIn this paper, the authors provide an incremental learning approach that prevents catastrophic forgetting. \nTheir approach can work on both balanced and unbalanced data.  \ncons: \nThe authors need to improve the presentation of the manuscript by providing more explanation. It could be confusing for readers who are not familiar with the topic. \nIt would be very helpful to publically share the code. \nI highly recommend adding the confusion matrix or F1 score in addition to the accuracies. \n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1863/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1863/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109101, "tmdate": 1606915793913, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1863/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1863/-/Official_Review"}}}, {"id": "u5qnAVszzt5", "original": null, "number": 3, "cdate": 1604246849766, "ddate": null, "tcdate": 1604246849766, "tmdate": 1605024341015, "tddate": null, "forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "invitation": "ICLR.cc/2021/Conference/Paper1863/-/Official_Review", "content": {"title": "A new heuristic for online learning from non-IID data", "review": "This paper proposes a new procedure for continual supervised learning from a non-IID data stream that assumes ability to maintain some of the stream examples in a buffer and use the buffer to improve updates of a prediction model. The proposed procedure is called Continual Prototype Evolution (CoPE), which controls evolution of prototypes to prevent catastrophic forgetting. \nStrenghts:\n- the paper presents very detailed experimental results\nWeaknesses:\n- the paper is not easy to read\n- the underlying assumptions about the data stream are not clearly defined. Thus, it remains unclear what problem the proposed algorithm is trying to solve.\n- the proposed algorithm is a collection of heuristics that are not clearly justified. There is no attempt to provide a theoretical insight about the behavior of the algorithm.\n- the experiments were performed for one particular synthetic setup on one benchmark data set and the proposed algorithm is compared to a very limited class of baselines. Thus, even the empirical evaluation is not very insightful.\n- there is no discussion about the computational cost \nOverall rating:\nThis paper has too many weaknesses and is not ready for publication\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1863/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1863/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams", "authorids": ["~Matthias_De_Lange1", "~Tinne_Tuytelaars1"], "authors": ["Matthias De Lange", "Tinne Tuytelaars"], "keywords": ["continual learning", "prototypical learning", "online learning", "incremental learning", "deep learning", "representation learning", "catastrophic forgetting", "concept drift"], "abstract": "Attaining prototypical features to represent class distributions is well established in representation learning. However, learning prototypes online from streams of data proves a challenging endeavor as they rapidly become outdated, caused by an ever-changing parameter space in the learning process. Additionally, continual learning assumes a non-stationary nature of the data stream, typically resulting in catastrophic forgetting of previous knowledge. As a first, we introduce a system addressing both problems, where prototypes evolve continually in a shared latent space, enabling learning and prediction at any point in time. In contrast to the major body of work in continual learning, data streams are processed in an online fashion, without additional task-information, and an efficient memory scheme provides robustness to imbalanced data streams. Besides nearest neighbor based prediction, learning is facilitated by a novel objective function, encouraging cluster density about the class prototype and increased inter-class variance. Furthermore, the latent space quality is elevated by pseudo-prototypes in each batch, constituted by replay of exemplars from memory. We generalize the existing paradigms in continual learning to incorporate data incremental learning from data streams by formalizing a two-agent learner-evaluator framework, and obtain state-of-the-art performance by a significant margin on eight benchmarks, including three highly imbalanced data streams.", "one-sentence_summary": "Continual Prototype Evolution (CoPE) establishes online adaptation of class-representative prototypes in non-stationary data streams, exploiting latent space representations in a novel loss to enhance the state-of-the-art in continual learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lange|continual_prototype_evolution_learning_online_from_nonstationary_data_streams", "pdf": "/pdf/d681c8d8855f986fd76870c24b56c59a0d6f9b7a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=95swpzM9Qt", "_bibtex": "@misc{\nlange2021continual,\ntitle={Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams},\nauthor={Matthias De Lange and Tinne Tuytelaars},\nyear={2021},\nurl={https://openreview.net/forum?id=Tt1s9Oi1kCS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Tt1s9Oi1kCS", "replyto": "Tt1s9Oi1kCS", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1863/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109101, "tmdate": 1606915793913, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1863/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1863/-/Official_Review"}}}], "count": 9}