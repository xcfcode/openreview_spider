{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396321743, "tcdate": 1486396321743, "number": 1, "id": "rycsof8Og", "invitation": "ICLR.cc/2017/conference/-/paper43/acceptance", "forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware.\nIn our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.", "pdf": "/pdf/de3d7a3678c6bd1f250b4066a8f2fd0b0b9da868.pdf", "paperhash": "ambai|ternary_weight_decomposition_and_binary_activation_encoding_for_fast_and_compact_neural_network", "keywords": ["Deep learning"], "conflicts": ["chubu.ac.jp", "d-itlab.co.jp", "denso.co.jp", "kumamoto-u.ac.jp"], "authors": ["Mitsuru Ambai", "Takuya Matsumoto", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "authorids": ["manbai@d-itlab.co.jp", "tmatsumoto@d-itlab.co.jp", "yamashita@cs.chubu.ac.jp", "hf@cs.chubu.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396322293, "id": "ICLR.cc/2017/conference/-/paper43/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396322293}}}, {"tddate": null, "tmdate": 1482187778400, "tcdate": 1482187778400, "number": 3, "id": "HJ5-4JL4e", "invitation": "ICLR.cc/2017/conference/-/paper43/official/review", "forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "signatures": ["ICLR.cc/2017/conference/paper43/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper43/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware.\nIn our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.", "pdf": "/pdf/de3d7a3678c6bd1f250b4066a8f2fd0b0b9da868.pdf", "paperhash": "ambai|ternary_weight_decomposition_and_binary_activation_encoding_for_fast_and_compact_neural_network", "keywords": ["Deep learning"], "conflicts": ["chubu.ac.jp", "d-itlab.co.jp", "denso.co.jp", "kumamoto-u.ac.jp"], "authors": ["Mitsuru Ambai", "Takuya Matsumoto", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "authorids": ["manbai@d-itlab.co.jp", "tmatsumoto@d-itlab.co.jp", "yamashita@cs.chubu.ac.jp", "hf@cs.chubu.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512718921, "id": "ICLR.cc/2017/conference/-/paper43/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper43/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper43/AnonReviewer3", "ICLR.cc/2017/conference/paper43/AnonReviewer2", "ICLR.cc/2017/conference/paper43/AnonReviewer1"], "reply": {"forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512718921}}}, {"tddate": null, "tmdate": 1481924417654, "tcdate": 1481924417654, "number": 2, "id": "rk9ryJzNx", "invitation": "ICLR.cc/2017/conference/-/paper43/official/review", "forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "signatures": ["ICLR.cc/2017/conference/paper43/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper43/AnonReviewer2"], "content": {"title": "Clarify my comments", "rating": "5: Marginally below acceptance threshold", "review": "I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware.\nIn our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.", "pdf": "/pdf/de3d7a3678c6bd1f250b4066a8f2fd0b0b9da868.pdf", "paperhash": "ambai|ternary_weight_decomposition_and_binary_activation_encoding_for_fast_and_compact_neural_network", "keywords": ["Deep learning"], "conflicts": ["chubu.ac.jp", "d-itlab.co.jp", "denso.co.jp", "kumamoto-u.ac.jp"], "authors": ["Mitsuru Ambai", "Takuya Matsumoto", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "authorids": ["manbai@d-itlab.co.jp", "tmatsumoto@d-itlab.co.jp", "yamashita@cs.chubu.ac.jp", "hf@cs.chubu.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512718921, "id": "ICLR.cc/2017/conference/-/paper43/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper43/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper43/AnonReviewer3", "ICLR.cc/2017/conference/paper43/AnonReviewer2", "ICLR.cc/2017/conference/paper43/AnonReviewer1"], "reply": {"forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512718921}}}, {"tddate": null, "tmdate": 1481881116835, "tcdate": 1481881116835, "number": 1, "id": "SySmUNZNg", "invitation": "ICLR.cc/2017/conference/-/paper43/official/review", "forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "signatures": ["ICLR.cc/2017/conference/paper43/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper43/AnonReviewer3"], "content": {"title": "Novel quantization method to reduce memory and complexity of pre-trained networks, but benefit over other methods is unclear", "rating": "4: Ok but not good enough - rejection", "review": "This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.\n\nMy major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.\n\n[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, https://arxiv.org/abs/1510.00149\n[2] Compressing Deep Convolutional Networks using Vector Quantization, https://arxiv.org/abs/1412.6115\n[3] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, https://arxiv.org/abs/1603.05279", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware.\nIn our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.", "pdf": "/pdf/de3d7a3678c6bd1f250b4066a8f2fd0b0b9da868.pdf", "paperhash": "ambai|ternary_weight_decomposition_and_binary_activation_encoding_for_fast_and_compact_neural_network", "keywords": ["Deep learning"], "conflicts": ["chubu.ac.jp", "d-itlab.co.jp", "denso.co.jp", "kumamoto-u.ac.jp"], "authors": ["Mitsuru Ambai", "Takuya Matsumoto", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "authorids": ["manbai@d-itlab.co.jp", "tmatsumoto@d-itlab.co.jp", "yamashita@cs.chubu.ac.jp", "hf@cs.chubu.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512718921, "id": "ICLR.cc/2017/conference/-/paper43/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper43/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper43/AnonReviewer3", "ICLR.cc/2017/conference/paper43/AnonReviewer2", "ICLR.cc/2017/conference/paper43/AnonReviewer1"], "reply": {"forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512718921}}}, {"tddate": null, "tmdate": 1481787833650, "tcdate": 1481787811627, "number": 1, "id": "B12iY6yNg", "invitation": "ICLR.cc/2017/conference/-/paper43/public/comment", "forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "signatures": ["~Sungho_Shin1"], "readers": ["everyone"], "writers": ["~Sungho_Shin1"], "content": {"title": "Suggestion for missing reference", "comment": "I suggest to refer the following two papers.\n\n- Kyuyeon Hwang and Wonyong Sung. \"Fixed-point feedforward deep neural network design using weights +1, 0, and \u22121.\" 2014 IEEE Workshop on Signal Processing Systems (SiPS). IEEE, 2014.\n\n- Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. \"X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks.\" 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014.\n\nThe retrain-based neural network quantization algorithm was first published in these two papers.\n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware.\nIn our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.", "pdf": "/pdf/de3d7a3678c6bd1f250b4066a8f2fd0b0b9da868.pdf", "paperhash": "ambai|ternary_weight_decomposition_and_binary_activation_encoding_for_fast_and_compact_neural_network", "keywords": ["Deep learning"], "conflicts": ["chubu.ac.jp", "d-itlab.co.jp", "denso.co.jp", "kumamoto-u.ac.jp"], "authors": ["Mitsuru Ambai", "Takuya Matsumoto", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "authorids": ["manbai@d-itlab.co.jp", "tmatsumoto@d-itlab.co.jp", "yamashita@cs.chubu.ac.jp", "hf@cs.chubu.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287753386, "id": "ICLR.cc/2017/conference/-/paper43/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ByOK0rwlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper43/reviewers", "ICLR.cc/2017/conference/paper43/areachairs"], "cdate": 1485287753386}}}, {"tddate": null, "tmdate": 1481315347941, "tcdate": 1481315347935, "number": 2, "id": "H13z45Omx", "invitation": "ICLR.cc/2017/conference/-/paper43/pre-review/question", "forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "signatures": ["ICLR.cc/2017/conference/paper43/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper43/AnonReviewer1"], "content": {"title": "Comparison", "question": "Do you have any quantitative comparison to existing methods such as introduced in Section 1.1?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware.\nIn our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.", "pdf": "/pdf/de3d7a3678c6bd1f250b4066a8f2fd0b0b9da868.pdf", "paperhash": "ambai|ternary_weight_decomposition_and_binary_activation_encoding_for_fast_and_compact_neural_network", "keywords": ["Deep learning"], "conflicts": ["chubu.ac.jp", "d-itlab.co.jp", "denso.co.jp", "kumamoto-u.ac.jp"], "authors": ["Mitsuru Ambai", "Takuya Matsumoto", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "authorids": ["manbai@d-itlab.co.jp", "tmatsumoto@d-itlab.co.jp", "yamashita@cs.chubu.ac.jp", "hf@cs.chubu.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481315348555, "id": "ICLR.cc/2017/conference/-/paper43/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper43/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper43/AnonReviewer2", "ICLR.cc/2017/conference/paper43/AnonReviewer1"], "reply": {"forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481315348555}}}, {"tddate": null, "tmdate": 1480714623280, "tcdate": 1480714623275, "number": 1, "id": "rJPYYwJXe", "invitation": "ICLR.cc/2017/conference/-/paper43/pre-review/question", "forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "signatures": ["ICLR.cc/2017/conference/paper43/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper43/AnonReviewer2"], "content": {"title": "Improve results ", "question": "I do need to see the results in a clear table. Original results and results when compression is applied. For all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10% etc...\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware.\nIn our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.", "pdf": "/pdf/de3d7a3678c6bd1f250b4066a8f2fd0b0b9da868.pdf", "paperhash": "ambai|ternary_weight_decomposition_and_binary_activation_encoding_for_fast_and_compact_neural_network", "keywords": ["Deep learning"], "conflicts": ["chubu.ac.jp", "d-itlab.co.jp", "denso.co.jp", "kumamoto-u.ac.jp"], "authors": ["Mitsuru Ambai", "Takuya Matsumoto", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "authorids": ["manbai@d-itlab.co.jp", "tmatsumoto@d-itlab.co.jp", "yamashita@cs.chubu.ac.jp", "hf@cs.chubu.ac.jp"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481315348555, "id": "ICLR.cc/2017/conference/-/paper43/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper43/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper43/AnonReviewer2", "ICLR.cc/2017/conference/paper43/AnonReviewer1"], "reply": {"forum": "ByOK0rwlx", "replyto": "ByOK0rwlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper43/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481315348555}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478155561990, "tcdate": 1478086272308, "number": 43, "id": "ByOK0rwlx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ByOK0rwlx", "signatures": ["~Mitsuru_Ambai1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Ternary Weight Decomposition and Binary Activation Encoding for Fast and Compact Neural Network", "abstract": "This paper aims to reduce test-time computational load of a deep neural network. Unlike previous methods which factorize a weight matrix into multiple real-valued matrices, our method factorizes both weights and activations into integer and noninteger components. In our method, the real-valued weight matrix is approximated by a multiplication of a ternary matrix and a real-valued co-efficient matrix. Since the ternary matrix consists of three integer values, {-1, 0, +1}, it only consumes 2 bits per element. At test-time, an activation vector that passed from a previous layer is also transformed into a weighted sum of binary vectors, {-1, +1}, which enables fast feed-forward propagation based on simple logical operations: AND, XOR, and bit count. This makes it easier to deploy a deep network on low-power CPUs or to design specialized hardware.\nIn our experiments, we tested our method on three different networks: a CNN for handwritten digits, VGG-16 model for ImageNet classification, and VGG-Face for large-scale face recognition. In particular, when we applied our method to three fully connected layers in the VGG-16, 15x acceleration and memory compression up to 5.2% were achieved with only a 1.43% increase in the top-5 error. Our experiments also revealed that compressing convolutional layers can accelerate inference of the entire network in exchange of slight increase in error.", "pdf": "/pdf/de3d7a3678c6bd1f250b4066a8f2fd0b0b9da868.pdf", "paperhash": "ambai|ternary_weight_decomposition_and_binary_activation_encoding_for_fast_and_compact_neural_network", "keywords": ["Deep learning"], "conflicts": ["chubu.ac.jp", "d-itlab.co.jp", "denso.co.jp", "kumamoto-u.ac.jp"], "authors": ["Mitsuru Ambai", "Takuya Matsumoto", "Takayoshi Yamashita", "Hironobu Fujiyoshi"], "authorids": ["manbai@d-itlab.co.jp", "tmatsumoto@d-itlab.co.jp", "yamashita@cs.chubu.ac.jp", "hf@cs.chubu.ac.jp"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 8}