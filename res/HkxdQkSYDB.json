{"notes": [{"id": "HkxdQkSYDB", "original": "B1xlwPhdwH", "number": 1622, "cdate": 1569439519922, "ddate": null, "tcdate": 1569439519922, "tmdate": 1583912041724, "tddate": null, "forum": "HkxdQkSYDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Sz-dXUzt6A", "original": null, "number": 4, "cdate": 1581447790786, "ddate": null, "tcdate": 1581447790786, "tmdate": 1581447790786, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Public_Comment", "content": {"title": "Questions related to the graph and some spelling/notation mistakes", "comment": "The paper mentions the creation of a matrix $F^t$ which is invariant to the ordering of nodes in the graph, but what happens in the experiments if one of the agents (or enemies) die? Are all adjacency values for all other agents zeroed for the entity that died? Conversely, what happens if a new agent is added to the environment? Can all the matrices and the overall model accommodate this?\n\nI missed a figure exemplifying how graphs are created, how they change over time or even what they look like. Are all nodes in the graph agents in the same team or does the graph model other things, such as adversaries and environment objects? Why are the edges determined by an arbitrary distance measure if the messaging between nodes is later weighted by self-attention? Couldn't the graph be complete and the attention weights learned to allow the model to learn what to ignore?\n\nProblem with notation: In page 3, L is used as \"the length of feature vector\". In page 5, L is used as number of enemies.\n\nThe acronym \"DGN\" is never defined. I suppose it was chosen to establish the model as a graph-convolutional variant of DQN, but it would be nice to define it, e.g. as \"Deep Graph Network\".\n\nThere were some spelling mistakes, but I suggest finding and fixing one instance of \"regularation\" in the text."}, "signatures": ["~Douglas_De_Rizzo_Meneghetti1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Douglas_De_Rizzo_Meneghetti1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192149, "tmdate": 1576860573842, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Public_Comment"}}}, {"id": "M-tH3al04L", "original": null, "number": 1, "cdate": 1576798728064, "ddate": null, "tcdate": 1576798728064, "tmdate": 1576800908477, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The work proposes a graph convolutional network based approach to multi-agent reinforcement learning. This approach is designed to be able to adaptively capture changing interactions between agents. Initial reviews highlighted several limitations but these were largely addressed by the authors. The resulting paper makes a valuable contribution by proposing a well-motivated approach, and by conducting extensive empirical validation and analysis that result in novel insights. I encourage the authors to take on board any remaining reviewer suggestions as they prepare the camera ready version of the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708854, "tmdate": 1576800257391, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Decision"}}}, {"id": "ByxZYj7hjr", "original": null, "number": 13, "cdate": 1573825400616, "ddate": null, "tcdate": 1573825400616, "tmdate": 1573825400616, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "BklJkMghjH", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Response", "comment": "\n\n>>> Clarifying \u2018these two methods require the objects in the environment are explicitly labeled, which is infeasible in many real-world applications.\u2019\n\nThese two methods use the entities in the environment as the nodes of the graph. So, they need explicitly know what the entities are and where they are in the environment to construct the graph at each timestep. However, in real-world applications, such information cannot be obtained.\n\n>>> About increasing the neighbourhood even more.\n\nWhen $|\\mathbb{B}|$=4, the receptive field of the second convolutional layer is 1+4*(1+4)=21. It is able to cover all of the 20 agents. And the experiments of $|\\mathbb{B}|$=1,2,3, and 4 have verified our claims about how the size of neighbors $|\\mathbb{B}|$ affects the performance of DGN. When increasing the neighborhood even more, the method will become a full communication method.  \n\n>>> \"I believe that if you make a claim which is not supported by the other two experiments, then the claim might be wrong. In this case, removing the ablation results does not add clarity, but hides the important information.\"\n\nIn the revise version, we have added the ablation study of DGN in jungle and routing in Appendix. Please refer to the last paragraph of Appendix and Figure 15 and 16 for details.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "BJeVi97hsH", "original": null, "number": 12, "cdate": 1573825180059, "ddate": null, "tcdate": 1573825180059, "tmdate": 1573825180059, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HJg1W8g3sS", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Response", "comment": "\n\n>>>  ...This makes it hard to learn abstract representations of mutual interplay between agents.\n\nFirst, the mutual interplay between agents is hard to be quantitatively represented. Moreover, multi-agent environments are changing quickly, which is a result caused by all agents, so it is hard to capture the pairwise relation. Our relation kernel is a neat method to quantitatively represent the pairwise relation.\n\n>>> \"It would be beneficial for the paper and all its readers if you write down the precise formalism. Is it Dec-POMDP?\"\n\nYes, it is Dec-POMDP. We have added the formalism at the first paragraph of Section 3.1.\n\n>>> \"I still don't understand why it is the case. Thanks for the additional experiments, however, I would like to see similar experiments for the other two domains.\"\n\nThe graph of the agents changes quickly. The change of the graph at next state will cause the change of target Q value. This is a problem of moving target, which is similar to the problem the target network addressed in DQN. That is the reason why  Q-function is difficult to converge. We really do not have enough time to perform additional experiments on other two scenarios before the deadline of rebuttal. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "HJg1W8g3sS", "original": null, "number": 11, "cdate": 1573811702593, "ddate": null, "tcdate": 1573811702593, "tmdate": 1573811702593, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "rygwdyD9sS", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Reviewer response", "comment": ">>>  ...This makes it hard to learn abstract representations of mutual interplay between agents.\n\nI don't understand why it is the case.\n\n>>> Our problem is a POMDP, where each agent gets a partial observation of the state and obtains a local reward. The objective is to maximize the sum of all agents\u2019 expected returns.\n\nIt would be beneficial for the paper and all its readers if you write down the precise formalism. Is it Dec-POMDP?\n\n>>> However, the graph changes quickly, which makes Q-function difficult to converge.\n\nI still don't understand why it is the case. Thanks for the additional experiments, however, I would like to see similar experiments for the other two domains."}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "BklJkMghjH", "original": null, "number": 10, "cdate": 1573810647306, "ddate": null, "tcdate": 1573810647306, "tmdate": 1573810647306, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "H1xLgxw9ir", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Reviewer's response", "comment": "I appreciate the time and effort the authors invested in improving their paper.\n\n>>> However, these two methods require the objects in the environment are explicitly labeled, which is infeasible in many real-world applications.\n\nCan you, please, clarify this?\n\n>>> We have performed additional experiments on bigger neighborhood.\n\nDo I understand correctly, that the total number of agents was 20 there? If yes, it would be interesting to increase the neighbourhood even more.\n\n>>> In other two scenarios, the same conclusions can also be drawn by ablation, but not as significant as in battle. Thus, we neglect the ablation results in these two scenarios for clarity.   \n\nI believe that if you make a claim which is not supported by the other two experiments, then the claim might be wrong. In this case, removing the ablation results does not add clarity, but hides the important information."}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "SJx0QUYOdB", "original": null, "number": 1, "cdate": 1570440742466, "ddate": null, "tcdate": 1570440742466, "tmdate": 1573726596489, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #3", "review": "This paper introduces Graph Convolutional Reinforcement Learning (referred to as DGN). DGN is a Deep Q-Learning (DQN) agent structured as a graph neural network / graph convolutional network with multi-head dot product attention as a message aggregation function. Graphs are obtained based on spatial neighborhoods (e.g. k nearest neighbors) or based on network structure in the domain. DGN considers a multi-agent setting with a centralized learning algorithm and shared parameters across all (controlled) agents, but individually allocated reward. Further, the paper considers environments where other non-learning agents are present which follow a pre-trained, stationary policy. In addition to the attention-based multi-agent architecture, the paper introduces a regularizer on attention weights similar to the use of target networks in DQN, to stabilize training. Results demonstrate that the proposed model architecture outperforms related earlier agent architectures that do not use attention or use a fully-connected graph.\n\nOverall, this paper addresses an interesting problem, introduces a novel combination of well-established architecture/agent building blocks and introduces a novel regularizer. The novelty and significance of the contributions, however is limited, as many recent works have explored using graph-structured representations and attention in multi-agent domains (e.g. VAIN: Hoshen (NeurIPS 2017), Zambaldi et al. (ICLR 2019), Tacchetti et al. (ICLR 2019)). The combination of these blocks and the considered problem setting is novel, but otherwise incremental. Nonetheless, the results are interesting, the overall architecture is simple (which I consider to be a good sign), and the attention regularizer is novel, hence I would rate this paper as relevant to the ICLR audience.\n\nMy main concern with this paper is clarity of writing: I have the feeling that important details are missing and some modeling decisions and formulas are difficult to understand. For example, I found section 3.3 difficult to read. The following sentences/statements need revision or further explanation:\n* \u201cIntuitively, if the relation representation produced by the relation kernel of upper layer truly captures the abstract relation between surrounding agents and itself, such relation representation should be stable/consistent\u201d (Please clarify)\n* \u201cWe use the attention weight distribution in the next state as the target for the current attention weight distribution\u201d (What is the reasoning behind this? Would an exponential moving average of attention logits/weights work as well?)\n* \u201cWhile RNN/LSTM forces consistent action, regardless of cooperation\u201d (unclear)\n* \u201cSince we only focus on the self-consistent of the relation representation based on the current feature extraction network we apply current network to the next state to produce the new relation representation instead of the target network as in deep Q learning\u201d (unclear)\n* The KL term in Eq. 4 is odd: z_i is defined as G^K and vice versa, neither of them appear to be distributions. I suppose one of the two arguments of the KL term should be the attention distribution for the current time step and the other argument for the next time step (if I understood the motivation in the earlier paragraph correctly), but this is not evident from Eq. 4.\n* KL is not symmetric -- what motivates the particular ordering in your case? Did you consider symmetric divergences such as Jensen-Shannon divergence (JSD)?\n\nI also wonder about the necessity of assembling adjacency matrices per node to create an intermediate ordered representation of the neighborhood on which, afterwards, an order-invariant operation such as mean pooling or self-attentive pooling is applied. Wouldn't it be more efficient to implement these operations directly using sparse scatter/gather operations as most recent GNN frameworks implement these techniques (e.g. PyTorch Geometric or DeepMind's graph_nets library)?\n\nFurther, important experimental details are missing, e.g., how observations / node features are represented / obtained from the environment and preprocessed. Do you encode position (continuous/discrete) and normalize in some way? It should further be mentioned that some of the baselines are trained with a different training algorithm and do not only differ in agent architecture (e.g. CommNet) \u2014 what is the effect of this?\n\nExperimentally, the results seem sound, but the variance in the results is suprisingly low (see e.g. Figure 7 DQN) \u2014 did you change the random seed between runs (both environment seed and the seed for initializing the agent weights)?\n\nOverall, this paper is interesting but needs revision in terms of clarity. Novelty is incremental, but if the paper would otherwise be very well written, I think it could qualify for acceptance. In its current state, I recommend a weak reject.\n\n\n--- UPDATE AFTER REVISION ---\nThe clarity in the revised manuscript is significantly improved and I feel confident in recommending acceptance of the paper. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575418059034, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Reviewers"], "noninvitees": [], "tcdate": 1570237734701, "tmdate": 1575418059045, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Review"}}}, {"id": "BJgLDYsciB", "original": null, "number": 9, "cdate": 1573726558346, "ddate": null, "tcdate": 1573726558346, "tmdate": 1573726558346, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "BJld6mP9or", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your clarifications and for updating the manuscript. The clarity in the revised manuscript is indeed significantly improved and I feel confident in recommending acceptance of the paper. \n\nOne minor comment: it might be useful to run the paper through a spelling/grammar checker (an automated service should suffice) to fix some small grammar mistakes, such as \"in dynamic environment where the neighbors of agent quickly change\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "BJld6mP9or", "original": null, "number": 8, "cdate": 1573708735922, "ddate": null, "tcdate": 1573708735922, "tmdate": 1573710046802, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "SJx0QUYOdB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Responses to Review #3", "comment": "\n\nWe have rewritten the section of temporal relation regularization to better present our idea. The first two paragraphs of Section 3.3 have addressed the reviewer\u2019s comments on temporal relation regularization. In the following, we first list the responses to these comments for reference, and then give the responses to other comments.  \n\n>>> \u2018Intuitively, if the relation representation produced by the relation kernel of upper layer truly captures the abstract relation between surrounding agents and itself, such relation representation should be stable/consistent\u2019 (Please clarify)\n\nIn CNNs/GCNs, higher layer learns more abstract representation. Similarly, in DGN, the relation representation captured by upper layer should be more abstract and stable. This is the motivation of applying temporal relation regularization to the upper layer.\n\n>>> \u2018We use the attention weight distribution in the next state as the target for the current attention weight distribution\u2019 (What is the reasoning behind this? Would an exponential moving average of attention logits/weights work as well?) \n\nCooperation is a persistent and long-term process. Who to cooperate with and how to cooperate should be consistent and stable for at least a short period of time even when the state/feature of surrounding agents changes. Thus, the attention weight distribution over the neighboring agents should be also consistent and stable for a short period of time. To make the learned attention weight distribution stable over timesteps, we use the attention weight distribution in the next state as the target for the current attention weight distribution. \n\nIn dynamic environment where the neighbors of agent quickly change, moving average and RNN structures cannot be performed on the attention weights of different neighbors and thus do not work. \n\n>>> \u2018Since we only focus on the self-consistent of the relation representation based on the current feature extraction network we apply current network to the next state to produce the new relation representation instead of the target network as in deep Q learning\u2019 (unclear)\n\nFor the calculation of KL divergence between relation representations in two timesteps, we apply current network to the next state to produce the target relation representation. This is because relation representation is highly correlated with the weights of feature extraction. But update of such weights in target network always lags behind that of current network, making the relation representation produced by target network not consistent with that produced by current network.\n\n>>> Explaining the arguments in the KL term in Eq. 4.\n\nYou understood it correctly. But we need to point out that $\\mathcal{G}_m^{\\kappa}$, the attention weights after the softmax (Equation 2), are actually a distribution over its neighbors, with the probability $\\alpha_{ij}$ for each neighbor $j$. Thus, we could use KL divergence to measure the difference.\n\n>>> \u201cKL is not symmetric -- what motivates the particular ordering in your case? Did you consider symmetric divergences such as JSD?\u201d\n\nSymmetry is not necessary. We use the attention weights in the next state as the target and only update the attention weights at the current state to make it close to the target. KL(current|target) measures how the current attention weight distribution is different from the target attention weight distribution. We also tested MSE, a symmetric metric, and it also works. However, the performance of KL divergence is better. \n\n>>>Assembling adjacency matrices\n\nAssembling adjacency matrices is not necessary but an easy way to implement multi-head attention. This technique is also used in GAT (ICLR 2018) (keras version). In fact, our implementation is also efficient and highly parallel on GPUs, since the computation of the algorithm is realized by dot product. GNN libraries might help the efficiency and are compatible with DGN. We will try these techniques to investigate the difference. \n\n>>> \u201cIt should further be mentioned that some of the baselines are trained with a different training algorithm and do not only differ in agent architecture (e.g. CommNet) \u2014 what is the effect of this?\u201d\n\nAll the baselines are trained with Q-learning and only differ in agent architecture. As pointed out in the CommNet paper, CommNet can be combined with standard RL algorithms or supervised learning. We use the Q-learning version as our baseline.\n\n>>> About observation and preprocessing\n\nIn the experiments that the observation of each agent is a square view with 11 \u00d7 11 grids centered at the agent and its own coordinates, which is provided by MAgent without preprocessing. \n\n>>> About the random seeds\n\nWe indeed changed the random seeds for each run. In routing, each router connects other three routers, and thus the action space of each agent (packet) is small. Therefore, the convergence process of each algorithm is prone to be similar under different random seeds. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "H1xLgxw9ir", "original": null, "number": 6, "cdate": 1573707758393, "ddate": null, "tcdate": 1573707758393, "tmdate": 1573709884515, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "SygTyGcOcr", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Responses to Review #4: part 1", "comment": "\n\n>>> Related work\n\nThanks for bringing up the missing references. MAGnet [Malysheva et al., 2018] learns relevance information in the form of a relevance graph, where the relation weights are learned by pre-defined loss function based on heuristic rules, but relation weights in DGN are learned by directly minimizing the temporal-difference error of value function end-to-end. Agarwal et al. (2019) used attention mechanism for communication and proposed a curriculum learning for transferable cooperation. However, these two methods require the objects in the environment are explicitly labeled, which is infeasible in many real-world applications. However, DGN agents only use their raw local observation. We have included these references and clarified the differences in the reversion.  \n\n>>> The metrics to determine the neighbor set.\n\nThe set of neighbors of an agent could be the agents in its local observation, or the agents within its communication range. It depends on specific scenarios. In the experiments, we use distance and select k-nearest agents as the neighbors and we have also investigated how the number of neighbors affects the performance.\n\n>>> Additional experiment to verify the claim that it may be costly and less helpful to take all other agents into consideration.\n \nThanks for your constructive suggestion. We have performed additional experiments on bigger neighborhood. As shown in Figure 9, when we set $|\\mathbb{B}|$ = 4, the performance drops. In addition, as shown in Figure 6, the full communication method, CommNet, has very limited performance. These verify that it may be less helpful and even negatively affect the performance to take all other agents into consideration. Due to limited time, we have not yet reconstructed the paper to incorporate Figure 8 and 9 in the main part of the paper. We will do that in the final version.\n\n>>> \u201cAt the end of Section 3.1, you mention the soft update of the target network. Later, in 3.3, you say that that you do not use the target network. Can you elaborate more on that?\u201d\n\nWe indeed use the target network to produce the target value for computing TD-error of Q function. However, for the calculation of the KL divergence between relation representations in two timesteps, we use current network instead of target network. The reason is explained in detail in the second paragraph of Section 3.3. \n\n>>> \u201cIn Equation 4, is it a separate KL for each of the attention heads? If yes, this is not clear from the formula.\u201d\n\nYes, it is the separate KL for each of the attention heads, we have made this clear in Equation 4 in the reversion.\n\n>>> Explaining the ablation experiments for other testbeds.\n\nIn other two scenarios, the same conclusions can also be drawn by ablation, but not as significant as in battle. Thus, we neglect the ablation results in these two scenarios for clarity.   \n\n>>> \u201cWhy does the DQN performance drop in the second half of the training in Figure 4 for all of the runs?\u201d\n\nThe enemy model built in MAgent is very powerful, making the Battle game difficult. We watched and analyzed their behaviors for all the runs. As described in Section 4.1, at the beginning, DQN agents learn sub-optimum strategies such as gathering at a corner to avoid to be attacked. These strategies might help at the beginning, and thus the reward is relatively high. But the agents at the edge of the group are easily attacked, receiving low reward and making the reward unevenly distributed among the group. Fitting the 'low reward data' produced by the sub-optimum policy, the DQN converges to more passive policy, e.g., moving disorderly. That is the reason that the mean reward decreases in the later phase. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "HylRNbw9iS", "original": null, "number": 7, "cdate": 1573708085964, "ddate": null, "tcdate": 1573708085964, "tmdate": 1573709840636, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "Byxv13sAFS", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Responses to Review #1", "comment": "\n\nDGN and all the baselines we compared with are Q-learning algorithms, but ATOC and TarMAC are actor-critic algorithms. Moreover, TarMAC uses a centralized critic that optimizes the shared global reward, which is different from other methods. Thus, they are not quite suitable for fair comparison. Nevertheless, we performed additional experiments to compare with them. As shown in Figure 11, DGN outperforms ATOC. The reason is that LSTM kernel is worse than multi-head attention kernel in capturing relation between agents. Like CommNet, TarMAC is also a full communication method. Similarly, DGN also outperforms TarMAC. The reason is that receiving redundant information may negatively affect the performance.  \n\nWe consider the case where the state information is unavailable and the agent can only use observations/encodings from other agents to learn to construct and exploit more centralized information. This is true in many real-world applications. However, collecting more observations/encodings incurs more costs and irrelevant observations/encodings can even negatively affect the performance. Because of this, as shown in Figure 9, the performance of $|\\mathbb{B}|$=4 is worse than $|\\mathbb{B}|$=3 or 2. RFM requires the true state information for supervise learning, and thus RFM is not suitable for fair comparison. \n\nMoreover, we have revised the paper to properly use the acronyms, cite the references, and fix the typos. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "SJlH50IcoS", "original": null, "number": 4, "cdate": 1573707404800, "ddate": null, "tcdate": 1573707404800, "tmdate": 1573708868250, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "To all the reviewers", "comment": "We appreciate the efforts made by the anonymous reviewers on reviewing our paper. Many thanks for the comments which are especially useful for us to improve the quality of this paper. In this revised version, we have carefully addressed the concerns of the reviewers by fixing the problems, performing additional experiments, rewriting the section of temporal relation regularization, and adding necessary references and explanations. We have also improved the writing of the paper. We hope that the reviewers will find our revision satisfactory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "rygwdyD9sS", "original": null, "number": 5, "cdate": 1573707630683, "ddate": null, "tcdate": 1573707630683, "tmdate": 1573707981048, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "SygTyGcOcr", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "Responses to Review #4: part 2", "comment": "\n\n>>> About the dynamic graph\n\nWe model the multi-agent environment as a graph, where each agent is a node and there is an edge between an agent and each neighbor. As agents keep moving and their neighbors changes quickly, the graph is highly dynamic. We have made this clear in the revision. \n\n>>> \u201cWhat do you mean precisely by easy to scale? Can you support this claim?\u201d\n\nSince all agents use the same neural network weights, we can directly apply the models trained with small-scale agents to the large-scale scenario. In routing, we apply the trained models (N=20) to the setting from N=40 to N=200. As illustrated in Table 3 and Figure 12, DGN continuously outperforms Floyd with BL up to N = 140.\n\n>>> About the testbed jungle\n\nJungle is a testbed designed by ourselves. It is a typical social dilemma where agents must learn to eat foods together and avoid to attack each other. \n\n>>> MDP formalism in partially observable environments.\n\nOur problem is a POMDP, where each agent gets a partial observation of the state and obtains a local reward. The objective is to maximize the sum of all agents\u2019 expected returns.\n\n\n>>> The meaning of \u2018more convolutional layers will not increase the local region of node i.\u2019\n\nWe mean regardless of how many convolutional layers are stacked, node i only communicates with its neighbors. This makes DGN practical in real-world applications, where each agent has limited communication range (e.g., wireless communication). \n\n\n>>> The difficulty of learning on the changing graph of agents. \n\nIdeally, Q-function should be learned on the changing graph of agents. However, the graph changes quickly, which makes Q-function difficult to converge. Fixing the graph in two successive timesteps mitigates the effect of changing graph and eases the learning difficulty. We performed additional experiments to investigate this. Figure 10 shows that fixing the graph indeed speed up the learning. Moreover, keeping the agent graph unchanged is also necessary for temporal relation regularization.    \n\n>>> Explaining the factorization by DGN.\n\nThe objective is to optimize the sum of all agents\u2019 expected returns. DGN factorizes the problem by each agent optimizes its own local reward, similar to CommNet, BiCNet, etc. Note that this factorization is different from VDN, QMIX and QTRAN where all agents share a global environmental reward and there is still a centralized Q-function that directly optimizes the shared reward during training. In DGN, CommNet and BiCNet, as each agent learns to optimize its own local reward, you could also see them as sophisticated independent Q-learning.\n\n\nMoreover, in Equation 3, we concatenate the output of each attention head, which is described in the paragraph above the equation. The size of the environment is 30x30.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "Byxv13sAFS", "original": null, "number": 2, "cdate": 1571892190658, "ddate": null, "tcdate": 1571892190658, "tmdate": 1572972444844, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an algorithm allowing \"cooperation\" between agents in multi-agent reinforcement learning, modeling agents as nodes in a graph. Each agent having only a partial view of the environment, the proposed algorithm uses multi-head attention as a (graph) convolution kernel but otherwise remains similar to the DQN algorithm. Performance is evaluated on three tasks using the MAgent framework.\n\nThe paper is reasonably well motivated, grounded and written. It addresses an interesting question: how to make agents cooperate in an efficient way? It does so by combining ideas from two lines of work, bringing incremental novelty. \n\nMy main concern relates to the experiments. It seems that ATOC and TarMAC would be the best baselines to compare against for a fair evaluation of the algorithm. Could they be added?\n\nOne question for the authors: at the beginning of Section 3, it is stated that \"it may be costly and less helpful to take all other agents into consideration\". It seems counter intuitive that DGN with several convolutional layers (to have a large receptive field) would be less costly than directly receiving global information? And isn't, in a sense, DGN also making use of global information when it has a large enough receptive field, even if indirectly? In this case, would it also make sense to more thoroughly compare DGN with RFM or other global state algorithms? Can this be clarified?\n\nFinally, readability is somewhat hindered by several small issues:\n- Acronyms used in the paper should really be introduced, at least when they are first used. DGN is never introduced, DGN-R/DGN-M are introduced several paragraphs after being first mentioned and BL needs some guessing.\n- Re-citing the same paper several times when mentioned in different sections is good practice. I found myself going over and over back to the related work section to find references and acronyms.\n\nSome typos:\nPage 1: among all agent -> among all agents\nPage 3: of S -> of size S\nPage 4: weighed -> weighted, concate -> concatenate\nPage 5: respecitvely -> respectively\nPage 6: regularation\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575418059034, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Reviewers"], "noninvitees": [], "tcdate": 1570237734701, "tmdate": 1575418059045, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Review"}}}, {"id": "SygTyGcOcr", "original": null, "number": 3, "cdate": 1572540901222, "ddate": null, "tcdate": 1572540901222, "tmdate": 1572972444799, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper addresses the problem of coordination in the multi-agent reinforcement learning setting. It proposes the value function factorization similar to independent Q-learning conditioning on the output of the graph convolutional neural network, where the graph topology is based on the agents\u2019 nearest neighbours. The paper is interesting and has some great ideas, for example, KL term to ensure temporal cooperation consistency. However, the paper has its drawbacks and I feel obliged to point them out below. I vote for the weak acceptance of this paper.\n\nOne of the main drawbacks of the paper is that it is extremely hard to grasp. Even the Abstract and Introduction are hard to understand without having a pass over the whole paper. The authors often use vague terms such as 'highly dynamic environments' or 'dynamics of the graph' which make it hard to understand what they mean. The paper would benefit from a more precise language. Some of the important notions of the paper are used before they are introduced, which make the general picture very hard to understand and to relate the work to the existing research.\n\n'Related Work' section seems to be missing some recent work applying graph neural networks to multi-agent learning settings:\n\u2022 Malysheva, Aleksandra, Tegg Taekyong Sung, Chae-Bong Sohn, Daniel Kudenko, and Aleksei Shpilman. \"Deep Multi-Agent Reinforcement Learning with Relevance Graphs.\" arXiv preprint arXiv:1811.12557 (2018).\n\u2022 Agarwal, Akshat, Sumit Kumar, and Katia Sycara. \"Learning Transferable Cooperative Behavior in Multi-Agent Teams.\" arXiv preprint arXiv:1906.01202 (2019).\n\nMy questions to the authors:\n\n\u2022 In section 3 you mention 'a set of neighbours ..., which is determined by distance or other metrics'. Can you elaborate on that? What are these metrics in your case?\n\u2022 Just before the Section 3.1, you say 'In addition, in many multi-agent environments, it may be costly and less helpful to take all other agents into consideration.' Have you run any experiments on that? In the appendix, you show, that making the neighbourhood smaller negatively affects the performance, but what if you make it bigger? Ideally, I would like to see an extended version of Figure 8 and 9 in the main part of the paper since they are very interesting and important for the claims the paper makes.\n\u2022 At the end of Section 3.1, you mention the soft update of the target network. Later, in 3.3, you say that that you do not use the target network. Can you elaborate more on that?\n\u2022 In Equation 4, is it a separate KL for each of the attention heads? If yes, this is not clear from the formula.\n\u2022 It will be useful to see the ablation experiments for all of the testbeds, not only for Battle.\n\u2022 Why do you think the DQN performance drops in the second half of the training in Figure 4 for all of the runs?\n\u2022 Have you tried summation instead of the mean aggregation step?\n\nI will put comments for particular parts of the paper below.\n\nABSTRACT\n\n>>> ...environments are highly dynamic\n\nWhat do you mean precisely here?\n\n>>> ...graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment\n\nWhat is the 'dynamics of the underlying graph'? What is the 'graph of the multi-agent environment'?\n\n>>> 'coordination is further boosted'\n\nNot sure that 'boosted' is the right word here.\n\nINTRODUCTION\n\n>>> '...where mutual interplay between humans is abstracted by their relations'\n\nNot sure what it means.\n\n>>> we consider the underlying graph of agents...\n\nThe agent graph has not been introduced yet.\n\n>>> DGN shares weights among all agent(s) making it easy to scale\n\nWhat do you mean precisely by 'easy to scale'? Can you support this claim?\n\n>>> We empirically show the learning effectiveness of DGN in jungle\n\nNeeds a reference to the testbed.\n\n>>>  ... interplay between agents and abstract relation representation\n\nWhat is 'abstract relation representation?\n\n>>> We consider partially observable environments.\n\nWhat do you mean precisely by that? What is the MDP formalism most suitable for your problem statement? What is objective under your formalism?\n\n>>> However, more convolutional layers will not increase the local region of node i.\n\nWhat do you mean by that?\n\n>>> As the number and position of agents vary over time, the underlying graph continuously changes, which brings difficulties to graph convolution.\n\nWhat kind of difficulties?\n\n>>> As the action of agent can change the graph at next timestep which makes it hard to learn Q function.\n\nWhy does it make it hard?\n\n>>> DGN can also be seen as a factorization of a centralized policy that outputs actions for all the agents to optimize the average expected return.\n\nIt would be useful for the reader to compare your approach with all the others type of the value function factorization. To me, your approach looks like a more sophisticated version of independent Q-learning, is that true?\n\nMinor comments:\n\n* In 3.2 it would be very helpful to put the dimensions for all of the variables for easier understanding.\n* The brackets in the equation 3 are not very clear (what are you concatenating across?)\n* In section 4, when describing an environment you say 'local observation that contains a square view with 11x11 grids'. What is the total size of the environment?\n* The performance plots for Battle include ablations before the ablation subsection is introduced. This is a bit confusing.\n* All figures/tables captions should be more detailed and descriptive.\n* \u2018However, causal influence is not directly related to the reward of environment.\u2019 Should be \u2018of the environment\u2019.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575418059034, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Reviewers"], "noninvitees": [], "tcdate": 1570237734701, "tmdate": 1575418059045, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Review"}}}, {"id": "Skga4mX4cS", "original": null, "number": 3, "cdate": 1572250421246, "ddate": null, "tcdate": 1572250421246, "tmdate": 1572250421246, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HJg3DQ7gcH", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"title": "RE: The released code lacks some key files. ", "comment": "There is a readme file in Battle and Jungle, respectively. Please check if it helps. Let us know if it does not. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "HJg3DQ7gcH", "original": null, "number": 3, "cdate": 1571988323873, "ddate": null, "tcdate": 1571988323873, "tmdate": 1571988393039, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Public_Comment", "content": {"title": "The released code lacks some key files.", "comment": "I am interested in your work and try to reproduce your work. However, there is no any comments about in the released code and the running environment.  Could you please update the code to make it easily be reproduced ?\n"}, "signatures": ["~Huiknight_Li1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Huiknight_Li1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192149, "tmdate": 1576860573842, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Public_Comment"}}}, {"id": "SygMD_i_uB", "original": null, "number": 2, "cdate": 1570449498192, "ddate": null, "tcdate": 1570449498192, "tmdate": 1570450480189, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "SklOSsFduH", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"comment": "We tested MSE and it also works. However, the performance of KL divergence is better. As mentioned in the paper, the relation in different timesteps should not be the same but similar, thus we use KL divergence to compute the distance between the distributions.", "title": "RE:KL divergence"}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "SklOSsFduH", "original": null, "number": 2, "cdate": 1570442047936, "ddate": null, "tcdate": 1570442047936, "tmdate": 1570442047936, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "BJxoQu_udS", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Public_Comment", "content": {"comment": "Thanks for the quick reply. Is there any specific reason for using KL divergence instead of any other divergence measures?", "title": "KL divergence"}, "signatures": ["~Hopeful_Rational2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hopeful_Rational2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192149, "tmdate": 1576860573842, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Public_Comment"}}}, {"id": "r1gtJmtu_r", "original": null, "number": 1, "cdate": 1570439904533, "ddate": null, "tcdate": 1570439904533, "tmdate": 1570440797240, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "BJxoQu_udS", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment", "content": {"comment": "After the softmax, the attention weight in the local region is a distribution, with the probability $\\alpha_{ij}$. We use KL divergence to measure the difference between the attention weight distributions in two timesteps. Please refer to the code for more details. ", "title": "Re: Use of KL divergence"}, "signatures": ["ICLR.cc/2020/Conference/Paper1622/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1622/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1622/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1622/Authors|ICLR.cc/2020/Conference/Paper1622/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153300, "tmdate": 1576860540282, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Official_Comment"}}}, {"id": "BJxoQu_udS", "original": null, "number": 1, "cdate": 1570437154679, "ddate": null, "tcdate": 1570437154679, "tmdate": 1570437154679, "tddate": null, "forum": "HkxdQkSYDB", "replyto": "HkxdQkSYDB", "invitation": "ICLR.cc/2020/Conference/Paper1622/-/Public_Comment", "content": {"comment": "Hi. The paper is nice. However, could you please elaborate more on the use of KL divergence for computing the distance between the attention weight distributions. Thanks.", "title": "Use of KL divergence"}, "signatures": ["~Hopeful_Rational2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Hopeful_Rational2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Convolutional Reinforcement Learning", "authors": ["Jiechuan Jiang", "Chen Dun", "Tiejun Huang", "Zongqing Lu"], "authorids": ["jiechuan.jiang@pku.edu.cn", "cd46@rice.edu", "tjhuang@pku.edu.cn", "zongqing.lu@pku.edu.cn"], "keywords": [], "abstract": "Learning to cooperate is crucially important in multi-agent environments. The key is to understand the mutual interplay between agents. However, multi-agent environments are highly dynamic, where agents keep moving and their neighbors change quickly. This makes it hard to learn abstract representations of mutual interplay between agents. To tackle these difficulties, we propose graph convolutional reinforcement learning, where graph convolution adapts to the dynamics of the underlying graph of the multi-agent environment, and relation kernels capture the interplay between agents by their relation representations. Latent features produced by convolutional layers from gradually increased receptive fields are exploited to learn cooperation, and cooperation is further improved by temporal relation regularization for consistency. Empirically, we show that our method substantially outperforms existing methods in a variety of cooperative scenarios.", "code": "https://github.com/PKU-AI-Edge/DGN/", "pdf": "/pdf/942511ecd9f475b12cfee205aa15e43861f41c41.pdf", "paperhash": "jiang|graph_convolutional_reinforcement_learning", "_bibtex": "@inproceedings{\nJiang2020Graph,\ntitle={Graph Convolutional Reinforcement Learning},\nauthor={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxdQkSYDB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/ad1c9e66b12fd2ab763193fb3fa61b362e1bf36d.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxdQkSYDB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504192149, "tmdate": 1576860573842, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1622/Authors", "ICLR.cc/2020/Conference/Paper1622/Reviewers", "ICLR.cc/2020/Conference/Paper1622/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1622/-/Public_Comment"}}}], "count": 22}