{"notes": [{"id": "IeuEO1TccZn", "original": "NVrR3eRRAW9", "number": 559, "cdate": 1601308068453, "ddate": null, "tcdate": 1601308068453, "tmdate": 1614985708925, "tddate": null, "forum": "IeuEO1TccZn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "63-nCYfDapI", "original": null, "number": 1, "cdate": 1610040434471, "ddate": null, "tcdate": 1610040434471, "tmdate": 1610474034947, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper aims to present a new representation learning framework for supervised learning based on finding a representation such that the input is conditionally independent given the representation, the components of the representations are independent and the representation is rotation-invariant. While there were both positive and negative assessments of this paper by the reviewers, there are 3 major concerns that lead me to recommend rejecting this paper:\n1. Most importantly, experiments do not seem to be conclusive as they do not properly ablate the specific aspects of this method. More specifically, the authors compare their deep learning based approach with non-deep learning approaches but do not compare against deep learning baselines. This makes it impossible to assess the merit of the proposed approach (which also appears to be complicated) over much simpler baselines.\n2. The required properties of the representations do not seem to be properly motivated. \n3. The paper refers to their produced representations as disentangled representations. As pointed out by AnonReviewer4, this appears not to be consistent with prior uses of that word in the community.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040434458, "tmdate": 1610474034931, "id": "ICLR.cc/2021/Conference/Paper559/-/Decision"}}}, {"id": "d6_dpfXUK9", "original": null, "number": 2, "cdate": 1605473027158, "ddate": null, "tcdate": 1605473027158, "tmdate": 1605753038154, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "B8tzfP4tVcJ", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We\u2019d like to thank you for taking the time to read our submission and for your comments. However, based on the comments we are afraid you may have misunderstood the key points of our paper and did not provide a fair assessment of our contribution. We hope our responses below may help clarify the key points of our work and its contribution.\nOur point-by-point responses are as follows.\n\nFirst of all, your following description of independence among the coordinates is totally wrong:\n\n3. That all it's coordinates are independent: Law(g(x)_i) = Law(g(x)_j) for all i, j\n\nA)\tOur work builds on the existing work, so we believe it is important to credit the researchers for their work. Sufficiency is a basic statistical property that a representation should have. This is a well-established principle in statistics.  Disentanglement is a condition that has long been recognized as an important desired condition for representation learning (see, for example, Bengio (2013) and Achille and Soatto (2018)). The rotation invariance property is motivated by image data and morphological data. Indeed, we should have better motivated this in addition to what we have learned from the existing literature.  Also, we proved that we can always transform the representation to have independent components and be invariant in distribution while preserving sufficiency (Lemma 2.1). So, there is no loss of information in a statistical sense to impose invariance on the representation. In the revision, we will better motivate the study and explain in more detail why we proposed the method to learn sufficient and disentangled representations. \n\nB)\tWe are not aware of any simple empirical risk criterion for representation learning in a nonparametric setting. To the best of our knowledge, we don\u2019t think such a criterion has been proposed in the literature. A challenge of supervised representation learning that distinguishes it from standard supervised learning tasks is the difficulty in formulating a clear and simple objective function. In classification, the objective is clear, which is to minimize the number of misclassifications; in regression, a least-squares criterion is usually used.  In representation learning, the objective is different from the ultimate objective, which is typically learning a classifier or a regression function for prediction. How to establish a good and simple criterion for supervised representation learning has remained an open question (Bengio, 2013). \n\nIn our work, we develop a criterion for supervised representation learning by formulating the problem as that of finding a representation that satisfies the conditional independence condition, and then regularizing it by requiring its components to be independent and rotation invariant (in distribution).  So we need a measure of conditional independence. Distance covariance is actually one of the statistically and computationally simplest ones for characterizing conditional independence. We mentioned two other possibilities\uff1amutual information and kernel covariance operator, but they are not any simpler than distance covariance. Specifically, mutual information involves the estimation of density ratios, in addition to the representation function. Nonparametric density-ratio estimation is a challenging problem in itself in high-dimensional settings. The computation of the empirical kernel covariance operator is expensive if not infeasible.  We would be very grateful if you could let us know a simple empirical risk function for characterizing conditional independence and disentanglement and hence can be used for learning a sufficient and disentangled representation nonparametrically.\n\nC)  We believe that the learning bounds provide strong support for the proposed method. We agree this is not directly related to training and application. However, this result shows that the proposed method at least is doing the right thing under the conditions provided. \n\nD)  The goal of our experiments is to demonstrate that the representations trained based on the proposed method perform well. Our proposed method is not trying to learn a classifier or a regression function directly, but rather to learn a representation that preserves all the information. So our experiments are designed to evaluate the performance of simply classification and regression methods using the representations we learned as input. The results demonstrate that a simple classification model using the representations we trained performs better than or comparably with the best classification method using deep CNN. \nThank you for your suggestions on the setting of regression problems. We will add experimental comparisons of NN-based neural network models in the revision according to your suggestions.\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper559/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IeuEO1TccZn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper559/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper559/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper559/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper559/Authors|ICLR.cc/2021/Conference/Paper559/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869667, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper559/-/Official_Comment"}}}, {"id": "nDBotXtdacz", "original": null, "number": 5, "cdate": 1605477261248, "ddate": null, "tcdate": 1605477261248, "tmdate": 1605478952686, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "vdmlGM5kSmy", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "We\u2019d like to thank you for taking the time to read our submission and for your comments. However, based on the comments we are afraid that you may have misunderstood the point of our work and did not provide a fair assessment of our contribution. We hope our responses below may help clarify the key points of our work and its contribution.\n\n\n\u2022A challenge of supervised representation learning that distinguishes it from standard supervised learning is the difficulty in formulating a clear and simple objective function.  We are not aware of any simple empirical risk criterion for representation learning.  The criteria for characterizing conditional independence we know of are all unfortunately complicated and involve some math. \n\nThe goal of our experiments is to demonstrate that the representations trained based on the proposed method perform well in terms of classification accuracy and prediction.  The results show that a simple classification model using the representations we trained performs better than or comparable with the best classification using deep CNN.  The comparison of classification experiments is apple-to-apple since the architecture of R_\\theta and most hyperparameters were shared across all three methods \u2013 SDRL, CN, and dCorAE. The depth of the DenseNet in the paper with code (https://paperswithcode.com/paper/densely-connected-convolutional-networks) is about twice that of our model, so this comparison is indeed a bit unfair to our method. The network architecture of DenseNet we used is shown in the appendix. We are conducting further experiments based on your comments. At present, SDRL can achieve 97.35% accuracy on CIFAR-10. \n\n\u2022We respectfully disagree. We use a narrow definition of disentangling: a representation is disentangled if the components of the representation are independent (see e.g., Achille and Soatto, 2018).  The rotation invariance (in distribution) condition is on top of the independence condition. So our definition is even stricter than the definition of disentangling as used in Achille and Soatto (2018).  For a given representation learned based on our method, different components (directions) are identifiable. What we mean by the representation being only identifiable up to orthogonal transformation is that if R is a learned representation, then any orthogonal transformation of R has exactly the same statistical property.\n\nThe definition of Higgins et al. (2018) is based on the decomposition of a symmetry group into subgroups.  However,  it is not clear how to establish an objective function based on their definition to learn disentangled representations empirically.  We also do not see any contradiction between their definition and the definition we used.  Importantly, their definition does not imply that a disentangled representation must be unique since the decomposition of a symmetric group is generally not unique. In their grid world example (Section 5.3 Higgins et al., 2018), the symmetry group G has two decompositions: G = Gx X Gy X Gc, where Gx is the set of all translation transformations along the x-axis, Gy is the set of all translation transformations along the y-axis, and Gc is the set of all color transformations. Another decomposition is G = Gp X Gc, where Gp is the subgroup of all positional changes. These two decompositions lead to two different disentangled representations. \n\nIn general, since we seek a representation in a space with a lower dimension than the dimension of the original data space, it is impossible to have uniqueness and identifiability in a nonparametric setting. Even if we restrict to linear representations with orthogonal directions, only space spanned by the linear representations is identifiable, but not the linear representation itself. This fact has long been established in the literature (See, for example, Cook, 2007; Fukumizu, Bach and Jordan, 2009 ).  Such representations satisfy the disentanglement definition of Higgins et al. (2018) by taking the symmetry group to be the group of orthogonal matrices.   In general, non-uniqueness and non-identifiability do not cause any problem for representation learning since the representations satisfying the required conditions are statistically equivalent and any one of them can be used for subsequent learning tasks.  While it would be nice if there existed a unique set of \u201ctrue\u201d latent factors in the world, we believe for most of the machine learning and statistical systems that have to be learned from empirical data, this is unfortunately not true.  There are multiple models and multiple representations that can lead to the same prediction accuracy (for a discussion about the multiplicity of good models, see e.g., Leo Breiman.  Statistical Modeling: The Two Cultures. Statistical Science, 16: 199-231, 2001). \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper559/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IeuEO1TccZn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper559/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper559/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper559/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper559/Authors|ICLR.cc/2021/Conference/Paper559/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869667, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper559/-/Official_Comment"}}}, {"id": "HOZiLitx2hy", "original": null, "number": 4, "cdate": 1605474150534, "ddate": null, "tcdate": 1605474150534, "tmdate": 1605478173668, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "y-6j1ib1WU9", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Official_Comment", "content": {"title": "Response to AnonReviewer 2", "comment": "We\u2019d like to thank you for the very detailed and constructive comments. Here are our point-by-point responses.\n\n\u2022 To achieve sufficiency, this paper proposes to use the distance covariance defined in Section 4.  This seems to be related with statistical correlation, can this be written out in the paper? As when I read the proof of  theorem 4.2, I do not know what is the \\rho(R,R*). It seems not  to be defined in the paper.  Also is there any relation between V[z,y] with the sufficient statistics (Fisher 1922), as mentioned in the introduction?  From (5) I then understand that this covariance V replies on the metric on X and Y.  For multi-class classification problems, it is not always clear how to choose a metric on Y. I recommend the authors to clarify this choice at least in the numerical experiments. \n\nThe population distance covariance is defined as the integral of the squared difference between the joint characteristic function and the product of the marginal characteristic functions. This is different from the usual ``statistical correlation\u2019\u2019 (e.g., Person\u2019s correlation, Spearman\u2019s rank correlation, or Kendal\u2019s tau). As can be seen from the definition of the distance correlation, it is a measure of dependence (not just correlation) of two random vectors. In particular, the distance covariance is zero if and only the two random vectors are independent. The usual \u201cstatistical correlation\u201d does not have such properties. The distance covariance does not have a direct connection with Fisher\u2019s sufficient statistics. The connection is between the use of conditional independence to define a sufficient representation and Fisher\u2019s sufficient statistics. Recall that for a given parametric model, a statistic is sufficient if the distribution of the data given this statistic is independent of the parameter, that is, a sufficient statistic contains all the information about the model parameter. The sufficient representation we seek to find is similar in the sense that given a sufficient representation, the input variable does contain any additional information about the response. \nThe \\rho(R, R*) is Pearson\u2019s correlation coefficient.  For multiple class classification problems, we use vector representation for classes. Suppose there are k classes, then Y is a k-vector. We will make this clear in the revision. \n\n\u2022 In the statement of Theorem 4.3, should \\lambda = O(1) be strictly positive? The objective (7) is formalized as a bi-level optimization problem, and the computation problem is addressed using recently proposed methods based on gradient flows. There is a typo 2 lines before Section 5, should the rate be (log n) n^{-2 / (2+d)} ? \n\nYes, \\lambda should be strictly positive. We will make this clear in the revision. Thanks for pointing out the typo. We will correct the typo in the revision. \n\n\u2022  Section 6: clarify what is Y, how the metric is chosen, and plots of learned representations\n\nWe will clarify the definition of Y in the numerical experiments and the metric used.  For the classification problem, Y is the label for image classification. For example, the label of MNIST is the corresponding number, and the label of CIFAR-10 is the type of object in the image.  In addition to the Gaussian distribution, we can choose a different distribution as our reference distribution for R*, such as a uniform distribution on the unit sphere. The point is that the components of this distribution are independent to meet the conditions of disentanglement. For the visualization of classification problems, the uniform spherical distribution is more suitable for displaying labeled low-dimensional representations than the Gaussian distribution. Therefore, in the experiment in Figure 1, we adopted the uniform spherical distribution. Since the dimension of the learned feature is 2, the visualization of uniform spherical distribution degenerates to the unit circle, which is what we show in Figure 1. We can see in Figure 1 that the points with different colors are well separated, which indicates that the features are well disentangled. We will also experiment with other ways for displaying the results as you suggested.\n\n\u2022What is the distance correlation computed in Figure 2?  \n\nThe squared distance correlation \\pho[z, y]^2 = V[z, y]^2/sqrt(V[z]^2 * V[y]^2), where V[z] , V[y] are the distance variances, V[z] = V[z, z], V[y] = V[y, y]. For more details, please see (Szekely et al., 2007). Table 3 is showing the test accuracy.\n\nTypo: \n\n\u2022\tin the definition of V[z,y] on page 4, the constant c_m should be c_q? \n\nWe will correct this typo in the revision.\n\n\u2022\t3 lines before (2), N_d (0, I_d) -> N (0, I_d)?\n\nWe will make the change in the revision\n\n\u2022\tThe (a) and (e) in Figure 1 are not the learned features, but the original data.\n\nWe will make the corrections in the revision.\n\n\u2022\tpage 19: matric entropy -> metric entropy\n\nWe will correct this typo in the revision.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper559/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IeuEO1TccZn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper559/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper559/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper559/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper559/Authors|ICLR.cc/2021/Conference/Paper559/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869667, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper559/-/Official_Comment"}}}, {"id": "PdLqyKh7KL1", "original": null, "number": 3, "cdate": 1605473505196, "ddate": null, "tcdate": 1605473505196, "tmdate": 1605473505196, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "vUZXZ64nI1H", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We\u2019d like to thank you for the very detailed and constructive comments. Here are our point-by-point responses.\n\nThank you for noticing the merits of our paper. \n\n\u2022 I could understand representations that are rotation-invariant are desirable in vision tasks, but I am wondering is this requirement too strong for other applications, e.g., in language and speech? Other than the technical convenience brought by this property, is there any other reason that could motivate this criterion?\n\nIndeed, for language and speech data, rotation invariance in distribution may not be desirable. Some other reference distributions other than normal may be more appropriate. In the proposed method, we can also push the distribution of the representation to other distribution such as uniform.  We can also think of pushing the distribution of the representation to the Gaussian distribution as a form of regularization. We will add these points to the revision. \n\n\u2022\tIn Eq. (2), shouldn't the domain of g be R^p instead of R^d? Otherwise the intersection of M and F would be empty.\n\nYes, the domain of g should be R^p in Eq. (2). Thanks for catching this error. \n\n\u2022\tIf my understanding about Lemma 2.1 is correct, it is the pushforward of \\mu under T to be an isotropic Gaussian distribution, right? The original Z = g(X) is only sufficient, but not disintangled nor rotation-invariant. If this is the case, then I am not sure I totally agree with the second comment after Theorem 4.2, where the authors remark that the discriminator network is the pushforward function T. My understanding is that the representer network R is used to approximate the composite of T and g, so that R_\\sharp \\mu_X = R^* in Eq. (3), while the discriminator network D is only served as the witness function in the definition of f-divergence in Eq. (6). Please clarify this point, thanks!\n\nYes, you are right. That comment is misleading. The discriminator network D only serves as a witness function as in GANs. Many thanks for pointing this out. \n\n\u2022\tThe line above Eq. (9), typo: no \\lambda is needed before the V term.\n\nYes, this is a typo. We will remove \\lambda in the revision. \n\n\u2022IMO the second assumption of Theorem 4.3 is quite strong: it requires the density to be lower bounded by a positive constant c_1, which does not even hold for Gaussians. On the other hand, the compactness of the domain helps to reconcile this assumption, and I understand that this is a standard assumption in the analysis of nonparametric density estimation/regression.\n\nWe agree this is a strong assumption. We now can weaken this assumption by only assuming the density to be lower bounded by a positive constant on a sufficiently large compact set. This will include Gaussian densities. We can use a truncation argument so that the original proof will only need minor modifications. We will make the changes in the revision. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper559/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IeuEO1TccZn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper559/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper559/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper559/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper559/Authors|ICLR.cc/2021/Conference/Paper559/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869667, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper559/-/Official_Comment"}}}, {"id": "vdmlGM5kSmy", "original": null, "number": 1, "cdate": 1603725726196, "ddate": null, "tcdate": 1603725726196, "tmdate": 1605024660612, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Official_Review", "content": {"title": "A GAN-like approach to representation learning that has little to do with disentangling", "review": "The authors present a new representation learning algorithm that trades off between a sufficiency condition (that is, the label should be independent of the input conditioned on the representation) and what they call a \"disentangling\" condition - that the representation vectors should be independent of one another and rotationally invariant. While the first condition has been used to define disentangled representations, the second is not standard. From the condition of rotational invariance, they require that the distribution over representations is isomorphic to a uniform Gaussian. They arrive at a loss with two terms, the first is a distance correlation between labels and representation, and the second is a divergence between the representation and a uniform Gaussian. In this sense, the regularization term looks quite similar to a VAE while the loss term looks quite similar to standard classification losses. The regularization is represented as a maximum over another loss, leading to a GAN-like coupled optimization problem.\n\nThe paper is written with quite a lot of complicated math. While I did not find anything wrong with the math, it did seem like at times it was meant more to obfuscate and impress and was often not really necessary. The experiments were almost entirely either comparisons against classic statistical methods (section 6.1 and 6.2, Table 1) or experiments on MNIST, FashionMNIST and CIFAR-10 (6.2, Table 3). Given the explosion of different deep learning methods for classification, representation learning, and disentangling in the last decade, it seems like the comparisons against classic statistical methods is missing the point - it seems highly likely that other deep learning methods could work just as well. In the experiments on MNIST, FashionMNIST and CIFAR-10, the authors set quite a high bar for themselves. These are 3 of the most over-studied datasets in all of machine learning - literally thousands, maybe tens of thousands of papers have been written on various deep learning algorithms applied to these datasets in the last several years. The experiments presented here do not seem to be a proper apples-to-apples comparison, however. A standard MLP is used for the SDRL (the method developed in this paper) while a DenseNet architecture trained with a classification loss is used for the \"convolutional network\" method. I was not able to find details on the dCorAE architecture. To properly reduce the number of possible confounding factors, the same architecture should be used with both the SDRL and the baseline methods, with different objective functions. Also, the classification numbers presented on CIFAR-10 do not seem to be state-of-the-art. According to Papers With Code, the DenseNet is able to achieve 96% accuracy on CIFAR-10, and even newer methods are able to reach 99% accuracy without additional data.\n\nMy biggest objection to the paper, however, is that it seems completely unrelated to disentangling. The only experiments presented are on visualization and classification - no results on standard disentangling tasks are presented. The rotational invariance condition seems completely against the grain of disentangling research. Despite disagreements over the exact definition of disentangling, there is at least broad agreement that a disentangled representation is one in which certain directions in latent space are privileged over others, and align with \"true\" latent factors in the world. While I disagree with the probabilistic definition of disentangling given in Locatello et al (2018), and prefer the geometric definition of Higgins et al (2018), I do agree with the point made in Locatello that, if different unique directions in latent space are not identifiable, then disentangling is not possible. Yet in this paper, the non-identifiability of different directions is given as a *necessary condition* for disentangling. This is the completely contrary to how the term is usually used. Even in the beta-VAE, which does have a rotationally-invariant loss function, the invariance is broken by requiring that the approximate posterior from the encoder has a diagonal covariance. Given this, I would recommend that the authors remove any reference to disentangling, and rewrite this purely as an alternative approach to supervised learning.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper559/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper559/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140438, "tmdate": 1606915782205, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper559/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper559/-/Official_Review"}}}, {"id": "y-6j1ib1WU9", "original": null, "number": 2, "cdate": 1603827171534, "ddate": null, "tcdate": 1603827171534, "tmdate": 1605024660543, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Official_Review", "content": {"title": "This framework is novel to me, but the paper needs to be revised", "review": "This paper proposes a new representation learning framework for supervised learning\u00a0\nIn order to achieve good prediction accuracies while maintaining some desired properties.\u00a0\nThe sufficiency and disentangled properties of the representation are formalized in\nthis paper to achieve this goal. Most results in the literature have focused on\nthe unsupervised setting, therefore this framework is novel to me.\nBelow I have a few comments to revise the paper.\u00a0\n\nTo achieve sufficiency, this paper proposes to use the distance covariance defined in Section 4.\u00a0\nThis seems to be related with statistical correlation,\ncan this be written out in the paper? As when I read the proof of\u00a0\ntheorem 4.2, I do not know what is the \\rho(R,R*). It seems not\u00a0\nto be defined in the paper.\u00a0\nAlso is there any relation between V[z,y]\u00a0with the sufficient statistics (Fisher 1922),\u00a0as mentioned in the introduction?\u00a0\nFrom (5) I then understand that this covariance V replies on the metric on X and Y.\u00a0\nFor multi-class classification problems, it is not always clear how to choose\na metric on Y. I recommend the authors to clarify this choice\nat least in the numerical experiments.\u00a0\n\nIn the statement of Theorem 4.3, should \\lambda = O(1) be\u00a0strictly positive? The objective (7) is formalized as a bi-level optimization problem, and the computation problem is addressed\u00a0using recently proposed methods based on gradient flows.\u00a0There is a typo 2 lines before Section 5, should\u00a0the rate be (log n) n^{-2 / (2+d)} ?\u00a0\n\nSection 6 presents numerical results. I recommend to\u00a0clarify what is Y in the classification problem\u00a0\nand how the metric is chosen.\u00a0Figure 1 should be significantly improved.\u00a0Figure 1 illustrates well that the learnt\u00a0features are sufficient, but\u00a0it is unclear whether the features are disentangled.\u00a0To make the results more convincing, I suggest to plot the learned representation directly (maybe\u00a0\nin Appendix) to check whether it looks like an isotropic 2d\u00a0Gaussian. Any statistical test of Gaussianity would be\u00a0more convincing.\u00a0\n\nWhat is the distance correlation computed in Figure 2?\u00a0\nV[z,y] is called the distance covariance, so I guess it is something\u00a0\nelse. Is the Table 3 showing the training or test accuracy?\u00a0\n\nTypo:\u00a0\n- in the definition of\u00a0V[z,y] on page 4, the constant c_m should be c_q?\u00a0\n- 3 lines before (2), N_d (0, I_d) -> N (0, I_d)?\n- The (a) and (e) in Figure 1 are not the learned features,\u00a0but the original data.\n- page 19: matric entropy -> metric entropy\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper559/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper559/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140438, "tmdate": 1606915782205, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper559/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper559/-/Official_Review"}}}, {"id": "vUZXZ64nI1H", "original": null, "number": 3, "cdate": 1603900924046, "ddate": null, "tcdate": 1603900924046, "tmdate": 1605024660466, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Official_Review", "content": {"title": "A solid work that aims to learn sufficient and disentangled representations, with strong motivations, sound theoretical justification and extensive empirical validations. ", "review": "This work proposes a method, SDRL, for learning sufficient and disentangled representations, with the additional property that the representations should also be rotation-invariant. Together with the disengtangled property, the ration-invariant property specifies the distribution of representations to be isotropic Gaussians. On the other hand, the repressentations are required to be sufficient for predicting the target labels. These two goals motivate the Lagrangian formulation of the objective function, based on which the authors apply two different estimators for these two goals. Experiments are extensive, with a good mixture of synthetic and real-world data. I particularly like the visualization of the learned representations on the synthetic data, which well corroborates the theoretical claims in the first part of the paper. \n\nOverall, I think the paper has a sound logic and clear presentation. The only downside is that the novelty is a bit limited: both the sufficiency criterion (see Tishby et al. 2015) and the disentangled criterion (mutual independency) have well been studied in the literature, despite using other divergence measures, e.g., mutual information and KL divergence. That being said, I haven't seen similar work using exactly the same divergences as the ones in this paper, e.g., distance correlation and f-divergence in this context. \n\nA few more minor comments and questions:\n-   I could understand representations that are rotation-invariant are desirable in vision tasks, but I am wondering is this requirement too strong for other applications, e.g., in language and speech? Other than the technical convenience brought by this property, is there any other reason that could motivate this criterion? \n\n-   In Eq. (2), shouldn't the domain of g be R^p instead of R^d? Otherwise the intersection of M and F would be empty. \n\n-   If my understanding about Lemma 2.1 is correct, it is the pushforward of \\mu under T to be an isotropic Gaussian distribution, right? The original Z = g(X) is only sufficient, but not disintangled nor rotation-invariant. If this is the case, then I am not sure I totally agree with the second comment after Theorem 4.2, where the authors remark that the discriminator network is the pushforward function T. My understanding is that the representer network R is used to approximate the composite of T and g, so that R_\\sharp \\mu_X = R^* in Eq. (3), while the discriminator network D is only served as the witness function in the definition of f-divergence in Eq. (6). Please clarify this point, thanks!\n\n-   The line above Eq. (9), typo: no \\lambda is needed before the V term.\n\n-   IMO the second assumption of Theorem 4.3 is quite strong: it requires the density to be lower bounded by a positive constant c_1, which does not even hold for Gaussians. On the other hand, the compactness of the domain helps to reconcile this assumption, and I understand that this is a standard assumption in the analysis of nonparametric density estimation/regression.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper559/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper559/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140438, "tmdate": 1606915782205, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper559/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper559/-/Official_Review"}}}, {"id": "B8tzfP4tVcJ", "original": null, "number": 4, "cdate": 1603987280747, "ddate": null, "tcdate": 1603987280747, "tmdate": 1605024660400, "tddate": null, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "invitation": "ICLR.cc/2021/Conference/Paper559/-/Official_Review", "content": {"title": "Review for \"Sufficient and Disentangled Representation Learning\"", "review": "This paper introduces SDRL, a representation learning algorithm for supervised learning.\nSDRL aims to enforce three constraints to the learned representation g(x) :\n1) That all the information from X about Y is preserved: Y independent to x given g(x)\n2) That it's distributionally rotation invariant: Law(O g(x)) = Law (g(x))\n3) That all it's coordinates are independent: Law(g(x)_i) = Law(g(x)_j) for all i, j\n\nSince 2 and 3 are equivalent to asking that g(x) is Gaussian, they look for a pushforward map g such that g(x) is Gaussian.\nIn particular, they pick the optimal transport one given by Brenier's theorem.\n\nI think the paper needs more work in the following on 4 very important aspects:\n\nA) The paper is not very well motivated in my opinion. Why do we want to learn a representation with statistically independent and rotation invariant coordinates? I know many works already try to achieve this goal, but this paper doesn't do a good job at motivating it. It merely cites a bunch of other papers that do so, but reading the paper I am left with the impression of \"we want to come up with a representation with properties 1, 2, 3. Why? Ask other people\". It doesn't feel like I'm given a sufficient explanation of why I should be reading this paper and the reality is that if I wasn't asked to review it I would have left it in the pile of papers in my desk half way through the introduction. This is very concretely seen in the sentence \"We also require that the representation is rotation invariant in distribution. Such an invariance property is an important characteristic in many visual and morphological tasks \\cite{}\". This is a third of the problem you're trying to solve, it is a bit crazy that all the motivation it has is a citation to two papers that are not mentioned anywhere else in the paper.\n\nB) The method is very complicated. Why use the distance covariance rather than the empirical risk to assure that the information from X about Y is preserved in G(X)? I know the authors have several choices for V, but the choice they pick is quite particular and a complicated one at it. Without much motivation or experiments ablating this choice it's hard to justify it. It's hard to convince the reader to bother like this.\n\nC) The learning bounds are pretty irrelevant. the n^-{2/(2+d)} bound, while minimax optimal, is useless in high dimensions (high meaning d > 10, which is usually what we are interested in representation learning). While minimax optimal ***in the distributionally worst case***, it is irrelevant in practice (and it is not an advance in any theoretical field). It's simply taking extra space in the paper, it really brings nothing to the table in my opinion.\n\nD) The experiments are extremely underwhelming. For such a complicated method does not bring any significant benefit in the real classification datasets, and a very minor one in the regression ones. In regression, furthermore, an important baseline is missing: the last layer of an NN trained with a regression and / or ordinal regression loss. It seems unfair to compete with the NN inductive bias in the author's algorithm vs linear algorithms.\n\nOverall, the authors take us through an interesting but overly complicated mathematical journey with a poor motivation, irrelevant learning bounds, and no empirical evidence that the journey was useful whatsoever.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper559/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper559/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Sufficient and Disentangled Representation Learning", "authorids": ["~Jian_Huang5", "yulingjiaomath@whu.edu.cn", "liaoxu@u.duke.nus.edu", "jin.liu@duke-nus.edu.sg", "zyu@stat.ecnu.edu.cn"], "authors": ["Jian Huang", "Yuling Jiao", "Xu Liao", "Jin Liu", "Zhou Yu"], "keywords": ["Conditional independence", "f-divergence", "rotation invariant", "neural network", "statistical guarantee"], "abstract": "We propose a novel approach to representation learning called sufficient and disentangled representation learning (SDRL). With SDRL, we seek a data representation that maps the input data to a lower-dimensional space with two properties: sufficiency and disentanglement. First, the representation is sufficient in the sense that the original input data is conditionally independent of the response or label given the representation. Second, the representation is maximally disentangled with mutually independent components and rotation invariant in distribution. We show that such a representation always exists under mild conditions on the input data distribution based on optimal transport theory. We formulate an objective function characterizing conditional independence and disentanglement. This objective function is then used to train a sufficient and disentangled representation with deep neural networks. We provide strong statistical guarantees for the learned representation by establishing an upper bound on the excess error of the objective function and show that it reaches the nonparametric minimax rate under mild conditions. We also validate the proposed method via numerical experiments and real data analysis.", "one-sentence_summary": "This paper proposes a sufficient and disentangled representation learning approach in the context of supervised learning. ", "pdf": "/pdf/8dfaa8ec06e9926c93599fab598cc406b961bcda.pdf", "supplementary_material": "/attachment/b0fb6b09d79343947943a87fcb63549e74e27629.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "huang|sufficient_and_disentangled_representation_learning", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=VYs_1ZoFQ1", "_bibtex": "@misc{\nhuang2021sufficient,\ntitle={Sufficient and Disentangled Representation Learning},\nauthor={Jian Huang and Yuling Jiao and Xu Liao and Jin Liu and Zhou Yu},\nyear={2021},\nurl={https://openreview.net/forum?id=IeuEO1TccZn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IeuEO1TccZn", "replyto": "IeuEO1TccZn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper559/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140438, "tmdate": 1606915782205, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper559/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper559/-/Official_Review"}}}], "count": 10}