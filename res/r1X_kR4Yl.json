{"notes": [{"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1492465666986, "tcdate": 1487359851334, "number": 114, "id": "r1X_kR4Yl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "r1X_kR4Yl", "signatures": ["~Ji_Gao1"], "readers": ["everyone"], "content": {"title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples", "abstract": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task.  To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.", "pdf": "/pdf/38abd32d7ea408ccce7366534ae953179972b01d.pdf", "TL;DR": "An effective defense approach which removes irrelevant features and thus increase the ability to deal with adversarial samples.", "paperhash": "gao|deepcloak_masking_deep_neural_network_models_for_robustness_against_adversarial_samples", "keywords": ["Deep learning"], "conflicts": ["cs.virginia.edu", "nju.edu.cn", "sjtu.edu.cn", "nec-labs.com", "fb.com"], "authors": ["Ji Gao", "Beilun Wang", "Zeming Lin", "Weilin Xu", "Yanjun Qi"], "authorids": ["jg6yd@virginia.edu", "bw4mw@virginia.edu", "xuweilin@virginia.edu", "yanjun@virginia.edu"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028608450, "tcdate": 1490028608450, "number": 1, "id": "rkluB_FTox", "invitation": "ICLR.cc/2017/workshop/-/paper114/acceptance", "forum": "r1X_kR4Yl", "replyto": "r1X_kR4Yl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples", "abstract": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task.  To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.", "pdf": "/pdf/38abd32d7ea408ccce7366534ae953179972b01d.pdf", "TL;DR": "An effective defense approach which removes irrelevant features and thus increase the ability to deal with adversarial samples.", "paperhash": "gao|deepcloak_masking_deep_neural_network_models_for_robustness_against_adversarial_samples", "keywords": ["Deep learning"], "conflicts": ["cs.virginia.edu", "nju.edu.cn", "sjtu.edu.cn", "nec-labs.com", "fb.com"], "authors": ["Ji Gao", "Beilun Wang", "Zeming Lin", "Weilin Xu", "Yanjun Qi"], "authorids": ["jg6yd@virginia.edu", "bw4mw@virginia.edu", "xuweilin@virginia.edu", "yanjun@virginia.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028609030, "id": "ICLR.cc/2017/workshop/-/paper114/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1X_kR4Yl", "replyto": "r1X_kR4Yl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028609030}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489588081165, "tcdate": 1489417571368, "number": 2, "id": "H1oPHENig", "invitation": "ICLR.cc/2017/workshop/-/paper114/public/comment", "forum": "r1X_kR4Yl", "replyto": "rkC_BFJjx", "signatures": ["~Ji_Gao1"], "readers": ["everyone"], "writers": ["~Ji_Gao1"], "content": {"title": "Thank you for your review!", "comment": "Dear Reviewer:\n\nThank you for your valuable comments!  We have revised our paper according to your suggestions. \n \nQ1: \u201cThe motivation of this method is to remove irrelevant feature. But it is questionable that this is the only reason of the existence of adversarial examples. It then leads to the question how important this factor is. \u201d\n\nA1: Thank you for pointing out this issue of writing. \n- We have revised the caption of Figure~2 into \u201cOne possible type of adversarial vulnerability when learning a linear classifier from unnecessary features.\u201d\n- Yes. We totally agree that unnecessary features may not be the only reason for the existence of adversarial examples.  \n- Methods like Jacobian-based saliency map approach, JSMA[1] limit the perturbation on a few number of feature dimensions. Figure-2 is a simple sketch of example attacks in this category. \n- To defend against similar attacks like JSMA, DeepCloak may be a quick and simple strategy by reducing the number of unnecessary features. We are in the process of adding JSMA based experimental results. \n\nQ2:\u201dDetermining irrelevant features by comparing the magnitude of the changes between original sample and the adversarial sample seems questionable. Further investigations are needed to verify this measurement.\u201d\n\nA2: We want to clarify the question and answer accordingly. \n- For each pair of seed sample and its adversarial sample (for a fix \\epsilon), we use an  entry-wise L1 to get a difference vector; then we get an accumulated vector by summing up difference vectors from all such pairs of training samples; then the mask is learned by assigning top $k$ entries of the accumulated vector into 0 and the rest into 1.   \n- The accumulated vector provides a summary of how each feature dimension (from g(x) layer) varies across a population of samples. \n- Presented in Figure-2, the basic motivation of DeepCloak is that the distance between an adversarial sample and its seed example will be small along necessary feature dimension (horizontal direction in Figure-2). The distance is relatively large along the unnecessary dimensions (vertical direction in Figure-2) for the current task. DNN combines feature extraction and classification in one model.  Therefore, we propose to remove unnecessary features for a DNN model (by masking the feature vector from the output of g(x) layer). This mask layer directly modifies DNN network structure and requires no retraining.\n- The feature difference vector between adversarial sample and seed sample is one simple and intuitive measurement that can find out which features are used more or less by adversarial samples. Currently, we are using the entry-wise L1 to capture such a relationship. We are going to try different distance measures as the next step.\n\nQ3:\u201dThe experimental results of the paper seems weak. Especially, although the results indicate improvement in robustness, it is not as good as the current results in the literature.\u201d\n\n\nA3: We agree that our current result is not as good as from adversarial training. \n- However, most recent defense approaches like adversarial training are retraining based methods;  \n- Adversarial training[2] injects adversarial samples with correct labels in the training set to retrain DNN model. It is computationally expensive and has been claimed to only works in defending against the attack that produced the added adversarial examples. \n- Our method requires no further training and directly modifies DNN model structure. This means our approach is complementary to other popular defense methods. \n- We will add our method on top of adversarial training to compare and check its effectiveness.\n\nQ4:\u201dSome comparative experiments should also be tested to better justify the new method. For example, replacing the masking layer by a layer of dropout or pooling, what would be the results? \u201d\nA4: Thank you for your comment. \n- We have added experimental result that compares DeepCloak with a random mask layer (which works the same as a dropout layer in the test phase) to justify our method. \n- The new results are added in the Appendix, Section 6.3.2 (Page 6 - Page 7). Our results indicate that RandomMask has almost no effect on adversarial accuracy. DeepCloak achieves much better performance in comparison with RandomMask.\n\nReference:\n[1]: Papernot, Nicolas, et al. \"The limitations of deep learning in adversarial settings.\" Security and Privacy (EuroS&P), 2016 IEEE European Symposium on. IEEE, 2016.\n[2]: Szegedy, Christian, et al. \"Intriguing properties of neural networks.\" arXiv preprint arXiv:1312.6199 (2013).\n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples", "abstract": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task.  To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.", "pdf": "/pdf/38abd32d7ea408ccce7366534ae953179972b01d.pdf", "TL;DR": "An effective defense approach which removes irrelevant features and thus increase the ability to deal with adversarial samples.", "paperhash": "gao|deepcloak_masking_deep_neural_network_models_for_robustness_against_adversarial_samples", "keywords": ["Deep learning"], "conflicts": ["cs.virginia.edu", "nju.edu.cn", "sjtu.edu.cn", "nec-labs.com", "fb.com"], "authors": ["Ji Gao", "Beilun Wang", "Zeming Lin", "Weilin Xu", "Yanjun Qi"], "authorids": ["jg6yd@virginia.edu", "bw4mw@virginia.edu", "xuweilin@virginia.edu", "yanjun@virginia.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487359851973, "tcdate": 1487359851973, "id": "ICLR.cc/2017/workshop/-/paper114/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper114/reviewers"], "reply": {"forum": "r1X_kR4Yl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487359851973}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489112008703, "tcdate": 1488995477399, "number": 1, "id": "rJ6c4T6qx", "invitation": "ICLR.cc/2017/workshop/-/paper114/official/review", "forum": "r1X_kR4Yl", "replyto": "r1X_kR4Yl", "signatures": ["ICLR.cc/2017/workshop/paper114/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper114/AnonReviewer2"], "content": {"title": "Interesting, but unclear applicability", "rating": "7: Good paper, accept", "review": "The paper proposes an approach that masks out those features preceding the last fully connected layer that are most sensitive to changes in adversarial examples in order to improve its robustness.\n\nAlthough, it is a workshop paper, some details should be clarified more. For example it is unclear whether the masking happens across locations or also it depends on the spatial location of the neuron, as it is applied on a max-pooling layer.\n\nAlthough it is irrelevant for the point they try to make, they claim the starting model they utilize is state of the art for accuracy, which is not the case.\n\nThe most interesting observation (and the weak point at the same time) is that adversarial robustness is greatly improved after masking a single feature output, but addition masking has relatively large negative effect on the final accuracy, while improving the adversarial robustness insignificantly. This could have several possible explanations that paper fails to explore, but accepts only the first explanation as correct:\n- This is a fundamental phenomenon that occurs generally \n- This is specific to this data set.\n- This is specific to this model\nGiven the lack of additional experimental evidence, it is hard to assess which possibility is most likely.\n\n[Edit]:  Given the latest updates to the paper that have significantly increased the quality, I have revised my score up.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples", "abstract": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task.  To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.", "pdf": "/pdf/38abd32d7ea408ccce7366534ae953179972b01d.pdf", "TL;DR": "An effective defense approach which removes irrelevant features and thus increase the ability to deal with adversarial samples.", "paperhash": "gao|deepcloak_masking_deep_neural_network_models_for_robustness_against_adversarial_samples", "keywords": ["Deep learning"], "conflicts": ["cs.virginia.edu", "nju.edu.cn", "sjtu.edu.cn", "nec-labs.com", "fb.com"], "authors": ["Ji Gao", "Beilun Wang", "Zeming Lin", "Weilin Xu", "Yanjun Qi"], "authorids": ["jg6yd@virginia.edu", "bw4mw@virginia.edu", "xuweilin@virginia.edu", "yanjun@virginia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489110390908, "id": "ICLR.cc/2017/workshop/-/paper114/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper114/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper114/AnonReviewer2", "ICLR.cc/2017/workshop/paper114/AnonReviewer1"], "reply": {"forum": "r1X_kR4Yl", "replyto": "r1X_kR4Yl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper114/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper114/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489110390908}}}, {"tddate": null, "tmdate": 1489110390309, "tcdate": 1489110390309, "number": 2, "id": "rkC_BFJjx", "invitation": "ICLR.cc/2017/workshop/-/paper114/official/review", "forum": "r1X_kR4Yl", "replyto": "r1X_kR4Yl", "signatures": ["ICLR.cc/2017/workshop/paper114/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper114/AnonReviewer1"], "content": {"title": "Official Review", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes another method to improve the robustness of DNN classifiers. The proposed method, deepcloak, attends to remove unneccesary features in the feature layer by inserting a 0-1 filter for the feature. The experimental results seem a little bit weak. \n\nThe writing of the paper is good. The idea of the deepcloak is new, but not quite convincing:\n1. The motivation of this method is to remove irrelevant feature. But it is questionable that this is the only reason of the existence of adversarial examples. It then leads to the question how important this factor is. \n2. Determining irrelevant features by comparing the magnitude of the changes between original sample and the adversarial sample seems questionable. Further investigations are needed to verify this measurement.\n\nPros:\n1. The idea of the proposed method, deepcloak, is new.\nCons:\n1. The motivation and the methodology of this paper seem questionable. It would need better justifications.\n2. The experimental results of the paper seems weak. Especially, although the results indicate improvement in robustness, it is not as good as the current results in the literature.\n3. Some comparative experiments should also be tested to better justify the new method. For example, replacing the masking layer by a layer of dropout or pooling, what would be the results? ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples", "abstract": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task.  To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.", "pdf": "/pdf/38abd32d7ea408ccce7366534ae953179972b01d.pdf", "TL;DR": "An effective defense approach which removes irrelevant features and thus increase the ability to deal with adversarial samples.", "paperhash": "gao|deepcloak_masking_deep_neural_network_models_for_robustness_against_adversarial_samples", "keywords": ["Deep learning"], "conflicts": ["cs.virginia.edu", "nju.edu.cn", "sjtu.edu.cn", "nec-labs.com", "fb.com"], "authors": ["Ji Gao", "Beilun Wang", "Zeming Lin", "Weilin Xu", "Yanjun Qi"], "authorids": ["jg6yd@virginia.edu", "bw4mw@virginia.edu", "xuweilin@virginia.edu", "yanjun@virginia.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489110390908, "id": "ICLR.cc/2017/workshop/-/paper114/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper114/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper114/AnonReviewer2", "ICLR.cc/2017/workshop/paper114/AnonReviewer1"], "reply": {"forum": "r1X_kR4Yl", "replyto": "r1X_kR4Yl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper114/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper114/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489110390908}}}, {"tddate": null, "tmdate": 1489103977004, "tcdate": 1489103977004, "number": 1, "id": "BkbuhP1ix", "invitation": "ICLR.cc/2017/workshop/-/paper114/public/comment", "forum": "r1X_kR4Yl", "replyto": "rJ6c4T6qx", "signatures": ["~Ji_Gao1"], "readers": ["everyone"], "writers": ["~Ji_Gao1"], "content": {"title": "Thank you for your review!", "comment": "Dear Reviewer:\n\nThank you for your valuable comments! We have revised our paper according to your suggestions. More specifically,\n\nA0:  We just realize that there is a famous project from Facebook Research also called DeepMask. Therefore we change our method name to DeepCloak.\n\nQ1: \u201cSome details should be clarified more. For example it is unclear whether the masking happens across locations or also it depends on the spatial location of the neuron, as it is applied on a max-pooling layer.\u201d\n\nA1:\nThank you for pointing out this unclear part. The mask layer is applied on the output of the max-pooling layer in the example shown in Figure 1. We have added necessary details as follows,\n1. We have revised the description of our method on Page 2.\n2. We have revised the schematic Figure 1 by adding more illustrations and made the figure clearer. \n3. We have modified the illustration of Algorithm 1 to make it clearer.\nAll above revisions we have made are included in Section 3 (Page 2 and Page 3).\n\nQ2: \u201cAlthough it is irrelevant for the point they try to make, they claim the starting model they utilize is state of the art for accuracy, which is not the case.\u201d\n\nA2:\nYou\u2019re definitely right. We have removed this wrong claim from our paper. The state-of-the-art algorithm on CIFAR-10 is Wide Residual Network[1]. In Table 1, we show the result of another popular model Residual Network.Our Residual Network is properly trained and achieves reasonable accuracy.  \nIn addition, we have added results of the Wide Residual Network into a new Table 3, in the appendix of the revised version of the paper.\n\n\nQ3: \u201cWhy adversarial robustness is greatly improved after masking a single feature output, but addition masking has relatively large negative effect on the final accuracy, while improving the adversarial robustness insignificantly.\u201d\n\nA3:\nThank you for bringing our attention to this interesting question. To give this question a proper answer, we have added results about three more DNN models. These experimental results have been put into the appendix.\nIn summary, our experiment currently cover 4 different models, including a small CNN on the MNIST dataset and VGG, ResNet, Wide ResNet on the CIFAR-10 dataset (See Section 6.3)\nOur experimental results indicate the effectiveness of DeepCloak is related to DNN model structure. \nModel structures like Residual Network reduce the number of the features in training. In such networks, only masking a small number of features can lead to a significant increase in adversarial performance. However, when more features are masked out, DeepCloak leads to the performance decrease since necessary features might be masked as well. \nNetworks like VGG model seem to include more unnecessary feature dimensions. So DeepCloak needs to mask more nodes to improve the adversarial performance. At the same time, masking more nodes doesn\u2019t lead to the performance decrease as fast as the case in Residual Network.\nWe will investigate the interesting question by implementing more experiments on different models and datasets in a longer version of our paper.\n\n[1]: Zagoruyko, Sergey, and Nikos Komodakis. \"Wide residual networks.\" arXiv preprint arXiv:1605.07146 (2016)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples", "abstract": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task.  To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.", "pdf": "/pdf/38abd32d7ea408ccce7366534ae953179972b01d.pdf", "TL;DR": "An effective defense approach which removes irrelevant features and thus increase the ability to deal with adversarial samples.", "paperhash": "gao|deepcloak_masking_deep_neural_network_models_for_robustness_against_adversarial_samples", "keywords": ["Deep learning"], "conflicts": ["cs.virginia.edu", "nju.edu.cn", "sjtu.edu.cn", "nec-labs.com", "fb.com"], "authors": ["Ji Gao", "Beilun Wang", "Zeming Lin", "Weilin Xu", "Yanjun Qi"], "authorids": ["jg6yd@virginia.edu", "bw4mw@virginia.edu", "xuweilin@virginia.edu", "yanjun@virginia.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487359851973, "tcdate": 1487359851973, "id": "ICLR.cc/2017/workshop/-/paper114/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper114/reviewers"], "reply": {"forum": "r1X_kR4Yl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487359851973}}}], "count": 6}