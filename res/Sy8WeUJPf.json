{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124471380, "tcdate": 1518456830368, "number": 169, "cdate": 1518456830368, "id": "Sy8WeUJPf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Sy8WeUJPf", "signatures": ["~Yash_Sharma1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Attacking the Madry Defense Model with $L_1$-based Adversarial Examples", "abstract": "The Madry Lab recently hosted a competition designed to test the robustness of their adversarially trained MNIST model. Attacks were constrained to perturb each pixel of the input image by a scaled maximal $L_\\infty$ distortion $\\epsilon$ = 0.3. This decision discourages the use of attacks which are not optimized on the $L_\\infty$ distortion metric. Our experimental results demonstrate that by relaxing the $L_\\infty$ constraint of the competition, the \\textbf{e}lastic-net \\textbf{a}ttack to \\textbf{d}eep neural networks (EAD) can generate transferable adversarial examples which, despite their high average $L_\\infty$ distortion, have minimal visual distortion. These results call into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrate the power of EAD at generating robust adversarial examples.", "paperhash": "sharma|attacking_the_madry_defense_model_with_l_1based_adversarial_examples", "keywords": ["Adversarial Attacks", "Adversarial Defenses", "Adversarial Training", "PGD", "EAD", "Distortion Metrics"], "_bibtex": "@misc{\n  sharma2018attacking,\n  title={Attacking the Madry Defense Model with $L_1$-based Adversarial Examples},\n  author={Yash Sharma and Pin-Yu Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy8WeUJPf}\n}", "authorids": ["sharma2@cooper.edu", "pin-yu.chen@ibm.com"], "authors": ["Yash Sharma", "Pin-Yu Chen"], "TL;DR": "EAD can generate minimally visually distorted adversarial examples which transfer to the Madry Defense Model, calling into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrating the power of EAD at generating robust adversarial examples.", "pdf": "/pdf/1bf85dec80ae5fe98b8bc17ca9169720a2642f2b.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582916071, "tcdate": 1520419168860, "number": 1, "cdate": 1520419168860, "id": "SktDZHT_M", "invitation": "ICLR.cc/2018/Workshop/-/Paper169/Official_Review", "forum": "Sy8WeUJPf", "replyto": "Sy8WeUJPf", "signatures": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer2"], "content": {"title": "interesting experiments on the adversarial attacks", "rating": "6: Marginally above acceptance threshold", "review": "This paper emphazises the issue of using L_\\infty norm when generating adversarial examples: the obtained attacks are less efficient that those using L_1.\nThis paper also provides experiments suggesting that adversarial examples generating by PGD algorithm using L_\\infty norm are less visualy similar than those generated by EAD algorithm.\n\nI suggest to authors to provide experiments to compare the visual similarity of adversarial examples generated using L_\\infty and L_1 norm. For increasing the clarity of the paper, I also suggest to explain what is a non-targeted attack.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attacking the Madry Defense Model with $L_1$-based Adversarial Examples", "abstract": "The Madry Lab recently hosted a competition designed to test the robustness of their adversarially trained MNIST model. Attacks were constrained to perturb each pixel of the input image by a scaled maximal $L_\\infty$ distortion $\\epsilon$ = 0.3. This decision discourages the use of attacks which are not optimized on the $L_\\infty$ distortion metric. Our experimental results demonstrate that by relaxing the $L_\\infty$ constraint of the competition, the \\textbf{e}lastic-net \\textbf{a}ttack to \\textbf{d}eep neural networks (EAD) can generate transferable adversarial examples which, despite their high average $L_\\infty$ distortion, have minimal visual distortion. These results call into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrate the power of EAD at generating robust adversarial examples.", "paperhash": "sharma|attacking_the_madry_defense_model_with_l_1based_adversarial_examples", "keywords": ["Adversarial Attacks", "Adversarial Defenses", "Adversarial Training", "PGD", "EAD", "Distortion Metrics"], "_bibtex": "@misc{\n  sharma2018attacking,\n  title={Attacking the Madry Defense Model with $L_1$-based Adversarial Examples},\n  author={Yash Sharma and Pin-Yu Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy8WeUJPf}\n}", "authorids": ["sharma2@cooper.edu", "pin-yu.chen@ibm.com"], "authors": ["Yash Sharma", "Pin-Yu Chen"], "TL;DR": "EAD can generate minimally visually distorted adversarial examples which transfer to the Madry Defense Model, calling into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrating the power of EAD at generating robust adversarial examples.", "pdf": "/pdf/1bf85dec80ae5fe98b8bc17ca9169720a2642f2b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582915858, "id": "ICLR.cc/2018/Workshop/-/Paper169/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper169/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper169/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper169/AnonReviewer3"], "reply": {"forum": "Sy8WeUJPf", "replyto": "Sy8WeUJPf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper169/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582915858}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582841890, "tcdate": 1520592339092, "number": 2, "cdate": 1520592339092, "id": "SJs0H1lYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper169/Official_Review", "forum": "Sy8WeUJPf", "replyto": "Sy8WeUJPf", "signatures": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer1"], "content": {"title": "Questioning L_\\infty-bounded adversarial perturbations", "rating": "7: Good paper, accept", "review": "The authors consider L_1 and L_2-bounded adversarial perturbations generated by Elastic-net Attack to DNNs (EAD) and observe that EAD gives better visually imperceptible and transferable attacks, despite high L_\\infty distortion. Thus, they argue that adversarial training against only L_\\infty-bounded adversarial attacks such as PGD (Projected Gradient Descent) and FGM (Fast Gradient Method) may not be sufficient.\n\nIn the targeted case, EAD achieves the same or better attack success rate than PGD and iterative-FGM with much lower L_1 and L_2 distortion. In the non-targeted case, PGD and iterative-FGM give similar attack success rate at lower L_\\infty distortion. However, in this case, the authors observe in Figure 1 that the examples generated by EAD have less visual distortion or noise.\n\nOverall, this paper has two contributions. First, it questions the choice of L_\\infty-bounded attacks to model visual imperceptible adversarial perturbations. Second, it compares the visual distortion of the L_1/L_2-bounded attacks generated by EAD and the L_\\infty-bounded attacks generated by PGD and iterative-FGM. I think both are decent contributions, worth a place in ICLR workshop. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attacking the Madry Defense Model with $L_1$-based Adversarial Examples", "abstract": "The Madry Lab recently hosted a competition designed to test the robustness of their adversarially trained MNIST model. Attacks were constrained to perturb each pixel of the input image by a scaled maximal $L_\\infty$ distortion $\\epsilon$ = 0.3. This decision discourages the use of attacks which are not optimized on the $L_\\infty$ distortion metric. Our experimental results demonstrate that by relaxing the $L_\\infty$ constraint of the competition, the \\textbf{e}lastic-net \\textbf{a}ttack to \\textbf{d}eep neural networks (EAD) can generate transferable adversarial examples which, despite their high average $L_\\infty$ distortion, have minimal visual distortion. These results call into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrate the power of EAD at generating robust adversarial examples.", "paperhash": "sharma|attacking_the_madry_defense_model_with_l_1based_adversarial_examples", "keywords": ["Adversarial Attacks", "Adversarial Defenses", "Adversarial Training", "PGD", "EAD", "Distortion Metrics"], "_bibtex": "@misc{\n  sharma2018attacking,\n  title={Attacking the Madry Defense Model with $L_1$-based Adversarial Examples},\n  author={Yash Sharma and Pin-Yu Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy8WeUJPf}\n}", "authorids": ["sharma2@cooper.edu", "pin-yu.chen@ibm.com"], "authors": ["Yash Sharma", "Pin-Yu Chen"], "TL;DR": "EAD can generate minimally visually distorted adversarial examples which transfer to the Madry Defense Model, calling into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrating the power of EAD at generating robust adversarial examples.", "pdf": "/pdf/1bf85dec80ae5fe98b8bc17ca9169720a2642f2b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582915858, "id": "ICLR.cc/2018/Workshop/-/Paper169/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper169/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper169/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper169/AnonReviewer3"], "reply": {"forum": "Sy8WeUJPf", "replyto": "Sy8WeUJPf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper169/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582915858}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582634528, "tcdate": 1520814995646, "number": 3, "cdate": 1520814995646, "id": "HyhcjH7Kf", "invitation": "ICLR.cc/2018/Workshop/-/Paper169/Official_Review", "forum": "Sy8WeUJPf", "replyto": "Sy8WeUJPf", "signatures": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer3"], "content": {"title": "An interesting read, few details missing", "rating": "7: Good paper, accept", "review": "The paper discusses a different attack strategy to assess the adversarial attack resistant model as proposed in Madry et al. Madry et al. suggested generating the adversarial samples by perturbing each pixel with a maximum L_\\infty distortion of 0.3. The authors uses elastic net distortion indeed and show that this can produce transferable adversarial samples with higher L_\\infty distortion yet visually similar to the original images. The proposed approach  is a generalization of the L_2 attack discussed by Carlini and Wagner. The paper is nicely written and presents the concepts and results well.\n\nMinor comments:\nDefine Confidence in Table 1\nPlease explain t in Eq. 1 and 2.", "confidence": "1: The reviewer's evaluation is an educated guess"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attacking the Madry Defense Model with $L_1$-based Adversarial Examples", "abstract": "The Madry Lab recently hosted a competition designed to test the robustness of their adversarially trained MNIST model. Attacks were constrained to perturb each pixel of the input image by a scaled maximal $L_\\infty$ distortion $\\epsilon$ = 0.3. This decision discourages the use of attacks which are not optimized on the $L_\\infty$ distortion metric. Our experimental results demonstrate that by relaxing the $L_\\infty$ constraint of the competition, the \\textbf{e}lastic-net \\textbf{a}ttack to \\textbf{d}eep neural networks (EAD) can generate transferable adversarial examples which, despite their high average $L_\\infty$ distortion, have minimal visual distortion. These results call into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrate the power of EAD at generating robust adversarial examples.", "paperhash": "sharma|attacking_the_madry_defense_model_with_l_1based_adversarial_examples", "keywords": ["Adversarial Attacks", "Adversarial Defenses", "Adversarial Training", "PGD", "EAD", "Distortion Metrics"], "_bibtex": "@misc{\n  sharma2018attacking,\n  title={Attacking the Madry Defense Model with $L_1$-based Adversarial Examples},\n  author={Yash Sharma and Pin-Yu Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy8WeUJPf}\n}", "authorids": ["sharma2@cooper.edu", "pin-yu.chen@ibm.com"], "authors": ["Yash Sharma", "Pin-Yu Chen"], "TL;DR": "EAD can generate minimally visually distorted adversarial examples which transfer to the Madry Defense Model, calling into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrating the power of EAD at generating robust adversarial examples.", "pdf": "/pdf/1bf85dec80ae5fe98b8bc17ca9169720a2642f2b.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582915858, "id": "ICLR.cc/2018/Workshop/-/Paper169/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper169/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper169/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper169/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper169/AnonReviewer3"], "reply": {"forum": "Sy8WeUJPf", "replyto": "Sy8WeUJPf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper169/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper169/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582915858}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573556071, "tcdate": 1521573556071, "number": 57, "cdate": 1521573555730, "id": "SJn2C00YG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Sy8WeUJPf", "replyto": "Sy8WeUJPf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attacking the Madry Defense Model with $L_1$-based Adversarial Examples", "abstract": "The Madry Lab recently hosted a competition designed to test the robustness of their adversarially trained MNIST model. Attacks were constrained to perturb each pixel of the input image by a scaled maximal $L_\\infty$ distortion $\\epsilon$ = 0.3. This decision discourages the use of attacks which are not optimized on the $L_\\infty$ distortion metric. Our experimental results demonstrate that by relaxing the $L_\\infty$ constraint of the competition, the \\textbf{e}lastic-net \\textbf{a}ttack to \\textbf{d}eep neural networks (EAD) can generate transferable adversarial examples which, despite their high average $L_\\infty$ distortion, have minimal visual distortion. These results call into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrate the power of EAD at generating robust adversarial examples.", "paperhash": "sharma|attacking_the_madry_defense_model_with_l_1based_adversarial_examples", "keywords": ["Adversarial Attacks", "Adversarial Defenses", "Adversarial Training", "PGD", "EAD", "Distortion Metrics"], "_bibtex": "@misc{\n  sharma2018attacking,\n  title={Attacking the Madry Defense Model with $L_1$-based Adversarial Examples},\n  author={Yash Sharma and Pin-Yu Chen},\n  year={2018},\n  url={https://openreview.net/forum?id=Sy8WeUJPf}\n}", "authorids": ["sharma2@cooper.edu", "pin-yu.chen@ibm.com"], "authors": ["Yash Sharma", "Pin-Yu Chen"], "TL;DR": "EAD can generate minimally visually distorted adversarial examples which transfer to the Madry Defense Model, calling into question the use of $L_\\infty$ as a sole measure for visual distortion, and further demonstrating the power of EAD at generating robust adversarial examples.", "pdf": "/pdf/1bf85dec80ae5fe98b8bc17ca9169720a2642f2b.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}