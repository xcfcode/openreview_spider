{"notes": [{"id": "yfKOB5CO5dY", "original": "cEedR5xEW00", "number": 2837, "cdate": 1601308314899, "ddate": null, "tcdate": 1601308314899, "tmdate": 1614985643187, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QMyqAHBEV8u", "original": null, "number": 1, "cdate": 1610040518648, "ddate": null, "tcdate": 1610040518648, "tmdate": 1610474127102, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper presents a PAC-Bayesian approach for meta-learning that utilizes information of the task distribution in the prior. The presented localized approach allows the authors to derive an algorithm directly from the bound - this is a worthwhile contribution. Nevertheless there are several concerns that were raised by the reviewers and in its current form the work is not ready to appear in ICLR.  \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040518634, "tmdate": 1610474127086, "id": "ICLR.cc/2021/Conference/Paper2837/-/Decision"}}}, {"id": "C2r23uj7e2t", "original": null, "number": 4, "cdate": 1604203218450, "ddate": null, "tcdate": 1604203218450, "tmdate": 1607191431490, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Official_Review", "content": {"title": "This paper naturally extends PAC-Bayesian analysis to construct a task-adaptive hyperprior, but its empirical justification is not strong.", "review": "Update: I appreciate the response to address the major concerns. The proposed approach doesn't follow the episodic training, so there exists a clear difference from advanced MAML approaches, which update the task-specific parameters in the episodic training. I still believe that more empirical justifications should be required to decide which approach performs better than the others. On a positive side, I completely agree that it is non-trivial to develop a localized meta-learning framework from a theoretical perspective. So, I increased my score by one point. \n\n**Summary**\nThis work proposes a localized meta-learning framework that adaptively determines a hyperprior for a specific task, derived from a PAC-Bayesian analysis. \n\n**Detailed comments**\nThe main idea of this approach is to adjust the global hyperposterior distribution by the information of the input task. Conceptually, this approach is very similar to previous ones:\n* Meta-learning with latent embedding optimization, ICLR\u201919.\n* Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace, ICML\u201918.\n\nBoth propose data-dependent parameter generative processes, in which a subset of parameters is used to solve the input task. But, the proposed approach has not been compared to them.\n\nThe proposed method has been validated in very limited few-shot classification scenarios (only 5-way 50-shot tasks). It\u2019s necessary to consider more standard experiments settings. I guess 1-shot / 5-shot on miniImagenet and tieredImagenet tasks should be included in experiments. Otherwise, it\u2019s very difficult to see that this approach works well in general settings. \n\nFor few-shot classification tasks, there exist many attempts to improve the performance. But, none of these approaches has not been compared to the proposed method. More empirical justification is needed.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2837/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2837/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087693, "tmdate": 1606915805356, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2837/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2837/-/Official_Review"}}}, {"id": "5eiSF3WID3j", "original": null, "number": 2, "cdate": 1603891653210, "ddate": null, "tcdate": 1603891653210, "tmdate": 1606313038097, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Official_Review", "content": {"title": "Review #1", "review": "----------Updates after rebuttal------------\n\nThe author did not provide a revised version and additional experiments to address my questions. \n\nI keep my original score.\n\n------------------------------------------\n\n\nSummary: \n\nThis paper proposed a new localized prior PAC-Bayes meta-learning. Specifically, they adopted local coordinate coding (LCC) to replace the previous global hyper-posterior predictive distribution.  Empirical results on few-shot tasks verified its practical benefits.\n\n------------------------------------------------------\n\nOverall review \n\nPros:\n\n[1] This problem is well-motivated. I think such a direction is meaningful. \n\n[2] The high level of proof is technically sound.\n\nCons:\n\n[1] The paper organization indeed requires a better refinement. I can follow the main proof but too many non-important details make the paper rather difficult to follow. \n\n[2] (main concern) The theoretical and practical benefits are not significant. It seems like a plug-in approach for LCC.\n\n[3] Some technical details need better justifications and discussions.\n\nBased on these, I recommend a weak rejection but encourage a major revision for resubmission.\n\n-----------------------------------------------------------\nDetailed explanations\n\n[1] Paper organization\n\nThe main contribution of this paper is Theorem 2 (and an improved Theorem 1). I think more intuitions/insights/discussions about these are expected. In contrast, the definition of Lipschitz and Smooth and Lemma 1,2 are not critical in understanding your main idea. \n\nThis is particularly important in Sec 3, where the main contributions occur. But it is quite difficult to follow the insights in this part. And this also influences the significance of the paper.\n\n[2] The significance of the results\n\nTheoretical perspective:\n\n[a] The author claimed they reach a tighter bound with $\\mathcal{O}(\\frac{1}{m})$ (Line 259). I respectfully disagree with this claim. In fact, the author chooses a specific function (smooth property) for deriving a tighter bound. But in the deep network due to the non-linear activation function, this condition can hardly be achieved in practice. Therefore the reason for improving the performance is not the proposed good theory.\n\n[b] I cannot understand why using LCC in meta-Pac-Bayes.  Why not using other clustering-based approaches to set anchor points such as the core-set approach? If there is a **particular** benefit to use LCC rather than other localized approaches, the explanations are highly expected. \n\nSince the whole proof idea is quite similar to Amit (2018) and Pentina (2014) with two-level Pac-Bayes theorems except the LCC part. From the current version, it seems a simple LCC plug-in approach, which hurts its significance.  \n\nPractical Perspective:\n\n[c] The empirical results indicate LCC based PAC-Bayes heavily depend on the number of anchor numbers. i.e if we do not choose the best anchor number, the results can be even worse than the simple baseline ML_AM. \n\n[d] The computational complexity of LML is not reported. I think it can be much higher than ML_AM/ML_PL, which restricts its practical utility.\n\n[e] The analysis of the result can be much better improved. For example, the author claimed better local information extraction. I would like to see clear evidence/analysis  (e.g. TSNE, visualization of anchor points, the relation of different tasks) rather than prediction accuracy curves. This also enhances the significance of LCC.\n\n\n[3] Other minor details\n\n[a] The loss in PAC-Bayes in $[0,1]$, the cross-entropy is surely not.\n\n[b] The Catoni bound (eq 25) the $\\pi$ must not depend on $x_1,\\dots x_n$. This may contradict your data-dependent bound?\n\n[c] Lemma 1 and Lemma 2 are straightforward. I do not think it does not provide strong practical insights into the proposed approach.  \n\n\n--------------------------------------------\n\nSuggestions \n\nI suggest a major revision on Sec 3, theoretical discussions, and empirical analysis (not only accuracy) on the benefits of LCC.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2837/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2837/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087693, "tmdate": 1606915805356, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2837/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2837/-/Official_Review"}}}, {"id": "Qt-GM9w5WWV", "original": null, "number": 5, "cdate": 1606279698098, "ddate": null, "tcdate": 1606279698098, "tmdate": 1606279698098, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "QWBV_TOaXXC", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Official_Comment", "content": {"title": "Response to Reviewer2", "comment": "Thank you for your thoughtful review. We answer your questions below.\n\nQ1: The theoretical novelty of this paper is limited.\nA1: Please refer to our response A2 to R1\n\nQ2: hyperprior and hyperposterior share the same variance.\nA2; Please refer to our response to A3 to R4\n\nQ3:  the prior w^P_i is determined from the samples S_i via LCC\nA3: Please refer to our response A5 to R1\n\nQ4: From Equation (10), I do not expect the task complexity term to be reduced necessarily.\nA4: Instead of using a fixed distribution, we formulate meta-knowledge as a flexible mapping function from task data distribution to specific prior. As claimed in line76, we provide a means to tighten the PAC-Bayes meta-learning bound. Empirical results in Fig 4(b) validate the efficacy of the proposed method. Specifically, to make the proposed prior predictor close to the posterior, we summarize the required properties in line166-176.\n\nQ5: What are the implications of c1 and c2?\nA5: c1, c2 can be treated as hyperparameters that balance the trade-off between empirical error and regularization terms.\n\nQ6: More baseline and dataset from meta-learning and few-shot learning\nA6; Please refer to our response to R3\n\nQ7: This algorithm lacks flexibility in handling regression problems.\nA7: We respectively disagree with it. We think the proposed LML could be easily extended to regression problems. AS we discussed it in line 166-176, the key step of designing a good prior predictor for classification problems should satisfy these three properties.  For the regression problem, the prior predictor should be regress task agnostic since it will be used in the new regression task."}, "signatures": ["ICLR.cc/2021/Conference/Paper2837/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yfKOB5CO5dY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2837/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2837/Authors|ICLR.cc/2021/Conference/Paper2837/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843997, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2837/-/Official_Comment"}}}, {"id": "1PPye_35DBf", "original": null, "number": 4, "cdate": 1606279520482, "ddate": null, "tcdate": 1606279520482, "tmdate": 1606279520482, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "5eiSF3WID3j", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We greatly appreciate your comments and thoughtful suggestions. We respond to each comment as follows.\n\nQ1:Why using LCC.\n A1: We have explained the properties required for constructing prior predictor in line 166-176. Here we propose an LCC-based prior predictor as an example. Because it provides a means to approximate the high-dimensional non-linear function with a global linear function with theoretical guarantees.\n\nQ2: The framework seems a simple LCC plug-in approach\nA2: We follow the two-level proof framework proposed in Amit (2018) and Pentina (2014), but we respectively disagree that it is a simple LCC plug-in approach. In the original setting, meta-knowledge is formulated as a hyperposterior distribution that generates the prior for each task. In LML, we formulate meta-knowledge as a mapping function from task data distribution to the prior, which makes it not trivial to address. Thus, we propose Lemma 1, 2, and LCC based prior predictor, which is novel in itself.\n\nQ3: The choice of anchor numbers. Computation complexity\nA3: As we claimed in line220-224, if we set anchor numbers to 1, it degenerates to the regular meta-learning framework. If we set it to a quite large number, it will suffer from the overfitting issue. We respectively disagree this flexibility is a flaw of LML.  As we described in line 240-243, we utilize an auto-encoder to extract the semantic information of data and then construct the LCC scheme based on the embeddings.  The extra computation cost scales linearly to anchor numbers and embedding size, which is relatively small compared to the base model parameter.\n\nQ4: The loss in PAC-Bayes in [0,1], the cross-entropy is surely not.\nA4: We discussed it in line 476-478.\n\nQ5: The Catoni bound (eq 25) the \u03c0 must not depend on x1,\u2026xn. This may contradict your data-dependent bound?\nA5: We respectively disagree with it. We discussed it in line191, we choose prior upon the task data distribution. Since it is unknown, we approximate it by its empirical counterpart.  A similar idea has been used in \u201cPAC-Bayes  bounds with data-dependent priors\u201d and \u201cData-dependent PAC-Bayes priors via differential privacy\u201d"}, "signatures": ["ICLR.cc/2021/Conference/Paper2837/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yfKOB5CO5dY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2837/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2837/Authors|ICLR.cc/2021/Conference/Paper2837/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843997, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2837/-/Official_Comment"}}}, {"id": "AyuZGn9NG0r", "original": null, "number": 3, "cdate": 1606279363875, "ddate": null, "tcdate": 1606279363875, "tmdate": 1606279363875, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "bPzJ3N9H6Yu", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Official_Comment", "content": {"title": "Response to Reviewer4", "comment": "We thank the reviewer for the valuable feedback and helpful comments. We will reorganize the structure to make it more clear in the next version. Here we answer your questions below.\n\nQ1: \u201cThe notion of 'localized learning' is key to the work\u2026require clear elaboration\u201d\nA1:  Your understanding is correct. We present the formal definition in line 157-165 and the localized meta-learning is implemented with an LCC-based prior predictor. We discuss the data-independent and data-dependent priors in line 140-155. Thanks for point out the paper \u201cPAC-Bayes analysis beyond the usual bounds\u201d. We will add and discuss it in our next version.\n\nQ2:\u201d How do local anchor points relate to the standard notion of a basis\u201d\nA2: Thanks for your suggestion. Due to space limitations, we have removed the example for LCC in the current version. We will put it in the appendix in our next version. Here the anchor point and the basis have the same meaning.\n\nQ3: \u201cBoth the prior and are spherical Gaussians with the same variance, i.e., they do not allow the variance to adapt through learning. Isn't this a significant limitation?\nA3: In the current version, we follow the setting in \u201cPAC-Bayes bounds with data-dependent priors.\u201d and formulate the prior predictor as a mapping function from task data distribution to the mean of the base model parameter. We only consider the mean for ease of presentation. It should be noted that our framework can be easily extended to the setting considering both the mean and the variance.\n\nQ4: Catoni's bound\nA4: As we claimed in line 121-122, the proof technique is agnostic to the specific single-task bound. To make a fair comparison, we adopt the same Catoni\u2019s bound to analyze the regular ML and the proposed LML framework.\n\nQ5: Further explanation about eq. (3) and (9)\nA5: Our explanation can be found in line 210-224\n\nQ6: The use of Latent Manifold assumption\nA6: Due to space limitation, we put the further analysis about lemma 1 in the appendix in line 429-431, which uses latent manifold assumption to derive the upper bound in eq. (14)\n\nQ7: Optimization of LCC\nA7: As described in line 241-243, we use an auto-encoder to extract the semantic information of image data and then construct the LCC scheme based on the embeddings. The LCC is optimized with an alternative update between C and \\gamma. More details for LCC training can be found in line 500-506."}, "signatures": ["ICLR.cc/2021/Conference/Paper2837/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yfKOB5CO5dY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2837/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2837/Authors|ICLR.cc/2021/Conference/Paper2837/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843997, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2837/-/Official_Comment"}}}, {"id": "yNv8Sz6NVAR", "original": null, "number": 2, "cdate": 1606278988134, "ddate": null, "tcdate": 1606278988134, "tmdate": 1606278988134, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "C2r23uj7e2t", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "We thank the reviewer for raising your concern in experiments. We agree that some existing meta-learning works follow the similar concept, however, they are not derived from the generalization bound. We discuss these in related work in line395-406. Thanks for pointing out these two works. We will include them in the related work section. We want to emphasize that we do not follow the episodic training to address few-shot learning but to explore the PAC-Bayes meta-learning framework [Amit et al. 2018]. As we discussed in line225, to make a fair comparison with PAC-Bayes baselines, we follow the same joint optimization method as Amit et al. 2018 to ensure that the benefit of LML is not from using an improved optimization method. Thus, we also follow the similar meta-learning environment setting as Amit et al. 2018. That is, the meta-learner observes a limited number of tasks (from 1 to 11) and each task has sufficient samples. In this scarce task setting, conventional meta-learning methods following the episodic training paradigm will suffer from the severe task-overfitting issue. We can observe LML significantly outperforms MAML and MatchingNet in Fig 5 and we explain it in line531-538. Thus, we do not compare any improved meta-learning method that follows the episodic training paradigm. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2837/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yfKOB5CO5dY", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2837/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2837/Authors|ICLR.cc/2021/Conference/Paper2837/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843997, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2837/-/Official_Comment"}}}, {"id": "bPzJ3N9H6Yu", "original": null, "number": 3, "cdate": 1603954566587, "ddate": null, "tcdate": 1603954566587, "tmdate": 1605024121425, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Official_Review", "content": {"title": "The overall idea of the paper is interesting, and, according to my understanding novel and promising. However, the lack of clarity and comparative analysis, renders my assement somewhat uncertain. ", "review": "The paper presents an algorithm for offline meta-learning, where tasks are drawn from a distribution and presented to a learner sequentially, the objective being to use accumulated knowledge in order to facilitate the learning of new tasks. The algorithm is motivated from the PAC-Bayes theory of generalization, extended in recent years to the meta-learning setup, and provides a new localized approach where the prior used for a novel task is allowed to depend on the data from the new task in addition to all previous tasks. They make use of a previously proposed local learning method (LCC) that leads to extra flexibility and to a tightening of the meta-learning bounds, and provide an algorithm based on these bounds from which a learning algorithm is derived based on minimization of the upper bound. Empirical results are provided demonstrating the efficacy of the method and comparing to other recent approaches.  \n\n\nStrong points The problems of meta-learning is of significant current interest and importance, and the theoretical formulation within the widely used and proven PAC-Bayes framework is well founded. Moreover, the authors go beyond current bounds by introducing a localized approach to data-dependent prior learning, deriving an algorithm directly from theoretical bounds, and demonstrating its utility.  \n\nWeak points The paper is difficult to follow making it hard to assess its true novelty. For example, the notion of 'localized learning' is key to the work, but is never defined clearly. Given its important it warrants a clear definition. For instance, in the caption of image 1 they state ''Instead of using global\nmeta-knowledge for all tasks, we tailor the meta-knowledge for various specific task.\" This is not very helpful. If I understand correctly, the main issue is using the LCC approach for determining a data-dependent prior for the new task. Moreover, given the most work on PAC-Bayes theory specifically requires that the prior be data-independent, and that task-dependent priors have only very recently been introduced and analyzed (e.g., Rivasplata et al., PAC-Bayes analysis beyond the usual bounds, NeurIPS 2020 ), this is a major issue that requires clear elaboration.\n\nSpecific issues\n\nThe definition of coordinate coding and the latent manifold require some further elaboration, perhaps through an example. How do local anchor points relate to the standard notion of a basis? \n\nThe authors assume that both the prior and posterior (lines 128-129) are spherical Gaussians with the same variance, i.e., they do not allow the variance to adapt through learning. Isn't this a significant limitation? The same holds for the distribution of v in line 160. Some discussion of this is called for. \n\nThe authors present a bounds based on Catoni's work claiming, correctly, that it allows for faster convergence at a rate of 1/m rather than 1/\\sqrt{m}. However, they refrain from noting that the empirical error is multiplied by a constant that is larger than 1, as opposed to previous bounds. While this may not be an issue for deep networks that have very low empirical error, this should be clearly stated. \n\nThe form of the task-complexity term in eq. (3) and (9) requires further explanation and elaboration given its pivotal role in the work.\n\nIn Lemma 1 and Theorem 2, it is not clear whether you used the  Latent Manifold assumption (Def 3) , and where does it come into play in the bound and in the analysis?\n\nThe optimization of LCC is not explained sufficiently.  Since it is the main part of the innovation, it should be explained more than is done in Appendix B.  Eq. 12 uses L2 distance on the data points, how effective this is in real data sets?\n\nGiven the multiple definitions required for the algorithm, I would urge the authors to include all necessary definitions and equations in Appendix F that contains the pseudo-code. This will greatly facilitate understanding the exact form of the algorithm. \n\nMinor points\nLine 49 - missing Fig number, Definition 3 - change order to  d_M:=|C|, Line 95 - delete b, Line 96 - any point in M (not in R^d). \n\nConcerns \nThe overall idea of the paper is interesting, and, according to my understanding novel and promising. However, I cannot be certain that I have fully understood the paper, given the concerns raised above about lack of clarity. I believe that a clear re-writing of the paper, highlighting the novel contributions of the work and putting it in context (e.g., re new results by Rivasplata mentioned above) is called for. I believe that such a rewriting, assuming the correctness of the proofs in the appendix that I have not checked, will greatly enhance the paper. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2837/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2837/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087693, "tmdate": 1606915805356, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2837/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2837/-/Official_Review"}}}, {"id": "QWBV_TOaXXC", "original": null, "number": 1, "cdate": 1603887175954, "ddate": null, "tcdate": 1603887175954, "tmdate": 1605024121356, "tddate": null, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "invitation": "ICLR.cc/2021/Conference/Paper2837/-/Official_Review", "content": {"title": "Limited theoretical novelty with some technical flaws", "review": "- Summary and Contributions\n    - In this paper, the authors proposed a localized version of PAC-Bayes bound for meta-learning. This paper is a direct extension of (Amit and Meir 2018) by customizing the prior of each task with Local Coordinate Coding. \n\t\t\n- Strengths\n    - The main text is easy to follow, with major theorems and intuitions well illustrated. \n    - The idea of localized prior for specific tasks is supported and inspired from the PAC-Bayes theory.\n\t\t\n- Weaknesses and questions:\n   - The theoretical novelty of this paper is limited - it is a combination of the theoretical analyses in (Amit and Meir 2018) and those in the work of LCC. \n   - Why do the hyperprior in Line 130 and the prior sampled from the hyper-posterior in Line 128 share the same variance? It does not make any sense. Actually, the two variances are different in the work of (Amit and Meir 2018).\n   - Following the PAC-Bayesian theory and Line 154-156, the prior must be independent from the samples but could be selected from the distribution. However, in this paper, the prior w^P_i is determined from the samples S_i via LCC. I expect the theoretical analysis is not valid. \n   - From Equation (10), I do not expect the task complexity term to be reduced necessarily. Could the authors elaborate any guarantee of this algorithm to learn an effective v^Q so that this term is reduced?\n   - What are the implications of c1 and c2?\n   - In fact, there are many gradient-based meta-learning algorithms working on adapting the global meta-knowledge to specific tasks, including [1] and [2]. The authors did not compare any of them. Moreover, I am not convinced by the results in E.4. Figure 5. I am curious about LML's performance on standard mini-ImageNet and Omniglot datasets. \n    - This algorithm lacks flexibility in handling regression problems. Could the authors provide an explanation and discussion on this?\n    - Could you illustrate the anchor points u empirically?\n\t\t\n[1] Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J Lim. Toward multimodal model-agnostic meta-learning. arXiv preprint arXiv:1812.07172, 2018.\n[2] Yao, H., Wei, Y., Huang, J., & Li, Z. (2019, May). Hierarchically Structured Meta-learning. In International Conference on Machine Learning (pp. 7045-7054).", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2837/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2837/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Localized Meta-Learning: A PAC-Bayes Analysis for Meta-Learning Beyond Global Prior", "authorids": ["~Chenghao_Liu1", "~Tao_Lu2", "~Doyen_Sahoo1", "~Yuan_Fang1", "~Kun_Zhang1", "~Steven_Hoi2"], "authors": ["Chenghao Liu", "Tao Lu", "Doyen Sahoo", "Yuan Fang", "Kun Zhang", "Steven Hoi"], "keywords": ["localized meta-learning", "PAC-Bayes", "meta-learning"], "abstract": "Meta-learning methods learn the meta-knowledge among various training tasks and aim to promote the learning of new tasks under the task similarity assumption. Such meta-knowledge is often represented as a fixed distribution; this, however, may be too restrictive to capture various specific task information because the discriminative patterns in the data may change dramatically across tasks. In this work, we aim to equip the meta learner with the ability to model and produce task-specific meta-knowledge and, accordingly, present a localized meta-learning framework based on the PAC-Bayes theory. In particular, we propose a Local Coordinate Coding (LCC) based prior predictor that allows the meta learner to generate local meta-knowledge for specific tasks adaptively. We further develop a practical algorithm with deep neural network based on the bound. Empirical results on real-world datasets demonstrate the efficacy of the proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "liu|localized_metalearning_a_pacbayes_analysis_for_metalearning_beyond_global_prior", "supplementary_material": "/attachment/002871d6464ef475ee1dd409003c6324a87653e1.zip", "pdf": "/pdf/a430ac66ab5a615a16afffd1e019b75452550bff.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=b2s5cw26qg", "_bibtex": "@misc{\nliu2021localized,\ntitle={Localized Meta-Learning: A {\\{}PAC{\\}}-Bayes Analysis for Meta-Learning Beyond Global Prior},\nauthor={Chenghao Liu and Tao Lu and Doyen Sahoo and Yuan Fang and Kun Zhang and Steven Hoi},\nyear={2021},\nurl={https://openreview.net/forum?id=yfKOB5CO5dY}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yfKOB5CO5dY", "replyto": "yfKOB5CO5dY", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2837/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087693, "tmdate": 1606915805356, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2837/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2837/-/Official_Review"}}}], "count": 10}