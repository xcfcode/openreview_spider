{"notes": [{"id": "Jimc6cIyRzY", "original": "EBHebna7kky", "number": 22, "cdate": 1615310252778, "ddate": null, "tcdate": 1615310252778, "tmdate": 1615313022308, "tddate": null, "forum": "Jimc6cIyRzY", "replyto": null, "invitation": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission", "content": {"title": "State Entropy Maximization with Random Encoders for Efficient Exploration", "authorids": ["ICLR.cc/2021/Workshop/SSL-RL/Paper22/Authors"], "authors": ["Anonymous"], "keywords": ["reinforcement learning", "deep learning", "exploration"], "TL;DR": "We use the representation space of a random encoder to estimate state entropy, which is used as an intrinsic reward for exploration.", "abstract": "Recent exploration methods have proven to be a recipe for improving sample-efficiency in deep reinforcement learning (RL). However, efficient exploration in high-dimensional observation spaces still remains a challenge. This paper presents Random Encoders for Efficient Exploration (RE3), an exploration method that utilizes state entropy as an intrinsic reward. In order to estimate state entropy in environments with high-dimensional observations, we utilize a $k$-nearest neighbor entropy estimator in the low-dimensional representation space of a convolutional encoder. In particular, we find that the state entropy can be estimated in a stable and compute-efficient manner by utilizing a randomly initialized encoder, which is fixed throughout training. Our experiments show that RE3 significantly improves the sample-efficiency of both model-free and model-based RL methods on locomotion and navigation tasks from DeepMind Control Suite and MiniGrid benchmarks. We also show that RE3 allows learning diverse behaviors without extrinsic rewards, effectively improving sample-efficiency in downstream tasks.", "pdf": "/pdf/b772e39edefd0f7559d5dfc9a68d9175d34d55ee.pdf", "paperhash": "anonymous|state_entropy_maximization_with_random_encoders_for_efficient_exploration", "_bibtex": "@inproceedings{\nanonymous2021state,\ntitle={State Entropy Maximization with Random Encoders for Efficient Exploration},\nauthor={Anonymous},\nbooktitle={Submitted to Self-Supervision for Reinforcement Learning Workshop - ICLR 2021},\nyear={2021},\nurl={https://openreview.net/forum?id=Jimc6cIyRzY},\nnote={under review}\n}"}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "signatures": {"values": ["ICLR.cc/2021/Workshop/SSL-RL"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Workshop/SSL-RL"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Workshop/SSL-RL"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1615310247528, "tmdate": 1615313016556, "id": "ICLR.cc/2021/Workshop/SSL-RL/-/Blind_Submission"}}, "tauthor": "~Super_User1"}], "count": 1}