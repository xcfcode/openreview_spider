{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396358852, "tcdate": 1486396358852, "number": 1, "id": "SJ1Riz8_x", "invitation": "ICLR.cc/2017/conference/-/paper104/acceptance", "forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be \"incremental\". \n \n Pros:\n - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work\n - Quality: The experimental results were thorough, \"very extensive and leaves no doubt that the proposed approach works well\".\n \n Mixed:\n - Novelty: There is appreciation that the work is novel. However as the work is somewhat \"application-specific\" the reviewers felt the technical contribution was not an overwhelming contribution.\n - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or \"main speed-up factor(s)\" were. \n \n This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396359430, "id": "ICLR.cc/2017/conference/-/paper104/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396359430}}}, {"tddate": null, "tmdate": 1485458712080, "tcdate": 1485458712080, "number": 9, "id": "Skg7pTPvl", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "rJFd3PUDe", "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "writers": ["~Michael_Auli1"], "content": {"title": "Re: acknowledge", "comment": "PixelCNN was applied to vision and CNNs have been known to work on vision tasks for quite some time. The same is not true for language. Our paper is the first work that successfully demonstrates that CNNs can be very competitive to LSTMs on language and in particular translation. It demonstrates that CNNs can model long-range dependencies for language."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}, {"tddate": null, "tmdate": 1485368447571, "tcdate": 1482186744618, "number": 3, "id": "BJ-bx1I4x", "invitation": "ICLR.cc/2017/conference/-/paper104/official/review", "forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "signatures": ["ICLR.cc/2017/conference/paper104/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper104/AnonReviewer4"], "content": {"title": "Final Review: good paper applying CNN to translation to match bi-LSTM baseline", "rating": "7: Good paper, accept", "review": "The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.\n\nKey ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).\n\nThe experimental results are well reported in detail.\n\nOne or two figures would definitely be required to help clarify the architecture.\n\nThis paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512695833, "id": "ICLR.cc/2017/conference/-/paper104/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper104/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper104/AnonReviewer1", "ICLR.cc/2017/conference/paper104/AnonReviewer3", "ICLR.cc/2017/conference/paper104/AnonReviewer4"], "reply": {"forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper104/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper104/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512695833}}}, {"tddate": null, "tmdate": 1485368433414, "tcdate": 1485368433414, "number": 5, "id": "rJFd3PUDe", "invitation": "ICLR.cc/2017/conference/-/paper104/official/comment", "forum": "BJAA4wKxg", "replyto": "HyatmhWIg", "signatures": ["ICLR.cc/2017/conference/paper104/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper104/AnonReviewer4"], "content": {"title": "acknowledge..", "comment": "I appreciate the additional comments about conference fit, although I do agree with AnonReviewer1 that, e.g. PixelCNN also demonstrates CNN's ability to model long-term structure (i.e. that RNN's are not unique in this regard)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726432, "id": "ICLR.cc/2017/conference/-/paper104/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper104/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper104/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726432}}}, {"tddate": null, "tmdate": 1484838887745, "tcdate": 1484838887745, "number": 4, "id": "r1gluUALe", "invitation": "ICLR.cc/2017/conference/-/paper104/official/comment", "forum": "BJAA4wKxg", "replyto": "ryiuoXhUl", "signatures": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "content": {"title": "Convolutons vs RNN", "comment": "My last 5 cents: the fact that deep CNNs can handle long-term dependencies has been already discussed in the works on Pixel RNNs and Pixel CNNs (https://arxiv.org/abs/1601.06759, https://arxiv.org/pdf/1606.05328v2.pdf). In my view this limits the contribution of this paper to showing that the same holds for the machine translation encoder."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726432, "id": "ICLR.cc/2017/conference/-/paper104/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper104/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper104/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726432}}}, {"tddate": null, "tmdate": 1484696434646, "tcdate": 1484696434646, "number": 8, "id": "ryiuoXhUl", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "rJC_2IqUl", "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "writers": ["~Michael_Auli1"], "content": {"title": "Re: ICLR 2017 conference paper104 AnonReviewer1", "comment": "We just updated the paper according to our reply to the reviewers on January 9th. \n\nWe respectfully disagree that requiring two stacks is the main finding of the paper. Our goal is to show that convolutional encoders with finite contexts can compare favorably to RNNs. This claim is supported with extensive experiments.\n\nWe also edited the discussion around the two-stack architecture. In particular, Figure 1a already presents extensive experimentation around the depth of the two stacks. At this point we do not see how to extend our analysis further."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484696247756, "tcdate": 1478223062231, "number": 104, "id": "BJAA4wKxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJAA4wKxg", "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "content": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484577910091, "tcdate": 1484577910091, "number": 3, "id": "rJC_2IqUl", "invitation": "ICLR.cc/2017/conference/-/paper104/official/comment", "forum": "BJAA4wKxg", "replyto": "B1-h4nZ8l", "signatures": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "content": {"title": "Reviewer response", "comment": "Thank you for your comments. As far as I understand there has been no next version of the paper, and I do not think I can increase the score without seeing it in this case. It is still very unclear for me why 1-layer CNN-a + 1-layer CNN-c can perform so much better than just a 1-layer CNN, this would require a  thorough investigation, as this seems to be the main finding of the paper.\n\nThe same applies for the reasons behind the faster decoding."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726432, "id": "ICLR.cc/2017/conference/-/paper104/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper104/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper104/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726432}}}, {"tddate": null, "tmdate": 1484010665556, "tcdate": 1484010665556, "number": 7, "id": "B1-h4nZ8l", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "Hytb1xzVe", "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "writers": ["~Michael_Auli1"], "content": {"title": "Re: ICLR 2017 conference paper104 AnonReviewer1", "comment": "Paper is application specific: we chose to focus on machine translation which is a challenging task with strong competition systems, dedicated product teams interested in improvements, and MT has been used in the past to introduce sequence to sequence models (e.g. Sutskever et al, Bahdanau et al). It is very likely that tasks relying on architectures similar to attention-based translation will benefit by our architecture as well, e.g. summarization, constituency parsing, or dialog modeling. In future work, we plan to evaluate on other sequence to sequence problems as well.\n\nTwo stacks: our experimentation with the two stacks suggests that the attention weights (CNN-a) need to integrate information from a wide context which can be done with a deep stack; at the same time, the vectors which are averaged (CNN-c) seem to benefit from a shallower, more local representation closer to the input words. Two stacks is an easy way to achieve these contradicting requirements. We will add this discussion to the text to make the point clearer. Inspired by LSTMs, we are currently exploring non-linearities with gating that could potentially derive a single representation satisfying both requirements.\n\nFaster evaluation: we will clarify this in the next version of the paper. Please refer to our previous answer to this question in the thread below."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}, {"tddate": null, "tmdate": 1484010472049, "tcdate": 1484010472049, "number": 6, "id": "r1xxEhZ8e", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "SJK13DGEg", "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "writers": ["~Michael_Auli1"], "content": {"title": "Re: ICLR 2017 conference paper104 AnonReviewer3", "comment": "We will add figures illustrating our model architecture better; our baseline biLSTM architecture closely follows [Zhou et al, TACL 2016] https://arxiv.org/pdf/1606.04199v3.pdf who have detailed figures illustrating the model. \nPosition embeddings are important. Without them our encoder has no notion of position. We investigated position embeddings numbered from the left to right and right to left but found that one direction was enough. \n\nRegarding conference fit: we show that RNNs are not necessary to perform encoding in a complex sequence to sequence task and that CNNs are comparable or better. Of course, this is of interest to the NLP community but also to the ML community. In term of representation learning, the two stack CNN highlight that attention weights and the input representation might benefit from different depth, which can impact other tasks, and other models. Our work also highlights  that the success of RNNs in MT cannot be attributed to their unique ability to model long term dependencies. We believe that those findings are of great interest to the ML community."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}, {"tddate": null, "tmdate": 1484010372869, "tcdate": 1484010372869, "number": 5, "id": "HyatmhWIg", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "BJ-bx1I4x", "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "writers": ["~Michael_Auli1"], "content": {"title": "Re: ICLR 2017 conference paper104 AnonReviewer4", "comment": "We will add figures illustrating the architecture. Please see our response regarding conference fit in the replies to the other reviewers below."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}, {"tddate": null, "tmdate": 1483816024679, "tcdate": 1483816024679, "number": 2, "id": "SJbwh2RSl", "invitation": "ICLR.cc/2017/conference/-/paper104/official/comment", "forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "signatures": ["ICLR.cc/2017/conference/paper104/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper104/areachair1"], "content": {"title": "Authors: Please post a rebuttal", "comment": "Authors, It would be great to have a rebuttal for this paper as reviewers will be discussing over the next week. Thanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726432, "id": "ICLR.cc/2017/conference/-/paper104/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper104/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper104/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726432}}}, {"tddate": null, "tmdate": 1481960416772, "tcdate": 1481960416772, "number": 2, "id": "SJK13DGEg", "invitation": "ICLR.cc/2017/conference/-/paper104/official/review", "forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "signatures": ["ICLR.cc/2017/conference/paper104/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper104/AnonReviewer3"], "content": {"title": "A well-executed NLP paper", "rating": "6: Marginally above acceptance threshold", "review": "This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.\n\nOne minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512695833, "id": "ICLR.cc/2017/conference/-/paper104/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper104/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper104/AnonReviewer1", "ICLR.cc/2017/conference/paper104/AnonReviewer3", "ICLR.cc/2017/conference/paper104/AnonReviewer4"], "reply": {"forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper104/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper104/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512695833}}}, {"tddate": null, "tmdate": 1481928449551, "tcdate": 1481928449551, "number": 1, "id": "Hytb1xzVe", "invitation": "ICLR.cc/2017/conference/-/paper104/official/review", "forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "signatures": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "content": {"title": "Good paper, but very incremental", "rating": "6: Marginally above acceptance threshold", "review": "The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. \n\nApart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.\nThe empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. \n\nThe experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. \n\nMy main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512695833, "id": "ICLR.cc/2017/conference/-/paper104/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper104/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper104/AnonReviewer1", "ICLR.cc/2017/conference/paper104/AnonReviewer3", "ICLR.cc/2017/conference/paper104/AnonReviewer4"], "reply": {"forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper104/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper104/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512695833}}}, {"tddate": null, "tmdate": 1480547011650, "tcdate": 1480547011645, "number": 4, "id": "Hk2T90nzx", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "ryUr3c_fx", "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "writers": ["~Michael_Auli1"], "content": {"title": "Re: ICLR 2017 conference paper104 AnonReviewer1", "comment": "**Why is the convnet encoder faster at decoding?**\nOur architecture works very well with smaller embedding sizes which allows us to match or outperform the accuracy of the LSTM baseline at lower capacity. In the particular experiment you are referring to, we use a smaller embedding size and encoder output dimensionality which makes computation faster despite having more layers. Also, the 2-layer bi-directional LSTM encoder baseline contains 4 LSTMs, because it is bi-directional. The accuracy of the baseline (Table 3b) has been tuned and the results are very competitive to other published work (cf. Table 2 WMT'15 English-German).\n\nThe smaller embedding size is not the only reason for the speed-up. In Table 3 (a), we compare a Conv 6/3 encoder and a BiLSTM with equal embedding sizes. The convolutional encoder is still 1.34x faster (at 0.7 higher BLEU) although it requires roughly 1.6x as many FLOPs. We believe that this is likely due to better cache locality for convolutional layers on CPUs: a LSTM with fused gates requires two big matrix multiplications with different weights as well as additions, multiplications and non-linearities  for each word in the input sentence, while each convolutional layer can be computed in a tight loop with the same weight matrix. Both architectures might benefit from a further optimized multi-core implementation, but in comparison to other NMT implementations our setup is quite fast already.\n\nOur bi-directional LSTM implementation is based on torch rnnlib which uses fused LSTM gates ( https://github.com/facebookresearch/torch-rnnlib/blob/master/rnnlib/cell.lua) and which we consider an efficient implementation. \n\n**Why are there two convolutional stacks in the encoder? **\nUsing only a single stack of convolutions was indeed much worse (22.9 BLEU vs 28.5 BLEU). The encoder output is used for two important tasks: first, it is matched to the decoder state to identify which parts of the input are relevant to generate the next token. Second, it actually needs to encode the source sentence to provide useful inputs to the decoder. Tackling both these tasks with a simple deep convolutional network is much harder than for a bi-directional LSTM which has a gating mechanism.  Our goal was to use very simple conv-tanh blocks but there may be a more complex setting that enables a single encoder stack.\n\n**Why do we claim our encoder model to be conceptually simpler? **\nWe acknowledge that this statement is subjective. We removed it and highlighted the differences between RNNs and convnets to support this statement, namely: (i) bi-directional LSTMs process past and future words in separate stacks which then need to be combined, whereas convnets process the surrounding context at every step, (ii) convnets perform the same number of non-linear operations for each word, whereas RNNs  apply a very large number of non-linearities to words far away, (iii) parallelization is easier with convnets."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}, {"tddate": null, "tmdate": 1480268862307, "tcdate": 1480268862303, "number": 1, "id": "ryUr3c_fx", "invitation": "ICLR.cc/2017/conference/-/paper104/pre-review/question", "forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "signatures": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "content": {"title": "A few questions", "question": "Hi, I have a few questions about your submission.\n\nFirst, can you please explain in more detail why your convnet model is faster for decoding? Is it because less FLOPs are required? Or is it because you were able to use less hidden units? Are both implementations efficient? It's not obvious to me why 15+5 layers of convolution are two times faster than 2 layers of LSTM when a single CPU thread is used.\n\nSecond, a very interesting finding of the paper is that two convolutional network are needed for good performance, CNN-a and CNN-c. Can you provide a qualitative explanation for this phenomenon?\n\nFinally, I think that the wording \"conceptually simpler encoder\" that you used in the conclusion is a very subjective judgement. Especially given that you have to use 2 different convnets."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959460076, "id": "ICLR.cc/2017/conference/-/paper104/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper104/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper104/AnonReviewer1"], "reply": {"forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper104/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper104/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959460076}}}, {"tddate": null, "tmdate": 1479374292155, "tcdate": 1479374246244, "number": 3, "id": "HyRsrljWx", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "signatures": ["~Ozan_Caglayan1"], "readers": ["everyone"], "writers": ["~Ozan_Caglayan1"], "content": {"title": "Small typo", "comment": "Hi,\n\nThe conditional input is denoted by both c_i and c_t in sections 2 and 3.1 respectively. c_i is reused in section 3.2\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}, {"tddate": null, "tmdate": 1479346964895, "tcdate": 1479346964891, "number": 2, "id": "HkTfjK5Zx", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "BkqbKptbl", "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "writers": ["~Michael_Auli1"], "content": {"title": "Re: multi-bleu.perl", "comment": "You are right. We fixed this in the latest version and now report detokenized BLEU for WMT'16 English-Romanian. Our conv encoder is 0.3 BLEU short of your result."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}, {"tddate": null, "tmdate": 1479297882613, "tcdate": 1479297282409, "number": 1, "id": "BkqbKptbl", "invitation": "ICLR.cc/2017/conference/-/paper104/public/comment", "forum": "BJAA4wKxg", "replyto": "BJAA4wKxg", "signatures": ["~Rico_Sennrich1"], "readers": ["everyone"], "writers": ["~Rico_Sennrich1"], "content": {"title": "multi-bleu.perl", "comment": "I want to draw attention to the fact that you compare tokenized BLEU results (with multi-bleu.perl) and detokenized BLEU results (with mteval-v13a.pl). The two should *never* be mixed in the same table, as the tokenization will have a big effect on results. Even when comparing systems that all have tokenized BLEU, and all use the Moses tokenizer, using different parameters (such as the \"-a\" option for aggressive hyphen splitting) will skew the results.\n\nDetokenized BLEU is standard for WMT, and reported by Sennrich et al. (2016a,b).\n\nI re-ran BLEU on our EN-RO system for comparison:\n\ndetokenized BLEU, mteval-v13a.pl: 28.1 BLEU\ntokenized BLEU, multi-bleu.perl: 29.4 BLEU\n\nyour reported result (multi-bleu.perl): 28.5"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "pdf": "/pdf/146750f1db6809ab8860ee03b596385437a13789.pdf", "TL;DR": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "paperhash": "gehring|a_convolutional_encoder_model_for_neural_machine_translation", "conflicts": ["fb.com"], "keywords": [], "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287726638, "id": "ICLR.cc/2017/conference/-/paper104/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJAA4wKxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper104/reviewers", "ICLR.cc/2017/conference/paper104/areachairs"], "cdate": 1485287726638}}}], "count": 19}