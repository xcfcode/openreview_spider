{"notes": [{"id": "HygbQaNYwr", "original": "rkgqw8CIDH", "number": 438, "cdate": 1569439000715, "ddate": null, "tcdate": 1569439000715, "tmdate": 1577168231394, "tddate": null, "forum": "HygbQaNYwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system", "authors": ["Shixian Wen", "Laurent Itti"], "authorids": ["shixianw@usc.edu", "itti@usc.edu"], "keywords": ["Adversarial Training", "Adversarial Examples"], "TL;DR": "Perturbation bias inside of the neural network helps us to achieve adversarial training with negligible cost; alleviate accuracy trade-off between clean and adversarial examples; and diversify adversarial perturbations.", "abstract": "Adversarial training, in which a network is trained on both adversarial and clean examples, is one of the most trusted defense methods against adversarial attacks. However, there are three major practical difficulties in implementing and deploying this method - expensive in terms of extra memory and computation costs; accuracy trade-off between clean and adversarial examples; and lack of diversity of adversarial perturbations. Classical adversarial training uses fixed, precomputed perturbations in adversarial examples (input space). In contrast, we introduce dynamic adversarial perturbations into the parameter space of the network, by adding perturbation biases to the fully connected layers of deep convolutional neural network. During training, using only clean images, the perturbation biases are updated in the Fast Gradient Sign Direction to automatically create and store adversarial perturbations by recycling the gradient information computed. The network learns and adjusts itself automatically to these learned adversarial perturbations. Thus, we can achieve adversarial training with negligible cost compared to requiring a training set of adversarial example images. In addition, if combined with classical adversarial training, our perturbation biases can alleviate accuracy trade-off difficulties, and diversify adversarial perturbations.", "pdf": "/pdf/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "paperhash": "wen|adversarial_training_embedding_adversarial_perturbations_into_the_parameter_space_of_a_neural_network_to_build_a_robust_system", "original_pdf": "/attachment/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "_bibtex": "@misc{\nwen2020adversarial,\ntitle={Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system},\nauthor={Shixian Wen and Laurent Itti},\nyear={2020},\nurl={https://openreview.net/forum?id=HygbQaNYwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "AoAAt75GL2", "original": null, "number": 1, "cdate": 1576798696424, "ddate": null, "tcdate": 1576798696424, "tmdate": 1576800939227, "tddate": null, "forum": "HygbQaNYwr", "replyto": "HygbQaNYwr", "invitation": "ICLR.cc/2020/Conference/Paper438/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to introduce perturbation biases as a counter-measure against adversarial perturbations. The perturbation biases are additional bias terms that are trained by a variant of gradient ascent. Serious issues were raised in the comments. No rebuttal was provided.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system", "authors": ["Shixian Wen", "Laurent Itti"], "authorids": ["shixianw@usc.edu", "itti@usc.edu"], "keywords": ["Adversarial Training", "Adversarial Examples"], "TL;DR": "Perturbation bias inside of the neural network helps us to achieve adversarial training with negligible cost; alleviate accuracy trade-off between clean and adversarial examples; and diversify adversarial perturbations.", "abstract": "Adversarial training, in which a network is trained on both adversarial and clean examples, is one of the most trusted defense methods against adversarial attacks. However, there are three major practical difficulties in implementing and deploying this method - expensive in terms of extra memory and computation costs; accuracy trade-off between clean and adversarial examples; and lack of diversity of adversarial perturbations. Classical adversarial training uses fixed, precomputed perturbations in adversarial examples (input space). In contrast, we introduce dynamic adversarial perturbations into the parameter space of the network, by adding perturbation biases to the fully connected layers of deep convolutional neural network. During training, using only clean images, the perturbation biases are updated in the Fast Gradient Sign Direction to automatically create and store adversarial perturbations by recycling the gradient information computed. The network learns and adjusts itself automatically to these learned adversarial perturbations. Thus, we can achieve adversarial training with negligible cost compared to requiring a training set of adversarial example images. In addition, if combined with classical adversarial training, our perturbation biases can alleviate accuracy trade-off difficulties, and diversify adversarial perturbations.", "pdf": "/pdf/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "paperhash": "wen|adversarial_training_embedding_adversarial_perturbations_into_the_parameter_space_of_a_neural_network_to_build_a_robust_system", "original_pdf": "/attachment/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "_bibtex": "@misc{\nwen2020adversarial,\ntitle={Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system},\nauthor={Shixian Wen and Laurent Itti},\nyear={2020},\nurl={https://openreview.net/forum?id=HygbQaNYwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HygbQaNYwr", "replyto": "HygbQaNYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729531, "tmdate": 1576800282137, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper438/-/Decision"}}}, {"id": "Hye_47mNYH", "original": null, "number": 1, "cdate": 1571201840349, "ddate": null, "tcdate": 1571201840349, "tmdate": 1572972595289, "tddate": null, "forum": "HygbQaNYwr", "replyto": "HygbQaNYwr", "invitation": "ICLR.cc/2020/Conference/Paper438/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nThis paper proposes to introduce adversarial perturbations into intermediate layers of a neural network, to achieve more efficient adversarial training.\nThe idea has been proposed before. The paper is based on, and perpetrates, a number of fundamental misconceptions about adversarial examples. A lack of many relevant citations indicates that the authors are not familiar with the related work done in this field over the past three years. This paper is currently clearly below the bar for ICLR.\n\nWe recommend that the authors read, understand, and cite at least the list of papers below before revising their paper:\n\n- Madry et al., \"Towards deep learning models resistant to adversarial attacks\", 2017\n- Sabour et al., \"Adversarial Manipulation of Deep Representations\", 2016\n- Tsipras et al., \"Robustness May Be at Odds with Accuracy\", 2019\n- Carlini & Wagner., \"Towards Evaluating the Robustness of Neural Networks\", 2017\n- Carlini et al., \"On evaluating adversarial robustness\", 2019\n\nIn particular, this should help clarify the following misconceptions:\n- Adversarial training does not require training on both clean and adversarial examples. The best results are often obtained by only training on adversarial examples.\n- Adversarial training does not require extra memory\n- While expensive, adversarial training does not require days of computation on hundreds of GPUs (not even on ImageNet)\n- There is no evidence that adversarial training with PGD produces \"non-diverse\" adversarial examples (for the chosen perturbation set)\n- FGSM is not a good benchmark for training or evaluation"}, "signatures": ["ICLR.cc/2020/Conference/Paper438/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper438/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system", "authors": ["Shixian Wen", "Laurent Itti"], "authorids": ["shixianw@usc.edu", "itti@usc.edu"], "keywords": ["Adversarial Training", "Adversarial Examples"], "TL;DR": "Perturbation bias inside of the neural network helps us to achieve adversarial training with negligible cost; alleviate accuracy trade-off between clean and adversarial examples; and diversify adversarial perturbations.", "abstract": "Adversarial training, in which a network is trained on both adversarial and clean examples, is one of the most trusted defense methods against adversarial attacks. However, there are three major practical difficulties in implementing and deploying this method - expensive in terms of extra memory and computation costs; accuracy trade-off between clean and adversarial examples; and lack of diversity of adversarial perturbations. Classical adversarial training uses fixed, precomputed perturbations in adversarial examples (input space). In contrast, we introduce dynamic adversarial perturbations into the parameter space of the network, by adding perturbation biases to the fully connected layers of deep convolutional neural network. During training, using only clean images, the perturbation biases are updated in the Fast Gradient Sign Direction to automatically create and store adversarial perturbations by recycling the gradient information computed. The network learns and adjusts itself automatically to these learned adversarial perturbations. Thus, we can achieve adversarial training with negligible cost compared to requiring a training set of adversarial example images. In addition, if combined with classical adversarial training, our perturbation biases can alleviate accuracy trade-off difficulties, and diversify adversarial perturbations.", "pdf": "/pdf/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "paperhash": "wen|adversarial_training_embedding_adversarial_perturbations_into_the_parameter_space_of_a_neural_network_to_build_a_robust_system", "original_pdf": "/attachment/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "_bibtex": "@misc{\nwen2020adversarial,\ntitle={Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system},\nauthor={Shixian Wen and Laurent Itti},\nyear={2020},\nurl={https://openreview.net/forum?id=HygbQaNYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygbQaNYwr", "replyto": "HygbQaNYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575439739776, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper438/Reviewers"], "noninvitees": [], "tcdate": 1570237752131, "tmdate": 1575439739787, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper438/-/Official_Review"}}}, {"id": "Hyey5xwaYB", "original": null, "number": 2, "cdate": 1571807367434, "ddate": null, "tcdate": 1571807367434, "tmdate": 1572972595256, "tddate": null, "forum": "HygbQaNYwr", "replyto": "HygbQaNYwr", "invitation": "ICLR.cc/2020/Conference/Paper438/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes perturbation biases as a counter-measure against adversarial perturbations. The perturbation biases are additional bias terms that are trained by a variant of gradient ascent. The method imposes less computational costs compared to most adversarial training algorithms. In their experimental evaluation, the algorithm achieved higher accuracy on both clean and adversarial examples.\n\nThis paper should be rejected because the proposed method is not well justified either by theory or practice. Experiments are weak and do not support the effectiveness of the proposed method.\n\nMajor comments:\nSince the evaluations of defense algorithms are often misleading [1], it requires throughout experiments or theoretical certifications to confirm the effectiveness of defense methods. However, the experiment configuration in this paper is not satisfactory to demonstrate the robustness of defended networks. The followings are a list of concerns.\n1) Experiments are limited to small datasets and networks. Since some phenomena only appear in larger datasets [2], there is a concern that the proposed method also works on other datasets.\n2) The attack algorithm used for the evaluation is weak. We can confirm this by observing the label leakage [2] in the experimental results. It is hard to judge which defenses are most effective, even within the tested datasets and models.\n3) The \"adversarial training\" baseline used in the experiment is weird. Adversarial training typically generates adversarial examples during the process of the neural networks' optimization instead of using precomputed adversarial examples. Baseline methods should be stronger, for example, adversarial training with PGD [3].\n\n[1] Athalye et al. \"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples.\" ICML 2018\n[2] Kurakin et al. \"ADVERSARIAL MACHINE LEARNING AT SCALE.\" ICLR 2017\n[3] Madry et al. \"Towards Deep Learning Models Resistant to Adversarial Attacks.\" ICLR 2018"}, "signatures": ["ICLR.cc/2020/Conference/Paper438/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper438/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system", "authors": ["Shixian Wen", "Laurent Itti"], "authorids": ["shixianw@usc.edu", "itti@usc.edu"], "keywords": ["Adversarial Training", "Adversarial Examples"], "TL;DR": "Perturbation bias inside of the neural network helps us to achieve adversarial training with negligible cost; alleviate accuracy trade-off between clean and adversarial examples; and diversify adversarial perturbations.", "abstract": "Adversarial training, in which a network is trained on both adversarial and clean examples, is one of the most trusted defense methods against adversarial attacks. However, there are three major practical difficulties in implementing and deploying this method - expensive in terms of extra memory and computation costs; accuracy trade-off between clean and adversarial examples; and lack of diversity of adversarial perturbations. Classical adversarial training uses fixed, precomputed perturbations in adversarial examples (input space). In contrast, we introduce dynamic adversarial perturbations into the parameter space of the network, by adding perturbation biases to the fully connected layers of deep convolutional neural network. During training, using only clean images, the perturbation biases are updated in the Fast Gradient Sign Direction to automatically create and store adversarial perturbations by recycling the gradient information computed. The network learns and adjusts itself automatically to these learned adversarial perturbations. Thus, we can achieve adversarial training with negligible cost compared to requiring a training set of adversarial example images. In addition, if combined with classical adversarial training, our perturbation biases can alleviate accuracy trade-off difficulties, and diversify adversarial perturbations.", "pdf": "/pdf/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "paperhash": "wen|adversarial_training_embedding_adversarial_perturbations_into_the_parameter_space_of_a_neural_network_to_build_a_robust_system", "original_pdf": "/attachment/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "_bibtex": "@misc{\nwen2020adversarial,\ntitle={Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system},\nauthor={Shixian Wen and Laurent Itti},\nyear={2020},\nurl={https://openreview.net/forum?id=HygbQaNYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygbQaNYwr", "replyto": "HygbQaNYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575439739776, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper438/Reviewers"], "noninvitees": [], "tcdate": 1570237752131, "tmdate": 1575439739787, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper438/-/Official_Review"}}}, {"id": "S1eAMK6pKH", "original": null, "number": 3, "cdate": 1571834134357, "ddate": null, "tcdate": 1571834134357, "tmdate": 1572972595214, "tddate": null, "forum": "HygbQaNYwr", "replyto": "HygbQaNYwr", "invitation": "ICLR.cc/2020/Conference/Paper438/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors try to tackle the problem of adversarial examples by introducing a special set of bias weights into the neural network. There are serious clarity issues with the writing of this paper. It cites Wen & Itti 2019 but provides few motivations and explanations of algorithmic choices. \n\nMy questions include: \n  - Is there a separate bias term for clean examples? When is the adversarial bias used? Is it only for adversarial examples? \n  - How is the adversarial bias term updated? In Algorithm 2, its gradient is a sum over samples, but over what samples? A mini-batch? \n  - What are the multi-modal weights? There is no forward equation on how they are used. We only have their updates in Algorithm 2. \n\nThese are all very confusing, given the central importance of Algorithm 2 to the paper. The authors should start with how the new bias terms affect inference and predictions, before going to their updates. \n\nThere are also limitations in the experiments. Only Fast Gradient Sign Method (FGSM) is used to generate adversarial examples. The more powerful projected gradient descent should be used for a better test against adversarial examples. Also, only MNIST and FashionMNIST are tested. The authors should consider including CIFAR10, CIFAR100, or other datasets. \n\nOverall I think the issues with clarity and experiments in the paper make it not ready for publication yet. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper438/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper438/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system", "authors": ["Shixian Wen", "Laurent Itti"], "authorids": ["shixianw@usc.edu", "itti@usc.edu"], "keywords": ["Adversarial Training", "Adversarial Examples"], "TL;DR": "Perturbation bias inside of the neural network helps us to achieve adversarial training with negligible cost; alleviate accuracy trade-off between clean and adversarial examples; and diversify adversarial perturbations.", "abstract": "Adversarial training, in which a network is trained on both adversarial and clean examples, is one of the most trusted defense methods against adversarial attacks. However, there are three major practical difficulties in implementing and deploying this method - expensive in terms of extra memory and computation costs; accuracy trade-off between clean and adversarial examples; and lack of diversity of adversarial perturbations. Classical adversarial training uses fixed, precomputed perturbations in adversarial examples (input space). In contrast, we introduce dynamic adversarial perturbations into the parameter space of the network, by adding perturbation biases to the fully connected layers of deep convolutional neural network. During training, using only clean images, the perturbation biases are updated in the Fast Gradient Sign Direction to automatically create and store adversarial perturbations by recycling the gradient information computed. The network learns and adjusts itself automatically to these learned adversarial perturbations. Thus, we can achieve adversarial training with negligible cost compared to requiring a training set of adversarial example images. In addition, if combined with classical adversarial training, our perturbation biases can alleviate accuracy trade-off difficulties, and diversify adversarial perturbations.", "pdf": "/pdf/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "paperhash": "wen|adversarial_training_embedding_adversarial_perturbations_into_the_parameter_space_of_a_neural_network_to_build_a_robust_system", "original_pdf": "/attachment/c8f02169241bb70ce899ff079ded6dacb5aa3de0.pdf", "_bibtex": "@misc{\nwen2020adversarial,\ntitle={Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system},\nauthor={Shixian Wen and Laurent Itti},\nyear={2020},\nurl={https://openreview.net/forum?id=HygbQaNYwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygbQaNYwr", "replyto": "HygbQaNYwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575439739776, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper438/Reviewers"], "noninvitees": [], "tcdate": 1570237752131, "tmdate": 1575439739787, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper438/-/Official_Review"}}}], "count": 5}