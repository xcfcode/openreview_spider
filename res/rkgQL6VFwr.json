{"notes": [{"id": "rkgQL6VFwr", "original": "S1gRzvuDDH", "number": 555, "cdate": 1569439051454, "ddate": null, "tcdate": 1569439051454, "tmdate": 1577168264619, "tddate": null, "forum": "rkgQL6VFwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning Generative Image Object Manipulations from Language Instructions", "authors": ["Martin L\u00e4ngkvist", "Andreas Persson", "Amy Loutfi"], "authorids": ["martin.langkvist@oru.se", "andreas.persson@oru.se", "amy.loutfi@oru.se"], "keywords": [], "abstract": "The use of adequate feature representations is essential for achieving high performance in high-level human cognitive tasks in computational modeling. Recent developments in deep convolutional and recurrent neural networks architectures enable learning powerful feature representations from both images and natural language text. Besides, other types of networks such as Relational Networks (RN) can learn relations between objects and Generative Adversarial Networks (GAN) have shown to generate realistic images. In this paper, we combine these four techniques to acquire a shared feature representation of the relation between objects in an input image and an object manipulation action description in the form of human language encodings to generate an image that shows the resulting end-effect the action would have on a computer-generated scene. The system is trained and evaluated on a simulated dataset and experimentally used on real-world photos.", "pdf": "/pdf/54891ff2c30230948a513990c45f44e42578656f.pdf", "code": "https://www.dropbox.com/s/fkaapcpbk06t8zi/pytorch.zip?dl=0", "paperhash": "l\u00e4ngkvist|learning_generative_image_object_manipulations_from_language_instructions", "original_pdf": "/attachment/54891ff2c30230948a513990c45f44e42578656f.pdf", "_bibtex": "@misc{\nl{\\\"a}ngkvist2020learning,\ntitle={Learning Generative Image Object Manipulations from Language Instructions},\nauthor={Martin L{\\\"a}ngkvist and Andreas Persson and Amy Loutfi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgQL6VFwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "iJdt1g4EBt", "original": null, "number": 1, "cdate": 1576798699648, "ddate": null, "tcdate": 1576798699648, "tmdate": 1576800936205, "tddate": null, "forum": "rkgQL6VFwr", "replyto": "rkgQL6VFwr", "invitation": "ICLR.cc/2020/Conference/Paper555/-/Decision", "content": {"decision": "Reject", "comment": "The submission proposes to train a model to modify objects in an image using language (the modified image is the effect of an action). The model combines CNN, RNN, Relation Nets and GAN and is trained and evaluated on synthetic data, with some examples of results on real images.\n\nThe paper received relatively low scores (1 reject and 2 weak rejects).  The authors did not provide any responses to the reviews and did not revise their submission.  Thus there was no reviewer discussion and the scores remained unchanged.\n\nThe reviewers all agreed that the submission addressed an interesting task, but there was no special insight in how the components were put together, and the work was limited in the experimental results.  Comparisons against additional baselines (AE, VAE), and ablation studies or examinations of how the components can be varied is needed.\n\nThe paper is currently too weak to be accepted at ICLR.  The authors are encouraged to improve their evaluation and resubmit to an appropriate venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Generative Image Object Manipulations from Language Instructions", "authors": ["Martin L\u00e4ngkvist", "Andreas Persson", "Amy Loutfi"], "authorids": ["martin.langkvist@oru.se", "andreas.persson@oru.se", "amy.loutfi@oru.se"], "keywords": [], "abstract": "The use of adequate feature representations is essential for achieving high performance in high-level human cognitive tasks in computational modeling. Recent developments in deep convolutional and recurrent neural networks architectures enable learning powerful feature representations from both images and natural language text. Besides, other types of networks such as Relational Networks (RN) can learn relations between objects and Generative Adversarial Networks (GAN) have shown to generate realistic images. In this paper, we combine these four techniques to acquire a shared feature representation of the relation between objects in an input image and an object manipulation action description in the form of human language encodings to generate an image that shows the resulting end-effect the action would have on a computer-generated scene. The system is trained and evaluated on a simulated dataset and experimentally used on real-world photos.", "pdf": "/pdf/54891ff2c30230948a513990c45f44e42578656f.pdf", "code": "https://www.dropbox.com/s/fkaapcpbk06t8zi/pytorch.zip?dl=0", "paperhash": "l\u00e4ngkvist|learning_generative_image_object_manipulations_from_language_instructions", "original_pdf": "/attachment/54891ff2c30230948a513990c45f44e42578656f.pdf", "_bibtex": "@misc{\nl{\\\"a}ngkvist2020learning,\ntitle={Learning Generative Image Object Manipulations from Language Instructions},\nauthor={Martin L{\\\"a}ngkvist and Andreas Persson and Amy Loutfi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgQL6VFwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkgQL6VFwr", "replyto": "rkgQL6VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729287, "tmdate": 1576800281851, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper555/-/Decision"}}}, {"id": "BygWQQcCYr", "original": null, "number": 1, "cdate": 1571885849309, "ddate": null, "tcdate": 1571885849309, "tmdate": 1572972580734, "tddate": null, "forum": "rkgQL6VFwr", "replyto": "rkgQL6VFwr", "invitation": "ICLR.cc/2020/Conference/Paper555/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a model that takes an image and a sentence as input, where the sentence is an instruction to manipulate objects in the scene, and outputs another image which shows the scene after manipulation. The model is an integration of CNN, RNN, Relation Nets, and GAN. The results are mostly on synthetic data, though the authors also included some results on real images toward the end.\n\nThis paper, despite studying an interesting problem, is limited in terms of its technical innovations and experimental results. My recommendation is a clear reject.\n\nThe model is simply an integration of multiple standard neural nets. To me, it's unclear how the system can inspire future research. Such an integration of neural nets won't generalize well. The authors have to pre-process real images in a very specific way for limited sim-to-real transfer. It's unclear how the model can work on more complex images, nor to mention scenes or sentences (or actions) beyond those available during training. \n\nThe experimental setup is very simple. The model is tested on scenes with a clean background and a few geometric primitives. There are only four actions involved. There are no comparisons with published, SOTA methods. All experiments are with the ablated model itself. Considering all this, I believe this paper cannot be accepted to a top conference such as ICLR.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper555/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper555/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Generative Image Object Manipulations from Language Instructions", "authors": ["Martin L\u00e4ngkvist", "Andreas Persson", "Amy Loutfi"], "authorids": ["martin.langkvist@oru.se", "andreas.persson@oru.se", "amy.loutfi@oru.se"], "keywords": [], "abstract": "The use of adequate feature representations is essential for achieving high performance in high-level human cognitive tasks in computational modeling. Recent developments in deep convolutional and recurrent neural networks architectures enable learning powerful feature representations from both images and natural language text. Besides, other types of networks such as Relational Networks (RN) can learn relations between objects and Generative Adversarial Networks (GAN) have shown to generate realistic images. In this paper, we combine these four techniques to acquire a shared feature representation of the relation between objects in an input image and an object manipulation action description in the form of human language encodings to generate an image that shows the resulting end-effect the action would have on a computer-generated scene. The system is trained and evaluated on a simulated dataset and experimentally used on real-world photos.", "pdf": "/pdf/54891ff2c30230948a513990c45f44e42578656f.pdf", "code": "https://www.dropbox.com/s/fkaapcpbk06t8zi/pytorch.zip?dl=0", "paperhash": "l\u00e4ngkvist|learning_generative_image_object_manipulations_from_language_instructions", "original_pdf": "/attachment/54891ff2c30230948a513990c45f44e42578656f.pdf", "_bibtex": "@misc{\nl{\\\"a}ngkvist2020learning,\ntitle={Learning Generative Image Object Manipulations from Language Instructions},\nauthor={Martin L{\\\"a}ngkvist and Andreas Persson and Amy Loutfi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgQL6VFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgQL6VFwr", "replyto": "rkgQL6VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576048422577, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper555/Reviewers"], "noninvitees": [], "tcdate": 1570237750437, "tmdate": 1576048422590, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper555/-/Official_Review"}}}, {"id": "ByxOfQOJcr", "original": null, "number": 2, "cdate": 1571943183555, "ddate": null, "tcdate": 1571943183555, "tmdate": 1572972580692, "tddate": null, "forum": "rkgQL6VFwr", "replyto": "rkgQL6VFwr", "invitation": "ICLR.cc/2020/Conference/Paper555/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an architecture for generating images with objects manipulated according to user-specified or conditional instructions. I find the domain very interesting and do believe that tasks like these are critical for learning human-like cognitive capabilities.\n\nThis paper is also very clear and easy to follow and understand what the authors have done.\n\nBut I do feel like this work could use more polishing. There are four components that are used in this work, CVAE, LSTM, RN, and GANs. It seems that those components are all taken straight out of the shelf and combined. It would be interesting to see what subtle changes were important in a combined system to further increase performance.\nFor example, why is RN only before decoding, could RN possibly help the decoder as well?\n\nWhat are some of the most frequent failure cases?\nThe qualitative results look reasonable, and I\u2019m quite surprised that only 10K images were used for training. Improvements in which areas would lead to perfect results?\n\nWould better performance be obtained if every module were to be trained separately first, rather than the proposed end-to-end approach?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper555/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper555/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Generative Image Object Manipulations from Language Instructions", "authors": ["Martin L\u00e4ngkvist", "Andreas Persson", "Amy Loutfi"], "authorids": ["martin.langkvist@oru.se", "andreas.persson@oru.se", "amy.loutfi@oru.se"], "keywords": [], "abstract": "The use of adequate feature representations is essential for achieving high performance in high-level human cognitive tasks in computational modeling. Recent developments in deep convolutional and recurrent neural networks architectures enable learning powerful feature representations from both images and natural language text. Besides, other types of networks such as Relational Networks (RN) can learn relations between objects and Generative Adversarial Networks (GAN) have shown to generate realistic images. In this paper, we combine these four techniques to acquire a shared feature representation of the relation between objects in an input image and an object manipulation action description in the form of human language encodings to generate an image that shows the resulting end-effect the action would have on a computer-generated scene. The system is trained and evaluated on a simulated dataset and experimentally used on real-world photos.", "pdf": "/pdf/54891ff2c30230948a513990c45f44e42578656f.pdf", "code": "https://www.dropbox.com/s/fkaapcpbk06t8zi/pytorch.zip?dl=0", "paperhash": "l\u00e4ngkvist|learning_generative_image_object_manipulations_from_language_instructions", "original_pdf": "/attachment/54891ff2c30230948a513990c45f44e42578656f.pdf", "_bibtex": "@misc{\nl{\\\"a}ngkvist2020learning,\ntitle={Learning Generative Image Object Manipulations from Language Instructions},\nauthor={Martin L{\\\"a}ngkvist and Andreas Persson and Amy Loutfi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgQL6VFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgQL6VFwr", "replyto": "rkgQL6VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576048422577, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper555/Reviewers"], "noninvitees": [], "tcdate": 1570237750437, "tmdate": 1576048422590, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper555/-/Official_Review"}}}, {"id": "Bkg6NF0Wqr", "original": null, "number": 3, "cdate": 1572100405023, "ddate": null, "tcdate": 1572100405023, "tmdate": 1572972580650, "tddate": null, "forum": "rkgQL6VFwr", "replyto": "rkgQL6VFwr", "invitation": "ICLR.cc/2020/Conference/Paper555/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "1. The paper aims to train a model to move objects in an image using language. For instance, an image with a red cube and blue ball needs to be turned into an image  with a red cube and red ball if asked to \"replace the red cube with a blue ball\". The task itself is interesting as it aims to modify system behavior through language. \n\nThe approach the authors  take is to encode an image with a CNN, encode the sentence with an RNN and use both representations to reconstruct  the frame (via a relational network and decoder) that solves this task. This process as described was already done in (Santoro 2017). The idea of using a CNN feature map and LSTM embedding to solve spatial reasoning tasks is not new. \n\nThe main contribution is to add a discriminative loss to turn the problem into a \"is this solution correct or not.\" This is interesting but does not perform much better than the baseline of not using the GAN loss (as suggested by the results in Table 2). This suggests that the GAN term is not adding as much value as the authors claim.\n\n2. Reject\n- Reason 1: The results in Table 2 show that the GAN does slightly better (0.0134 vs 0.0144) in RMSE against the non-GAN version. This improvement does not seem statistically significant enough to warrant the added GAN complexity.\n- Reason 2: Other baselines need to be considered, AE, VAE or other variations.\n- Reason 3: No ablations on the impact of the parameters to eq 1.\n\n3. To improve the paper I suggest adding other baselines such as VAE, AE. In addition, consider using more negative samples instead of the single negative image."}, "signatures": ["ICLR.cc/2020/Conference/Paper555/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper555/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Generative Image Object Manipulations from Language Instructions", "authors": ["Martin L\u00e4ngkvist", "Andreas Persson", "Amy Loutfi"], "authorids": ["martin.langkvist@oru.se", "andreas.persson@oru.se", "amy.loutfi@oru.se"], "keywords": [], "abstract": "The use of adequate feature representations is essential for achieving high performance in high-level human cognitive tasks in computational modeling. Recent developments in deep convolutional and recurrent neural networks architectures enable learning powerful feature representations from both images and natural language text. Besides, other types of networks such as Relational Networks (RN) can learn relations between objects and Generative Adversarial Networks (GAN) have shown to generate realistic images. In this paper, we combine these four techniques to acquire a shared feature representation of the relation between objects in an input image and an object manipulation action description in the form of human language encodings to generate an image that shows the resulting end-effect the action would have on a computer-generated scene. The system is trained and evaluated on a simulated dataset and experimentally used on real-world photos.", "pdf": "/pdf/54891ff2c30230948a513990c45f44e42578656f.pdf", "code": "https://www.dropbox.com/s/fkaapcpbk06t8zi/pytorch.zip?dl=0", "paperhash": "l\u00e4ngkvist|learning_generative_image_object_manipulations_from_language_instructions", "original_pdf": "/attachment/54891ff2c30230948a513990c45f44e42578656f.pdf", "_bibtex": "@misc{\nl{\\\"a}ngkvist2020learning,\ntitle={Learning Generative Image Object Manipulations from Language Instructions},\nauthor={Martin L{\\\"a}ngkvist and Andreas Persson and Amy Loutfi},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgQL6VFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgQL6VFwr", "replyto": "rkgQL6VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper555/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576048422577, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper555/Reviewers"], "noninvitees": [], "tcdate": 1570237750437, "tmdate": 1576048422590, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper555/-/Official_Review"}}}], "count": 5}