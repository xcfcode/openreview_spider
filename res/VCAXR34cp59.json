{"notes": [{"id": "VCAXR34cp59", "original": "XCcuEosptNn", "number": 3064, "cdate": 1601308339782, "ddate": null, "tcdate": 1601308339782, "tmdate": 1614985731180, "tddate": null, "forum": "VCAXR34cp59", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BoXGDxyOKbw", "original": null, "number": 1, "cdate": 1610040408283, "ddate": null, "tcdate": 1610040408283, "tmdate": 1610474005278, "tddate": null, "forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "invitation": "ICLR.cc/2021/Conference/Paper3064/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper evaluates the extent to which disentangled representations can be recovered from pre-trained GANs with style-based generators by finding an orthogonal basis in the space of style vectors, and then training an encoder to map images to coordinates in the resulting latent space. To construct the orthogonal basis, the authors consider 3 recently proposed methods for controllable generation, along with a newly developed generalization of one of these methods. The authors evaluate metrics for disentanglement for 4 datasets, consider an abstract visual reasoning task, and compute unfairness scores.\n\nReviewers expressed diverging opinions on this paper. R2 is in support of acceptance,  R3 finds the paper borderline but is leaning towards acceptance, whereas R4 is critical. R2 and R4 engaged in a relatively detailed discussion, but maintained their scores. \n\nHaving read the paper, the metareviewer feels this submission indeed has strengths and weaknesses. On the one hand, the main results are notable; it is worth reporting that disentangled representations can be recovered from pretrained GANs is a relatively straightforward manner. In this context, the metareviewer feels that some comments by R4 are more critical than is warranted. The authors do not necessarily have to show that GAN-based methods uniformly improve upon VAE-based methods, either in terms of disentanglement metrics or in terms of sensitivity to hyperparameters. The main claim in this submission is that GAN-based methods are mostly comparable to VAE-based methods, and this claim is both sufficiently notable and sufficiently supported by experimental results. \n\nAt the same time, this submission is not without flaws.  The writing is on the rough side, and as R4 notes the authors have removed all white space between paragraphs. The metareviewer also feels it is not satisfactory to show a box plot for GAN-based methods in Figure 2 and ask the reader to compare these plots to the violin plots for VAE-based methods in the Locatello paper. The authors need to find a way to make a more direct comparison here. R4's comments about the comparison in the abstract-reasoning setting are also well-taken \u2013\u2013 here the baseline employs standard (entangled) models, so it is unclear what conclusions we should draw from this experiment. Similarly the unfairness results once again appeal to an indirect comparison to results in the  Locatello paper on this topic.\n\nOn balance, the metareviewer is inclined to say that this submission, in its current form, falls just below the threshold for acceptance. These results are clearly of note to the community and worth reporting, but the presentation has enough flaws that another round of reviews is warranted based on a revised manuscript. The metareviewer hopes to see this paper appear a conference in the (near) future. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "tags": [], "invitation": {"reply": {"forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040408270, "tmdate": 1610474005261, "id": "ICLR.cc/2021/Conference/Paper3064/-/Decision"}}}, {"id": "PcgyaVdEX7g", "original": null, "number": 3, "cdate": 1604103178408, "ddate": null, "tcdate": 1604103178408, "tmdate": 1607371405749, "tddate": null, "forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "invitation": "ICLR.cc/2021/Conference/Paper3064/-/Official_Review", "content": {"title": "Interesting Idea ", "review": "[Summary]\nThis paper proposes a new approach to learn disentangled representations from Generative models that were trained without a loss that explicitly enforces disengagement. The motivation behind this work is to address the computational time cost regarding hyperparameter tuning that is typically required for VAEs trained with a disentangled loss. The proposed approach can be summarized in 3 steps: (1) Training a GAN (in an unsupervised way) (2) Find  \u2018k\u2019 new basis vectors in \u2018W\u2019 space (3) Train a new encoder on the synthetic dataset {(z,G(z))} with loss that encourages the encodings to correspond to the subspace spanned by the basis vectors. Furthermore, the authors propose a new approach for identifying meaningful basis vectors which the authors claim that it is the generalized version of ClosedFrom. For evaluation, the authors then perform a set of experiments involving disentanglement, abstract reasoning, fairness on a variety of datasets such as 3Dshapes, MPI, and Cars3D. \n\n[Clarity]\nOverall, I think the paper reads well. I would however encourage the authors to add a background section that maybe explains a bit more regarding disentanglement, controllable generation, and in particular the style gan generator. The related work section touches on some of these, but not enough in my opinion.  \nAs an example, I am personally not familiar with the GAN literature including the architecture of StyleGan decoder so I had to go and read [1] to see what is the difference between Z-space and W-space and what is the mapping. \n\nClarification on notation: In the related work section, the vector arithmetic is written as \u201cz\u2019 = z + a * n\u201d. In the methodology, it is written as \u201cw\u2019 = w + a * n\u201d. Is this just a change in notation, or \u2018w\u2019 here corresponds to the W-space? (If this is the case, why \u2018w\u2019 is later referred to as sample noise vectors. Also this doesn\u2019t match the methodology Section in [2]). \n\n[Main Comment]\nOverall, I find the idea behind this paper quite interesting. While it is relatively simple and straightforward to implement, the idea of learning a disentangled representation from an entangled latent  is novel and makes sense.\n\nRegarding the experiments section,  I applaud the authors for doing the fairness and abstract reasoning experiments in addition to reporting standard disentanglement metrics. I would however encourage the authors to directly compare with disentangled VAE baselines. While the caption Fig 1 claims that the results are better, the VAE results are not reported here. (I had to double check the results in [3] to make sure the results are indeed better). \n\nOne key weakness of their approach is that it relies on training the encoder based on \u2018generated\u2019 samples. This could mean that the proposed approach might not be very effective for models where the latent space might be reasonably disentangled, but the generated samples don\u2019t look good (maybe blurry), given that the encoders job will presumnaly get harder as sample quality gets worse. However, as the authors state, this approach certainly can be applied for existing GANs that are known to generate high fidelity images. \n\n\n[Questions]\n- As far as I can see, this approach is not specific to GANs and can be applied to the latent space of VAEs as well. I\u2019m wondering about the results from applying this approach to a trained VAE with a style gan decoder. Could the authors explain the focus on GANs?\n- Could the authors elaborate on the proposed approach with the impossibility result in [1]. While the results suggest that we indeed were successful in learning a disentangled representation, it is not clear to me what inductive bias (or supervision) are we imposing here that encourages disentanglement. Is it simply the empirical evidence regarding controllable generation on GANs?\n- It is unclear to me how the authors computed singular vectors at the deeper convolutional layers. Computing the Jacobian does not sound like an easy computation, and heavily depending on the base point in this case. Could the authors elaborate on this? \n\n[References]\n[1] Karras, Tero, Samuli Laine, and Timo Aila. \"A style-based generator architecture for generative adversarial networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2019.\n[3] Shen, Yujun, and Bolei Zhou. \"Closed-Form Factorization of Latent Semantics in GANs.\" arXiv preprint arXiv:2007.06600 (2020).\n[3] Locatello, Francesco, et al. \"Challenging common assumptions in the unsupervised learning of disentangled representations.\" international conference on machine learning. 2019.\n\n\n-------------\nUPDATE\n-------------\n\nI thank the authors for their response. The revised version certainly looks better. I'm still happy to recommend this paper for acceptance. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3064/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083028, "tmdate": 1606915774936, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3064/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3064/-/Official_Review"}}}, {"id": "tbHfSI_mCIB", "original": null, "number": 1, "cdate": 1603884758279, "ddate": null, "tcdate": 1603884758279, "tmdate": 1606821478317, "tddate": null, "forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "invitation": "ICLR.cc/2021/Conference/Paper3064/-/Official_Review", "content": {"title": "Potentially a useful contribution, but provided experiments are incomplete and have several other issues", "review": "### Summary\n\nThis paper proposes a non-conventional approach to learning disentangled representations through finding interpretable directions in the latent space of a pre-trained Style-GAN (Karras et al., 2018). To find these directions, several existing techniques from the recent literature on controllable image generation are considered. To apply this technique to existing (real) images, an encoder network is trained on the generated images to learn a mapping  from images to disentangled representations.\n\nAs the main contribution it is shown how this approach allows one to recover disentangled representations on several standard datasets, which perform similar to conventional techniques for learning disentangled representations based on VAEs. Nonetheless, the paper argues that this approach is preferred since it does not rely on hyper-parameter tuning using ground-truth factors, which is typically necessary for conventional approaches (Locatello et al., 2018). Finally, the paper also contributes its own approach for discovering interpretable latent directions, but which is a simple heuristic derived from the \u201cClosedForm\u201d method proposed by Shen & Zhou (2020). \n\n### Pro\u2019s / Con\u2019s / Justification\n\nThe paper is reasonably well written, although I should note that it violates the required style guide for formatting paragraphs. By my count this paper would otherwise have exceeded the page limit of 8 pages by about 22 lines and I have previously reached out to the AC about this. \n\nI am somewhat conflicted about the significance of the contribution. On the one hand, I like the approach as an alternative to VAE-based approaches and I think that it is valuable to connect the field of disentangled representation learning with the field of controllable image generation. On the other hand, the insight that there is a correspondence between interpretable latent directions and disentangled factors of variation is not new as is evident from the recent literature cited here. Indeed, I would argue that the main contribution is simply the application of these techniques to a standard benchmark for learning disentangled representations. Here I also note that the proposed approach for going from disentangled representations for generated images to disentangled representations for real image is trivial. Nonetheless I would still consider the application itself to be a significant contribution. However, the current experiments are incomplete and have several other issues, which I will comment on next. \n\nFirst, I note that while the paper claims to investigate an alternative route to learning disentangled representations based on \u201cstate-of-the-art GANs\u201d, only two variations of Style-GAN are considered. This is problematic, since it leaves it unclear to what extent the evaluated approach applies to other GANs. For example, were it found that the same techniques do not yield disentangled representations for other GANs, one would end up in the very same situation as when using conventional (VAE-based) approaches, namely by having to use ground-truth factors of variation to perform model-selection. Since this is the main argument as to why the proposed approach is preferred it is vital that this is investigated.\n\nSecond, and related to this, is the quality of the provided comparison to VAE-based approaches. The reader is required to compare the boxplot in Figure 1 to the violin plots in Figure 13 in Locatello et al. (2018). Other than this being far from ideal and only allowing for a rough comparison, it is not clear to me that these results can be compared since Locatello et al. (2018) reports an average that additionally includes six different regularization strengths for the VAE-based methods. While it can be argued that this is a limitation of the VAE-based approach (i.e. having to consider multiple regularization strengths) it is not clear to what extend the style-based generators, the choice of encoder, and the hyper-parameters of the various techniques for discovering latent directions considered (such as the number of latent directions to search for) here are already \u201coptimal\u201d for these problems (in which case they should also be varied to allow for a fair comparison). Moreover, the results for \u201cDeepSpectral\u201d are reported separately for each choice of layer, while they should be aggregated into a single score to allow for a fair comparison. I note that in that case, the results don\u2019t look all that convincing. \u201cGS\u201d works well on some datasets, but worse on others, \u201cDS\u201d has large variance on all datasets and performs average, \u201cCF\u201d has large variance and mixed results for MIG across data sets, and \u201cLD\u201d generally tends to perform bad. Can it really be argued that the GAN-based approach is more stable and does not require model-selection from these results? Certainly there is no clear best method for interpretable latent discovery. It is also not clear how the size of the interval for the regularization parameter in Locatello et al. (2018) compares to that explored for DS. To facilitate a better comparison I would argue that a comparison of various VAEs having the _optimal_ regularization strength, to the reported results when using pre-trained GANs is needed. I would also like to see the authors aggregate results across methods for interpretable latent discovery and compare that to an aggregate over possible VAEs (and regularization strengths). Based on the reported results I would expect the variance to be equally large in both cases, which would leave it unclear whether this pre-trained GAN-based approach is preferred.\n\nMy final concern regarding the experimental evaluation is concerned with the results presented in section 4.2.1, where the learned representations using the approach presented here are applied to abstract visual reasoning tasks and to a fairness task. These tasks were used to demonstrate the benefits of representations that are disentangled, but here they are used to validate the learned representation. For example, Figure 2 is compared to Figure 11 in van Steenkiste et al. (2019) and it is observed that the representations acquired from pre-trained GANs are better than VAE-based representations by comparing the spread. However, the \u201cpre-trained\u201d line in Figure 11 in van Steenkiste et al. (2019) only contains a random selection of representations learned by VAEs _without_ considering their disentanglement. Hence, it is not clear what can be concluded from this other than that more disentangled representations perform better on this task, but this was already observed in  van Steenkiste et al. (2019).\n\nThe remaining contribution that is \u201cDeepSpectral\u201d currently feels ad-hoc and more of a heuristic, even though there is a correspondence to GS in the linear case. One reason for this is that it performs about average in all cases and has relatively large variance (if averaging across the main hyper-parameter). Perhaps this part of the contribution can be strengthened by providing additional analysis, why this is a reasonable approach, i.e. by analyzing its behavior in the linear case where there is a correspondence. It could also be useful to see whether the number of layers can be chosen dynamically based on some internal statistic, but that does not require ground-truth disentanglement factors. \n  \n\n### Detailed comments\n\n* The title is too broad and not very informative. Please narrow it down to using pre-training GANs and methods for latent direction discovery\n\n* It is often claimed that by using the latent-discovery approach one gets rid of essentially all critical hyper-parameters. However, there is not enough evidence provided that the remaining hyper-parameter values chosen do not affect the quality of the discovered representations. For example, the base point in GS is always fixed. Similarly, for LD only default hyper-parameters are used and the effect of the regularization parameter is not explored.\n\n* W is sometimes used to refer to weights (eg. in the first layer of the transformation) and other times to the latent space learned by style GAN from which interpretable directions are recovered. I think in the \u201cClosedForm\u201d paper they use A in place of W for the network weights, which may be more clear. \n\n* In general, please report the full experimental details used, even though they are the \u201cdefault values\u201d. For example, currently it is unclear to me how many training steps were sed to obtain the results in Figure 2.\n\n* Please take a moment to go through the references and correctly cite papers that have been published.\n\n### Post Rebuttal\n\nI have read the other reviews and the author's response. I also had another look at the revision. While I appreciate the work that was put in the revision, my main concerns remain:\n\n* This paper proposes an alternative to VAE-based approaches to learning disentangled representations, which are known to require hyper-parameter selection based on ground-truth factors (as pointed out in Locatello et al., 2019). Hence, to improve over VAEs in this regard, it is crucial that the proposed framework does not suffer from this same issue or least consistently performs better compared to VAEs. Unfortunately this is simply not the case: across Figure 2 and Table 1 there is no variation (among CF, GS, DS, LD) that consistently performs well or consistently outperforms VAEs. Similarly in Figure 3c it can be seen how the GAN-based approach and VAE-based approach perform similarly (especially if the outliers on the top would be included in the mean). Similar fluctuations can essentially be observed across all figures and which does not take into account yet the choice of GAN or other hyper-parameters that were kept fixed (as per my initial review). Further, notice how in Table 7 (when using ProGAN as opposed to StyleGAN), the best performing variation (DS) now performs more than 50% worse according to MIG -- which is not factored in the comparison to VAEs.\n* The abstract visual reasoning experiment is flawed, since the comparison to van Steenkiste et al. (2019) figure 11 considers a random selection of pre-trained VAEs for which it is unclear whether they were disentangled or not.\n* DS is a simple heuristic that generally does not outperform other approaches. Indeed, notice how not reporting the results for DS separate for each layer (as was done previously) now results in large fluctuations. In particular, for the four datasets considered in Figure 1 DS outperforms other methods only on 3D shapes. Further, although the authors in their rebuttal argue that DS is not a simple heuristic since they \u201cbuild [DS] on the intuition that singular vectors of the Jacobian provide a set of locally ``independent directions with respect to the perceptual metric\u201d, there is no evidence provided that this intuition is correct. I had suggested ways according to which this part of the contribution could be strengthened (i.e. by analyzing it in the linear case, or attempting to automatically select a good layer) but this was not further explored.\n\nI do think that there is significant value in a systematic analysis of GAN-based approaches applied for disentanglement in this way, as it could serve as a useful benchmark for existing (VAE-based) approaches to compare against. However, the current set-up falls short at this as it is not sufficiently systematic and certain variations remain insufficiently unexplored (like other kinds of GANs). Further, this is not how this work is currently motivated in the paper or how the comparisons are performed.\n\nThese concerns are irrespective of whether the reader needs to perform a manual comparison to Locatello et. al (2019). Although I noted in my review that this is far from ideal, I can understand how this may be necessary if Locatello et al. (2019) can not provide a smaller range of hyper-parameters. In that sense, I believe that the authors did a good job incorporating VAE results at such short notice.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3064/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083028, "tmdate": 1606915774936, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3064/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3064/-/Official_Review"}}}, {"id": "054vCQdjL2x", "original": null, "number": 7, "cdate": 1606219713933, "ddate": null, "tcdate": 1606219713933, "tmdate": 1606219713933, "tddate": null, "forum": "VCAXR34cp59", "replyto": "tbHfSI_mCIB", "invitation": "ICLR.cc/2021/Conference/Paper3064/-/Official_Comment", "content": {"title": "Response to AR4", "comment": "We thank the reviewer for the review and address each of the comments below: \n\n[Alternative GAN architectures.] We chose StyleGAN 2 since it is the current state-of-the-art for an unconditional generation. For less powerful models, however, our approach also achieves decent performance. In Appendix F, we have added results for ProGAN [A] on the 3D Shapes dataset. The results are slightly inferior to StyleGAN2, and we attribute this behavior to the larger gap between real data and ProgGAN samples compared to StyleGAN2 samples. Since the encoder in our scheme is trained on the synthetic data, the quality of samples is essential; therefore, it is important to use the best available model. Overall, we argue that the choice of the GAN model for our approach does not require ground truth factors of variation, and the current SOTA GAN architecture established in the community can be used for a particular unsupervised disentanglement task.\n\n[Comparison to VAE.] We have added the direct comparison in Section 4.2 in the new version. For these plots, we used the pretrained models for Cars3D provided in https://github.com/google-research/disentanglement_lib.\nWe asked the authors (Locatello et al., ICML\u201919) for exact numbers for other datasets, but they could not provide them. Unfortunately, we are not able to re-train thousands of models during the discussion phase.\n\n[the choice of encoder, and the hyper-parameters of the various techniques for discovering latent directions considered...should also be varied to allow for a fair comparison] We aimed to perform the evaluation in the setup, which is as comparable to the setup in (Locatello et al., ICML\u201919) as possible. Locatello et al. computed the variance only with respect to regularization strengths and random seeds, all other hyperparameters being fixed. These hyperparameters include the dimensionality of the latent space, the architecture, and the details of the training protocol, such as learning rate, batch size, etc. For this reason, we also considered these hyperparameters as fixed. Meanwhile, we sweep over random seeds, which appear in GAN training, discovering latent directions, and encoder training.\n\n[the results for \u201cDeepSpectral\u201d are reported separately for each choice of layer, while they should be aggregated into a single score to allow for a fair comparison] We agree with this point and report the aggregated results for DS in the new revision.\n\n[I would also like to see the authors aggregate results across methods for interpretable latent discovery and compare that to an aggregate over possible VAEs (and regularization strengths).] We provide the results of this experiment in Figure 3. In this case, the average scores obtained by our method are generally higher. \n\n[\u0421omparison of various VAEs having the optimal regularization strength] As proposed by the reviewer, we compare our approach with VAEs, given the optimal regularization strength. Note that in this case, the comparison is not completely fair since the knowledge of ground truth generative factors is used in VAEs. The results are provided in Figure 3, demonstrating that CF and GS can achieve competitive performance even in this setup.\n\n[Abstract visual reasoning tasks.] In this experiment, we verify if the representations produced by our approach can be used as alternative unsupervised representations to solve abstract reasoning tasks. Rather than using proxies (e.g., MIG, Modularity, etc.), we show that our representations result in more stable abstract reasoning performance.\n\n[Simplicity of DS] We respectfully disagree with the reviewer's comments on the DS's simplicity. While it indeed can be reduced to CF in a basic case, this does not imply that it is a \u201csimple heuristic\u201d. Conceptually, while the authors of CF argue that empirically the first layer of generators determines the values of important factors of variations, we build our method on the intuition that singular vectors of the Jacobian provide a set of locally ``independent directions with respect to the perceptual metric, which is the desired property of disentangled representations. From a computational point of view, our method additionally takes significantly more effort to implement since the efficient realization requires, e.g., Pearlmutter\u2019s trick.\n\n[Changing the title] In the new revision, we have changed the title to be more specific.\n\n[Hyperparameters and other details] Note that all the hyperparameters and training settings for our setup were initially provided in Appendix B.\n\n\n[A] [Progressive growing of GANs for improved quality, stability, and variation](https://arxiv.org/abs/1710.10196)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3064/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VCAXR34cp59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3064/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3064/Authors|ICLR.cc/2021/Conference/Paper3064/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841503, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3064/-/Official_Comment"}}}, {"id": "7RzEWJ72K7O", "original": null, "number": 5, "cdate": 1606219079233, "ddate": null, "tcdate": 1606219079233, "tmdate": 1606219607458, "tddate": null, "forum": "VCAXR34cp59", "replyto": "PcgyaVdEX7g", "invitation": "ICLR.cc/2021/Conference/Paper3064/-/Official_Comment", "content": {"title": "Response to AR2", "comment": "We thank the reviewer for the review and constructive comments. Here we address each of the concerns:\n\n[Could the authors explain the focus on GANs?] The main reason is that GANs achieve higher generation performance compared to VAEs, especially when it comes to small visual details. These details can correspond to essential factors of variation, crucial for proper disentanglement. For instance, Figure 1h in [A] demonstrates that VAE samples often do not capture the form of an object held by the robotic arm in the MPI3D dataset, which is one of the ground truth generative factors.\n\n[Elaborate on the proposed approach with the impossibility result in Locatello et al.] A strong inductive bias comes from the StyleGAN architecture, which is known to be an excellent model of the real image manifold (over 1300 citations since CVPR\u20192019). Another source of inductive bias comes from the success of controllable generation, as mentioned by the reviewer.\n\n[Computing singular vectors at the deeper convolutional layers] To compute singular vectors of the Jacobian, we use the iterative method (in fact, the one available in scipy), which requires only two operations: efficient multiplication of the Jacobian by a vector and the same operation for the Jacobian transposed. These operations can be very efficiently implemented (without forming the full Jacobian matrix) using automatic differentiation via the so-called Pearlmutter\u2019s trick [B, C]. In practice, it takes only a few seconds to obtain the top 10 singular vectors, even for deeper layers of GANs we studied.\n\n[Direct comparison with disentangled VAE baselines] We agree entirely with this point and have added the direct comparison in Section 4 in the revised version. \nFor added plots, we used the pretrained models for Cars3D provided in https://github.com/google-research/disentanglement_lib.\nWe asked the authors (Locatello et al., ICML\u201919) for exact numbers for other datasets, but they could not provide them. Unfortunately, we cannot re-train thousands of models during the discussion phase, thus resort to a cross-paper comparison for 3DShapes.\n\n[More details on GANs and controllable generation.] Thanks. We have extended the \u201cRelated work\u201d section and include the necessary details to make the submission fully self-contained.\n\n\n\n[A] [A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation](https://www.jmlr.org/papers/v21/19-976.html)\n\n[B] http://www.bcl.hamilton.ie/~barak/papers/nc-hessian.pdf\n\n[C] https://j-towns.github.io/2017/06/12/A-new-trick.html\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3064/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VCAXR34cp59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3064/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3064/Authors|ICLR.cc/2021/Conference/Paper3064/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841503, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3064/-/Official_Comment"}}}, {"id": "pVxRdfEAFq", "original": null, "number": 6, "cdate": 1606219247225, "ddate": null, "tcdate": 1606219247225, "tmdate": 1606219247225, "tddate": null, "forum": "VCAXR34cp59", "replyto": "ut55ugwtFsA", "invitation": "ICLR.cc/2021/Conference/Paper3064/-/Official_Comment", "content": {"title": "Response to AR3", "comment": "We thank the reviewer for the feedback. In the new revision, we significantly polished the writing and believe that the current paper is easier to follow. We also added a visualization of our approach (see Figure 1)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3064/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VCAXR34cp59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3064/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3064/Authors|ICLR.cc/2021/Conference/Paper3064/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841503, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3064/-/Official_Comment"}}}, {"id": "07i14y9YeYa", "original": null, "number": 4, "cdate": 1606218817771, "ddate": null, "tcdate": 1606218817771, "tmdate": 1606218817771, "tddate": null, "forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "invitation": "ICLR.cc/2021/Conference/Paper3064/-/Official_Comment", "content": {"title": "General comment", "comment": "We thank the reviewers for their time and useful suggestions. We have uploaded a new revision of our paper with changes described in the individual answers below. For this revision, we have trained a new set of StyleGAN 2 models for 64x64 datasets; specifically, we have now trained eight models for each, for double the number of iterations compared to our previous setup. This significantly improved the stability and quality of our method. Additionally, we have found a mistake in the ordering of labels provided for the $\\texttt{Isaac3D}$ dataset. After fixing this bug, our scores significantly improved on this dataset as well. We have also polished the writing, focusing on clearer notations, a better explanation of our method, and making the paper more self-contained."}, "signatures": ["ICLR.cc/2021/Conference/Paper3064/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "VCAXR34cp59", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3064/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3064/Authors|ICLR.cc/2021/Conference/Paper3064/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841503, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3064/-/Official_Comment"}}}, {"id": "ut55ugwtFsA", "original": null, "number": 2, "cdate": 1603943808571, "ddate": null, "tcdate": 1603943808571, "tmdate": 1605024075641, "tddate": null, "forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "invitation": "ICLR.cc/2021/Conference/Paper3064/-/Official_Review", "content": {"title": "Interesting empirical observations, needs better writing ", "review": "This paper proposes a method to learn disentangled representations. The idea is to use an existing GAN generator and use an existing controllable generation algorithm (such as ClosedForm or GANspace) to find a set of \u201cimportant\u201d directions in the latent space. These subspaces spanned by these directions could already represent disentangled factors of variation.  The main contribution of the paper is to propose learning a mapping from input to this subspace (that inverts the generator). \n\nPro:\n\nThe empirical evaluation shows that the unsupervised controlled generation methods are able to find a set of directions in the latent space that represent disentangled factors of variation and can achieve on-par performance on standard disentanglement benchmark datasets. \n\nThe experiments are fairly comprehensive (with multiple datasets, and several controllable generation methods)\n\nCon:\n\nThe success of the approach depends on whether the original algorithm (such as Closedform or GANspace) is able to find disentangled subspaces. If I understand correctly the paper only proposes to invert the generation process from the disentangled subspace (which is learned by existing methods) to the image. This somewhat limits the novelty. (Though I think the empirical finding that existing controllable generation algorithms also work well for disentanglement is also interesting) \n\nThe writing seems fairly unpolished. For example, it took some effort to understand that the authors are defining a new basis with columns of A, and the vectors w_i are defined under the coordinate system of the basis. These should be formally defined and clearly stated to avoid ambiguity. As another example, it is unclear what \u201csources of randomness\u201d means. Sometimes this refers to hyper-parameters, while other times this refers to the randomly sampled variables used in the algorithm. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3064/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3064/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Disentangled Representations Extracted from Pretrained GANs", "authorids": ["~Valentin_Khrulkov1", "~Leyla_Mirvakhabova1", "~Ivan_Oseledets1", "~Artem_Babenko1"], "authors": ["Valentin Khrulkov", "Leyla Mirvakhabova", "Ivan Oseledets", "Artem Babenko"], "keywords": ["Disentanglement", "representations", "GANs"], "abstract": "Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision.\n\nThis paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art GANs trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations, while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task.\nAll our code and models are publicly available.", "one-sentence_summary": "We show that non-disentangled GANs can be used to obtain high quality disentangled representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khrulkov|on_disentangled_representations_extracted_from_pretrained_gans", "pdf": "/pdf/b7ae357f3b2f29b7acb6e54bd2888050cd1055a4.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Yk81FsGV73", "_bibtex": "@misc{\nkhrulkov2021on,\ntitle={On Disentangled Representations Extracted from Pretrained {\\{}GAN{\\}}s},\nauthor={Valentin Khrulkov and Leyla Mirvakhabova and Ivan Oseledets and Artem Babenko},\nyear={2021},\nurl={https://openreview.net/forum?id=VCAXR34cp59}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "VCAXR34cp59", "replyto": "VCAXR34cp59", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3064/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083028, "tmdate": 1606915774936, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3064/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3064/-/Official_Review"}}}], "count": 9}