{"notes": [{"id": "IUaOP8jQfHn", "original": "cN5tWY7cO0", "number": 3040, "cdate": 1601308337172, "ddate": null, "tcdate": 1601308337172, "tmdate": 1614985738192, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2Z0NUIViikS", "original": null, "number": 1, "cdate": 1610040400127, "ddate": null, "tcdate": 1610040400127, "tmdate": 1610473995954, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper received 4 reviews with mixed initial ratings: 7, 5, 4, 5. The main concerns of R1, R2 and R4, who gave unfavorable scores, included: lack of methodological novelty (analysis-only paper), absence of experiments on real data (3 synthetic-only benchmarks), missing baselines and an overall inconclusive discussion. At the same time R5 notes that the offered fair comparison between SOTA methods was indeed \"much needed\", and the paper can \"serve an important role\" in guiding future developments in the community. In response to that, the authors submitted a new revision and provided detailed answers to each of the reviews separately. R1, R2 and R4 did not participate in the discussion, and R5 stayed with the positive rating.\nAC agrees with R5 that the provided analysis is insightful, and the effort put into organizing the research community around a single set of benchmarks and metrics is indeed valuable. However, given a simplistic nature of the proposed datasets and lack of other methodological contributions, the submission is not meeting the acceptance bar for ICLR. After discussion with PCs, the final recommendation is to reject."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040400112, "tmdate": 1610473995938, "id": "ICLR.cc/2021/Conference/Paper3040/-/Decision"}}}, {"id": "FHAgH2ENHP", "original": null, "number": 7, "cdate": 1605877219995, "ddate": null, "tcdate": 1605877219995, "tmdate": 1605877219995, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "v_57zmIUwnj", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment", "content": {"title": "Clarification of contributions", "comment": "Thank you for reviewing our paper and providing critical feedback. We believe your assessment of the strengths of the paper is spot on \u2013 in particular the first point: The relatively young field of object-centric representation learning has been lacking a common framework for evaluating methods and, as a consequence, also the evaluation itself (see also AnonReviewer5). Our paper fills this gap. We believe that such an evaluation is currently one of the most critical missing pieces for the field, and at least as valuable as proposing \u201cyet another method.\u201d\n\nWe evidently did not do a good job at describing our contributions clearly. Let us clarify.\n\nContribution 1: Establish that there is a problem. The main contribution of our paper is showing that none of the methods work as promised in the original papers. We did not state this point that explicitly, perhaps in an attempt at being diplomatic. Specifically:\n\n- All models rely heavily on color and struggle with objects of similar color, which is a strange result since color is a noiseless, almost perfect cue in the sprites datasets. A simple edge detector followed by a non-zero threshold and some trivial morphological operations would already segment objects perfectly, yet current neural network models do not learn to disentangle similarly colored objects despite thousands of training samples (cp. Fig. 4e). This is clearly a suboptimal solution since not separating even similarly colored objects both incurs a higher reconstruction error as well as requires a more expressive latent space than properly disentangling them.  \n\n- Despite explicitly encoding depth, neither TBA nor SCALOR handle occlusion more gracefully than our simpler ViMON model, showing that inferring depth is still an unsolved problem even on such simple synthetic datasets.  \n\n- OP3, while being the model most robust to occlusion, suffers from unstable training and generates many false positives when there are fewer objects in the scene than the model has slots.\n\nDocumenting these findings in the publication record is important, because it affects how people entering the field will choose what problems to work on, which affects how fast the community as a whole can make progress (as opposed to only insiders who may have realized some of these points independently already).\n\nContribution 2: Establish a way to measure progress. The state of the field before our paper was that it wasn\u2019t clear how existing methods fare relative to each other in different respects (tracking, occlusion handling, depth reasoning, segmentation, \u2026). We provide this comparison along with a dataset (VMDS) with several challenge sets that establish quantitative metrics to measure progress in the future. This dataset (and, similarly, VOR) is loosely inspired by earlier work in the sense that it uses the same four basic sprites, but apart from that is entirely novel: we create videos with smoothly moving sprites, add color as an additional cue and create several challenge sets that systematically vary certain properties of the movies to allow us to disentangle sources of difficulty for the models.\n\nContribution 3: Provide suggestions for how to move forward. We realized that many of the conclusions in the Discussion section were implicit. We therefore revised it to provide more explicit suggestions as pointed out by AnonReviewer1.\n\nIn summary, we kindly but strongly disagree with your assessment of the level of contribution for ICLR. In our opinion, the paper fills one of the key missing pieces in the field of object-centric representation learning at the moment and therefore clearly constitutes an important contribution to a conference focused on representation learning such as ICLR.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IUaOP8jQfHn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3040/Authors|ICLR.cc/2021/Conference/Paper3040/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment"}}}, {"id": "U0LzAy_YY5p", "original": null, "number": 6, "cdate": 1605876984638, "ddate": null, "tcdate": 1605876984638, "tmdate": 1605876984638, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "sJ386HAzlUb", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment", "content": {"title": "Response to comments 2+3", "comment": "Thanks again for your critical feedback. Here we would like to address your second and third concern.\n\nWe realize that many of our conclusions were only implicit in the discussion, but we missed spelling them out explicitly. Based on your feedback, we updated the discussion and formulated the implications of our findings more concretely; we also list them again below:\n\n- Occlusion handling is a key component object-centric video models need to master, but not yet sufficiently do. Moving forward, the models that don\u2019t include a potent interaction module like OP3\u2019s that takes pairwise interaction between objects (including occlusion) into account, could be improved by incorporating one. Preferable, though, would be making sure that the depth reasoning of the models works as intended (see next point).  \n- Teaching models to make use of amodal masks in combination with proper depth reasoning would enable much more compact latents, which are necessary to make proper use of the intuitions behind object-centric learning, namely that decomposing a scene into objects is more efficient than representing multiple or all of them at once. This would help prevent multiple objects from getting segmented in one slot as well as facilitate reasoning about occlusion.  \n\n- To scale to more natural data and prevent the methods from missing objects that are similar to the background, the pixel-wise reconstruction might need to be replaced, for instance by using contrastive learning [1] or perceptual loss functions [2,3].   \n\n- Choosing a class of models is dependent on the dataset one wants to apply it to as well as on the computational resources at one's disposal. According to our experiments, datasets that feature a high number of objects (>10) that are well separated from each other make a method like SCALOR, which can process objects in parallel, advisable. On datasets with a lower number of objects per scene which feature heavy occlusion, methods like OP3 and ViMON will likely achieve better results, but require a high computational budget for training. Having said that, no model was able to handle all tracking scenarios in our benchmark gracefully, suggesting that in order for the field to move forward advantages of the different models need to be combined, as well as new models need to be rigorously evaluated on these challenging, systematic evaluation scenarios.  \n\nRegarding your third point that we should rather do some minor extensions: Resolving the issues we demonstrate is a logical next step. However, these are difficult problems that are not resolved by minor tweaks. Moreover, to test whether an extension actually helps at resolving X, we need datasets and evaluation protocols that test for X. Furthermore, we need to know how the existing approaches perform. The value of our paper lies in establishing such datasets, protocols and performing a comprehensive evaluation that enables the broader community to work on novel approaches and make progress.\n\n\n[1] Kipf et al. Contrastive learning of structured world models. ICLR 2020.  \n\n[2] Gatys et al. A neural algorithm of artistic style. arXiv.org, 1508.06576, 2015.  \n\n[3] Hou et al. Deep feature consistent variational autoencoder. WACV 2017.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IUaOP8jQfHn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3040/Authors|ICLR.cc/2021/Conference/Paper3040/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment"}}}, {"id": "3YZ_AkhLIw4", "original": null, "number": 5, "cdate": 1605876700193, "ddate": null, "tcdate": 1605876700193, "tmdate": 1605876700193, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "tkbFYWlIjG", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment", "content": {"title": "Response to comments", "comment": "Thank you for your constructive feedback and stressing that our work is much needed, important and competently executed. \n\nTwo quick points regarding the weaknesses you state:\n\n1. Note that we do make a technical contribution by introducing ViMON, which is a competitive model that is simpler than many of its competitors.\n\n2. As you correctly point out, there is no overall best model in our benchmark. Selecting an overall winner was not the intention of our benchmark. Given that object-centric models so far are only working on toy data and are not deployable in real-world scenarios, our paper is focused on basic research that aims at understanding fundamental problems. Therefore, we tried to give a nuanced assessment of the models under different circumstances to point out their respective strengths and weaknesses to guide future research. We revised our discussion to emphasize these implications more clearly."}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IUaOP8jQfHn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3040/Authors|ICLR.cc/2021/Conference/Paper3040/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment"}}}, {"id": "Vcc2GEN_8r", "original": null, "number": 4, "cdate": 1605183141432, "ddate": null, "tcdate": 1605183141432, "tmdate": 1605183205299, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "sJ386HAzlUb", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment", "content": {"title": "Clarification of dataset selection", "comment": "Thank you for your constructive comments. We would like to clarify and get your input on your first criticism that we use only synthetic datasets; a response to the other points will follow.  \n\n- We focus on synthetic datasets, because current object-centric models are not capable of modeling the visual complexity of real-world videos. This limitation is well-known in the literature [1] and existing papers focus on synthetic datasets (or use preprocessing like background removal to essentially turn them into sprite-like datasets [2]). \n- Synthetic stimuli enable us to precisely generate challenging scenarios in a controllable manner (occlusion, role of color, \u2026). Such analyses would not be possible with real-world datasets, even if the models were able to handle them.\n\nHaving said that, it is possible that we are unaware of a suitable dataset on which current object-centric learning models succeed. If you think that is the case, which dataset would you suggest we add to our benchmark?  \n\n[1] Greff et al. Multi-object representation learning with iterative variational inference. In Proc. of the International Conf. on Machine learning (ICML), 2019.  \n\n[2] He et al. Tracking by animation: Unsupervised learning of multi-object attentive trackers. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IUaOP8jQfHn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3040/Authors|ICLR.cc/2021/Conference/Paper3040/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment"}}}, {"id": "m7rFGLLAY8e", "original": null, "number": 3, "cdate": 1605169894122, "ddate": null, "tcdate": 1605169894122, "tmdate": 1605169894122, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "aUAlcH6LG4H", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment", "content": {"title": "Response to comments; feedback welcome.", "comment": "Thank you for reviewing our paper and your valuable comments.  \n\nCould you please clarify your sentence \u201cthe paper contains a very small amount of information to make use of the benchmark\u201d? Does this statement refer to points 1\u20133 below or what kind of additional information are you missing? Note that we will make our code for models and evaluation public, as well as a leaderboard to which future methods can be added.  \n\nBelow we respond to your three comments. In particular for the third one, we would be interested in your feedback.  \n\n1. The SpMOT dataset is taken from He et al. (2019) and the only dataset that we did not create ourselves (it is also the dataset with the lowest visual complexity). VMDS and VOR are two datasets that we generated by taking existing image datasets and animating them to create video sequences. VMDS was generated by using the shapes from the dSprites dataset (Matthey et al., 2017). dSprites is a black-and-white image dataset with one object per image. We used that to create colored videos with multiple objects per video by sampling shapes from dSprites, randomly coloring them with RGB colors and sampling a trajectory for each object from a Gaussian process. Additionally to the training, validation and test set, we create seven challenge sets that feature different challenging tracking scenarios. VOR is a video dataset that we newly generated using the graphics engine OpenGL. Again, we used the basic 3D shapes of an earlier image dataset ObjectRoom and created animated videos by moving the camera. Please see also Appendix B1-B3 for a more detailed description of the dataset generation. We are happy to provide additional clarification if needed.  \n\n2. We are sorry if our format of submission caused any confusion. We are happy to merge the main paper and the appendix into one document. \nRegarding our description of the models in the appendix, it was meant to serve two purposes: (1) We wanted the paper to be as self-contained as possible without requiring the reader to have to go back to the original model publications. (2) We unified the mathematical notation of the methods in order to facilitate comparison between them.  \n\n3. Note that the baseline you suggested (SPACE) and methods they compare to in their paper (SPAIR, IODINE, GENESIS) are models for images, not videos. As such, these models have no mechanisms built-in for tracking objects through time, which is one of the major topics of our analysis. We used MONet merely as a simple sanity check that image-based models are indeed insufficient. It serves this purpose well since it is structurally otherwise identical to ViMON, showing that indeed temporal processing is required. Therefore, we are not sure that adding additional image-based models as baselines would be very insightful, because we expect these models to be very far from being competitive. Could you please let us know whether you still think that image-based models are important baselines and, if so, what insights you expect to gain from such baselines?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "IUaOP8jQfHn", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3040/Authors|ICLR.cc/2021/Conference/Paper3040/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Comment"}}}, {"id": "aUAlcH6LG4H", "original": null, "number": 1, "cdate": 1603897344589, "ddate": null, "tcdate": 1603897344589, "tmdate": 1605024080482, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Review", "content": {"title": "The paper provides a benchmark for unsupervised object representations with 5 state-of-the-art methods and 3 simulation datasets. ", "review": "Overall, this paper is interesting in setting up a benchmark for unsupervised object representations which is a very important problem in computer vision, reinforcement learning, etc. But the paper contains a very small amount of information to make use of the benchmark. Following are my comments:\n\n1. The paper needs to clearly set the contributions. The video datasets are from self-made simulations or taken from other sources because the appendix cites many references for each part of the dataset. \n\n2. The appendices are linked in the paper but given in the supplementary. It is good to combine both in one paper.  The appendix doesn't require information of existing methods and the current version contains of appendix contain mostly these explanations. \n\n3. Apart from these, the paper is well written and useful in the community. But the empirical evaluation is not convincing. There could be many baseline approaches for this, for example, paper [1] and the methods it is comparing with. This is my major concern on the paper and if stated well, the recommendation can change. \n\n[1] SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition, ICLR 2020\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083520, "tmdate": 1606915772645, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3040/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Review"}}}, {"id": "v_57zmIUwnj", "original": null, "number": 2, "cdate": 1604166036536, "ddate": null, "tcdate": 1604166036536, "tmdate": 1605024080421, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Review", "content": {"title": "A nice benchmark, but not enough", "review": "The paper proposes a benchmark for the evaluation of unsupervised learning of object-centric representation. The benchmark consists of three datasets, multi-object tracking metrics and of the evaluation of four methods. The proposed dataset consists of three sets of video sequences, procedurally generated, which are either generated from slight variations of existing works (Sprites-MOT) or on the basis of existing datasets (dSpirites, Video Object Room). For evaluation, authors propose to use a slight variation of the protocol of the MOT challenge for evaluation (with the addition of a Mostly Detected measure which does not penalize ID switches). As part of the  paper, they also evaluate and discuss the performances of four object-centric representation models, one of them (Video MONet) being an extension of an existing approach, proposed as part of this paper, and the remaining being state of the art approaches for the task.\n\n**Paper Strengths**\n- Although I am not familiar with unsupervised learning of object-centric representation, I like the idea of proposing a common protocol for evaluation. Potentially, this can be useful for the community and can help to create a common ground for evaluation.\n- The benchmark is comprehensive, in that it contains both data and evaluation measures. The adoption of MOT metrics also appears to be a reasonable choice.\n- The comparison between different models is interesting, and authors have also added a custom designed novel method (Video MONet). They investigate a set of challenging scenarios and carry out out-of-distribution tests.\n- The paper is a pleasure to read, and authors have also made a good effort in writing a clear and comprehensive supplemental.\n\n**Paper Weaknesses**\nMy main concern about the paper is the lack of novelty. While I realize that the objective of the paper is to create a common evaluation background, I fail to see a sufficient level of quality and of novelty. In particular:\n- the dataset associated with the benchmark are mostly based on existing works - they might be appropriate for evaluating this task, but the level of contribution is a bit limited;\n- on the metrics, the only contribution is to suggest using MOT metrics, which are again pre-existing;\n- the experimental and the insights it gives, again, can foster the community towards better model, but it's not a sufficient contribution for ICLR in my view.\n\nOverall, I think the paper could be a nice contribution to the literature on unsupervised learning of object-centric representation, but it lacks sufficient contribution for ICLR, in my view. I would therefore suggest to reject the paper. ", "rating": "4: Ok but not good enough - rejection", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083520, "tmdate": 1606915772645, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3040/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Review"}}}, {"id": "sJ386HAzlUb", "original": null, "number": 3, "cdate": 1604392769926, "ddate": null, "tcdate": 1604392769926, "tmdate": 1605024080356, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Review", "content": {"title": "The paper provides a framework to compare five models on four synthetic datasets for object detection/segmentation and tracking .", "review": "The paper is actually very well written and tries to answer the question of how various unsupervised learning of object-centric representations to on controlled tasks (synthetic datasets). The positives:\n1. Designs a benchmark of three datasets of varying complexity\n2. Compare a single image model and four video models (total five) \n3. Defined clear metrics that are around precision, detection, segmentation, tracking \n\nThe cons to the paper:\n1. All of the datasets are synthetic and it would have been good to at least pick a real world dataset and confirm the conclusions stand\n2. Since, the core goal of the paper is not novelty but a better understanding of various models I would have liked for the discussion to have some clear conclusions and better structure (use X in scenario Y, Model Z needs to be extended for scenarios Y etc.). I feel this section was short and mostly verbose without a clear conclusion\n3. Instead of focusing on such a comprehensive set of things - 3 datasets, five models and multiple metrics it would have been better if authors did some minor extensions of the models and showcase novel directions. But, instead the paper is mostly an understand only work and i worry that the conclusions don't necessarily give clear future directions for other researchers to build on.\n\nGiven the cons and specifically on not a clear actionable suggestion on how to improve models and no analysis beyond synthetic datasets I am leaning towards a rating of below acceptance threshold.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083520, "tmdate": 1606915772645, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3040/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Review"}}}, {"id": "tkbFYWlIjG", "original": null, "number": 4, "cdate": 1604691845532, "ddate": null, "tcdate": 1604691845532, "tmdate": 1605024080297, "tddate": null, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "invitation": "ICLR.cc/2021/Conference/Paper3040/-/Official_Review", "content": {"title": "A timely and well-executed evaluation of existing methods", "review": "The paper presents an empirical evaluation of a number of recent models for unsupervised object-based video modelling. Five different models are evaluated on three (partially novel) benchmarks, providing a unifying perspective on the relative performance of these models. Several common issues are identified and highlighted using challenge datasets: The reliance on color as a cue for object segmentation, occlusion, object size, and change in object appearance. The paper concludes with several ideas for alleviating these issues.\n\nStrengths:\n 1. The paper represents a much needed comparison of several related models which have previously not been evaluated on common benchmarks. Given the rapidly increasing number of competing models in this space, I believe analysis papers like this one serve an important role.\n 2. The paper highlights important weaknesses of unsupervised object models, such as the overreliance on color or the difficulties with handling occlusion. While I believe that some of these weaknesses were already known to the people working with these methods, they have not always been formally documented in the respective publications. This paper rectifies this, and also provides guidance as to the relative vulnerability of the different methods.\n 3. The methodology is convincing and thorough. Architecture and hyperparameter choices are clearly documented in the appendix, and the datasets have been published. \n\nWeaknesses:\n1. As an analysis paper, this publication does not provide specific new technical contributions to the issues it is evaluating. \n2. The results are not entirely conclusive, in that there is no clear best model, and their relative quality varies with datasets and metrics. That said, some things can be very clearly observed, for instance the importance of object size for the performance of TBA.\n\nOverall, the paper serves an important role in consolidating the ecosystem of unsupervised object representations. Given the increasing need for such analysis papers, and the competent execution, I recommend acceptance.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3040/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3040/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Benchmarking Unsupervised Object Representations for Video Sequences", "authorids": ["~Marissa_A._Weis1", "~Kashyap_Chitta1", "~Yash_Sharma1", "~Wieland_Brendel1", "~Matthias_Bethge2", "~Andreas_Geiger3", "~Alexander_S_Ecker1"], "authors": ["Marissa A. Weis", "Kashyap Chitta", "Yash Sharma", "Wieland Brendel", "Matthias Bethge", "Andreas Geiger", "Alexander S Ecker"], "keywords": ["Unsupervised learning", "object-centric representations", "benchmark", "tracking"], "abstract": "Perceiving the world in terms of objects and tracking them through time is a crucial prerequisite for reasoning and scene understanding. Recently, several methods have been proposed for unsupervised learning of object-centric representations. However, since these models have been evaluated with respect to different downstream tasks, it remains unclear how they compare in terms of basic perceptual abilities such as detection, figure-ground segmentation and tracking of individual objects. To close this gap, we design a benchmark with three datasets of varying complexity and seven additional test sets which feature challenging tracking scenarios relevant for natural videos. Using this benchmark, we compare the perceptual abilities of four unsupervised object-centric learning approaches: ViMON, a video-extension of MONet, based on a recurrent spatial attention mechanism, OP3, which exploits clustering via spatial mixture models, as well as TBA and SCALOR, which use an explicit factorization via spatial transformers. Our results suggest that architectures with unconstrained latent representations and full-image object masks such as ViMON and OP3 are able to learn more powerful representations in terms of object detection, segmentation and tracking than the explicitly parameterized spatial transformer based architecture of TBA and SCALOR. We also observe that none of the methods are able to gracefully handle the most challenging tracking scenarios despite their synthetic nature, suggesting that our benchmark may provide fruitful guidance towards learning more robust object-centric video representations.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "weis|benchmarking_unsupervised_object_representations_for_video_sequences", "one-sentence_summary": "We quantitatively analyze the performance of four object-centric representation learning models for video sequences on challenging tracking scenarios. ", "supplementary_material": "/attachment/b559cc50b98fec0191df5a91dea9ef17114b1a01.zip", "pdf": "/pdf/6f9d7fac9271c0a287b97957f3b552388cc5e98a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=JRp7h9KQY6", "_bibtex": "@misc{\nweis2021benchmarking,\ntitle={Benchmarking Unsupervised Object Representations for Video Sequences},\nauthor={Marissa A. Weis and Kashyap Chitta and Yash Sharma and Wieland Brendel and Matthias Bethge and Andreas Geiger and Alexander S Ecker},\nyear={2021},\nurl={https://openreview.net/forum?id=IUaOP8jQfHn}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "IUaOP8jQfHn", "replyto": "IUaOP8jQfHn", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3040/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538083520, "tmdate": 1606915772645, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3040/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3040/-/Official_Review"}}}], "count": 11}