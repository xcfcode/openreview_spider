{"notes": [{"id": "rJe2syrtvS", "original": "SyeOUyJFDB", "number": 1930, "cdate": 1569439652283, "ddate": null, "tcdate": 1569439652283, "tmdate": 1587935127482, "tddate": null, "forum": "rJe2syrtvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Xsw8PQb0R", "original": null, "number": 1, "cdate": 1576798736158, "ddate": null, "tcdate": 1576798736158, "tmdate": 1576800900213, "tddate": null, "forum": "rJe2syrtvS", "replyto": "rJe2syrtvS", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "This is a very interesting paper which discusses practical issues and solutions around deploying RL on real physical robotic systems, specifically involving questions on the use of raw sensory data, crafting reward functions, and not having resets at the end of episodes.\n\nMany of the issues raised in the reviews and discussion were concerned with experimental details and settings, as well as relation to different areas of related work. These were all sufficiently handled in the rebuttal, and all reviewers were in favour of acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJe2syrtvS", "replyto": "rJe2syrtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710354, "tmdate": 1576800259343, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Decision"}}}, {"id": "r1l7Uvpn_r", "original": null, "number": 1, "cdate": 1570719563072, "ddate": null, "tcdate": 1570719563072, "tmdate": 1574167961868, "tddate": null, "forum": "rJe2syrtvS", "replyto": "rJe2syrtvS", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The paper takes seriously the question of having a robotic system learning continuously without manual reset nor state or reward engineering. The authors propose a first approach using vison-based SAC, shown visual goals and VICE, and show that it does not provide a satisfactory solution. Then they add a random pertubation controller which brings the robot or simulated system away from the goal and a VAE to encode a compressed state, and show that it works better.\n\nThe paper is a nice read, it contains useful messages thus I'm slightly in favor of accepting it, but I may easily change my mind as it suffers from serious weaknesses.\n\nFirst, and most importantly, the experimental study is very short, the authors have chosen to spend much more space on careful writing of the problem they are investigating.\n\nTo mention a few experimental weaknesses, in Section 6.2 the authors could have performed much more detailed ablation studies and stress in more details the impact of using the VAE alone versus using the random pertubation controller alone, they could say more about the goals they show to the system, etc. There is some information in Figure 7, but this information is not exploited in a detailed way. Furthermore, Figure 7 is far to small, it is hard to say from the legend which system is which.\n\nAbout Fig.8, we just have a qualitative description, the authors claim that without instrumenting they cannot provide a quantitative study, which I don't find convincing: you may instrument for the sake of science (to measure the value of what you are doing, even if the real-world system won't use this instrumentation).\n\nSo the authors have chosen to spend more space on the positionning than on the empirical study, which may speak in favor of sending this paper to a journal or magazine rather than a technical conference. But there is an issue about the positionning too: the authors fail to mention a huge body of literature trying to address very close or just similar questions. Namely, their concern is one the central leitmotives of Developmental Robotics and some of its \"subfields\", such as Lifelong learning, Open-ended learning, Continual learning etc.  The merit of the paper in this respect is to focus on a specific question and provide concrete results on this question, but this work should be positionned with respect to the broader approaches mentioned above. The authors will easily find plenty of references in these domains, I don't want to give my favorite selection here.\n\nKnowing more about the literature mentioned above, the authors could reconsider their framework from a multitask learning perspective: instead of a random perturbation controller, the agent may learn various controllers to bring the system into various goal states (using e.g. goal-conditioned policies), and switching from goal to goal to prevent the system fro keeping stuck close to some goal.\n\nMore local points:\n\nIn the middle of page 5, it is said that the system does not learn properly just because it is stuck at the goal. This information comes late, and makes the global message weaker.\n\nin Fig. 4, I would like to know what is the threshold for success.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1930/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1930/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJe2syrtvS", "replyto": "rJe2syrtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575373315603, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1930/Reviewers"], "noninvitees": [], "tcdate": 1570237730230, "tmdate": 1575373315616, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Official_Review"}}}, {"id": "r1xnxPghjS", "original": null, "number": 5, "cdate": 1573811955934, "ddate": null, "tcdate": 1573811955934, "tmdate": 1573811955934, "tddate": null, "forum": "rJe2syrtvS", "replyto": "HylKcAZijH", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment", "content": {"title": "Updates", "comment": "\u201cI would recommend for more improvements for publication (or a future submission) you increase the number of trials, and/or use the bootstrap method Henderson employs to make better confidence intervals.\u201d\n-> We have attempted to run more random seeds since your comment to address these concerns. Due to limited time before the end of the rebuttal period, we have been able to complete 5 additional seeds on the bead manipulation task, but additional seeds for the other two tasks have not finished. We have updated Fig 7 accordingly. We will add in the remaining seeds once they have completed running. To make it more clear that our method provides statistically significant results, we have also updated Fig 7 to show 95% bootstrap confidence intervals. These plots make it clear that the previous insights carry over to the case with confidence intervals and additional seeds as well. \n\n\u201crun VICE for as many hours as you do for your method\u201d\n-> We ran a longer run of VICE on hardware and updated the paper accordingly in Figs 8 and 14. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1930/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe2syrtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1930/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1930/Authors|ICLR.cc/2020/Conference/Paper1930/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148812, "tmdate": 1576860540468, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment"}}}, {"id": "HylKcAZijH", "original": null, "number": 4, "cdate": 1573752464811, "ddate": null, "tcdate": 1573752464811, "tmdate": 1573752464811, "tddate": null, "forum": "rJe2syrtvS", "replyto": "SJgaARl5oH", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment", "content": {"title": "Thanks", "comment": "Thank you for the response,\n\nI didn't mean to suggest you were claiming VAE is the best for this application. It was more a question of what else you have tried and motivations from using VAE. Again, I think it is a fine choice but I appreciate the added discussion to highlight this is just an algorithmic choice.\n\nI'm happy with the added experiments in the appendix, and think this makes the work more concrete. I'm a bit worried about the lack of trials in the simulated domains (5 runs is not enough see \"Deep RL that Matters from Henderson https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16669). I would recommend for more improvements for publication (or a future submission) you increase the number of trials, and/or use the bootstrap method Henderson employs to make better confidence intervals.\n\nFor Figure 8 (for valve rotation), I would make sure to run VICE for as many hours as you do for your method, or mention why they are different.\n\nAgain, thank you for the response."}, "signatures": ["ICLR.cc/2020/Conference/Paper1930/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1930/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe2syrtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1930/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1930/Authors|ICLR.cc/2020/Conference/Paper1930/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148812, "tmdate": 1576860540468, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment"}}}, {"id": "Skgx4yZqoH", "original": null, "number": 3, "cdate": 1573682983955, "ddate": null, "tcdate": 1573682983955, "tmdate": 1573682983955, "tddate": null, "forum": "rJe2syrtvS", "replyto": "r1l7Uvpn_r", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for their detailed, insightful and constructive feedback! We acknowledge a number of clarity issues in the presentation and positioning of our results, which make the actual results somewhat hard to understand. We have updated the paper to make several of the points much more clear and have run additional hardware experiments to address the points raised in the review, as described in detail below: \n\n\u201cthey could say more about the goals they show to the system, etc.\u201d\n-> We have added some visualizations about goals provided to the system to Appendix C. \n\n\u201cyou may instrument for the sake of science (to measure the value of what you are doing, even if the real-world system won't use this instrumentation).\u201d \n-> Yes, this is a good point! We have now performed these experiments on the hardware and have included additional comparisons to baselines on hardware in Section 6.3, Fig 8. We find that the same trends observed in simulation hold on the hardware as well. \n\n\u201cIn Fig. 4, I would like to know what is the threshold for success.\u201d\n-> Fig. 4 analyzes the sample complexity for the task of valve rotation. The experiment is considered successful when the learned policy achieves average training performance of less than 0.15 in pose distance (defined in Appendix C.1.3) across 3 seeds. Fig. 4 has now been updated to clarify this.\n\n\u201cIn Section 6.2 the authors could have performed much more detailed ablation studies and stress in more details the impact of using the VAE alone versus using the random perturbation controller alone\u201d\n-> We have modified the Figure 7 legend and caption to make it more legible, and a discussion on the effects of the individual components based on these ablation experiments is now included in Section 6.2 in the updated manuscript. We have also updated the results after removing a small visual artifact in the environment, which allows the baselines to perform a bit better, but still maintains the same trends. We agree that the presentation of data in Figure 7 was hard to parse, and many of the comparisons (including the two requested by the reviewer) that we did actually already perform were hard to discern from the figure. The methods marked [VAE + VICE] and [RND + VICE] show the performance curves corresponding to the ablations suggested. A discussion on comparisons to explicit goal-based reset mechanisms and goal-conditioned policies has also been added to Section 6.2.\n\n\u201cthere is an issue about the positioning too: the authors fail to mention a huge body of literature trying to address very close or just similar questions...central motives of Developmental Robotics and some of its \"subfields\" \n-> We have expanded our related work with appropriate discussion with respect to the field of developmental robotics. The goal of our work is to enable reinforcement learning systems to handle the practicalities of learning in the real world without human instrumentation or interruption, even for a single task setting, without multi-task considerations. The insights we make should also be applicable for developmental robotics algorithms! Though our investigation doesn\u2019t touch on all aspects of developmental robotics such as lifelong learning, open-ended learning, psychology, cognition etc., our proposed work R3L does bear strong relationship with respect to continual learning, intrinsic motivation, perceptual development, and sensory-motor development involving proprioceptive manipulation. We thank the reviewer for bringing out this interesting connection, and have added appropriate citations in the text. \n\n\u201cauthors could reconsider their framework from a multitask learning perspective...agent may learn various controllers to bring the system into various goal states and switching from goal to goal to prevent the system for keeping stuck close to some goal.\u201d\n-> We agree that this is indeed an interesting and valuable perspective on this problem, and we have added some discussion of this to Section 6.2. We found in our experimental study that when we consider the case of using 2 goals, and switching between them (the Eysenbach et al comparison in Fig 7), it was not as effective and robust as using the perturbation controller. While this scheme chooses between only 2 goal options, and a more involved scheme could be chosen to pick multiple different goals, the performance of such an algorithm is dependent on the specific choice of goals. We find that the simpler solution via the perturbation controller can be very effective without the need for multiple meaningful alternative goals to be specified, although a better algorithm for self-supervised multi-goal selection is an interesting avenue for future work. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1930/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe2syrtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1930/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1930/Authors|ICLR.cc/2020/Conference/Paper1930/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148812, "tmdate": 1576860540468, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment"}}}, {"id": "S1x6e1-5sB", "original": null, "number": 2, "cdate": 1573682933119, "ddate": null, "tcdate": 1573682933119, "tmdate": 1573682933119, "tddate": null, "forum": "rJe2syrtvS", "replyto": "Hyl29EPXtr", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for their encouraging feedback! We are excited about further exploring the possibilities of this line of research! "}, "signatures": ["ICLR.cc/2020/Conference/Paper1930/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe2syrtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1930/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1930/Authors|ICLR.cc/2020/Conference/Paper1930/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148812, "tmdate": 1576860540468, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment"}}}, {"id": "SJgaARl5oH", "original": null, "number": 1, "cdate": 1573682900632, "ddate": null, "tcdate": 1573682900632, "tmdate": 1573682900632, "tddate": null, "forum": "rJe2syrtvS", "replyto": "r1gseP0nKH", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for their insightful and constructive feedback! We have run additional hardware comparisons and quantitative evaluations as requested (Section 6.3) and have updated the paper according to your suggestions and comments to better discuss related work. We respond to individual concerns in detail below:\n\n\u201cdiscussion of the real world tasks from the appendix to appear in the main text.\u201d\n-> We have moved this discussion from the Appendix to Section 6.3. Additional comparisons to a VICE (Fu et al) baseline have been added for real world experiments in Section 6.3, Fig 8. We see that our algorithm is able to outperform this baseline on the real world tasks. \n\n\u201cIt is not clear if a VAE is the best choice for unsupervised representation learning for RL agents.\u201c\n-> While a VAE works well in the domains we considered in this paper, we certainly agree that a VAE is not necessarily the optimal choice for all RL domains. We have updated Section 4.2 to reflect this explicitly, and have included references to Anand et al, Hjelm et al. and Lee et al as you pointed out as alternative methods for representation learning. We did not mean to claim that VAE\u2019s were the only representation learning scheme that might suffice in this scenario, and many of the schemes suggested might also be effective. \n\n\u201cI would like some more details on your simulation experiments\u2026\u201d\n-> We have updated Section 6 and Appendix C to include details about the experimental setup, both in simulation and in the real world. We have updated Fig 7 after removing a small visual artifact in the environment, which allows the baselines to perform a bit better, but still maintains the same trends. \n-- The plots are averaged over 5 random seeds for each method and task \n-- The (shaded) error regions correspond to the variance of the seeds for each curve\n-- Appendix B has been updated to include information on ranges of hyperparameters tuned, in addition to the optimal values used to generate the plots in figures 7 & 8\n\n\u201cQ1: Did you try any of the other approaches on the real robotics system? Or was there no way to deploy these algorithms to your specific setup without instrumentation?\u201d\n-> Yes we did add a new real-world comparison to the VICE baseline, as requested, in Fig 8. We have updated Section 6.3 with a comparison in the real world on the valve rotation and bead manipulation tasks. A quantitative evaluation corroborates findings from the simulated environments and shows that our method outperforms these methods in terms of sample efficiency and robustness. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1930/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe2syrtvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1930/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1930/Authors|ICLR.cc/2020/Conference/Paper1930/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148812, "tmdate": 1576860540468, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1930/Authors", "ICLR.cc/2020/Conference/Paper1930/Reviewers", "ICLR.cc/2020/Conference/Paper1930/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Official_Comment"}}}, {"id": "Hyl29EPXtr", "original": null, "number": 2, "cdate": 1571153043610, "ddate": null, "tcdate": 1571153043610, "tmdate": 1572972405253, "tddate": null, "forum": "rJe2syrtvS", "replyto": "rJe2syrtvS", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper presents approaches to handle three aspects of real-world RL on robotics: (1)learning from raw sensory inputs (2) minimal reward design effort (3) no manual resetting. Key components:(1) learn a perturbation policy that allows the main policy to explore a wide variety of state. (2) learn a variational autoencoder to transform images to low dimensional space.\n\nExperiments in simulation on the physical robots are performed to demonstrate the effectiveness of these components. Close related work is also used for comparison. The only concern I have is that the tasks considered involve robots that can automatically reset themselves pretty easily. I doubt that this will scale to unstable robots such as biped/quadruped, where once they fail, the recovering/resetting tasks will be as much or more difficult than the main locomotion tasks. But I understand this is too much to address in one paper and limitation is also briefly discussed in the final section.\n\nOverall I think this is a good paper and valuable to the community.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1930/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1930/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJe2syrtvS", "replyto": "rJe2syrtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575373315603, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1930/Reviewers"], "noninvitees": [], "tcdate": 1570237730230, "tmdate": 1575373315616, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Official_Review"}}}, {"id": "r1gseP0nKH", "original": null, "number": 3, "cdate": 1571772146932, "ddate": null, "tcdate": 1571772146932, "tmdate": 1572972405217, "tddate": null, "forum": "rJe2syrtvS", "replyto": "rJe2syrtvS", "invitation": "ICLR.cc/2020/Conference/Paper1930/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "  *Synopsis*:\n  This paper focuses on current limitations of deploying RL approaches onto real world robotic systems. They focus on three main points: the need to use raw sensory data collected by the robot, the difficulty of handcrafted reward functions without external feedback, the lack of algorithms which are robust outside of episodic learning. They propose a complete system which addresses these concerns, combining approaches from the literature and novel improvements. They then provide an empirical evaluation and ablation testing of their approach and other popular systems, and show a demonstration on a real robotic system.\n \n  Main Contributions:\n  - A discussion of the current limitations of RL on real robotic systems\n  - A framework for doing real world robotic RL without extra instrumentation (outside of the robot).\n\n  *Review*: \n  Overall, I think the paper is well written and provides some nice analysis of the current state of RL and robotics. I am not as familiar with the RL for robotics literature, but from some minor snooping around I believe these ideas to be novel and useful for the community. I have a few suggestions for the authors, and a few critical pieces I would like added to the main text.\n\n  Critical additions:\n  1. I would like some more details on your simulation experiments. Specifically:\n    - How many runs were your experiments? \n    - What are the error bars on your plots?\n    - What ranges of hyper-parameters did you test for tuning?\n\n  2. I would quite like the discussion of the real world tasks from the appendix to appear in the main text. Specifically, giving the evaluation metrics you mentioned in the appendix. \n\n  Suggestions/Questions:\n\n  S1: It is not clear if a VAE is the best choice for unsupervised representation learning for RL agents. Although a reasonable choice, Yashua Bengio recently released a look at several unsupervised techniques for representation learning in Atari which you may want to look at: https://arxiv.org/pdf/1906.08226.pdf. \n\n  Q1: Did you try any of the other approaches on the real robotics system? Or was there no way to deploy these algorithms to your specific setup without instrumentation?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1930/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1930/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["henryzhu@berkeley.edu", "justinvyu@berkeley.edu", "abhigupta@berkeley.edu", "shah@eecs.berkeley.edu", "kristian.hartikainen@gmail.com", "avisingh@cs.berkeley.edu", "vikashplus@gmail.com", "svlevine@eecs.berkeley.edu"], "title": "The Ingredients of Real World Robotic Reinforcement Learning", "authors": ["Henry Zhu", "Justin Yu", "Abhishek Gupta", "Dhruv Shah", "Kristian Hartikainen", "Avi Singh", "Vikash Kumar", "Sergey Levine"], "pdf": "/pdf/c9e6956612c7e18c323ae939026d8457431406cb.pdf", "TL;DR": "System to learn robotic tasks in the real world with reinforcement learning without instrumentation", "abstract": "The success of reinforcement learning in the real world has been limited to instrumented laboratory scenarios, often requiring arduous human supervision to enable continuous learning. In this work, we discuss the required elements of a robotic system that can continually and autonomously improve with data collected in the real world, and propose a particular instantiation of such a system. Subsequently, we investigate a number of challenges of learning without instrumentation -- including the lack of episodic resets, state estimation, and hand-engineered rewards -- and propose simple, scalable solutions to these challenges. We demonstrate the efficacy of our proposed system on dexterous robotic manipulation tasks in simulation and the real world, and also provide an insightful analysis and ablation study of the challenges associated with this learning paradigm.", "keywords": ["Reinforcement Learning", "Robotics"], "paperhash": "zhu|the_ingredients_of_real_world_robotic_reinforcement_learning", "_bibtex": "@inproceedings{\nZhu2020The,\ntitle={The Ingredients of Real World Robotic Reinforcement Learning},\nauthor={Henry Zhu and Justin Yu and Abhishek Gupta and Dhruv Shah and Kristian Hartikainen and Avi Singh and Vikash Kumar and Sergey Levine},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe2syrtvS}\n}", "original_pdf": "/attachment/53544a88edb00982d547268ae7b77fab3a34c592.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJe2syrtvS", "replyto": "rJe2syrtvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1930/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575373315603, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1930/Reviewers"], "noninvitees": [], "tcdate": 1570237730230, "tmdate": 1575373315616, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1930/-/Official_Review"}}}], "count": 10}