{"notes": [{"id": "ggNgn8Fhr5Q", "original": "HjQEE1bsLR", "number": 459, "cdate": 1601308058583, "ddate": null, "tcdate": 1601308058583, "tmdate": 1614985623447, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "xQr0tIp3cpX", "original": null, "number": 1, "cdate": 1610040536358, "ddate": null, "tcdate": 1610040536358, "tmdate": 1610474146385, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper analyses the behaviour of Neural Processes in the frequency domain and, in particular, how it suppresses high-frequency components of the input functions. While this is entirely intuitive, the paper adds some theoretical analysis via the Nyquist-Shannon theorem. But the analysis remains too generic and it is not clear it will be of broad interest to the community. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040536345, "tmdate": 1610474146370, "id": "ICLR.cc/2021/Conference/Paper459/-/Decision"}}}, {"id": "ZvEa9hhrWuy", "original": null, "number": 2, "cdate": 1603875629587, "ddate": null, "tcdate": 1603875629587, "tmdate": 1606769569463, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Review", "content": {"title": "Interesting approach, but insufficiently supported claims", "review": "This paper addresses an interesting and timely problem, which is to understand how Neural Processes work to learn a representation of a function space. Offering a closer investigation into a recently introduced framework, this work will likely be of interest to the ICLR community. The work focuses on the 1-dimensional case and tries to analyze the simplest case in a rigorous way, which I think is a good approach in general.\n\nHowever, I have some concerns about the main claims of this paper, as listed below:\n\n- One of the main findings of the paper is an observation that Neural Processes perform a \"frequency decomposition\". However, I think this is an insufficiently supported, and even misleading, over-statement. Indeed, Figure 2 shows that there are different modes dominated by varying characteristic frequencies, where a higher-rank mode shows a more slowly varying feature; but there is no further evidence that the decomposition is actually based on the frequency of the signal. One would get a similar result by simply doing a Principal Component Analysis too. When you say \"frequency decomposition\" it carries a clear mathematical meaning, and it is a much stronger statement than what the paper reports empirically.\n    - That said, I agree that the empirical observations are interesting. Perhaps the observations in the paper's experiments may be better described in a frame of global mode decomposition (CNP) vs. local feature detection (NP)?\n\n- I also think that the claim about the theoretical upper bound on the frequency is overstated, the way it is stated currently. The validity of the statement (Theorem 3.1) really depends on the assumption of uniform sampling, which is mentioned as a note after Theorem 3.1. Of course, I fully agree that it is an important starting step to get rigorous results in simplified conditions. But those conditions should be mentioned as part of the statement, especially when it is highly likely that the conditions are not met in the use case (there is no reason to expect that the x values in the context set is close to uniform). For example, it is possible to encode functions with a localized feature whose (local) frequency is higher than your derived bound, by using more samples around that high-frequency feature.\n\nThis paper will get views, partly because it is actually asking an interesting question, and partly because of the boldness and attractiveness of the claims made. How exciting is it to discover a naturally emerging Fourier transform? Except... that's not exactly what one can say just yet (I think). I believe the authors should either support the paper's claims by further work, or tone down their overall framing \u2014 major changes either way. While I think this work is headed to a promising direction, given the concerns described above, I recommend a rejection at this time.\n\n**UPDATE:** I appreciate the authors' responses and the engaged discussion. However, I still think that the claims of the paper are not sufficiently supported by the presented results, and maintain my original rating.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142689, "tmdate": 1606915810139, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper459/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Review"}}}, {"id": "Cpf-cfnRskR", "original": null, "number": 16, "cdate": 1606255946447, "ddate": null, "tcdate": 1606255946447, "tmdate": 1606255946447, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "Szock3BMhms", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Thanks for clarifications", "comment": "Thank you for the clarifications. After reading the authors replies and the comments of the other reviewers there are no more open questions. The approach is academically interesting; however, further analyses are needed to gain a clearer picture about the potential of the method."}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "CHH_LZT2Tw7", "original": null, "number": 15, "cdate": 1606206049131, "ddate": null, "tcdate": 1606206049131, "tmdate": 1606206049131, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "T8LOMUG8gRe", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Response", "comment": "Dear R2, thanks for the clarification. We still think that (ii) is what actually happens, hopefully we will find a way to demonstrate it more clearly :)"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "T8LOMUG8gRe", "original": null, "number": 14, "cdate": 1606191048756, "ddate": null, "tcdate": 1606191048756, "tmdate": 1606191048756, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "zn-C7b8f6MK", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Dear authors, I can't say much about the results that are not shown, but yes - I think what could be called a frequency decomposition would naturally look very much like a Fourier transform. If you show that that's what NP does, indeed it would be an interesting contribution. However, from what I understand, I feel that it is most likely not the case; frequencies may not be the essence of your findings. I may be wrong, but for the sake of completing the discussion let me try to make the point one last time. Suppose that your signals are local bumps in the time space, of fixed widths but varying center positions and heights. Would the decomposition (i) find a basis that includes single bumps at various positions, so that each signal is explained by ~1 of these, or (ii) a bunch of frequency modes where each signal is a superposition of _many_ modes? I would call only (ii) a frequency decomposition, but is that what NP does?"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "zn-C7b8f6MK", "original": null, "number": 13, "cdate": 1605788159153, "ddate": null, "tcdate": 1605788159153, "tmdate": 1605788159153, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "zGljMfTAEOT", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Thanks for the discussion", "comment": "Dear R2,\n\nthanks for taking the time to discuss (even though our paper will certainly be rejected)!\n\nWhat we were trying to show is that the NP _becomes_ this mechanism, without any external incentive, and if we understood you correctly, we're not quite managing to do that, but if we did, it would in fact be a very interesting contribution?\nIn your definition, would a frequency decomposition only be one that actually separates _individual_ frequencies?  Is it one that uses sin/cos, i.e. essentially a Fourier transform? We're currently trying to enforce orthogonality in the representations, if we can then show that the representation is approximately a Fourier basis, would you say that our claim is validated?\nOr is the problem rather the fact that we only present evidence that _suggests_ a frequency decomposition is learned, but doesn't _prove_ it?\n* Frequency bound -> Suggests a notion of frequency, but can probably be observed in other kinds of representations.\n* Visualizations of Representations -> Looks like frequency decomposition (depending on definition), but certainly only a qualitative result.\n* Band filter -> In our opinion very strong evidence that learned representations reside in frequency space, but probably not impossible to get this behaviour in other ways.\n\nSorry if we're a bit slow to grasp your point."}, "signatures": ["ICLR.cc/2021/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "zGljMfTAEOT", "original": null, "number": 12, "cdate": 1605784464551, "ddate": null, "tcdate": 1605784464551, "tmdate": 1605784464551, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "oOO8Ozv6ZU", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Dear Authors,\n\nI have absolutely no problem calling this dictionary example (and also your finding in the paper) a decomposition. What I feel uncomfortable about is calling it a _frequency_ decomposition. When an operation is called a \"frequency decomposition\", I think it is strongly suggestive of a _mechanism_ that is based on the notion of frequency, which is not what you are presenting. I feel that the more appropriate way to talk about these findings would be _descriptive_: you discovered an interesting decomposition (I agree), and you could describe that the modes have different characteristic frequencies (I see that).\n\nTo be fair, though, I came to realize that this may be a subjective interpretation of the language, possibly related to the fact that I was trained as a physicist."}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "FeIOM__T90y", "original": null, "number": 11, "cdate": 1605723435383, "ddate": null, "tcdate": 1605723435383, "tmdate": 1605723435383, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "W2kU6ViwQh9", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Dear authors,\n\nThank you for your clarifications.\n\nIn response to ` \"is that what you mean by \"limited practical setting\"?\", yes indeed.\nI agree that your work is useful for the academic community it belongs to but at the end of the day, it would be interesting to also mention practical applications.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "oOO8Ozv6ZU", "original": null, "number": 10, "cdate": 1605609874683, "ddate": null, "tcdate": 1605609874683, "tmdate": 1605609874683, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "hJvbcQwujE", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Thanks for the Clarification", "comment": "Dear R2, thanks for elaborating,\n\n> a model could achieve the same result by simply learning a dictionary whose elements happen to consist of functions of different timescales.\n\nOk, but if a signal is now (re-)constructed as a combination of those dictionary elements, which are functions of different timescales (=functions with different frequency), that's exactly what we're saying... Is the point here that we can't be sure that the signal is actually \"decomposed\" in any way, but could also be memorized?\n\nThanks for elaborating more on the PCA point, we get what you mean! It could be that a PCA would behave similarly, but we can't be certain until we try, so we'll look into it. If it does, however, we still don't see how that would invalidate our findings or make them overly obvious.\n\nThanks again!"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "hJvbcQwujE", "original": null, "number": 9, "cdate": 1605606023296, "ddate": null, "tcdate": 1605606023296, "tmdate": 1605606023296, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "TRZ7tsf6dFJ", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Still don't think that this is a frequency decomposition", "comment": "Dear authors, thank you for the response. To answer your comments first:\n\n> \u2026 the band filter experiment, which should only be possible if the representations are indeed learned in frequency space.\n\nI don't quite agree\u2026 a model could achieve the same result by simply learning a dictionary whose elements happen to consist of functions of different timescales.\n\n> could you elaborate on your comment that a similar result would be achieved with a PCA?\n\nIf one performs a PCA on some smooth (but mixed-frequency) synthetic timeseries, for example generated from a gaussian process, I wouldn't be surprised to find that the PC1 resembles a \"low-frequency\" mode, and the following PCs increasingly \"higher-frequency\".\n\nI am not changing the original recommendation; I still think that this result is not really a frequency decomposition. There should be a better, and more accurate, way of describing the findings. The CNP result finds modes with different characteristic timescales, but they are not identified with any specific frequencies, nor have any reason to be so. Maybe one could say that these modes are more strongly _localized_ in frequency than in time.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "JO8CLaolcCM", "original": null, "number": 8, "cdate": 1605600905050, "ddate": null, "tcdate": 1605600905050, "tmdate": 1605600905050, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "-nZRGcsWHcd", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Thanks for the Clarification", "comment": "So essentially you'd like to see the tightness of the bound evaluated, as a function of error tolerance? That's certainly a good point, thank you. We did try to evaluate the tightness of the bound in general, but haven't yet come up with a way that's as rigorous as we'd like it to be.\n\nYes, the comma is meant to be a cartesian product.\n\nThanks again!"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "-nZRGcsWHcd", "original": null, "number": 7, "cdate": 1605562262129, "ddate": null, "tcdate": 1605562262129, "tmdate": 1605562262129, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "iUVRUOjTEj", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Not to flip recommendation", "comment": "Thanks for the authors' response.\n\nThough the NS bound is true, the novelty is still unclear. Regarding the relationship between the representability and the error tolerance in Definition 3.1, my concern was, since the NS bound has nothing to do with the error tolerance, it would be a significant contribution if you can relate them in the context of NPs.\n\nDoes the comma in \"a map from C, X to Y\" indicates the Cartesian product? \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "iUVRUOjTEj", "original": null, "number": 6, "cdate": 1605522307836, "ddate": null, "tcdate": 1605522307836, "tmdate": 1605548674596, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "xix2Dkwp85s", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Response to Review", "comment": "Hi! Thanks for taking the time to review our paper!\n\nIt is true that the Nyquist-Shannon theorem is not specific to NPs, but we don't see how that takes away from our contribution? The derivation of our theorem might not seem complicated in hindsight, but the key was to recognize that finding in Wagstaff et al., which concerns arbitrary set-valued functions, can be combined with Nyquist-Shannon in scenarios where the sets contain evaluations of continuous functions, as is the case here. We did not evaluate the error tolerance quantitatively because a) it's usually something that would depend on a particular use case; b) it's not really relevant to the main finding that NPs automatically find a representation in function space and c) the results would essentially be the same as those presented in the appendix, where we show the reconstruction error as a function of representation size and GP kernel lengthscale. You point out correctly that we only look at scalar functions, and we agree that behaviour for higher dimensional observations is an interesting avenue for future work (we do acknowledge this in the discussion). However, the analyses we perform here would have been very hard to interpret for higher-dimensional problems, which is why we decided to stay in 1D.\n\nAllow us to answer your other points in an itemized fashion as well:\n* The derivation of NPs in the original paper was probably made in a way to be close to GPs, but in the end it is stated that NPs learn a distribution over random functions (p.2 second to last paragraph), which is essentially the same as what we say. The NP is obviously defined by data, as is virtually every DL model, but the trained NP model is in fact a map from C, X to Y, as we state in our paper.\n* We will add exemplary sources in the final version / next iteration, thanks for the suggestion\n* Discrete measurements just means that we have individual points of a signal/function that is assumed to be continuous. Does that answer your question?\n* Yes, we mean sampling from a Gaussian Process, but we say prior to make it clear that it is not the posterior, which would already be conditioned on some points. The GP is just used as one source of random functions, it doesn't have any other meaning (Bayesian or other) in our context.\n* That's a good point, we will address this in the next version: the predictive distribution is almost always narrow enough to no be visible in the plots, but we should have stated that of course.\n* Yes, y is a function of x, but think of all the functions y(x) of a given distribution/function space occupying a certain area of the (x,y) space. We now sample this space with both x and y going from -3 to 3 in 50 steps (think of np.linspace) and then just encode all those points to create a visualization of r_i(x, y) to understand how that space is now represented in the NP.\n* Thanks for the suggestion\n\nWe hope we were able to answer your questions!\n\nKind regards, the authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "TRZ7tsf6dFJ", "original": null, "number": 5, "cdate": 1605520132997, "ddate": null, "tcdate": 1605520132997, "tmdate": 1605520132997, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "ZvEa9hhrWuy", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Response to Review", "comment": "Dear R2, thank you four your detailed input,\n\nregarding your first point, we agree to the extent that the visualization of the representations can only be considered qualitative evidence for the hypothesis that a frequency decomposition occurs. That is why we included the band filter experiment, which should only be possible if the representations are indeed learned in frequency space. Do you have suggestions on how to show more directly that this is a frequency decomposition? And could you elaborate on your comment that a similar result would be achieved with a PCA? We don't see how that's the case.\n\nOn the concern regarding the equidistant sampling requirement in the Nyquist-Shannon theorem, that's not actually a hard requirement, signals can also be reconstructed from random sampling points if the average sampling rate meets the Shannon limit. Because we sample x from a uniform distribution, we can thus use the same derivation as if they were equidistantly sampled. We do agree that we should have discussed this in more detail! Your idea to test the behaviour with non-uniform sampling is very interesting and we will look into it.\n\nWe'd also like to thank you for your encouraging words on the direction of our work, they do mean a lot :)\n\nKind regards, the authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "Szock3BMhms", "original": null, "number": 4, "cdate": 1605518402211, "ddate": null, "tcdate": 1605518402211, "tmdate": 1605518402211, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "KikP9tDQaX8", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Response to Review", "comment": "Dear R3, thank you for your review!\n\nregarding your question on the computational requirements w.r.t to a possible use of NPs as filters, that's certainly something that can be looked into further. Our goal was not to propose NPs as learnable band filters, we just thought that's a cool consequence of our main finding (that NPs automatically learn a frequency decomposition) and can be considered further evidence for it. Your second point---the significance and the consequences being unclear---seems to go in the same direction. And the truth is, we're not completely sure if there are any immediate use cases our contribution enables. It is more about the general understanding of how NPs represent function spaces, something that has never been looked into before. In our opinion, such advancements are as important as those that are focused more on practical utility.\n\nThanks again and kind regards,\nthe authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "W2kU6ViwQh9", "original": null, "number": 3, "cdate": 1605288432320, "ddate": null, "tcdate": 1605288432320, "tmdate": 1605288432320, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "V7dLlnEeZjT", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment", "content": {"title": "Response to Review", "comment": "Hi! Thanks for taking the time to read and review our paper :)\n\nRegarding your comment on the types of functions we used, the choices were relatively straightforward: the GP samples were used in the original NP work, and the Fourier series gave us control about the frequencies in the signals. But we agree, more variety would have undoubtedly strengthened our paper. The implementation we use (fully connected + tanh activation) only allows for smooth functions, but it's certainly worth exploring other implementations as well!\n\nWith respect to how our work can be helpful for the community, we believe it contributes significantly to the general understanding of Neural Processes. To the best of our knowledge, there is no other publication that tries to understand how function spaces are represented in these models. We do suggest one possible practical benefit of our discovery (learnable band filters), but our main contribution is still the finding that function spaces are represented via a frequency decomposition that emerges automatically, not so much the question how that translates to practical use of NPs. Is that what you mean by \"limited practical setting\"?\n\nKind regards,\nthe authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ggNgn8Fhr5Q", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper459/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper459/Authors|ICLR.cc/2021/Conference/Paper459/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870769, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Comment"}}}, {"id": "xix2Dkwp85s", "original": null, "number": 1, "cdate": 1603817738010, "ddate": null, "tcdate": 1603817738010, "tmdate": 1605024684954, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Review", "content": {"title": "Recommendation to reject", "review": "This paper presents an analysis on the neural processes in the signal processing point of view and gives a bound on the highest frequency of the function that a neural process can represent.\n\nI recommend to reject this manuscript. My comments are below.\n\nThe key point of this work is Theorem 3.1. However the theorem itself is just a direct outcome of the Nyquist\u2013Shannon sampling theorem, and it is generally true to not only neural processes but also to all the other approaches. Meanwhile, the authors did not talk about the relationship quantitatively between the representability and the error tolerance in Definition 3.1. In addition, the analysis is limited to only scalar-valued function on a 1D interval. The writing could also be improved.\n\nConcerns:\n- The definition of neural processes in the background section is confusing. Despite the way of defining a map, P is a mathematical object defined by a set of tuples and a map,  meaning that the neural processes are also defined by data. In the original paper, the neural processes were however defined as random functions.\n\n- In the background section, the words say 'some sources define ...'. Could the authors give the sources?\n\n- In Def 3.1, what do the authors mean by 'discrete measurements'?\n\n- In the experiment section, do the authors mean sampling from a Gaussian process by saying GP prior? I don't see a GP plays the role of prior in terms of Bayesian inference.\n\n- The examples given in the experiment section lack quantitative results. It is better for evaluating the reconstruction by showing the posterior or predictive distribution instead of single reconstructions.\n\n- In Sec. 4.2. how did the authors sample regular grid on the 2D plane as y is determined by x. \n\n- Eq.11 is defined in the appendix. Better to use separate numbering.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142689, "tmdate": 1606915810139, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper459/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Review"}}}, {"id": "V7dLlnEeZjT", "original": null, "number": 3, "cdate": 1604015863809, "ddate": null, "tcdate": 1604015863809, "tmdate": 1605024684809, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Review", "content": {"title": "Interesting paper on neural processes, unsure about applications and robustness of experiments", "review": "The paper tries to analyze the behavior of Neural Processes in the frequency domain and concludes that such Processes can only represent oscillations up to a certain frequency.\n\nWhile drawing a parallel between Neural Processes and signal processes, I think that there is some weakness in the experiments of the paper. In particular, the authors only seem to consider the exponential quadratic kernel to generate examples which would mostly show examples of smooth functions as would sampling Fourier linear combinations.\n\nI am also unsure how this paper could be helpful to our community in its present form as it sheds some light on the inner workings of Neural Processes but only in a very limited practical setting.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142689, "tmdate": 1606915810139, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper459/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Review"}}}, {"id": "KikP9tDQaX8", "original": null, "number": 4, "cdate": 1604061980558, "ddate": null, "tcdate": 1604061980558, "tmdate": 1605024684736, "tddate": null, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "invitation": "ICLR.cc/2021/Conference/Paper459/-/Official_Review", "content": {"title": "Empirical studies confirm theoretical conclusions; however, what are the consequences?", "review": "The work examines properties of Neural Processes (NP). More precisely, of deterministic NPs and how they for finite-dimensional representations of infinite-dimensional function spaces. NP learn functions f that best represent/fit discrete sets of points in space. Based on signal theoretic aspects of discretisation, authors infer a maximum theoretical upper bond of frequencies of functions f that can be used to represent the points. The bond depends on the latent dimension/representation size and the finite interval spawn by the points. Simulations are computed to test the validity of the upper bond. Authors find that NPs behave like a Fourier Transform and decompose the spectrum of the signal. Since the representation during training learns to represent specific frequencies, NPs can be used as band pass/stop filter.\n\nThe paper is well written, and the basic approach is clearly outlined. The quality of the work and the evaluation are good and support the authors claims. However, it is not fully clear to which extend the claims translate to other data or generalise well. The finding that NPs interpret points in space as signals and implement a frequency decomposition like Fourier/Wavelet transforms seems reasonable. Not sure, however, if an application as filter is ecological in terms of computational complexity. \n\nThe paper provides a strong theoretical foundation of the method and authors support their claims by  empirical stimulation. Also, explainability and more importantly interpretability of how methods generate results is essential. So, the message the paper sends is relevant. However ,the relevance and significance of the findings, and the consequences thereof are not clear.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper459/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper459/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Frequency Decomposition in Neural Processes", "authorids": ["~Jens_Petersen2", "~Paul_F_Jaeger1", "~Gregor_Koehler1", "~David_Zimmerer1", "~Fabian_Isensee1", "~Klaus_Maier-Hein1"], "authors": ["Jens Petersen", "Paul F Jaeger", "Gregor Koehler", "David Zimmerer", "Fabian Isensee", "Klaus Maier-Hein"], "keywords": [], "abstract": "Neural Processes are a powerful tool for learning representations of function spaces purely from examples, in a way that allows them to perform predictions at test time conditioned on so-called context observations. The learned representations are finite-dimensional, while function spaces are infinite-dimensional, and so far it has been unclear how these representations are learned and what kinds of functions can be represented. We show that deterministic Neural Processes implicitly perform a decomposition of the training signals into different frequency components, similar to a Fourier transform. In this context, we derive a theoretical upper bound on the maximum frequency Neural Processes can reproduce, depending on their representation size. This bound is confirmed empirically. Finally, we show that Neural Processes can be trained to only represent a subset of possible frequencies and suppress others, which makes them programmable band-pass or band-stop filters.", "one-sentence_summary": "We show that deterministic Neural Processes decompose signals into different frequency components, a behaviour that can be controlled to turn them into programmable band-pass or band-stop filters.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "petersen|frequency_decomposition_in_neural_processes", "pdf": "/pdf/850da096c63d720fe08f8e9792aafaee9a93cd17.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=UQzuwNQYZv", "_bibtex": "@misc{\npetersen2021frequency,\ntitle={Frequency Decomposition in Neural Processes},\nauthor={Jens Petersen and Paul F Jaeger and Gregor Koehler and David Zimmerer and Fabian Isensee and Klaus Maier-Hein},\nyear={2021},\nurl={https://openreview.net/forum?id=ggNgn8Fhr5Q}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ggNgn8Fhr5Q", "replyto": "ggNgn8Fhr5Q", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper459/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538142689, "tmdate": 1606915810139, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper459/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper459/-/Official_Review"}}}], "count": 20}