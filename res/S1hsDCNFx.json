{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487361955757, "tcdate": 1487361955757, "number": 122, "id": "S1hsDCNFx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1hsDCNFx", "original": "r1br_2Kge", "signatures": ["~Kunal_Talwar1"], "readers": ["everyone"], "content": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/2be929948a84d1b73643e88cddafaa69244c7c7c.pdf", "TL;DR": "r sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1478272134239, "tcdate": 1478244408692, "number": 136, "id": "r1br_2Kge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1br_2Kge", "signatures": ["~Kunal_Talwar1"], "readers": ["everyone"], "content": {"title": "Short and Deep: Sketching and Neural Networks", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "pdf": "/pdf/12c6f024bbb3f85ab2cd5f9508d5b6f14eafce85.pdf", "TL;DR": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "paperhash": "daniely|short_and_deep_sketching_and_neural_networks", "keywords": ["Theory"], "conflicts": ["google.com"], "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 1}