{"notes": [{"id": "Eql5b1_hTE4", "original": "yvvEEq4opfk", "number": 1387, "cdate": 1601308154798, "ddate": null, "tcdate": 1601308154798, "tmdate": 1615775776851, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "eDYjuhVqs1L", "original": null, "number": 2, "cdate": 1612579998796, "ddate": null, "tcdate": 1612579998796, "tmdate": 1612580084957, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "RNSc2L6aP7f", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Comment", "content": {"title": "Response", "comment": "Dear Xinshao:\n\nThanks for your comments and we are agree with you, i.e., the new optimality criterion is sufficient, but is not necessary. We focus on learning with noisy labels in this paper. The necessity of  the new optimality criterion does not affect the effectiveness of the proposed method.  We will add discussions in the final version to address your concern and enhance this paper. \n\nBest,\\\nAuthors"}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Eql5b1_hTE4", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1387/Authors|ICLR.cc/2021/Conference/Paper1387/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649451252, "tmdate": 1610649451252, "id": "ICLR.cc/2021/Conference/Paper1387/-/Comment"}}}, {"id": "RNSc2L6aP7f", "original": null, "number": 1, "cdate": 1611003603417, "ddate": null, "tcdate": 1611003603417, "tmdate": 1611003700285, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "5ACBXd9y-SZ", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Comment", "content": {"title": "Concern on the OPTIMALITY CRITERION", "comment": "Dear authors: \n\nGreat work! Your idea on dividing parameters to critical ones and non-critical ones is very interesting for me. \n\nIn the section 3.1, G(t) = L(tW; S),  grad_G(1) = grad_L(W;S) * W.tranpose(), \nI find that the OPTIMALITY CRITERION is sufficient, but may be not necessary. \n\n**Sufficient condition**:\n1. When  grad_L(W;S) = zero vector, we have grad_G(1) = zero scalar. \n\n\nHowever, **it is not a necessary condition**: \n\n2. When grad_G(1) = zero scalar, grad_L(W;S) may not be a zero vector. \n\nTherefore, according to my personal understanding so far, the optimality criterion is improper.  Could you kindly explain more to address my concern?  \nI am looking forward to your ideas and discussing with you. Please kindly and feel free to correct me if I am wrong.   \n\nMany thanks. "}, "signatures": ["~Xinshao_Wang1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Xinshao_Wang1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Eql5b1_hTE4", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1387/Authors|ICLR.cc/2021/Conference/Paper1387/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649451252, "tmdate": 1610649451252, "id": "ICLR.cc/2021/Conference/Paper1387/-/Comment"}}}, {"id": "5ACBXd9y-SZ", "original": null, "number": 1, "cdate": 1610040409148, "ddate": null, "tcdate": 1610040409148, "tmdate": 1610474006285, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper presents a novel method for learning with noisy labels based on an interesting insight into the learning dynamics of deep neural networks. \n\nReviewers unanimously vote for acceptance. I agree with their assessment, and it is my pleasure to recommend the paper for acceptance. \n\nIf I can draw attention to one comment, I strongly agree with R1 that the criterion in Eq. (3) is somewhat poorly motivated. I believe the paper would benefit from a clearer exposition of this part. \n\nPlease make sure to address all reviewers' remarks in the camera-ready version. Thank you for submitting your work to ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040409135, "tmdate": 1610474006266, "id": "ICLR.cc/2021/Conference/Paper1387/-/Decision"}}}, {"id": "FSR2Al847iN", "original": null, "number": 8, "cdate": 1606216496289, "ddate": null, "tcdate": 1606216496289, "tmdate": 1606216496289, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment", "content": {"title": "Revised draft uploaded", "comment": "Dear reviewers and all,\n\nWe have revised our draft according to your constructive comments. Major revisions are highlighted in green. We sincerely thank all the reviewers. We would highly appreciate it if you could read our responses and revisions. Please feel free to let us know if further details/explanations would be helpful.\n\nBest,\n\nAuthors"}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Eql5b1_hTE4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1387/Authors|ICLR.cc/2021/Conference/Paper1387/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860296, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment"}}}, {"id": "0PEdtcEGilF", "original": null, "number": 2, "cdate": 1605426890098, "ddate": null, "tcdate": 1605426890098, "tmdate": 1605434097233, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "9EOuYrUqLzS", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment", "content": {"title": "The response to Ehsan Amid", "comment": "Hello, Ehsan, \n\nThank you for your interesting work. We will study them after the rebuttal session. \n\nBest,\\\nPaper 1387 authors"}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Eql5b1_hTE4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1387/Authors|ICLR.cc/2021/Conference/Paper1387/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860296, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment"}}}, {"id": "H86TYTjdIt5", "original": null, "number": 6, "cdate": 1605433531155, "ddate": null, "tcdate": 1605433531155, "tmdate": 1605433531155, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "VIojxqDxeMz", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment", "content": {"title": "The response to Reviewer4", "comment": "Thanks for your review and suggestions. \n\n1. Why does the proportion of non-critical parameters is assumed to be the same as the noise rate?\n- Thank you for your nice concern! We have the intuition that if the label noise rate is high, we will have a large set of incorrect labels and a small set of correct labels. Then, there could be a large number of non-critical parameters which overfit the incorrect labels; while a small number of critical parameters which fit the correct labels. We conjecture that the number of non-critical parameters is proportional to the label noise rate and therefore use the label noise rate to help identify the non-critical parameters. We agree with the reviewer that the non-critical parameters may not rely strictly on the label noise rate, which needs further investigations. For the proposed method, which firstly studies the critical and non-critical parameters, it empirically works well as supported by comprehensive experimental results. We will add similar discussions in the paper to reflect the reviewer\u2019s insight. \n\n2. What will the performance be like if the proportion of non-critical parameters is assumed to be a constant number or estimation of the noise rate is too bad?\n- Thanks for the suggestion! First of all, as shown in the answer to the above question, it is intuitive to relate the proportion of non-critical parameters to the label noise rate, which is task-dependent and can provide a reliable solution. However, it is interesting to see how it works if we set the proportion of non-critical parameters as a constant number. We would like to mention that it may be hard to set or tune the constant number for the proportion because we only exploit noisy data in this paper. In the paper, we use the noisy validation set for early stopping. Note that the correct labels are dominating in each noisy class and that label noise is random, the accuracy on the noisy validation set and the accuracy on the clean test data set are positively correlated. The noisy validation set therefore can be employed. We will try to use the noisy validation set to locate constants for the proportion of the non-critical parameters on MNIST, F-MNIST, and CIFAR10 in the updated version. Specifically, we will compare the constants and will discuss the relationship between the constants and the label noise rate.\n\n- If the estimation is largely different from the real noise rate, a too large estimation error may hurt the performance of the proposed method. However in fact, the noise rate can be effectively estimated [1-2]. In this paper, we use the estimated noise rate to achieve the superior performance. When the estimation error is within an acceptable range, our method is robust as shown in the ablation study. Your comments about setting the proportion of non-critical parameters are really constructive. We agree that it is an important point needed in-depth study. \n\n3. Why does the proposed method use a noisy validation set for early stopping?\n- Our work follows the often used practices in the literature of learning from noisy labels, using a noisy validation set for early stopping. The intuition to do so is discussed in the answer to the above question. It empirically works well as also supported by the existing methods, e.g., GCE [3], Forward [4], and T-Revision [5]. A more in-depth theoretical analysis of this aspect is worth further learning. \n\n[1] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE TPAMI, 2016.\\\n[2] Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, and Dacheng Tao. An efficient and provable approach for mixture proportion estimation using linear independence assumption. CVPR, 2018. \\\n[3] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. NeurIPS, 2018.\\\n[4] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. CVPR, 2017.\\\n[5] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? NeurIPS, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Eql5b1_hTE4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1387/Authors|ICLR.cc/2021/Conference/Paper1387/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860296, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment"}}}, {"id": "JgyFJyipZK-", "original": null, "number": 5, "cdate": 1605433307640, "ddate": null, "tcdate": 1605433307640, "tmdate": 1605433307640, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "i_fZV9ttgbw", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment", "content": {"title": "The response to Reviewer2", "comment": "Thank you for your comments! We will answer the questions one by one. \n\n1. Add illustrations of the experimental results. \n- We agree with you that figures in some cases can better demonstrate the effectiveness of our method. We will add some figures in the supplementary materials. We will keep you informed of the change once done. \n\n2. Could the authors add descriptions for the baselines and experimental results? \n- Yes. We will update the descriptions accordingly. \n\n3. The symbol \\tidle{S} makes readers misunderstand. \n-  We emphasize it after Eq.(5) to make it clear. \n\n4. Some typos need to be corrected. \n- Thanks! We have checked it and updated it. \n\n5. Some minor comments. \n- To increase the readability of the paper, we will take your advice to modify the tables and figures. \n- Yes, it is really worthy of learning. We consider that the AutoML technique used in S2E may be useful for setting the parameter automatically. We will explore in this direction to improve our work in future. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Eql5b1_hTE4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1387/Authors|ICLR.cc/2021/Conference/Paper1387/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860296, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment"}}}, {"id": "Ne5oO_yE8ot", "original": null, "number": 4, "cdate": 1605432972892, "ddate": null, "tcdate": 1605432972892, "tmdate": 1605433128420, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "wtHgi5ZzzX", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment", "content": {"title": "The response to Reviewer3", "comment": "Thank you for the valuable feedback! Your suggestions will make this paper better without doubt. \n\n1. Change the symbol to avoid the confusion.\n- Thank you for this suggestion! We have changed the symbol of the learnable parameters.\n\n2. The reasons for using the $\\ell_1$ regularizer. \n- Thank you for raising this concern. Recall our proposed method and the optimality criterion, the non-critical parameters are penalized to be zero (or close to zero), which makes them to have much less effect on generalization. For the negative update rule in this paper, when we exploit the $\\ell_1$ regularizer, it is effective to achieve that non-critical parameters are pushed to be very sparse and be very close to zero. This theory is also support by [1]. \n\n3. The issue about hyper-parameter $\\tau$. \n- If the noise rate is unknown, we can estimate it with [2-3]. The methods for estimating noise rate are widely used in existing work, e.g., Co-teaching and Co-teaching+. Additionally, the proposed method is insensitive to the estimation result of the noise rate as you mentioned. \n\n[1] Bishop, Christopher M. Pattern recognition and machine learning. Springer, 2006.\\\n[2] Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE TPAMI, 2016.\\\n[3] Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, and Dacheng Tao. An efficient and provable approach for mixture proportion estimation using linear independence assumption. CVPR, 2018. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Eql5b1_hTE4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1387/Authors|ICLR.cc/2021/Conference/Paper1387/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860296, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment"}}}, {"id": "UCp2KDmD4cd", "original": null, "number": 3, "cdate": 1605432741343, "ddate": null, "tcdate": 1605432741343, "tmdate": 1605432741343, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "5OuwXvLJRUD", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment", "content": {"title": "The response to Reviewer1", "comment": "Thanks for your constructive suggestions! We have checked and will address the issues as mentioned in \u201cCons\u201d to improve this paper. \n\n1. Add details or citations for better understanding the optimality criterion.\n- We will add detailed analyses and explanations for the optimality criterion. \n\n2. Some modifiabilities in the experiments.\n- We agree with your comment, and place the baseline S2E after Co-teaching+. \n- To make the results more convincing, we add the settings of the baseline methods in Section 4.2. \n\n3. The advice for future work. \n- Exciting suggestions! It is very interesting and valuable to effectively detect the critical/non-critical parameters during the whole training process, and further achieve a robust classifier. We will investigate this in future work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Eql5b1_hTE4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1387/Authors|ICLR.cc/2021/Conference/Paper1387/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860296, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Comment"}}}, {"id": "9EOuYrUqLzS", "original": null, "number": 1, "cdate": 1605042687139, "ddate": null, "tcdate": 1605042687139, "tmdate": 1605042687139, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Public_Comment", "content": {"title": "Please consider referencing/comparing to these more recent works", "comment": "I would like to point out that our work (Amid et al. 2019a) extends the Generalized CE loss (Zhang and Sabuncu 2018) by introducing two temperatures t1 and t2 which recovers GCE when t1 = q and t2 = 1. Our more recent work, called the bi-tempered loss (Amid et al. 2019b) extends these methods by introducing a proper (unbiased) generalization of the CE loss and is shown to be extremely effective in reducing the effect of noisy examples. Please consider referencing/comparing to these papers.\n\n(Amid et al. 2019a) Amid et al. \"Two-temperature logistic regression based on the Tsallis divergence.\" In The 22nd International Conference on Artificial Intelligence and Statistics (AISTATS), 2019. \n\n(Amid et al. 2019b) Amid et al. \"Robust bi-tempered logistic loss based on Bregman divergences.\" In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n"}, "signatures": ["~Ehsan_Amid1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Ehsan_Amid1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Eql5b1_hTE4", "readers": {"description": "User groups that will be able to read this comment.", "values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed."}}, "expdate": 1605630600000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/Authors", "ICLR.cc/2021/Conference/Paper1387/Reviewers", "ICLR.cc/2021/Conference/Paper1387/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1605024972891, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Public_Comment"}}}, {"id": "VIojxqDxeMz", "original": null, "number": 1, "cdate": 1603182037529, "ddate": null, "tcdate": 1603182037529, "tmdate": 1605024458286, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Review", "content": {"title": "New technique for learning with noisy labels", "review": "This paper proposes a method for deep learning with noisy labels, which distinguishes the critical parameters and non-critical parameters for fitting clean labels and updates them by different rules. The method is easy to implement and the empirical results are promising. Experiments on both simulated and real-world datasets show it reaches new state-of-the-arts results.\n\nQuestions:\n- I don\u2019t think the ablation is convincing \u2013 why does the proportion of non-critical parameters is assumed to be the same as the noise rate? The ablation only shows the method is insensitive when the estimation of the noise rate is not precise. However, why do we need to estimate the noise rate? What if the proportion of non-critical parameters is assumed to be a constant number? In other words, what will the performance be like if the estimation is largely different from the real noise rate? \n- Since the validation set is also noisy, why does the early stopping criterion adopts the minimum classification error on it? ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119846, "tmdate": 1606915775179, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1387/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Review"}}}, {"id": "i_fZV9ttgbw", "original": null, "number": 2, "cdate": 1603720831586, "ddate": null, "tcdate": 1603720831586, "tmdate": 1605024458204, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Review", "content": {"title": "Review 2 for \"Robust early-learning: Hindering the memorization of noisy labels\"", "review": "------Overall------\nThis paper utilizes the memorization effects of deep models and aims to improve their robustness to noisy labels before early stopping. As the deep models fit training data with clean labels in the early stage of training, the authors propose a novel method to identify those more important parameters for fitting clean labels. They then deactivate the unimportant parameters to reduce side effects brought by noisy labels, which enhances the fitting to clean labels implicitly. I think this paper is interesting and makes sense. The major comments and issues are as follows: \n\n------Major comments------\n1. Different from other complex methods for learning with noisy labels , this work discusses that standard cross entropy loss can achieve competitive performance with early stopping. We can therefore focus the training stage before early stopping to handle noisy labels. The authors skillfully allow the optimality to be checked by a scalar, and then judge the importance of the parameters by analyzing the influence of the parameters on this scalar. The idea of this paper is novelty and meaningful. \n\n2. The paper is very well-written. The description of its motivation and technical details is clear and flows smoothly, which makes it easy for readers to understand the core idea of this paper and follow its implementation details.\n\n3. The experimental results are convincing. The authors provide a very detailed description of experimental settings. Besides, this paper exploits multiple methods for comparison and considers various noise settings to verify the effectiveness of the proposed method. The experimental results on synthetic and real-world datasets are convincing. The authors also perform an ablation study to present the proposed method is insensitive to the estimation of noise rate. \n\n------Issues------\n1. I only find the illustration of comparison between CE and CDR in the case of noisy CIFAR-100. This paper aims to reduce the side effect of noisy labels before early stopping, thus CE is an importance baseline in this paper. Can the authors add illustrations of the experimental results in other cases like Figure.2? \n\n2. The baselines and experimental results are sufficient. Could the authors add some introduction for the baselines and more detailed discussion for experimental results. \n\n3. The authors may need add some explanation for Eq.(2) and Eq.(5). The proposed method makes use of the memorization effects of deep models. The authors directly write \\tilde{S} rather than S in Eq.(5). However, this may be easy to misunderstand. I suggest that the authors can emphasize it or change it. \n\n4. Some typos need to be corrected. (1) \u201cThe underlying issue of directly using the gradient of......\u201d; (2) \u201cRobust positive update uses the gradients to update the critical ones......\u201d.\n\n5. Some minor comments. (1) The experimental results in Table 1 are too dense. (2) The figures are not readable, especially the title is small for me. This makes it a little hard to match the figures with specific cases. (3) The parameter (noise rate) $\\tau$ still needs to be estimated, which may be challenging. It will be promising to automatically set this parameter during training. Thus, a more in-depth analysis is worthy of further learning. \n\nI hope the authors can address these issues carefully to improve this work. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119846, "tmdate": 1606915775179, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1387/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Review"}}}, {"id": "wtHgi5ZzzX", "original": null, "number": 3, "cdate": 1603723677693, "ddate": null, "tcdate": 1603723677693, "tmdate": 1605024458134, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Review", "content": {"title": "Interesting idea with convincing experimental performance", "review": "This paper aims to exploit the early stopping method to solve the problem of learning with noisy labels. Specifically, this paper finds that only partial parameters (critical parameters) are important for fitting clean labels and generalize well; while the other parameters (non-critical parameters) tend to fit noisy labels and cannot generalize well. Based on this observation, this paper proposes to divide all parameters into the critical parameters and non-critical ones, and perform different update rules for the two types of parameters, in each iteration. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the effectiveness of the proposed method.\n\nThis paper has the following advantages:\n1.\tThis paper is well-written and the motivation is very clear. This paper has clearly explained the two types of parameters, i.e., critical parameters and non-critical parameters.\n2.\tThis paper proposes a novel method with an interesting idea. Unlike many existing methods that are aggregations of multiple techniques, this paper only focuses on one concept. It is simple but effective. The view of updating different parameters by different rules is quite novel for learning with noisy labels. I think this view may bring some new insights to the area of learning with noisy labels.\n3.\tExperiments are quite thorough and results on both synthetic and real-world datasets validate the effectiveness of the proposed method.\n\nThis paper also has some minor issues:\n1.\tI would suggest the authors to carefully check the notations used in the paper. For example, $\\mathbf{w}$ denotes all the learnable parameters of the model in the paper, while $\\mathbf{w}$ usually means a vector. So I would suggest the authors to use another symbol to denote the set of all the learnable parameters of the model, e.g., $\\mathcal{W}$.\n2.\tIt is not well justified why this paper chooses the $\\ell_1$ regularizer. Is there any consideration to use the $\\ell_1$ regularizer except that could be associated with weight decay?\n3.\tThe hyper-parameter $\\tau$ (noise rate) needs to be known. It may not be a big issue as the ablation study in this paper demonstrates that the proposed method is insensitive to this hyper-parameter.\n\nOverall, I think this is a good paper with an interesting idea and convincing experimental performance. So I prefer to accept it.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119846, "tmdate": 1606915775179, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1387/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Review"}}}, {"id": "5OuwXvLJRUD", "original": null, "number": 4, "cdate": 1603939445407, "ddate": null, "tcdate": 1603939445407, "tmdate": 1605024458067, "tddate": null, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "invitation": "ICLR.cc/2021/Conference/Paper1387/-/Official_Review", "content": {"title": "A novel and effective method for learning with noisy labels", "review": "This paper tackles the problem of learning with noisy labels and proposes a novel method CDR which is inspired by the lottery ticket hypothesis. In particular, the proposed method categorizes the parameters into two parts, including critical parameters and non-critical parameters, and applies different update rules to these parameters. Using comprehensive experiments on synthetic datasets and real-world datasets, the authors verify that the proposed method can improve the robustness of the classifiers against noisy labels.\n\nPros.\n1. The proposed method is interesting in its design. The authors provide an alternative interpretation for the optimality criterion, which reveals that the value of the parameter can also be used to check the optimality. It is novel and of significance. Meanwhile, the proposed method is easy to implement. This method can also be applied to existing algorithms to further improve their robustness.\n2. Overall, this paper is very well written and well organized. The technical details are easy to follow, and Algorithm 1 helps understand the procedures. \n3. Extensive experiments are performed to verify that the proposed method indeed helps over the baselines at fighting noisy labels. I like the details of experimental settings, which really can help reproduce these experimental results.\n\nCons.\n1. The new interpretation for the optimality criterion is important, but lacks detailed explanation. I find that there is only simple analysis. Perhaps the authors could add some details or citations for better understanding.\n2. Though the baseline S2E uses AutoML, it seems that S2E is an improvement on co-teaching or co-teaching+? It is not suitable to place it at the end. \n3. For some baselines such GCE and Joint, there are hyperparameters to consider. The authors should explain how to set their values for a fair comparison. For APL, there are multiple combinations of loss functions. In this experiment, which one did you choose? I suggest that the authors add such explanation in Section 4.2, which will make the results more convincing. \n4. The proposed method implicitly exploits the memorization effects of deep models, and can reduce the side effect of noisy labels before early stopping. After early stopping, noisy labels still affect the performance. How to reduce the side effect during the whole training by using this idea? I personally think this is an interesting and meaningful direction. The authors can regard this as future work to improve this paper. \n\nOverall, I think this paper makes sense in learning with noisy labels. The proposed method is novel and effective. I recommend to accept this paper, and hope that the authors can address the above issues carefully.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1387/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1387/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Robust early-learning: Hindering the memorization of noisy labels", "authorids": ["~Xiaobo_Xia1", "~Tongliang_Liu1", "~Bo_Han1", "~Chen_Gong5", "~Nannan_Wang1", "~Zongyuan_Ge1", "yichang@jlu.edu.cn"], "authors": ["Xiaobo Xia", "Tongliang Liu", "Bo Han", "Chen Gong", "Nannan Wang", "Zongyuan Ge", "Yi Chang"], "keywords": [], "abstract": "The \\textit{memorization effects} of deep networks show that they will first memorize training data with clean labels and then those with noisy labels. The \\textit{early stopping} method therefore can be exploited for learning with noisy labels. However, the side effect brought by noisy labels will influence the memorization of clean labels before early stopping. In this paper, motivated by the \\textit{lottery ticket hypothesis} which shows that only partial parameters are important for generalization, we find that only partial parameters are important for fitting clean labels and generalize well, which we term as \\textit{critical parameters}; while the other parameters tend to fit noisy labels and cannot generalize well, which we term as \\textit{non-critical parameters}. Based on this, we propose \\textit{robust early-learning} to reduce the side effect of noisy labels before early stopping and thus enhance the memorization of clean labels. Specifically, in each iteration, we divide all parameters into the critical and non-critical ones, and then perform different update rules for different types of parameters. Extensive experiments on benchmark-simulated and real-world label-noise datasets demonstrate the superiority of the proposed method over the state-of-the-art label-noise learning methods.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xia|robust_earlylearning_hindering_the_memorization_of_noisy_labels", "pdf": "/pdf/8bcbd9a8ffef76580768ad0f329dcc1cae3be97e.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxia2021robust,\ntitle={Robust early-learning: Hindering the memorization of noisy labels},\nauthor={Xiaobo Xia and Tongliang Liu and Bo Han and Chen Gong and Nannan Wang and Zongyuan Ge and Yi Chang},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Eql5b1_hTE4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Eql5b1_hTE4", "replyto": "Eql5b1_hTE4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1387/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538119846, "tmdate": 1606915775179, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1387/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1387/-/Official_Review"}}}], "count": 15}