{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582961371, "tcdate": 1520131547348, "number": 1, "cdate": 1520131547348, "id": "B171R0_dz", "invitation": "ICLR.cc/2018/Workshop/-/Paper3/Official_Review", "forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "signatures": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer2"], "content": {"title": "Interesting results but not surprissing", "rating": "4: Ok but not good enough - rejection", "review": "This paper analyzes parameter redundancy in current large networks and how networks using lower number of parameters can still do a decent job in generalization.\n\nWhile this is interesting, it is not new. It is well known that redundancy exists and that is why compression among other (post-processing) techniques exist. The paper also suggests some potential ways to make use of the extra computational capacity, however, there are only ideas and no results. Would be nice to see these ideas in practice. Redundancy is mostly needed for helping the optimizer, while reducing the number of parameters helps in some tasks there is no proof that it works in larger datasets (ImageNet or larger).\n\nIn summary, this is a nice analysis of redundancy in neural networks in small datasets but there is not strong take home message. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing while Learning Next to Nothing", "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most  cases, training involves iterative modification of all weights inside the network via back-propagation. In this paper, we propose to take an extreme approach and fix \\emph{almost all weights} of a deep convolutional neural network in their randomly initialized values, allowing only a small portion to be learned. As our experiments show, this often results in performance which is on par with the performance of learning all weights. The implications of this intriguing property or deep neural networks are discussed and we suggest ways to harness it to create more robust representations. ", "pdf": "/pdf/416498d17515b62b1ca9157745c60ade59d8551e.pdf", "TL;DR": "Convnets can achieve good performance even when only a fraction of parameters are learned. ", "paperhash": "rosenfeld|intriguing_properties_of_randomly_weighted_networks_generalizing_while_learning_next_to_nothing", "keywords": ["Random Networks", "Extreme Learning", "Compact Representations"], "authors": ["Amir Rosenfeld", "John K. Tsotsos"], "authorids": ["amir.rosenfeld@gmail.com", "tsotsos@cse.yorku.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582961182, "id": "ICLR.cc/2018/Workshop/-/Paper3/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper3/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper3/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper3/AnonReviewer1"], "reply": {"forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582961182}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582826787, "tcdate": 1520608837575, "number": 2, "cdate": 1520608837575, "id": "B1ABU7xFz", "invitation": "ICLR.cc/2018/Workshop/-/Paper3/Official_Review", "forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "signatures": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer3"], "content": {"title": "Interesting work", "rating": "7: Good paper, accept", "review": "The authors train several network architectures while fixing parts of the parameters to the initalization. They show that even when fixing a large subset of the trainable parameters, the networks perform reasonably (but worse than when training parameters).\n\nThe work seems solid but could be improved by a few comparisons. Especially I'm wondering how much the fixed parameters contribute to the network performance. The zeroing-out-experiments explore this question but I would be interested to see how good the models perform when zeroing out the fixed parameters already while training (of course this doesn't work when fixing a full layer). This could serve as a simple baseline with the same number of parameters. For example, I'm not sure how impressive the performance on CIFAR100 is when fixing more than 60% of the weights. Also I'm missing the performance of the fully trained models (e.g. is a highly reduced densenet still better than a fully trained AlexNet?).\n\nThe authors explore a very interesting idea and I think especially the implications of interpreting representations are highly relevant.\n\nOverall I think this work is worth presenting at the ICLR workshops.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing while Learning Next to Nothing", "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most  cases, training involves iterative modification of all weights inside the network via back-propagation. In this paper, we propose to take an extreme approach and fix \\emph{almost all weights} of a deep convolutional neural network in their randomly initialized values, allowing only a small portion to be learned. As our experiments show, this often results in performance which is on par with the performance of learning all weights. The implications of this intriguing property or deep neural networks are discussed and we suggest ways to harness it to create more robust representations. ", "pdf": "/pdf/416498d17515b62b1ca9157745c60ade59d8551e.pdf", "TL;DR": "Convnets can achieve good performance even when only a fraction of parameters are learned. ", "paperhash": "rosenfeld|intriguing_properties_of_randomly_weighted_networks_generalizing_while_learning_next_to_nothing", "keywords": ["Random Networks", "Extreme Learning", "Compact Representations"], "authors": ["Amir Rosenfeld", "John K. Tsotsos"], "authorids": ["amir.rosenfeld@gmail.com", "tsotsos@cse.yorku.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582961182, "id": "ICLR.cc/2018/Workshop/-/Paper3/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper3/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper3/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper3/AnonReviewer1"], "reply": {"forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582961182}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582689833, "tcdate": 1520715919321, "number": 3, "cdate": 1520715919321, "id": "BJv9upbKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper3/Official_Review", "forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "signatures": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer1"], "content": {"title": "final review", "rating": "5: Marginally below acceptance threshold", "review": "Learning a fraction of filter weights while with other weights not updated is an interesting setting. The paper shows that such partially trained network is still able to produce reasonably well results. The authors also noted that zeroing the freezed weights would hurt the performance. I'm curious to see what happens if these un-learned weights are zeroed before training starts. If that still leads to worse results, than it means the distribution for those random weights matters. Probably the paper can further explore when and how the random weights help training even they are not trained.  Overall, I think the results in this paper are still quite preliminary, and not much conclusion can be drawn from them.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing while Learning Next to Nothing", "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most  cases, training involves iterative modification of all weights inside the network via back-propagation. In this paper, we propose to take an extreme approach and fix \\emph{almost all weights} of a deep convolutional neural network in their randomly initialized values, allowing only a small portion to be learned. As our experiments show, this often results in performance which is on par with the performance of learning all weights. The implications of this intriguing property or deep neural networks are discussed and we suggest ways to harness it to create more robust representations. ", "pdf": "/pdf/416498d17515b62b1ca9157745c60ade59d8551e.pdf", "TL;DR": "Convnets can achieve good performance even when only a fraction of parameters are learned. ", "paperhash": "rosenfeld|intriguing_properties_of_randomly_weighted_networks_generalizing_while_learning_next_to_nothing", "keywords": ["Random Networks", "Extreme Learning", "Compact Representations"], "authors": ["Amir Rosenfeld", "John K. Tsotsos"], "authorids": ["amir.rosenfeld@gmail.com", "tsotsos@cse.yorku.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582961182, "id": "ICLR.cc/2018/Workshop/-/Paper3/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper3/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper3/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper3/AnonReviewer1"], "reply": {"forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582961182}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573581532, "tcdate": 1521573581532, "number": 164, "cdate": 1521573581192, "id": "HJL0R0RKf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing while Learning Next to Nothing", "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most  cases, training involves iterative modification of all weights inside the network via back-propagation. In this paper, we propose to take an extreme approach and fix \\emph{almost all weights} of a deep convolutional neural network in their randomly initialized values, allowing only a small portion to be learned. As our experiments show, this often results in performance which is on par with the performance of learning all weights. The implications of this intriguing property or deep neural networks are discussed and we suggest ways to harness it to create more robust representations. ", "pdf": "/pdf/416498d17515b62b1ca9157745c60ade59d8551e.pdf", "TL;DR": "Convnets can achieve good performance even when only a fraction of parameters are learned. ", "paperhash": "rosenfeld|intriguing_properties_of_randomly_weighted_networks_generalizing_while_learning_next_to_nothing", "keywords": ["Random Networks", "Extreme Learning", "Compact Representations"], "authors": ["Amir Rosenfeld", "John K. Tsotsos"], "authorids": ["amir.rosenfeld@gmail.com", "tsotsos@cse.yorku.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1519530286801, "tcdate": 1519530286801, "number": 3, "cdate": 1519530286801, "id": "BJPNWnk_f", "invitation": "ICLR.cc/2018/Workshop/-/Paper3/Official_Comment", "forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "signatures": ["ICLR.cc/2018/Workshop/Paper3/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper3/Authors"], "content": {"title": "Code Available", "comment": "An preliminary version of code to reproduce the experiments is now available on GitHub.\nhttps://github.com/rosenfeldamir/pytorch-classification"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing while Learning Next to Nothing", "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most  cases, training involves iterative modification of all weights inside the network via back-propagation. In this paper, we propose to take an extreme approach and fix \\emph{almost all weights} of a deep convolutional neural network in their randomly initialized values, allowing only a small portion to be learned. As our experiments show, this often results in performance which is on par with the performance of learning all weights. The implications of this intriguing property or deep neural networks are discussed and we suggest ways to harness it to create more robust representations. ", "pdf": "/pdf/416498d17515b62b1ca9157745c60ade59d8551e.pdf", "TL;DR": "Convnets can achieve good performance even when only a fraction of parameters are learned. ", "paperhash": "rosenfeld|intriguing_properties_of_randomly_weighted_networks_generalizing_while_learning_next_to_nothing", "keywords": ["Random Networks", "Extreme Learning", "Compact Representations"], "authors": ["Amir Rosenfeld", "John K. Tsotsos"], "authorids": ["amir.rosenfeld@gmail.com", "tsotsos@cse.yorku.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450701, "id": "ICLR.cc/2018/Workshop/-/Paper3/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hy-w-2PSf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper3/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper3/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper3/Reviewers", "ICLR.cc/2018/Workshop/Paper3/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450701}}, "tauthor": "amir.rosenfeld@gmail.com"}, {"tddate": null, "ddate": null, "tmdate": 1519530048372, "tcdate": 1519529957366, "number": 2, "cdate": 1519529957366, "id": "Skayx21uM", "invitation": "ICLR.cc/2018/Workshop/-/Paper3/Official_Comment", "forum": "Hy-w-2PSf", "replyto": "S15V4Lydz", "signatures": ["ICLR.cc/2018/Workshop/Paper3/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper3/Authors"], "content": {"title": "Reply", "comment": "The authors are thankful for the comments. \nA much more detailed version, with more experiments an analysis is now available on arXiv,\nhttps://arxiv.org/abs/1802.00844 \nRegarding previous work: To which work does the reviewer refer, showing that training only the last layer is effective?\nThis was show, and to a limited extent, in the context of transfer learning, which is not quite the context here. Indeed, the experiments include attempting to train only the last layer, which results in very poor performance w.r.t many of the explored alternatives. \nAdditionally, none of the previous works have tried to train only a subset of features in each layer, let alone try to train only a middle layer, or only the first layer, as the proposed work.  \nThe exact number of parameters is not a good predictor of network performance, for example densenet vs. wide-residual networks.\nWhile the experiments did not include an exact version of what is suggested here, training smaller networks with the same number of trainable parameters, we did train on a variety of architectures, varying the number of trainable parameters in each, as well as showing that the untrained parameters are not left unused, as simply zeroing them out significantly hinders the network performance.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing while Learning Next to Nothing", "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most  cases, training involves iterative modification of all weights inside the network via back-propagation. In this paper, we propose to take an extreme approach and fix \\emph{almost all weights} of a deep convolutional neural network in their randomly initialized values, allowing only a small portion to be learned. As our experiments show, this often results in performance which is on par with the performance of learning all weights. The implications of this intriguing property or deep neural networks are discussed and we suggest ways to harness it to create more robust representations. ", "pdf": "/pdf/416498d17515b62b1ca9157745c60ade59d8551e.pdf", "TL;DR": "Convnets can achieve good performance even when only a fraction of parameters are learned. ", "paperhash": "rosenfeld|intriguing_properties_of_randomly_weighted_networks_generalizing_while_learning_next_to_nothing", "keywords": ["Random Networks", "Extreme Learning", "Compact Representations"], "authors": ["Amir Rosenfeld", "John K. Tsotsos"], "authorids": ["amir.rosenfeld@gmail.com", "tsotsos@cse.yorku.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450701, "id": "ICLR.cc/2018/Workshop/-/Paper3/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hy-w-2PSf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper3/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper3/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper3/Reviewers", "ICLR.cc/2018/Workshop/Paper3/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450701}}, "tauthor": "amir.rosenfeld@gmail.com"}, {"tddate": null, "ddate": null, "tmdate": 1519506481652, "tcdate": 1519506481652, "number": 1, "cdate": 1519506481652, "id": "S15V4Lydz", "invitation": "ICLR.cc/2018/Workshop/-/Paper3/Official_Comment", "forum": "Hy-w-2PSf", "replyto": "Hy-w-2PSf", "signatures": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper3/AnonReviewer1"], "content": {"title": "interesting finding, but incompelete experiements", "comment": "This paper shows that deep networks with a large portion of fix weights can still generalize quite well. The oberservation is quite interesting, but not that suprising. As existing work (including some of the references in this paper) has already shown that random networks with only the last layer trained are quite effective. In addition, it would be important to compare with a small network with the same number of trainable parameters. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing while Learning Next to Nothing", "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most  cases, training involves iterative modification of all weights inside the network via back-propagation. In this paper, we propose to take an extreme approach and fix \\emph{almost all weights} of a deep convolutional neural network in their randomly initialized values, allowing only a small portion to be learned. As our experiments show, this often results in performance which is on par with the performance of learning all weights. The implications of this intriguing property or deep neural networks are discussed and we suggest ways to harness it to create more robust representations. ", "pdf": "/pdf/416498d17515b62b1ca9157745c60ade59d8551e.pdf", "TL;DR": "Convnets can achieve good performance even when only a fraction of parameters are learned. ", "paperhash": "rosenfeld|intriguing_properties_of_randomly_weighted_networks_generalizing_while_learning_next_to_nothing", "keywords": ["Random Networks", "Extreme Learning", "Compact Representations"], "authors": ["Amir Rosenfeld", "John K. Tsotsos"], "authorids": ["amir.rosenfeld@gmail.com", "tsotsos@cse.yorku.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222450701, "id": "ICLR.cc/2018/Workshop/-/Paper3/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hy-w-2PSf", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper3/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper3/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper3/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper3/Reviewers", "ICLR.cc/2018/Workshop/Paper3/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222450701}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1516908888975, "tcdate": 1516908888975, "number": 3, "cdate": 1516908888975, "id": "Hy-w-2PSf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Hy-w-2PSf", "signatures": ["~Amir_Rosenfeld1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Intriguing Properties of Randomly Weighted Networks: Generalizing while Learning Next to Nothing", "abstract": "Training deep neural networks results in strong learned representations that show good generalization capabilities. In most  cases, training involves iterative modification of all weights inside the network via back-propagation. In this paper, we propose to take an extreme approach and fix \\emph{almost all weights} of a deep convolutional neural network in their randomly initialized values, allowing only a small portion to be learned. As our experiments show, this often results in performance which is on par with the performance of learning all weights. The implications of this intriguing property or deep neural networks are discussed and we suggest ways to harness it to create more robust representations. ", "pdf": "/pdf/416498d17515b62b1ca9157745c60ade59d8551e.pdf", "TL;DR": "Convnets can achieve good performance even when only a fraction of parameters are learned. ", "paperhash": "rosenfeld|intriguing_properties_of_randomly_weighted_networks_generalizing_while_learning_next_to_nothing", "keywords": ["Random Networks", "Extreme Learning", "Compact Representations"], "authors": ["Amir Rosenfeld", "John K. Tsotsos"], "authorids": ["amir.rosenfeld@gmail.com", "tsotsos@cse.yorku.ca"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 8}