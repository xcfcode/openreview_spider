{"notes": [{"id": "rJgQkT4twH", "original": "S1e7i8BSvB", "number": 295, "cdate": 1569438938792, "ddate": null, "tcdate": 1569438938792, "tmdate": 1583912035997, "tddate": null, "forum": "rJgQkT4twH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification", "authors": ["Bennet Breier", "Arno Onken"], "authorids": ["b.breier@sms.ed.ac.uk", "aonken@inf.ed.ac.uk"], "keywords": ["convolutional neural networks", "neural network transparency", "AI explainability", "deep Taylor decomposition", "supervised classification", "zebrafish", "transparency", "behavioral research", "optical flow"], "TL;DR": "We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.", "abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.", "pdf": "/pdf/dfe7f66b0eb1c5259f8fca781eab1295c4953f6e.pdf", "code": "https://github.com/Benji4/zebrafish-learning.git", "paperhash": "breier|analysis_of_video_feature_learning_in_twostream_cnns_on_the_example_of_zebrafish_swim_bout_classification", "_bibtex": "@inproceedings{\nbreier2020analysis,\ntitle={Analysis of Video Feature Learning in Two-Stream {\\{}CNN{\\}}s on the Example of Zebrafish Swim Bout Classification},\nauthor={Bennet Breier and Arno Onken},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgQkT4twH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e569c3c019df2d53bbc8c939222ead9397d58e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "intYQZqp3z", "original": null, "number": 1, "cdate": 1576798692561, "ddate": null, "tcdate": 1576798692561, "tmdate": 1576800942809, "tddate": null, "forum": "rJgQkT4twH", "replyto": "rJgQkT4twH", "invitation": "ICLR.cc/2020/Conference/Paper295/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper presents a case study of training a video classifier and subsequently analyzing the features to reduce reliance on spurious artifacts. The supervised learning task is zebrafish bout classification which is relevant for biological experiments. The paper analyzed the image support for the learned neural net features using a previously developed technique called Deep Taylor Decomposition. This analysis showed that the CNNs when applied to the raw video were relying on artifacts of the data collection process, which spuriously increased classification accuracies by a \"clever Hans\" mechanism. By identifying and removing these artifacts, a retrained CNN classifier was able to outperform an older SVM classifier. More importantly, the analysis of the network features enabled the researchers to isolate which parts of the zebrafish motion were relevant for the classification.\n\nThe reviewers found the paper to be well-written and the experiments to be well-designed. The reviewers suggested a some changes to the phrasing in the document, which the authors adopted. In response to the reviewers, the authors also clarified their use of ImageNet for pre-training and examined alternative approaches for building saliency maps.\n\nThis paper should be published as the reviewers found the paper to be a good case study of how model interpretability can be useful in practice. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification", "authors": ["Bennet Breier", "Arno Onken"], "authorids": ["b.breier@sms.ed.ac.uk", "aonken@inf.ed.ac.uk"], "keywords": ["convolutional neural networks", "neural network transparency", "AI explainability", "deep Taylor decomposition", "supervised classification", "zebrafish", "transparency", "behavioral research", "optical flow"], "TL;DR": "We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.", "abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.", "pdf": "/pdf/dfe7f66b0eb1c5259f8fca781eab1295c4953f6e.pdf", "code": "https://github.com/Benji4/zebrafish-learning.git", "paperhash": "breier|analysis_of_video_feature_learning_in_twostream_cnns_on_the_example_of_zebrafish_swim_bout_classification", "_bibtex": "@inproceedings{\nbreier2020analysis,\ntitle={Analysis of Video Feature Learning in Two-Stream {\\{}CNN{\\}}s on the Example of Zebrafish Swim Bout Classification},\nauthor={Bennet Breier and Arno Onken},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgQkT4twH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e569c3c019df2d53bbc8c939222ead9397d58e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJgQkT4twH", "replyto": "rJgQkT4twH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720461, "tmdate": 1576800271292, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper295/-/Decision"}}}, {"id": "S1edFKl_jS", "original": null, "number": 5, "cdate": 1573550463627, "ddate": null, "tcdate": 1573550463627, "tmdate": 1573550463627, "tddate": null, "forum": "rJgQkT4twH", "replyto": "Skl0dCZk5r", "invitation": "ICLR.cc/2020/Conference/Paper295/-/Official_Comment", "content": {"title": "Reply to reviewer", "comment": "Thank you for your kind review and your accurate assessment. That is exactly what we wanted to demonstrate."}, "signatures": ["ICLR.cc/2020/Conference/Paper295/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification", "authors": ["Bennet Breier", "Arno Onken"], "authorids": ["b.breier@sms.ed.ac.uk", "aonken@inf.ed.ac.uk"], "keywords": ["convolutional neural networks", "neural network transparency", "AI explainability", "deep Taylor decomposition", "supervised classification", "zebrafish", "transparency", "behavioral research", "optical flow"], "TL;DR": "We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.", "abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.", "pdf": "/pdf/dfe7f66b0eb1c5259f8fca781eab1295c4953f6e.pdf", "code": "https://github.com/Benji4/zebrafish-learning.git", "paperhash": "breier|analysis_of_video_feature_learning_in_twostream_cnns_on_the_example_of_zebrafish_swim_bout_classification", "_bibtex": "@inproceedings{\nbreier2020analysis,\ntitle={Analysis of Video Feature Learning in Two-Stream {\\{}CNN{\\}}s on the Example of Zebrafish Swim Bout Classification},\nauthor={Bennet Breier and Arno Onken},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgQkT4twH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e569c3c019df2d53bbc8c939222ead9397d58e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgQkT4twH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference/Paper295/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper295/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper295/Reviewers", "ICLR.cc/2020/Conference/Paper295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper295/Authors|ICLR.cc/2020/Conference/Paper295/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173526, "tmdate": 1576860528556, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference/Paper295/Reviewers", "ICLR.cc/2020/Conference/Paper295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper295/-/Official_Comment"}}}, {"id": "HJlYwYedsS", "original": null, "number": 4, "cdate": 1573550432543, "ddate": null, "tcdate": 1573550432543, "tmdate": 1573550432543, "tddate": null, "forum": "rJgQkT4twH", "replyto": "r1gLu9xO5S", "invitation": "ICLR.cc/2020/Conference/Paper295/-/Official_Comment", "content": {"title": "Reply to reviewer", "comment": "Thank you for your detailed review and for pointing out issues that need further clarification. We hope our revisions will address your concerns.\n\nWe revised problematic wording in the manuscript. Terms like \"black box\" and \"cheating\" were unnecessary exaggerations which we have now replaced with expressions such as \"hidden if not further analyzed\" and \"found a shortcut\" that better reflect our intentions.\n\nFurthermore as you suggested, we added a justification for why we used pre-trained weights from ImageNet as initialization values for our network and clarified the description of our initialization strategy:\n\"We initialized both streams with weights pre-trained on ImageNet\\footnote{http://www.vlfeat.org/matconvnet/models/imagenet-vgg-m-2048.mat}. This has become a common initialization strategy in various classification tasks (\\cite{Shin2016, Lee2016, VanHorn2017}), due to the utility of general features learnt from natural images, such as edge detection. While \\cite{Simonyan2014} did not pre-train flow, \\cite{Carreira2017} found a pre-trained temporal stream to reach superior performance. With this initialization we hoped that training would require only fine-tuning and therefore less data and fewer epochs.\"\n\nAs you correctly pointed out, it is common practice to train a 2-neuron output layer, which is exactly what we did. We did not average the outputs but the weights of the 500 output units in order to initialize the weights of 2 neurons. We clarified this in the manuscript:\n\"Regarding outputs, we averaged the pre-trained weights of 500 units on the output layer to obtain the weights of two output neurons, because we dealt with 2 classes instead of 1,000 as in ImageNet.\"\n\nFinally, to address your concern regarding additional explainability techniques besides DTD, we have now created saliency maps as well as heatmaps from Guided BackProp. We found that these other two techniques allow similar insights, although slightly fuzzier. Importantly, these techniques also uncover the \"Clever Hans\" prediction. We now briefly mention this additional analysis in the main text and added a section with additional figures to the Appendix."}, "signatures": ["ICLR.cc/2020/Conference/Paper295/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification", "authors": ["Bennet Breier", "Arno Onken"], "authorids": ["b.breier@sms.ed.ac.uk", "aonken@inf.ed.ac.uk"], "keywords": ["convolutional neural networks", "neural network transparency", "AI explainability", "deep Taylor decomposition", "supervised classification", "zebrafish", "transparency", "behavioral research", "optical flow"], "TL;DR": "We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.", "abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.", "pdf": "/pdf/dfe7f66b0eb1c5259f8fca781eab1295c4953f6e.pdf", "code": "https://github.com/Benji4/zebrafish-learning.git", "paperhash": "breier|analysis_of_video_feature_learning_in_twostream_cnns_on_the_example_of_zebrafish_swim_bout_classification", "_bibtex": "@inproceedings{\nbreier2020analysis,\ntitle={Analysis of Video Feature Learning in Two-Stream {\\{}CNN{\\}}s on the Example of Zebrafish Swim Bout Classification},\nauthor={Bennet Breier and Arno Onken},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgQkT4twH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e569c3c019df2d53bbc8c939222ead9397d58e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgQkT4twH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference/Paper295/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper295/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper295/Reviewers", "ICLR.cc/2020/Conference/Paper295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper295/Authors|ICLR.cc/2020/Conference/Paper295/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173526, "tmdate": 1576860528556, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference/Paper295/Reviewers", "ICLR.cc/2020/Conference/Paper295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper295/-/Official_Comment"}}}, {"id": "ryxPpdeusr", "original": null, "number": 3, "cdate": 1573550270638, "ddate": null, "tcdate": 1573550270638, "tmdate": 1573550270638, "tddate": null, "forum": "rJgQkT4twH", "replyto": "SkxFdq2Y9r", "invitation": "ICLR.cc/2020/Conference/Paper295/-/Official_Comment", "content": {"title": "Reply to reviewer", "comment": "Thank you for your concise and insightful review.\n\nYour concern regarding the appropriateness of ICLR as a venue for publication is very understandable considering that the paper is built around behavioral zebrafish research. Nevertheless, our work supports the conference in exploring relevant topics around supervised representation learning, visualization and interpretation of learned representations, and applications in neuroscience. This is in line with the non-exhaustive list of subject areas in the ICLR Call for Papers at https://iclr.cc/Conferences/2020/CallForPapers . This list includes:\n- unsupervised, semi-supervised, and supervised representation learning   \n- visualization or interpretation of learned representations\n- applications in vision, audio, speech, natural language processing, robotics,\n- neuroscience, computational biology, or any other field\n\nAs a matter of reference, here is a small selection of comparable papers that were accepted at ICLR in previous years:\n\nhttps://openreview.net/forum?id=HJvvRoe0W\nThe paper shows the benefits of a novel pre-processing technique for a new CNN architecture specializing in predicting the folding structure of DNA molecules.\n\nhttps://openreview.net/forum?id=ryl5khRcKm\nThe authors present a tailor-made CNN architecture in the application domain of protein localization, which outperforms incumbent architectures as well as domain experts.\n\nhttps://openreview.net/forum?id=HJtEm4p6Z\nThe presented model improves upon state-of-the-art text-to-speech systems by allowing more efficient training. The authors further tackle common problems of the synthesis task.\n\nConformably, our paper validates some of the most recent explainability techniques for CNNs in the domain of behavioral biology and demonstrates their value. Furthermore, it details and justifies the approach taken, which can be very valuable for practitioners in other fields. For these reasons, we believe ICLR to be a suitable venue for the publication of our paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper295/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification", "authors": ["Bennet Breier", "Arno Onken"], "authorids": ["b.breier@sms.ed.ac.uk", "aonken@inf.ed.ac.uk"], "keywords": ["convolutional neural networks", "neural network transparency", "AI explainability", "deep Taylor decomposition", "supervised classification", "zebrafish", "transparency", "behavioral research", "optical flow"], "TL;DR": "We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.", "abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.", "pdf": "/pdf/dfe7f66b0eb1c5259f8fca781eab1295c4953f6e.pdf", "code": "https://github.com/Benji4/zebrafish-learning.git", "paperhash": "breier|analysis_of_video_feature_learning_in_twostream_cnns_on_the_example_of_zebrafish_swim_bout_classification", "_bibtex": "@inproceedings{\nbreier2020analysis,\ntitle={Analysis of Video Feature Learning in Two-Stream {\\{}CNN{\\}}s on the Example of Zebrafish Swim Bout Classification},\nauthor={Bennet Breier and Arno Onken},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgQkT4twH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e569c3c019df2d53bbc8c939222ead9397d58e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJgQkT4twH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference/Paper295/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper295/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper295/Reviewers", "ICLR.cc/2020/Conference/Paper295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper295/Authors|ICLR.cc/2020/Conference/Paper295/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173526, "tmdate": 1576860528556, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper295/Authors", "ICLR.cc/2020/Conference/Paper295/Reviewers", "ICLR.cc/2020/Conference/Paper295/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper295/-/Official_Comment"}}}, {"id": "Skl0dCZk5r", "original": null, "number": 1, "cdate": 1571917430334, "ddate": null, "tcdate": 1571917430334, "tmdate": 1572972613503, "tddate": null, "forum": "rJgQkT4twH", "replyto": "rJgQkT4twH", "invitation": "ICLR.cc/2020/Conference/Paper295/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "the paper uses model interpretation techniques to understand blackbox CNN fit of zebrafish videos. they show that, relying on the technique of deep taylor decomposition, their CNN relies its prediction on a different part of zebra fish than existing understanding. it is also able to detect the use of experimental artifacts, whose removal improves predictive performance. \n\nthe idea of a case study about the usefulness of model interpretation techniques is interesting. while the experimental studies rely on our belief that the interpretation technique indeed interprets, the result that removing experimental features and improving predictive performance is convincing and interesting. it illustrates how model interpretability and human intuition and domain knowledge can be useful."}, "signatures": ["ICLR.cc/2020/Conference/Paper295/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper295/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification", "authors": ["Bennet Breier", "Arno Onken"], "authorids": ["b.breier@sms.ed.ac.uk", "aonken@inf.ed.ac.uk"], "keywords": ["convolutional neural networks", "neural network transparency", "AI explainability", "deep Taylor decomposition", "supervised classification", "zebrafish", "transparency", "behavioral research", "optical flow"], "TL;DR": "We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.", "abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.", "pdf": "/pdf/dfe7f66b0eb1c5259f8fca781eab1295c4953f6e.pdf", "code": "https://github.com/Benji4/zebrafish-learning.git", "paperhash": "breier|analysis_of_video_feature_learning_in_twostream_cnns_on_the_example_of_zebrafish_swim_bout_classification", "_bibtex": "@inproceedings{\nbreier2020analysis,\ntitle={Analysis of Video Feature Learning in Two-Stream {\\{}CNN{\\}}s on the Example of Zebrafish Swim Bout Classification},\nauthor={Bennet Breier and Arno Onken},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgQkT4twH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e569c3c019df2d53bbc8c939222ead9397d58e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgQkT4twH", "replyto": "rJgQkT4twH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575663309107, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper295/Reviewers"], "noninvitees": [], "tcdate": 1570237754204, "tmdate": 1575663309122, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper295/-/Official_Review"}}}, {"id": "r1gLu9xO5S", "original": null, "number": 2, "cdate": 1572502125590, "ddate": null, "tcdate": 1572502125590, "tmdate": 1572972613458, "tddate": null, "forum": "rJgQkT4twH", "replyto": "rJgQkT4twH", "invitation": "ICLR.cc/2020/Conference/Paper295/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "SUMMARY: explore the use of CNN in a binary task on images of zebrafish\n\nIt is important to note that researchers in the field of AI and deep learning are themselves aware of the fallacies of deep learning, and are striving everyday to overcome these themselves. The hype over deep learning has caused certain disdain among a section of the research community over the workings of deep neural networks. This is evident in this paper with the authors calling CNNs \"black box\" and the learnings of a neural network \"cheating\". Perhaps the authors are not aware that CNNs are hardly black boxes, their inner workings quite transparent in mathematical terms, which the submitted paper itself explores. Perhaps the authors are also not aware that the fallacies that causes CNNs to overfit on some characteristics in the input data are also present in other machine learning tools such as SVMs. Perhaps the intention of the authors is to bring more relevance to the dangers of spurious correlations, especially when the applications are critical. I hope the community can work together in improving the state of the art while improving transparency and explainability.\n\nBACKGROUND:\nHypothesis: Prey movements in zebrafish are characterized by specific motions, that are triggered by a specific pathway involving an area called AF7.\nValidation: Semmelhack et. al. (2014) removed the AF7 neuropil and observed that they failed to respond to prey stimuli\nAI: the prey stimuli was a characteristic movement that an SVM was trained to detect.\n\nGood to know that the authors will share their code.\n\nIt is not explained why the authors chose to pretrain on ImageNet, since ImageNet does not have any image classes that are comparable to the dataset the authors use. They then proceed to do a hyperparameter sweep to fine-tune on their dataset.\n\nIt is also not explained why they chose to average outputs of 500 output nodes to get two outputs, instead of simply replacing the last layer with a 2-neuron layer and finetune, as is general practice.\n\nThe level of detail in the training procedure is very helpful to reproduce the setting as well as establish a reference for any future work in this direction. This itself is a notable achievement and a good use of significant research time. Furthermore, the authors conduct one good analysis (DTD) to explain the results of their CNN. The conclusion has many repeated points from previous sections, but it is a good summary.\n\nGiven their premise about explainability in machine learning, perhaps more significance was to be given to DTD, and other methods also performed to check if the results coincide with those from DTD."}, "signatures": ["ICLR.cc/2020/Conference/Paper295/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper295/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification", "authors": ["Bennet Breier", "Arno Onken"], "authorids": ["b.breier@sms.ed.ac.uk", "aonken@inf.ed.ac.uk"], "keywords": ["convolutional neural networks", "neural network transparency", "AI explainability", "deep Taylor decomposition", "supervised classification", "zebrafish", "transparency", "behavioral research", "optical flow"], "TL;DR": "We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.", "abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.", "pdf": "/pdf/dfe7f66b0eb1c5259f8fca781eab1295c4953f6e.pdf", "code": "https://github.com/Benji4/zebrafish-learning.git", "paperhash": "breier|analysis_of_video_feature_learning_in_twostream_cnns_on_the_example_of_zebrafish_swim_bout_classification", "_bibtex": "@inproceedings{\nbreier2020analysis,\ntitle={Analysis of Video Feature Learning in Two-Stream {\\{}CNN{\\}}s on the Example of Zebrafish Swim Bout Classification},\nauthor={Bennet Breier and Arno Onken},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgQkT4twH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e569c3c019df2d53bbc8c939222ead9397d58e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgQkT4twH", "replyto": "rJgQkT4twH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575663309107, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper295/Reviewers"], "noninvitees": [], "tcdate": 1570237754204, "tmdate": 1575663309122, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper295/-/Official_Review"}}}, {"id": "SkxFdq2Y9r", "original": null, "number": 3, "cdate": 1572616817126, "ddate": null, "tcdate": 1572616817126, "tmdate": 1572972613417, "tddate": null, "forum": "rJgQkT4twH", "replyto": "rJgQkT4twH", "invitation": "ICLR.cc/2020/Conference/Paper295/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper presents a case study of training a video classifier using convolutional networks, and on how the learned features related to previous, hand-designed ones. The particular domain considered is of importance for biologists/medical doctors/neuroscientists: zebra fish swim bout classification.\n\nIn order to identify which particular features the neural networks are paying attention to, the paper used Deep Taylor Decomposition, which allowed the authors to identify \"clever-hans\"-type phenomena (network attending to meaningless experimental setup differences that actually gave a way the ground truth classes). This allowed for the authors to mask out such features and make the network attend to more meaningful ones. In particular observations like \" looking for salient features in the trunk of the tail while largely disregarding the tip\", are typically absent from most deep learning studies and it's quite interesting.\n\nOverall the paper is well written, the experiments are well designed; everything seems very rigorous and  well executed. It makes for a very good quality practitioner-level case study on video understanding, which may also be useful for people studying zebra fish or related simple life forms. My main concern with the paper is whether ICLR is an appropriate venue, as it does not provide pure machine learning contributions in the form of new techniques of generally applicable insights. "}, "signatures": ["ICLR.cc/2020/Conference/Paper295/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper295/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Analysis of Video Feature Learning in Two-Stream CNNs on the Example of Zebrafish Swim Bout Classification", "authors": ["Bennet Breier", "Arno Onken"], "authorids": ["b.breier@sms.ed.ac.uk", "aonken@inf.ed.ac.uk"], "keywords": ["convolutional neural networks", "neural network transparency", "AI explainability", "deep Taylor decomposition", "supervised classification", "zebrafish", "transparency", "behavioral research", "optical flow"], "TL;DR": "We demonstrate the utility of a recent AI explainability technique by visualizing the learned features of a CNN trained on binary classification of zebrafish movements.", "abstract": "Semmelhack et al. (2014) have achieved high classification accuracy in distinguishing swim bouts of zebrafish using a Support Vector Machine (SVM). Convolutional Neural Networks (CNNs) have reached superior performance in various image recognition tasks over SVMs, but these powerful networks remain a black box. Reaching better transparency helps to build trust in their classifications and makes learned features interpretable to experts. Using a recently developed technique called Deep Taylor Decomposition, we generated heatmaps to highlight input regions of high relevance for predictions. We find that our CNN makes predictions by analyzing the steadiness of the tail's trunk, which markedly differs from the manually extracted features used by Semmelhack et al. (2014). We further uncovered that the network paid attention to experimental artifacts. Removing these artifacts ensured the validity of predictions. After correction, our best CNN beats the SVM by 6.12%, achieving a classification accuracy of 96.32%. Our work thus demonstrates the utility of AI explainability for CNNs.", "pdf": "/pdf/dfe7f66b0eb1c5259f8fca781eab1295c4953f6e.pdf", "code": "https://github.com/Benji4/zebrafish-learning.git", "paperhash": "breier|analysis_of_video_feature_learning_in_twostream_cnns_on_the_example_of_zebrafish_swim_bout_classification", "_bibtex": "@inproceedings{\nbreier2020analysis,\ntitle={Analysis of Video Feature Learning in Two-Stream {\\{}CNN{\\}}s on the Example of Zebrafish Swim Bout Classification},\nauthor={Bennet Breier and Arno Onken},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rJgQkT4twH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/e569c3c019df2d53bbc8c939222ead9397d58e79.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJgQkT4twH", "replyto": "rJgQkT4twH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575663309107, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper295/Reviewers"], "noninvitees": [], "tcdate": 1570237754204, "tmdate": 1575663309122, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper295/-/Official_Review"}}}], "count": 8}