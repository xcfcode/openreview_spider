{"notes": [{"id": "ryxDUs05KQ", "original": "HJlb1wAOYm", "number": 185, "cdate": 1538087759372, "ddate": null, "tcdate": 1538087759372, "tmdate": 1545355412243, "tddate": null, "forum": "ryxDUs05KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkeFF53R1E", "original": null, "number": 1, "cdate": 1544632960765, "ddate": null, "tcdate": 1544632960765, "tmdate": 1545354502196, "tddate": null, "forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper185/Meta_Review", "content": {"metareview": "The paper presents a GAN for learning a target distribution that is defined as the difference between two other distributions.\n\nThe reviewers and AC note the critical limitation of novelty and appealing results of this paper to meet the high standard of ICLR. \n\nAC thinks the proposed method has potential and is interesting, but decided that the authors need more works to publish.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Lack of novelty "}, "signatures": ["ICLR.cc/2019/Conference/Paper185/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper185/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper185/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353305304, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper185/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper185/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper185/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353305304}}}, {"id": "rklu4Xex0m", "original": null, "number": 1, "cdate": 1542615856282, "ddate": null, "tcdate": 1542615856282, "tmdate": 1542695139998, "tddate": null, "forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper185/Official_Comment", "content": {"title": "We upload the revised manuscript .", "comment": "Thanks for all the comments.\n\nWe list all the modifications as following:\n\n1. We add Sec. 1.1 to describe our motivation and scenario.\n\n2. We reorganize Sec. 2.2 to clarify the main idea of DSGAN through several case studies.\n\n3. We move Sec. 3.1 in the original manuscript to appendix C in the new one.\n\n4. We rephrase Sec. 5.1 for better understanding. Moreover, we replace Tables 4 and 5 in the original manuscript with Figs. 7, 9 and 10 in the new one for more comprehensive evaluation.\n\n5. We have more discussion to our model in Sec. 5.1.2 and Sec. 5.2.2.\n\n6. We add Sec. 6 to discuss related works."}, "signatures": ["ICLR.cc/2019/Conference/Paper185/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper185/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper185/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618015, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxDUs05KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference/Paper185/Reviewers", "ICLR.cc/2019/Conference/Paper185/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper185/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper185/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper185/Authors|ICLR.cc/2019/Conference/Paper185/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper185/Reviewers", "ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference/Paper185/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618015}}}, {"id": "SyxWewllA7", "original": null, "number": 4, "cdate": 1542616808955, "ddate": null, "tcdate": 1542616808955, "tmdate": 1542616808955, "tddate": null, "forum": "ryxDUs05KQ", "replyto": "BkxL26einQ", "invitation": "ICLR.cc/2019/Conference/-/Paper185/Official_Comment", "content": {"title": "Thanks for your review.", "comment": "We revise the manuscript to answer the comments.\n\n\n>> \u201cSome statements in the introduction felt quite arbitrary. For example, the authors state that PixelCNN \"does not have a latent representation\" in a manner that makes it sound as if that is a bad thing. If indeed it is, then why so? It would be very helpful to motivate the setting more and to provide a couple of examples of where this method would be useful, in the introduction.\u201d\n>> \u201cWhen is this method useful to readers? For solving which problems and under what conditions? And also, when is this method bad and should not be used?\u201d\n\nWe rephrase the first paragraph in original paragraph to delete some discussions which may be out-of-scope such as PixelCNN and VAE in this paper. We also add a subsection Sec. 1.1 to emphasize our motivations along with the scenario, where the proposed DSGAN will have the ability to generate diverse unseen data that is helpful for one-class classification, semi-supervised learning, and adversarial attacks. Those techniques are important and involve many real applications shown in Fig. 8 in the revised manuscript. \n\n\n>> \u201cThe theoretical results of section 3 are just stated/listed, but are not connected to algorithm 1. Please connect them to the different parts of the algorithm and state in a couple sentences what they imply for the algorithm.\u201d \n\nThe theoretical results in Sec. 3 aims to explain why the objective function in Eq. (2) can learn the wanted distribution. But, Algorithm 1 is to explain the training procedure which may be not directly related to theoretical results. \n\n\n>>\u201dYou should mention in the caption of table 4, what quantity you are computing\u201d \n\nWe replace table 4 (in the original manuscript) with Fig. 7 (in the revised manuscript) for the comprehensive evaluation on both robustness and performance of the models. And the quantity we computed as x-axis (same as the quantity in original table 4) is \u201cepsilon\u201d, which is the l_2 (or l_inf) norm between original image and corresponding adversarial example. The description of epsilon is also added in the caption of Fig. 7.\n\n\n>> \u201cSection 5.1 is hard to follow and I don't quite get how it connects to the rest.\u201d\n\nWe rephrase the descriptions of Sec 5.1 and hope it is more understandable. Briefly saying, Dai et al. (2017) claim that if the generator can satisfy the conditions Eq. (8) and Eq. (9), it can help the semi-supervised learning through the objective function Eq. (5). In our method, if we set the p_{\\bar{d}} to the linear combination of labeled and unlabeled data, then the generator can learn the distribution which satisfies both Eq. (8) and Eq. (9). We use this generator to help semi-supervised learning further.\n\n\n>> \u201cAlso, in section 5.1.2 you mention that in comparison to Dai et al. (2017) your method does not need to rely on an additional density estimation network. Even if that is true, I cannot see how it is a useful remark given that the method of Dai et al. seems to always beat your method.\u201d\n\nFor comparison with Dai et al. (2017) in Sec. 5.1.2, additional density estimation network may not exist always and be expensive to train (also discussed in Lee et al. (2018)). In the contrast, DSGAN is a simple and effective way for generating complementary data. \nWe also claim that the generator might not perfectly learn the distribution we assigned, however, this problem will be attenuated since there are a number of researches on solving this problem.  In Dai et al. (2017), their method relies on feature matching, which aim to match the first-order statistics of the distribution, and it cannot take all the advantages of the progress of GANs. Due to this reason, our method has more benefits in the long term.  The discussion in detail is also in Sec. 5.1.2.\n\n\n>> \u201cIn figure 1, no labels or legends are provided making it hard to figure out what's going on at a glance. It would be very helpful to include labels and a legend.\u201d\n\nWe add the legends to those figures and put more illustrations for our idea. Hope those are helpful for understanding our methods.\n\n\n>>The reformulation of section 3.1 is never justified. What led you to use this reformulation and why do you think it is more stable in practice?\n\nFrom the view of theory, the reformulation is equivalent to original formula. The original reason is that we can use 2m samples instead of 3m samples for each mini-batch. However, we found that the performance also becomes better. Thus, we conjecture that the equivalence may be based on the linearity of expectation, but mini-batch stochastic gradient descent in practical training may lead to the different outcomes. Due to the limit of space, we move Sec. 3.1 (in the original manuscript) to Appendix C (in the revised manuscript).\n\n\n>>Right after theorem 1, which assumption are you referring to when you say \"the assumption in Theorem 1\"?\n\nWe rephrase this sentence \u201cthe assumption in Theorem 1\u201d as \"The assumption, \u03b1 p_{d}(x) \u2264 p_{\\bar{d}}(x) for all x\u2019s in Theorem 1\".\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper185/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper185/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper185/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618015, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxDUs05KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference/Paper185/Reviewers", "ICLR.cc/2019/Conference/Paper185/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper185/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper185/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper185/Authors|ICLR.cc/2019/Conference/Paper185/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper185/Reviewers", "ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference/Paper185/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618015}}}, {"id": "rJlhD4gxAm", "original": null, "number": 3, "cdate": 1542616164435, "ddate": null, "tcdate": 1542616164435, "tmdate": 1542616164435, "tddate": null, "forum": "ryxDUs05KQ", "replyto": "HJlU9T4ph7", "invitation": "ICLR.cc/2019/Conference/-/Paper185/Official_Comment", "content": {"title": "Thanks for your review.", "comment": "We revise the manuscript to answer the comments.  \n\n\n>> \u201cThe main problem of this paper is that it does not provide sufficient results on any real applications that can support its problem&model formulations. For example, in which scenarios would the users of the model need to train a GAN to mimic a target distribution p_t which is a difference of another two distributions (with examples available there but unavailable in p_t)? It would be good to show significant results on real applications to show the problem and the method useful.\u201d \n\u201cTwo applications on semi-supervised classification and adversarial training are discussed. While both seem to be very artificial IMO if considering the used dataset and designed experiments. The results are also not convincing even for the shown two experiments compared to baselines.\u201d\n\nFor responding the suggestions, we add a subsection Sec. 1.1 to emphasize our motivations. Our goal is to generate unseen data, which is absent during training process and is difficult to collect. We show that the proposed DSGAN will have the ability to generate diverse unseen data that is helpful for one-class classification, semi-supervised learning, and adversarial attacks. Those techniques are important and involve many real applications shown in Fig. 8 in the revised manuscript.  \n\n\n>>\u201dNo related works on addressing the similar problems have been discussed nor compared in experiments.\u201d\n\nWe also add Sec. 6 in the revised manuscript to discuss related works about generating unseen data. In sum, DSGAN is a simple and effective way for our objective. The results compared with other methods are discussed in Sec. 5.1.2 and 5.2.2 corresponding to the two experiments.\n\n\n>> \u201cWhile the authors claim new theoretical results, in fact, I didn't see any contributions here as the theories developed in section 3 are mostly rather straightforward. There have been some similar theories being developed in previous papers where a component in GAN exhibits mixture-modeled forms, such as in TripleGan (Li et al. NIPS'17). So I would not recommend the authors to claim contributions here.\u201d\n\nAbout the contribution of theoretical proofs (in Sec. 1.2 in the revised manuscript), we remove the sentences \u201cwhen the optimum of the Jensen-Shannon divergence is attained such that the generator distribution is equal to the target distribution,\u201d which has been covered under [Goodfellow et al. 14]. Instead we show that the proposed DSGAN can learn the distribution which support set is difference of support sets of two distributions shown in Proposition 2. \n\n\n>> The paper does not seem to be polished. It may not be necessary to exceed 8 pages as many spaces in this paper could be easily squeezed (apparently). The organization could be better; Some parts are vague and difficult to understand; the writing could be improved to be more clearly demonstrate the contributions of this paper. \n\nConsequently, in the revised manuscript, we reorganize some sections and rephrase some sentences for better understandings."}, "signatures": ["ICLR.cc/2019/Conference/Paper185/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper185/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper185/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618015, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxDUs05KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference/Paper185/Reviewers", "ICLR.cc/2019/Conference/Paper185/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper185/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper185/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper185/Authors|ICLR.cc/2019/Conference/Paper185/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper185/Reviewers", "ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference/Paper185/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618015}}}, {"id": "BkgHQNggRQ", "original": null, "number": 2, "cdate": 1542616092628, "ddate": null, "tcdate": 1542616092628, "tmdate": 1542616092628, "tddate": null, "forum": "ryxDUs05KQ", "replyto": "SkeKjN28aQ", "invitation": "ICLR.cc/2019/Conference/-/Paper185/Official_Comment", "content": {"title": "Thanks for your review.", "comment": "We revise the manuscript to answer the comments.\n\n>> \u201cThe proposed model is quite straightforward in its formulation, and since the paper is not addressing the importance of, or any challenges with the problem they are trying to solve, the contribution of this work appears minor.\u201d \n>> \u201cThe motivation is very unclear when reading the introduction section, and Figure 1 does not do a good job of providing it. \u201c\n\nWe add a subsection Sec. 1.1 to emphasize our motivations. Our goal is to generate unseen data, which is absent during training process and is difficult to collect. We show that the proposed DSGAN will have the ability to generate diverse unseen data that is helpful for one-class classification, semi-supervised learning, and adversarial attacks. Those techniques are important and involve many real applications shown in Fig. 8 in the revised manuscript.  \n\n\n>> \u201cThe authors list theoretical results as contributions, but they are rather straightforward replacement of the p_d and p_g terms in the original theorems on optimality in [Goodfellow et al. 14] with the target distributions in this paper, that has nothing to do with what the authors claim in this paper. Thus they add nothing to the value of the paper.\u201d\n\nWe rephrase the contribution about theoretical proofs (in Sec. 1.2 in the revised manuscript).  We remove the sentences \u201cwhen the optimum of the Jensen-Shannon divergence is attained such that the generator distribution is equal to the target distribution,\u201d which has been covered under [Goodfellow et al. 14]. Instead we show that the proposed DSGAN can learn the distribution which support set is difference of support sets of two distributions shown in Proposition 2. \n\n\n>> \u201cThe experimental validation is lacking in many aspects. I think the main results should show that the difference-seeking GAN can learn distributional differences but the authors jump straight to the applications. Also, the current experimental section simply reports performances on the two tasks, without much analysis showing why it works well and how it works differently from other models. \u201c\n\nWe rearrange the figures, where Figs. 2-6 in the revised manuscript illustrate that DSGAN learn distributional difference on both toy-data and MNIST. We also add more discussions to compare our method with other methods in Sec 5.1.2 and Sec 5.2.2, and emphasize the advantages in DSGAN in the last paragraph of Sec. 5.1.2. "}, "signatures": ["ICLR.cc/2019/Conference/Paper185/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper185/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper185/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618015, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryxDUs05KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference/Paper185/Reviewers", "ICLR.cc/2019/Conference/Paper185/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper185/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper185/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper185/Authors|ICLR.cc/2019/Conference/Paper185/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper185/Reviewers", "ICLR.cc/2019/Conference/Paper185/Authors", "ICLR.cc/2019/Conference/Paper185/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618015}}}, {"id": "SkeKjN28aQ", "original": null, "number": 3, "cdate": 1542010017022, "ddate": null, "tcdate": 1542010017022, "tmdate": 1542010017022, "tddate": null, "forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper185/Official_Review", "content": {"title": "Interesting but straightforward idea that needs more development.", "review": "The paper presents DS-GAN, which aims to learn the difference between any two distributions whose samples are difficult or impossible to collect. To this end they simply model the target distribution such that adding it to one of the distribution results in another, and propose a min-max objective based on it. To show the effectiveness of the proposed DS-GAN, the authors validate it on semi-supervised learning and adversarial training tasks, on which it performs reasonably well in generating the difference between the two distributions. \n\nPros\n- The idea of learning the difference between two distributions is novel to my knowledge. Similar ideas have been explored in prior work such as [Li et al. 17] but are not doing exactly what the authors try to do. \n- The proposed method works reasonably well on semi-supervised learning and adversarial learning tasks, and thus it seems practically useful. \n\nCons\n- The proposed model is quite straightforward in its formulation, and since the paper is not addressing the importance of, or any challenges with the problem they are trying to solve, the contribution of this work appears minor. \n- The authors list theoretical results as contributions, but they are rather straightforward replacement of the p_d and p_g terms in the original theorems on optimality in [Goodfellow et al. 14] with the target distributions in this paper, that has nothing to do with what the authors claim in this paper.  Thus they add nothing to the value of the paper.\n- The motivation is very unclear when reading the introduction section, and Figure 1 does not do a good job of providing it.  \n- The experimental validation is lacking in many aspects. I think the main results should show that the difference-seeking GAN can learn distributional differences but the authors jump straight to the applications. Also, the current experimental section simply reports performances on the two tasks, without much analysis showing why it works well and how it works differently from other models. \n\nIn sum, the idea seems nice and interesting but the model is straightforward and the current results are very weak in analysis in order to make a good paper. I would recommend the authors to perform further analysis of the model either theoretically or experimentally.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper185/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper185/Official_Review", "cdate": 1542234519778, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper185/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335667255, "tmdate": 1552335667255, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper185/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlU9T4ph7", "original": null, "number": 2, "cdate": 1541389709951, "ddate": null, "tcdate": 1541389709951, "tmdate": 1541534213452, "tddate": null, "forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper185/Official_Review", "content": {"title": "An interesting problem formulation, but the method doesn't seem innovative and the empirical results not very convincing", "review": "- Summary:\nThis paper considers the problem of learning a GAN to capture a target distribution p_t with only very few training samples from p_t available.\n\n- Good\nAn interesting problem formulation. \nThe proposed approach is not new, but seems to be a sensible and simple solution to the problem formulated in this paper. I would see the contributions of the paper: (1) an interesting problem formulation on how to learn p_t (with a few assumptions) (2) a sensible adaptation of GANs on this problem (with minor modifications to GANs which have been observed/adopted in many GAN literatures in the last two years)\nThe training appaoches/tricks are rather straightforward and not new as well.\n\n- Suggestions\nThe main problem of this paper is that it does not provide sufficient results on any real applications that can support its problem&model formulations. \nFor example, in which scenarios would the users of the model need to train a GAN to mimic a target distribution p_t which is a difference of another two distributions (with examples available there but unavailable in p_t)? It would be good to show significant results on real applications to show the problem and the method useful.\n\nTwo applications on semi-supervised classification and adversarial training are discussed. While both seem to be very artificial IMO if considering the used dataset and designed experiments. The results are also not convincing even for the shown two experiments compared to baselines.\n\nNo related works on addressing the similar problems have been discussed nor compared in experiments.\n\n- Theoretical results:\nWhile the authors claim new theoretical results, in fact, I didn't see any contributions here as the theories developed in section 3 are mostly rather straightforward. There have been some similar theories being developed in previous papers where a component in GAN exhibits mixture-modeled forms, such as in TripleGan (Li et al. NIPS'17). So I would not recommend the authors to claim contributions here.\n\n- Writing:\nThe paper does not seem to be polished. It may not be necessary to exceed 8 pages as many spaces in this paper could be easily squeezed (apparently). The organization could be better; Some parts are vague and difficult to understand;  the writing could be improved to be more clearly demonstrate the contributions of this paper. ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper185/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper185/Official_Review", "cdate": 1542234519778, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper185/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335667255, "tmdate": 1552335667255, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper185/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkxL26einQ", "original": null, "number": 1, "cdate": 1541242285769, "ddate": null, "tcdate": 1541242285769, "tmdate": 1541534213172, "tddate": null, "forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper185/Official_Review", "content": {"title": "Potentially Interesting Method but the Paper Needs Polishing", "review": "The paper presents a new Generative Adversarial Network (GAN) for learning a  \ntarget distribution that is defined as the difference between two other \ndistributions. Applications in semi-supervised learning and adversarial training \nare considered in the experimental evaluation and results are presented in \ncomputer vision tasks. \n\nThe paper is not very well written and can be hard to follow. One very important \nissue for me was motivation for defining the target distribution as a difference \nbetween two other distributions. I am not familiar with this area, but reading \nthrough the introduction it was never clear to me why this is a useful scenario, \nin practice. Furthermore, some statements in the introduction felt quite \narbitrary. For example, the authors state that PixelCNN \"does not have a latent \nrepresentation\" in a manner that makes it sound as if that is a bad thing. If \nindeed it is, then why so? It would be very helpful to motivate the setting more \nand to provide a couple of examples of where this method would be useful, in the \nintroduction. Also, regarding the MNIST example in the end of page 1, what is \nthe \"universal set\"? This paragraph also felt a bit arbitrary and unclear.\n\nSome comments about the rest of the paper:\n  - The theoretical results of section 3 are just stated/listed, but are not \n    connected to algorithm 1. Please connect them to the different parts of the \n    algorithm and state in a couple sentences what they imply for the algorithm.\n  - Right after theorem 1, which assumption are you referring to when you say \n    \"the assumption in Theorem 1\"?\n  - The reformulation of section 3.1 is never justified. What led you to use \n    this reformulation and why do you think it is more stable in practice?\n  - You should mention in the caption of table 4, what quantity you are \n    computing.\n\nNote that my evaluation for this paper is based mainly on the way it is written \nas, in its current state, it is hard for me to judge what is novel and what is \nuseful, and what readers are supposed to take in by reading this paper. The main \nquestion that the paper definitely needs to answer, but does not do so currently \n(in my opinion) is:\n\n  When is this method useful to readers? For solving which problems and under \n  what conditions? And also, when is this method bad and should not be used?\n\n== Experiments ==\n\nSection 5.1 is hard to follow and I don't quite get how it connects to the rest.\n\nAlso, in section 5.1.2 you mention that in comparison to Dai et al. (2017) your \nmethod does not need to rely on an additional density estimation network. Even \nif that is true, I cannot see how it is a useful remark given that the method of \nDai et al. seems to always beat your method.\n\n== Style ==\n\nIn figure 1, no labels or legends are provided making it hard to figure out \nwhat's going on at a glance. It would be very helpful to include labels and a \nlegend.\n\nEquation 2 is not written correctly. The equals sign only refers to \"V(G, D)\" \nand not the min-max of that, right? Please make that explicit by first defining \n\"V(G, D)\" alone.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper185/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Difference-Seeking Generative Adversarial Network", "abstract": "We propose a novel algorithm, Difference-Seeking Generative Adversarial Network (DSGAN), developed from traditional GAN. DSGAN considers the scenario that the training samples of target distribution, $p_{t}$, are difficult to collect.\n\nSuppose there are two distributions  $p_{\\bar{d}}$ and $p_{d}$ such that the density of the target distribution can be the differences between the densities of $p_{\\bar{d}}$ and $p_{d}$. We show how to learn the target distribution $p_{t}$ only via samples from $p_{d}$ and $p_{\\bar{d}}$ (relatively easy to obtain).\n\nDSGAN has the flexibility to produce samples from various target distributions (e.g. the out-of-distribution). Two key applications, semi-supervised learning and adversarial training, are taken as examples to validate the effectiveness of DSGAN. We also provide theoretical analyses about the convergence of DSGAN.", "keywords": ["Generative Adversarial Network", "Semi-Supervised Learning", "Adversarial Training"], "authorids": ["r06942076@ntu.edu.tw", "parvaty316@hotmail.com", "peisc@ntu.edu.tw", "lcs@iis.sinica.edu.tw"], "authors": ["Yi-Lin Sung", "Sung-Hsien Hsieh", "Soo-Chang Pei", "Chun-Shien Lu"], "TL;DR": "We proposed \"Difference-Seeking Generative Adversarial Network\" (DSGAN) model to learn the target distribution which is hard to collect training data.", "pdf": "/pdf/174eb4bc502c1a4ba9d8418a3bc1fe69141698c6.pdf", "paperhash": "sung|differenceseeking_generative_adversarial_network", "_bibtex": "@misc{\nsung2019differenceseeking,\ntitle={Difference-Seeking Generative Adversarial Network},\nauthor={Yi-Lin Sung and Sung-Hsien Hsieh and Soo-Chang Pei and Chun-Shien Lu},\nyear={2019},\nurl={https://openreview.net/forum?id=ryxDUs05KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper185/Official_Review", "cdate": 1542234519778, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryxDUs05KQ", "replyto": "ryxDUs05KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper185/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335667255, "tmdate": 1552335667255, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper185/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}