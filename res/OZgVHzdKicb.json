{"notes": [{"id": "OZgVHzdKicb", "original": "aN5qXaVpa0p", "number": 2833, "cdate": 1601308314460, "ddate": null, "tcdate": 1601308314460, "tmdate": 1614985698879, "tddate": null, "forum": "OZgVHzdKicb", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 26, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "vNHFAwedUo-", "original": null, "number": 1, "cdate": 1610040447766, "ddate": null, "tcdate": 1610040447766, "tmdate": 1610474049532, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Summary:\nThis paper introduces a method to try to learn in environments where a person specifies successful outcomes  but there is no environmental reward signal.\n\nI'd personally be interested in knowing where people were able to easily provide such successful outcomes instead of, for instance, providing demonstrations or reward feedback. Similarly, I'd be interested in how other methods of providing human prior knowledge compared.\n\nDiscussion:\nReviewers agreed the paper was interesting, but none of the 4 thought the paper should be accepted.\n\nRecommendation:\nWhile I do not think this paper should be accepted in its current form, I hope the authors will find the comments and constructive criticism useful."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040447752, "tmdate": 1610474049516, "id": "ICLR.cc/2021/Conference/Paper2833/-/Decision"}}}, {"id": "NutPG60XnTO", "original": null, "number": 4, "cdate": 1604555480651, "ddate": null, "tcdate": 1604555480651, "tmdate": 1606803177332, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Review", "content": {"title": "some interesting ideas but require deeper analysis", "review": "This paper considers the problem of learning a policy for an MDP with unspecified reward, given user-provided goal states. To this end, a reward model and a policy are jointly learned: the reward model is the conditional normalized maximum likelihood (CNML) learned from a training set consisting of the example goal states as positive examples, and the policy trajectories as negative examples; the policy is trained to optimize the MDP using the learned reward. Meta-learning is applied to reduce the cost of learning the CNML models.\n\nPros\n+ The idea of using CNML to obtain a smoother reward as compared to a single model (together with the efficient meta-learning approximation) is interesting.\n+ The algorithm is compared with several baselines and seem to perform well. Ablation study suggests that goal examples and meta-learning are important for the proposed approach.\n\nCons\n- The paper is unnecessarily hard to read. A high-level description of the approach early in the paper will be helpful. This is also related to the comments below: it is not clear why the algorithm should work.\n- A claimed contribution of the paper is to \"produce more tractable RL problems and solve more challenging classes of tasks\". The limited feedback provided by the example goal states perhaps make the RL problem more tractable, but how is it possible to solve more challenging classes of problems when less information is available?\n- To learn a useful reward model from the example goal states, the CNML approach alone seems insufficient, and it seems necessary to require a good reward to be a smooth function of feature vectors. For example, if we work in a grid world with random rewards, does the approach still work?\n- Both the paragraph before Sec 3.2 and Alg 1 mention that the set of negative examples keeps growing. This implies that the reward model will become more and more sparser (values closer to zero), even for the goal states? How is such a reward model still useful?\n- Another question about the reward model is that when the policy becomes better, it is more likely to reach the goal states, thus the goal states are more likely to be labeled as both positive and negative. Thus the reward model is more likely to assign lower reward to goal states when more training is done?\n- Fig. 1 seems to be overstating the problem with MLE. What are the features used and what is the classifier model? If the feature is the real-valued position, and a regularized logistic regression model is used, then MLE will not produce such a sparse reward as in (b)?\n- The experiments section should provide more details about the experimental setup: the choice of candidate classifier models, explanation of the baselines (e.g. Sparse Reward seems not mentioned in the text at all), detailed description of the performance evaluation metric. Is Manhattan distance to goal a sensible performance metric for maze navigation?\n\nMinor comments\n- \"OpenAI et al.\": wrong citation format\n- Define L in Eq. (2)\n\nPost-rebuttal\nAfter reading the rebuttal and other reviewers' comments, my score remained the same. The rebuttal helped to clarify some issues, but it is still not clear to me why the algorithm should work. I agree with other reviewers that a more careful revision of the paper, and a further analysis on the algorithm will be beneficial.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087754, "tmdate": 1606915785944, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2833/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Review"}}}, {"id": "0mkkGx_usM", "original": null, "number": 2, "cdate": 1603936405845, "ddate": null, "tcdate": 1603936405845, "tmdate": 1606796008413, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Review", "content": {"title": "An interesting paper, but some concerns regarding motivation and experiments", "review": "Summary\n-------\n\nThis paper addresses a reinforcement learning problem where the reward\nfunction is learned through a classifier that decides whether states are\nsuccessful or not based on previous examples (i.e. RL after inverse RL).\nThe authors show that this requires uncertainty-aware predictions, which\nare difficult with neural networks. An algorithm, BayCLR, is proposed\nthat uses MAML to meta-learn the conditional normalized maximum\nlikelihood, i.e. the \"maximum likelihood distribution\". Connections to\nthe proposed algorithm and exploration methods are discussed before\nusing the algorithm to solve various robotics tasks.\n\nDecision\n--------\n\nAlthough I liked this paper overall, I am rating it tentatively as\nmarginal below the acceptance threshold. The paper is very well written\nand addresses a relatively clear problem (inverse RL with classifier)\nwith an interesting method (meta-learning CNML). I have some issues with\nunclear statements in the motivation and method that should be\naddressed. While the experiments provide some insight, I think the\nconclusions the authors draw from them are far stronger than the results\nimply.\n\nOriginality\n-----------\n\nI am not very familiar with CNML, but this paper seems very original. In\nparticular, the application of meta-learning to conditional normalized\nseems novel, as well as its application to inverse RL.\n\nQuality and Clarity\n-------------------\n\nThe paper is well-written, most statements are clear and easy to follow.\n\nStrengths\n---------\n\n-   The approach of meta-learning CNML is interesting, and if anything\n    deserves further analysis outside of inverse RL / RL.\n-   Although I have some issues with the motivation (see below), I think\n    the authors do a good job of explaining their rationale for using\n    CNML. In particular, quantifying neural network uncertainty through\n    posterior analysis is quite difficult.\n-   The experiments seem comprehensive. However, I am not very familiar\n    with the environment suite used. Due to the lack of work in this\n    area, there are not many possible baselines, and so the VICE\n    baseline seems like the best choice. The baseline is further kept\n    fair by adding exploration heuristics. I especially like Section 6.2\n    that analyzes BayCRL on the zigzag maze task, which I assume is a\n    tabular environment. In addition, there is a simple ablation but I\n    would prefer more work on this.\n\nWeaknesses\n----------\n\n-   In Section 4, some motivating statements are unclear to me (see\n    Detailed Comments).\n\n-   A single data-point being added to the dataset may not change the\n    distribution much, and this crucial point is only addressed (in an\n    ad-hoc way) in the appendix. It would be good to see an ablation\n    study on this, or perhaps a plot of the average difference between\n    different query points. Perhaps Figure 6 may indirectly explain\n    this, but it should be explicitly addressed.\n\n-   In Section 6, many statements about BayCLR seem stronger than the\n    results imply. Many confidence intervals overlap, and the results\n    themselves are hard to parse with so many lines in each plot.\n    Perhaps some of these baselines which are not competitive can be\n    excluded, or perhaps different linestyles should be used.\n\nDetailed comments\n-----------------\n\n-   Early mentions of exploration (i.e. in the abstract) seem out of\n    place. While you elaborate on the connection to exploration methods,\n    it does not seem like the main point of this paper is to address\n    exploration. It seems to me that you are tackling a novel RL problem\n    where the task is specified through goal states. Specifically, you\n    learn classifier in place of a reward function, and this is\n    exploited to shape an otherwise sparse reward.\n\n-   Section 4.1, \"To create effective shaping, we need to impose a prior\n    on our classifier so that it provides a more informative reward when\n    evaluated at rarely vis- ited states that lie on the path to\n    successful outcomes.\"\n\n    Why is a prior strictly necessary for reward shaping? Unless you\n    mean prior in a very general sense, not a Bayesian prior, I don't\n    see why a prior is strictly necessary.\n\n-   Section 4.1, \"\\[CNML\\].. is essentially imposing a uniform prior\n    over the space of possible outcomes\". This is not obvious to me, and\n    perhaps further explanation is needed.\n\n-   Section 4.2, Theorem 4.1: Perhaps I misunderstand but if $G(s) > 0$,\n    shouldn't $p(e = 1 | s) = 1$? Further, why is that when the agent\n    visit a successful state, i.e. $N(s)$ increases, $p(e = 1 | s)$\n    decreases after each visit.\n\n-   Section 5.1, \"This algorithm, which we call meta-NML, allows us to\n    obtain normalized likelihood estimates without having to retrain\n    maximum likelihood to convergence at every single query point, since\n    the model can now solve maximum likelihood problems of this form\n    very quickly. \"\n\n    Is it correct to say that meta-NML does not need to retrain to\n    convergence at every query point? The second part of the sentence\n    elaborates that metatraining allows you to solve the problem very\n    quickly, but it seems that you still need to solve it at every query\n    point.\n\n-   Section 6.2, Figure 4: I don't think its fair to say that BayCLR\n    performs substantially better. The confidence intervals overlap in\n    all but Spiral Maze and Sawyer 3d Pick-and-Place. Other subtleties\n    are not addressed, such as why RND/count-bonus actually hurt VICE in\n    sawyer 2d push. Other statements such as \"significantly more\n    efficiently\" need explanation as well.\n\n-   Section 6.3, Figure 5: for reproducibility, you should include\n    exactly how many gradient steps are used in the model without\n    meta-learning.\n\n-   Section 6.4, Figure 6: I'm unfamiliar with the environment being\n    used, so some additional details explaining what z means would be\n    helpful.\n\n    \"Furthermore, meta-NML is able to reasonably approximate the\n    idealized NML rewards with just one gradient step\u2026\"\n\n    How is this shown in Figure 6? I don't see anything showing the\n    idealized NML rewards.\n\nMinor Comments\n--------------\n\n-   Section 4, Line 3: missing space: \"ples.For example,\"\n\nPost Rebuttal\n--------------\n\nAfter reading the comments by the other reviewers, I have decided to keep my score at a 5. The authors reply, and the updated manuscript, helped my understanding of the paper. I was considering raising my score, however, the reviewers were nearly unanimous in their confusion regarding the framework or application of CNML. For future iterations of the paper, I suggest that the authors describe CNML, event-based control and their connection more explicitly. If the main contribution is using CNML as the classifier in event-based control, then it would also help to conduct experiments on meta-learning CNML in a supervised learning setting to further elucidate its effectiveness in the reinforcement learning application. I think your paper is very interesting, and I hope that the authors are able to use this feedback to improve their paper.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087754, "tmdate": 1606915785944, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2833/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Review"}}}, {"id": "KdS-k4Y6b4q", "original": null, "number": 1, "cdate": 1603854532081, "ddate": null, "tcdate": 1603854532081, "tmdate": 1606678517838, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Review", "content": {"title": "Review", "review": "This paper studies how to solve RL problems with a set of success states instead of a standard reward function. The central idea is to firstly train a Bayesian classifier from both the input success examples and the on-policy sampling using the conditional normalized maximum likelihood (CNML) and then use the learned classifier as a reward function to guide exploration. It is proved that in a tabular case, the success classifier trained with CNML is equivalent to a version of count-based exploration and it is claimed that with function approximation, the classifier attains non-negligible generalization. Empirically, it is claimed that this approach outperforms existing algorithms on a number of navigation and robotic manipulation domains.\n\nThe novelty of this work lies in the use of CNML to train a more regularized success classifier and further use meta-learning to implement CNML in practice. \n\nThere are several concerns I have:\n1. Can the authors indicate the following information in the experiment:\n- Which algorithms are provided with success examples as prior knowledge in all testing domains? The descriptions in Sec. 6.1 and 6.2 are a little confusing.\n- In Figure 4, I didn\u2019t see the lines for VICE+count-bonus.\n\n\n2. The claim that BayCRL outperforms uninformed, task-agnostic exploration (VICE+count-bonus and VICE + RND ?) is not surprising since the former has prior knowledge.\n\n\n3. The authors claimed that the proposed approach can achieve both effective reward-shaping and exploration. I agree with the first point by comparing it with other IRL methods. But how is the latter true? I think this needs to be further demonstrated in a different setting such as the one in the next point.\n\n\n4. All the tested domains have only one success state. Thus, the example set is informational complete. I wonder about the robustness of the algorithm if the example set does not contain all success states while a reward function (which is surely super sparse) is provided. For example, if there are 10 success ground-truth states and only 5 are provided in the example set and the uncovered states are quite remote from the provided ones (but a reward function is available, i.e., if we reach these hidden success states, a positive reward will be received), then how would this degenerate the performance of the proposed approach comparing with other methods? I think this also testifies the generalization/exploration ability of the algorithm from another perspective.\n\nI vote for weak reject since both CNML and MAML including the reformulation of the problem follow the prior works, which kind of limits the novelty of this paper as applying known algorithms to a defined problem. But I am open to adjusting the score if the rebuttal can address my concerns.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087754, "tmdate": 1606915785944, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2833/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Review"}}}, {"id": "SRTYvjm1djG", "original": null, "number": 26, "cdate": 1606281777547, "ddate": null, "tcdate": 1606281777547, "tmdate": 1606281777547, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "VX-yNYjtNor", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Thank you for these clarifications", "comment": "Thank you for these clarifications. I will re-evaluate the paper in the light of these details, the revisions and your discussions between the other reviewers."}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "JCLmUikd1sm", "original": null, "number": 2, "cdate": 1605337336860, "ddate": null, "tcdate": 1605337336860, "tmdate": 1606201382789, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Shared Response to Reviewers", "comment": "We thank the reviewers for their thoughtful comments and suggestions. To address the concerns brought up by reviewers, we conducted a number of additional experiments, and edited the paper for clarity and exposition.  We describe these below\n\nChanges in text:\n\nWe added a more clear method overview in Section 1, added significantly more details about the experiments and baselines in Section 6.1 and Appendix A.4, made clarifications about the expected behavior of the algorithm in Section 5.2, and added in clarifications about the computational complexity of meta-NML in Table 1 and Appendix A.3.3. We have also toned down the language in Section 6.2 describing the performance of BayCRL compared to prior algorithms, so that the improvements can be interpreted more objectively from the plots instead.\n\nNew experiments and Visualizations: \n\n- **Computational efficiency:** To demonstrate the substantial efficiency improvement of our meta-learned CNML approach compared to standard CNML, we computed the runtime per input point and per epoch of RL for the two methods. These runtimes are available in the tables in Appendix A.3.3. Meta-CNML achieves a speedup of 1600 to 2300 times the runtime of CNML from scratch.\n\n- **Single datapoint impact:** We also visualize the impact of a single added datapoint in training, which is quite substantial as seen in Fig 3 as well as Fig 11 & 12 in Appendix A.3.1, both qualitatively and quantitatively. \n\n- **Quantitative visualization of exploration:** We added visualizations of trajectories and state coverage on the spiral maze task for BayCRL vs. a normal classifier training scheme (VICE (Fu et al 2018)) in Fig 8. We also introduce a better quantitative evaluation of exploration, showing the fraction of states visited with and without using a Bayesian classifier. We see that using the Bayesian classifier increases the number of states visited by approximately 30%, and is also directed towards the goal rather than exploring all states naively. \n\n- **Additional visualizations of meta-NML:** We have provided additional comparisons of the classifiers resulting from meta-NML as compared to ideal NML, regularized MLE, and unregularized MLE in Fig 13 in Appendix A.3.2. These new figures show that meta-NML better guides the agent towards the goal by rewarding underexplored states. We also added a visualization of the rewards assigned by our meta-CNML model before and after taking a gradient step at evaluation time, showing the impact of adaptation. We also added a plot of average difference between MLE and CNML classifier goal probabilities as we take more gradient steps for CNML in Fig 12.\n\n- **More clear plotting of experiments:** To make the experimental comparisons to other RL algorithms easier to understand, we plot the success rate in Figure 4 instead of a distance metric over time. These plots are less cluttered and more representative of overall performance, since many of the baseline methods were actually getting stuck in local optima and the distance metric may not make this clear. The original distance plots are available in Appendix A.4.2, along with more detailed explanations of the success thresholds and distance metrics used.\n\n- **Learning in a discrete, randomized environment:** following the suggestion of Reviewer 2, we have added a new experiment in Appendix A.5.1 demonstrating BayCRL's ability to learn in environments where the states are not smooth or correlated dynamically, due to the worst-case performance guaranteed by its exploration effects.\n\n- **Finding \"hidden\" rewards not indicated by success examples:** as suggested by Reviewer 3, we have added a new experiment in Appendix A.5.2 where the environment has multiple potential goals, some of which are not provided to the algorithm as success examples. We see that BayCRL is able to consider multiple directions of movement and find the most convenient sparse rewards to reach (despite not knowing about those states initially) due to its built-in exploration ability, whereas prior classifier-based methods fail to do so.\n\n- **Details on importance weighting for CNML adaptation:** We have provided additional justification in Appendix A.2.1 for the importance weighting scheme used during CNML test-time adaptation, as well as a plot showing its concrete improvements to learning stability compared to standard minibatch gradient descent.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "kLB-kDjODxx", "original": null, "number": 25, "cdate": 1606201036108, "ddate": null, "tcdate": 1606201036108, "tmdate": 1606201187655, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "Dg2FJnkax57", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Regarding definition of S-", "comment": "Good observation! You are correct that defining S- as negatives from the collection of all previous policies results in wider coverage of visited states, which is useful for our approach which must estimate uncertainty in the rewards for each state. Since this is indeed what we do in practice (representing S- as a fairly large replay buffer of previous states) as described in Algorithms 1 & 2, we are able to ensure that at convergence, the goal has reward ~0.5 which is higher than that of all other previously visited regions."}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "HyBQ9df5ADb", "original": null, "number": 24, "cdate": 1606200938801, "ddate": null, "tcdate": 1606200938801, "tmdate": 1606200938801, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "dqfl6ywvfC", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Clarifying evidence for exploration and reward shaping", "comment": "Thanks for your feedback! Can you please clarify why the empirical results are only promising on the maze tasks and not on the Sawyer robot environments? The intent of the three Sawyer environments was to evaluate our method on tasks that represent a wider variety of general RL problems on which classifier-based algorithms would be useful \u2014 all of them involve fairly difficult exploration and require nontrivial reward shaping to solve the task with standard methods. Were there other types of general RL problems you were hoping to see?\n\nWe would also like to emphasize that we have included what we believe is an overwhelming amount of empirical evidence for how our algorithm \"advances the exploration and goal-oriented reward shaping for general RL problems\": \n- Theorem 4.1 establishes a connection between CNML and count-based exploration in the discrete case, without any generalization across states.\n- Figure 5 in Section 6.2 shows that BayCRL outperforms existing algorithms on navigation and robotic manipulation, providing evidence for improvements in both reward shaping and exploration.\n- (*Added in response to reviews*) Figure 7 and the \u201cBayCRL and Reward Shaping\u201d analysis in Section 6.4 visualizes the rewards given by CNML on the challenging 3D pick-and-place task as compared to a standard MLE classifier.\n- (*Added in response to reviews*) Figure 8 and the \u201cBayCRL and Exploration\u201d analysis in Section 6.4 looks at the state coverage of BayCRL vs. standard classifier-based methods both visually and quantitatively.\n- (*Added in response to reviews*) Appendix A.3.2 compares the reward shaping given by various classifier methods on the maze task qualitatively. \n- (*Added in response to reviews*) The ablation in Appendix A.5.1 shows how even in a discrete environment BayCRL reduces to count-based exploration."}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "dqfl6ywvfC", "original": null, "number": 23, "cdate": 1606167735450, "ddate": null, "tcdate": 1606167735450, "tmdate": 1606167735450, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "EevTAO5IkCT", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for the detailed response. Though the empirical results look promising on maze tasks, the paper could benefit from convincing analysis that shows how the CNML advances the exploration and goal-oriented reward shaping for general RL problems. Therefore, I may retain my original score."}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "Dg2FJnkax57", "original": null, "number": 22, "cdate": 1606157586201, "ddate": null, "tcdate": 1606157586201, "tmdate": 1606157586201, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "Lj4LMQbAlP", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Thanks for the revision", "comment": "Thank you for revising the paper and I do think it helps me understand the algorithm better. I only have one left confusion:\n\nYou mentioned that *the* policy is the current running policy instead of the collection of all previous policies. Then by Thm. 4.1, at convergence, a success state will have a reward 0.5 and a never-visit unsuccessful state under the current policy will also have a reward 0.5 (since G(s) = N(s) = 0)? Does this mean the reward encourages exploration as much as reaching the goal? Will this push the agent away from success states? \n\nIf the above is true, I actually think using the collection of all previous policies, i.e., do not clear S-, makes more sense. In that way, there is a wider cover over the state space and the case of a never-visit unsuccessful state merely happens so that *at convergence*, we guarantee that the successful states can have larger rewards than unsuccessful states?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "Lj4LMQbAlP", "original": null, "number": 19, "cdate": 1606066568409, "ddate": null, "tcdate": 1606066568409, "tmdate": 1606066568409, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "D_nKA6mil4m", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Implemented suggestions", "comment": "Thank you very much for your detailed feedback!  We have addressed your suggestions as detailed below:\n\n> The main algorithm is presented in section 5. Maybe a primitive version (i.e., without meta-training but only CNML) can be displayed in Section 4 to help the readers understand\n\nThanks for the suggestion \u2014 we have added pseudocode to section 4.1, describing a basic CNML-based RL algorithm without metalearning, to aid with understanding. (A diagram of classifier-based RL methods in general is also provided in Figure 1.) Please let us know if this helps make the section more clear.\n\n> In Figure 1, should the numerator of r(s) be the union with (s,1) instead of (s,0)? I thought we want r(s) = p(e=1|s).\n\nGood catch! We do indeed want r(s) = p(e=1|s) rather than p(e=0|s). This has been fixed in Figure 1.\n\n> When I read the paper, it keeps telling me what a good reward-shaping should be\u2026 Maybe it is better to give a more straight-to-the-point and complete explanation in one place?\n\nYour understanding is correct. We have added a clearer summary of the main advantages of CNML in Section 4.4, which gives an overview of all the material that was introduced over the course of Section 4.\n\n> In Section 4, the authors are using 'the' policy. Is it the policy \\pi that indeed changes each iteration as in Algorithm 1 or a collection of policies that we have rolled-out in total? Since in Algorithm 1, the set S- is never cleared, I guess it collects all states that can be visited by all previous policies?\n\nIn theory, our negative examples should come from the state distribution of the current policy \\pi(s). However, you are right that our practical algorithm does not clear the set \\mathcal{S}_- of visited states, and thus it includes states that were visited by previous policies. In fact, the practical implementation we use for \\mathcal{S}_- is actually a replay buffer, from which we sample visited states as negatives for training the classifier. We based this design decision on prior classifier-based RL methods like [1].\n\n> I saw other reviewers also asked about the seemingly reward decay question on successful states in Thm. 4.1. I think it would be better if a primitive algorithm can be presented in this section and the notations e.g., N(s) and G(s), can be defined accordingly.\n\nWe have added pseudocode for a basic CNML-based RL algorithm (without metalearning) in Section 4.1 as suggested. If this is still unclear after our revisions, please let us know!\n\n> In Section 4.3, it is stated that the classifier is akin to an exploration bonus. I personally think this is not very accurate since a bonus is something added to the true reward function but there is no original reward in this setting.\n\nAlthough there is no original reward function, we can actually think of the discrete CNML classifier rewards as a combination of a sparse reward function (which is 0 everywhere except the goal) and an exploration bonus. At non-goal states, discrete CNML would output 1/(N(s) + 2) as described in Theorem 4.1, which corresponds to a reward of 0 plus an inverse count bonus. At goal states, it would output (G(s) + 1)/(N(s) + G(s) + 2), which corresponds to the sparse reward function outputting a non-zero value for reaching the goal.\n\nWe have clarified this point in Sections 4.2 and 4.3.\n\n---\n\n[1] Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition, Fu et al. 2018. https://arxiv.org/abs/1805.11686"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "VX-yNYjtNor", "original": null, "number": 18, "cdate": 1606027926406, "ddate": null, "tcdate": 1606027926406, "tmdate": 1606027926406, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "YtgcML0R5lK", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Response to Additional Questions", "comment": "We appreciate your additional feedback! To respond to your questions:\n\n> Single datapoint impact: My original concern was the detail in Appendix A.2.1 on importance weighting used for the query point. It is not immediately obvious how this relates to low density states (as someone not familiar with CNML literature), and the connection to Figure 3 and the figures in the Appendix A.3.1.\n\nOur use of importance weighting as described in Appendix A.2.1 is intended to lower the variance while training with SGD. When taking gradient steps at test time for CNML, we care about the adapted model\u2019s outputs on the single query point x_q, yet x_q will not be present in most of the batches during stochastic optimization. While in principle training to convergence would still allow our model to perform well on the query point, due to imperfections in optimization, the query point would likely be \u201cdrowned out\u201d by the original dataset. By instead including the query point in every batch and downweighting appropriately, the objective remains unbiased but lower variance, since the query point is in every gradient update.\n\nWe have added a more detailed description of this in Appendix A.2.1 as well as a plot comparing the performance of gradient descent with and without importance sampling. The version with importance weighting learns significantly faster and does not have as much variance in losses across batches. Please let us know if this helps!\n\nAs for the connection to the points you mentioned:\n- *Low density states*: there is no explicit connection to low density states, since importance weighting is meant to help with adapting to any query point in general. However, low density query points do occur less frequently in the training distribution, so reducing variance of the MLE training process by including them in every batch ensures that adaptation can happen consistently for those states as well. \n- *Figure 3 and Appendix A.3.1*: both of these use importance weighting, i.e. the query point is downweighted and included in every batch at test time. Had we not done this, the progression in A.3.1 would have been less stable, and some of the steps may have brought us closer to the MLE solution (a sparse reward) rather than the true CNML solution if those batches did not happen to include the query point.\n\n> How does this encourage the agent to go to the goal if the rewards are 0 everywhere except at the states that are successful outcomes? Also, what is the connection to this and gradients?\n\nThe rewards are only 0 everywhere except the goals *at convergence*, i.e. if the agent has found the goal and the policy no longer changes much between iterations. Importantly, we don\u2019t train the classifier to convergence immediately and use it to assign all future rewards; instead, we jointly optimize the policy (using a standard RL algorithm) and the classifier (by training on visited states and goal examples), so the rewards are continually changing based on the data collected by the agent. A general progression would be something like this:\n- The rewards are initially very smooth, and increase in the direction of the goal examples, providing guidance on which direction to go\n- As the agent visits more states, the rewards for those visited states drop. However, as long as the goal has not yet been reached, there will still be shaping towards the goal\n- After the agent has found the goal and is able to consistently reach it, most other states will have their rewards brought down to 0 and the goal will have reward close to 0.5. At this point, we have converged to a sparse reward function, but this is okay because the learned policy is able to reach the goal as desired and should no longer be altered. \n\nWe note that this idea is not originated by us and is common in the inverse RL literature. Previously, [1] showed a connection between GAN-style training and maximum entropy IRL, both of which have discriminators (or reward classifiers) which output 0.5 at convergence on the desired distribution of states. This was then generalized in [2] to the case where only goal examples are provided. Regarding the connection to gradients, we apologize for the confusing wording. The more precise way to say this would be: since we train our classifier on visited states and goal examples, under reasonable assumptions about generalization and an appropriately regularized classifier, the output success probability (and therefore the reward) would be a smoothly increasing function as we get closer to the goal.\n\n--\n\n[1] A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models, Finn et al. 2016. https://arxiv.org/abs/1611.03852\n\n[2] Variational Inverse Control with Events: A General Framework for Data-Driven Reward Definition, Fu et al. 2018. https://arxiv.org/abs/1805.11686"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "ceqd-qYB6uy", "original": null, "number": 16, "cdate": 1605923803113, "ddate": null, "tcdate": 1605923803113, "tmdate": 1605923803113, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "DfhATmoBU4F", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "About my initial concerns", "comment": "I finished reading the experiment part of the revised version and my initial concerns are addressed. I will consider adjusting my score after communicating with other reviewers and the area chair."}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "YtgcML0R5lK", "original": null, "number": 15, "cdate": 1605851844954, "ddate": null, "tcdate": 1605851844954, "tmdate": 1605851844954, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "g4dLxUPGPGa", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Two brief comments", "comment": "Thank you for your detailed reply. This indeed addresses many of my\ninitial concerns. Below are some general comments with respect to your\nreply and the shared response.\n\n-   Single datapoint impact: My original concern was the detail in\n    Appendix A.2.1 on importance weighting used for the query point. It\n    is not immediately obvious how this relates to low density states\n    (as someone not familiar with CNML literature), and the connection\n    to Figure 3 and the figures in the Appendix A.3.1.\n\n-   p(e = 1 | s ) \" at convergence p(e=1|s) for successful outcomes\n    would be 0.5 (balanced large numbers of positives and negatives),\n    and every other state will have p(e=1|s) will be 0. So this still\n    encourages the agent to go towards the goal, and provides the agent\n    with the correct gradient.\"\n\n    How does this encourage the agent to go to the goal if the rewards\n    are 0 everywhere except at the states that are successful\n    outcomes? Also, what is the connection to this and gradients?\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "D_nKA6mil4m", "original": null, "number": 14, "cdate": 1605836404335, "ddate": null, "tcdate": 1605836404335, "tmdate": 1605837768337, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "DfhATmoBU4F", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "After reading the revision", "comment": "I want to thank the authors for their efforts to address my concerns. I have been reading the revised version and I have some extra comments and questions:\n\n1. The main algorithm is presented in section 5. Maybe a primitive version (i.e., without meta-training but only CNML) can be displayed in Section 4 to help the readers understand the content in Section 4? It's a little hard to catch the learning process.\n\n2. In Figure 1, should the numerator of r(s) be the union with (s,1) instead of (s,0)? I thought we want r(s) = p(e=1|s).\n\n3. When I read the paper, it keeps telling me what a good reward-shaping should be but it takes me several times to go over and collect the information about why CNML works. The information is scattered in some intuitive description about CNML on classification task in Section 3.2, an illustration in Section 4.1 second paragraph, and the relation established with count-based exploration, etc. From all the above information I gathered, the reasons are:\n 1) assume the states are represented by features in a meaningful metric space;\n 2) if the unknown states are close to either positive or negative samples then label it accordingly with higher certainty;\n 3) if the unknown states are not close to either negative or positive then explore with a uniform distribution,\n\nwhere the second point shows task-specification guidance which outperforms the task-agnostic exploration and the third point provides exploration/regularization which outperforms standard MLE. Basically, CNML is more lenient on unknown states by providing them with probability close to 0.5 instead of completely arbitrary compared with non-regularized MLE. \n\nPlease correct me if there is a misunderstanding. Maybe it is better to give a more straight-to-the-point and complete explanation in one place? \n\n4. In Section 4, the authors are using 'the' policy. Is it the policy \\pi that indeed changes each iteration as in Algorithm 1 or a collection of policies that we have rolled-out in total? Since in Algorithm 1, the set S- is never cleared, I guess it collects all states that can be visited by all previous policies? \n\n5. I saw other reviewers also asked about the seemingly reward decay question on successful states in Thm. 4.1. I think it would be better if a primitive algorithm can be presented in this section and the notations e.g., N(s) and G(s), can be defined accordingly. \n\n6. In Section 4.3, it is stated that the classifier is akin to an exploration bonus. I personally think this is not very accurate since a bonus is something added to the true reward function but there is no original reward in this setting.\n\nI am still reading the experiment part and will update the comment later."}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "DfhATmoBU4F", "original": null, "number": 9, "cdate": 1605821686123, "ddate": null, "tcdate": 1605821686123, "tmdate": 1605824733295, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "PtvRZQq3ux", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Additional Experiment and Revisions", "comment": "We have added the results and additional analysis for your suggested experiment in point 4 (multiple sparse reward states with some not provided as goals) to the Appendix in section A.5.2. The results show that BayCRL is able to uncover \u201csuccess states\u201d that are not given to the algorithm explicitly and are very far from the provided goals, due to its built-in exploration behavior that causes it to consider multiple possible directions of movement before finding a goal. Meanwhile, prior classifier-based methods do not have this behavior and will move only towards the incomplete set of provided success states.\n\nHowever, we would like to clarify a few things about your original point:\n- Typically, the setup for BayCRL (and classifier-based RL algorithms in general) assumes that no reward function exists, and the user provides success examples to alleviate the need for manually specifying such a reward function. However, to create a realistic setting for your proposed experiment, we can assume that there is a reward function which is not known to the human expert, and that the examples provided by the expert therefore may not cover all the regions with high reward.\n- You mentioned using this to test the \u201cgeneralization/exploration ability of the algorithm\u201d. To clarify, the results of our new experiment demonstrate BayCRL\u2019s exploration ability; this is what allows it to continue considering multiple unexplored pathways despite there being no goal examples in one of the directions, leading it to eventually find the sparse rewards. It does not necessarily imply an ability to generalize to very different goals which have not been provided. Intuitively, if the unknown sparse reward is very far from any provided goals in state space, we would not expect a function approximator to generalize from one goal to another. In practice, if the environment has multiple intended goals which are far from each other, the human can provide each of these as success examples as done in [1], which would cause the learned classifier to have several high-reward regions.\n\n[1] End-to-End Robotic Reinforcement Learning without Reward Engineering, Singh et al. 2019. https://arxiv.org/abs/1904.07854\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "z50atj_uHZo", "original": null, "number": 13, "cdate": 1605822430249, "ddate": null, "tcdate": 1605822430249, "tmdate": 1605823892622, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "KdS-k4Y6b4q", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Follow-up on concerns", "comment": "We have added a number of additional experiments, clarifications and revisions to the paper to address the reviewers' concerns. These provide more evidence of the exploration and reward shaping benefits of BayCRL, and the added analysis helps provide a clearer picture of the behavior of the algorithm.\n\nAs we have not heard from the reviewers since the beginning of the discussion period, we would like to ask whether your concerns have been addressed and whether there are any additional questions or clarifications? We have attempted to address reviewer concerns as thoroughly as possible, and we would be very happy to engage in further discussion and improvements to our work. Your feedback so far is greatly appreciated!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "7A6YymAe8l1", "original": null, "number": 12, "cdate": 1605822408580, "ddate": null, "tcdate": 1605822408580, "tmdate": 1605823882841, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "0mkkGx_usM", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Follow-up on concerns", "comment": "We have added a number of additional experiments, clarifications and revisions to the paper to address the reviewers' concerns. These provide more evidence of the exploration and reward shaping benefits of BayCRL, and the added analysis helps provide a clearer picture of the behavior of the algorithm.\n\nAs we have not heard from the reviewers since the beginning of the discussion period, we would like to ask whether your concerns have been addressed and whether there are any additional questions or clarifications? We have attempted to address reviewer concerns as thoroughly as possible, and we would be very happy to engage in further discussion and improvements to our work. Your feedback so far is greatly appreciated!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "EevTAO5IkCT", "original": null, "number": 11, "cdate": 1605822383453, "ddate": null, "tcdate": 1605822383453, "tmdate": 1605823871761, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "mLCde7AjY24", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Follow-up on concerns", "comment": "We have added a number of additional experiments, clarifications and revisions to the paper to address the reviewers' concerns. These provide more evidence of the exploration and reward shaping benefits of BayCRL, and the added analysis helps provide a clearer picture of the behavior of the algorithm.\n\nAs we have not heard from the reviewers since the beginning of the discussion period, we would like to ask whether your concerns have been addressed and whether there are any additional questions or clarifications? We have attempted to address reviewer concerns as thoroughly as possible, and we would be very happy to engage in further discussion and improvements to our work. Your feedback so far is greatly appreciated!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "4XbxC0PFRF", "original": null, "number": 10, "cdate": 1605822347607, "ddate": null, "tcdate": 1605822347607, "tmdate": 1605823853806, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "NutPG60XnTO", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Follow-up on concerns", "comment": "We have added a number of additional experiments, clarifications and revisions to the paper to address the reviewers' concerns. These provide more evidence of the exploration and reward shaping benefits of BayCRL, and the added analysis helps provide a clearer picture of the behavior of the algorithm.\n\nAs we have not heard from the reviewers since the beginning of the discussion period, we would like to ask whether your concerns have been addressed and whether there are any additional questions or clarifications? We have attempted to address reviewer concerns as thoroughly as possible, and we would be very happy to engage in further discussion and improvements to our work. Your feedback so far is greatly appreciated!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "PtvRZQq3ux", "original": null, "number": 6, "cdate": 1605338188989, "ddate": null, "tcdate": 1605338188989, "tmdate": 1605822474562, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "KdS-k4Y6b4q", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Response to Reviewer Concerns", "comment": "Thank you for your insightful suggestions and comments! To address the concerns you raised, we ran several new experiments and visualizations as described in the shared response. Please find responses to questions and concerns below:\n\n> \u201climits the novelty of this paper as applying known algorithms to a defined problem.\u201d\n\nThe observation that a Bayesian classifier can be applied to the problem of reward inference in RL is a key novel contribution of this work. We:\n1. Explain that CNML can serve as an appropriate bayesian method for reward inference\n2. Instantiate an entirely novel, practical way to obtain fast CNML estimates via meta-learning (this has not been proposed > before in any algorithm \u2014 MAML is typically an algorithm for few-shot classification, and it needs to be reformulated for the problem of CNML estimation)\n3. Theoretically and empirically show how this provides connections to exploration and reward shaping. \n\nAlso, although CNML is a well-established concept, it is intractable on deep neural networks \u2014 as shown in our new table in Appendix A.3.3, our meta-CNML algorithm achieves around a 2000x speedup compared to naive CNML, and is what makes evaluation on RL problems possible. We feel that this constitutes a significant set of novel contributions. \n\n> \u201cExperimental details:\u201d \n\nThe VICE comparisons, with and without exploration bonuses, the DDL algorithm are all provided with access to the success example. The point we make in these experiments is that BayCRL actually provides us an effective way to leverage this prior knowledge more effectively than the other techniques. We have added this description into Section 6.1 as well as the analysis in Figure 8.\n\n> \u201cMissing comparisons to VICE + count bonus\u201d\n\nThank you for the pointer, we have updated Fig 5 accordingly. The VICE + count bonus lines were previously shown in an incorrect color; this is now fixed.\n\n> \u201cThe claim that BayCRL outperforms uninformed, task-agnostic exploration is unsurprising\u201d\n\nWe definitely agree that the methods mentioned perform task-agnostic exploration, but the point of our comparison here is to show that BayCRL provides a direct way to actually provide task-directed exploration. We note that VICE + an exploration bonus, which we compare to in our experiments, also has access to goal examples, but does not use it for the exploration bonus itself. We can see the advantages of our method the better empirical performance on task success in Fig 5, and the reward shaping and exploration visualizations in Fig 7 and 8. \n\n> \u201cHow does the bayCRL algorithm help with exploration?\n\nAn easy way to understand the benefit of BayCRL for exploration is by noting the direct connection that BayCRL has to inverse counts, as noted in Theorem 4.1. Basically, as a state is seen more and more in the dataset and given a label of 0, the resulting likelihood that CNML assigns to it becomes lower and lower. The exact connection in the absence of function approximation is 1/(N + 2) where N is the number of times a state is visited. This connection directly reveals that the CNML likelihoods in the absence of function approximation give an inverse count bonus, which is what many exploration algorithms do [1]. To visualize this quantitatively, we also plot the number of uniquely visited states vs the number of iterations of training and see that BayCRL trained policies visit far more unique states than a standard MLE classifier (Fig 8).\n\n> \u201cSuggestion of sparse reward + goal states (point 4)\u201d\n\nThank you for the suggestion! We are in the process of running this comparison and will add in these results shortly. (Update 11/19: this experiment has been added, please see the next comment below for details)\n\n[1] Unifying Count-Based Exploration and Intrinsic Motivation, Bellemare et al 2016\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "T9aS_61UTd-", "original": null, "number": 3, "cdate": 1605337456368, "ddate": null, "tcdate": 1605337456368, "tmdate": 1605668984122, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "NutPG60XnTO", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Response to Reviewer Concerns", "comment": "Thank you for your comments and suggestions! We have performed a number of new experiments and visualizations to address your concerns, which have been added to the paper as described in the shared response. Please find detailed responses to questions below:\n\n> \u201cA high-level description of the approach early in the paper will be helpful\u201c: \n\nWe have added a high level overview section (Section 4.4), and made the overview figure which explains the entire pipeline appear earlier and more prominently. Please let us know if this helps with clarity of the exposition in Section 4. \n\n> \u201chow is it possible to solve more challenging classes of problems when less information is available?\u201d: \n\nExcellent question! The intuition is that the general class of RL problems can have arbitrary (sparse) reward, and in the worst case would need complete state coverage. Our approach assumes that we know what successful outcomes look like. When this information is provided, the algorithm can extract a signal that guides it towards success, whereas in the most general RL problem, the reward may be completely uninformative until the agent randomly chances upon a successful outcome. For example, a standard RL agent in a 2D grid may need to exhaustively explore the state space before observing a reward. However, successful outcomes given at the start of training can use these examples to know the direction to go. BayCRL can exploit this extra information, whereas standard RL cannot. We can see this type of behavior in our analysis of BayCRL in Section 6.4. \n\n> \u201cseems necessary to require a good reward to be a smooth function of feature vectors\u201d: \n\nFor the reward shaping that results from the classifier to be meaningful, the feature vectors need to be smooth and structured such that a CNML classifier would be able to appropriately extract shaped reward. In practice, this is the case with many RL environments such as those we consider in Section 6. However, even if the features are not structured and completely lack identity, then the CNML classifier would still reduce to an exploration-centric reward bonus, as indicated by Theorem 4.1, ensuring reasonable worst-case performance. We are in the process of running an empirical validation of this and will post results shortly. (Update 11/17: this experiment has been added, please see the next comment for details)\n\n> \u201cthe reward model will become more and more sparse , even for the goal states?\u201d\n\nWe will clarify this in the text. The dataset for training is balanced to have an equal number of positives and negatives, sampled from the entire set of positives and negatives seen thus far. If the policy learns to perfectly reach the goal states, then the rewards will be pushed down to zero and the goal states will converge to 0.5 since there will be a roughly equal number of positives and negatives at the goal. However, as long as the policy is imperfect, the rewards will not converge to this sharp reward, ensuring that the agent continues towards the goal. As noted briefly in Section 3.1, prior work has shown that such a scheme corresponds to optimizing a generalized inverse reinforcement learning objective, and is a principled way to do reward inference.\n\n> \u201cFig. 1 seems to be overstating the problem with MLE. Logistic regression with regularization would not provide sparse reward\u201d\n\nGood observation! We will correct the scope of claims and note that Fig 1 is a conceptual diagram meant to portray the issues with an unregularized MLE model. In the case with standard regularization such as L2 regularization, we get a reward that is dependent on the particular form of the regularizer, and can often lead to spurious local optima on more complicated examples. On the other hand, an adaptive CNML based regularizer allows for much better shaping as well as inherent exploration to avoid getting stuck in these local optima. To illustrate this, we have added a few figures to our paper:\n- Figure 8 shows the visitations on the spiral maze task for BayCRL as compared to VICE (Fu et al), which does use an L2 regularized classifier, and demonstrates how VICE gets stuck on the other side of the wall. We note that this is also reflected in the resulting performance in our experimental plots in Fig 5.  \n- Appendix A.3.2 which compares additional classifier training schemes, including both unregularized and regularized MLE, on a concrete 2D maze dataset.\n\n> \u201cThe experiments section should provide more details\u201d\n\nWe have added a detailed description of the baselines and evaluation metrics in Appendix A.4 of the updated paper.  We use standard multi-layer perceptrons for the classifier, trained with Adam using standard hyperparameters. Manhattan distance in this setup is referring to the shortest distance along *valid* paths to the goal, respecting the walls. This is reflective of good performance, and it will correctly identify if an agent is in a local optimum.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "0RFfhc7HA15", "original": null, "number": 8, "cdate": 1605668733174, "ddate": null, "tcdate": 1605668733174, "tmdate": 1605668733174, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "T9aS_61UTd-", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Additional Experiment and Revisions", "comment": "We have added an experiment in the Additional Ablations section (Appendix A.5.1) to address your question about the performance of BayCRL in a grid world environment without smooth feature vectors. In this new variant of the Zigzag Maze task, states are first discretized to a 16\u00d716 grid, then \"shuffled\" so that the xy representation of a state does not correspond to its true coordinates and the states are not correlated dynamically. We see that BayCRL still solves the task, whereas a standard classifier method fails. This provides empirical justification for Theorem 4.1 which states that in the absence of any generalization across states, CNML reduces to a count-based exploration bonus, ensuring that the agent continues to explore until it finds the goal.\n\nHowever, when this performance is compared with the performance using semantically meaningful input features, as described in our main results (comparable plot in Appendix A.4.2, Fig 14), we see that the performance with shuffled states is not as consistent. This suggests that the performance of BayCRL cannot simply be attributed to exploration but also to the reward shaping induced by generalization with the Bayesian classifier. We note that this is not a limitation in practice, because in most practical continuous domains (including the ones we consider in our paper), we can expect some degree of smoothness in the features of the environment rather than the completely random gridworld we have constructed in this ablation.\n\nAdditionally, we have added a clarification in Section 6.1 of the paper, indicating that the comparisons with sparse reward and L2 reward refer to running the base Soft Actor-Critic algorithm with reward being either sparse or L2. This comparison is meant to show that even with the algorithm kept the same, the form of the reward is particularly important for facilitating learning."}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "_VhR7tdnRb8", "original": null, "number": 4, "cdate": 1605337807917, "ddate": null, "tcdate": 1605337807917, "tmdate": 1605537785928, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "mLCde7AjY24", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Response to Reviewer Concerns", "comment": "Thank you for your comments and feedback! We have added new visualizations and experiments as described in the shared response. Please find specific responses below: \n\n> \u201cexplaining the connection and improvement on the work of Fu et al.\u201d\n\nWe have added a clarification to Section 6.1. The key difference is indeed the fact that the classifier is trained via CNML, rather than a standard classifier trained with MLE and L2 regularization. As seen in results comparing VICE (Fu et al) and BayCRL (ours) in Fig 5, this makes a significant difference in learning progress and improves reward shaping and exploration.\n\n> \u201cConnection of Theorem 4.1 to develop the algorithm or explain empirical findings.\u201d\n\nTheorem 4.1 connects the Bayesian classifier to exploration, since it shows that the classifier rewards are equivalent to inverse counts as exploration bonuses in the absence of meaningful generalization. This provides intuition for why BayCRL is able to solve challenging exploration tasks such as the mazes. To illustrate this connection, we visualize the rewards in Fig 13 in Appendix A.3.2. (In practice, BayCRL does not simply output inverse counts; the goal examples and generalization additionally allows for better shaping towards the goal, as Fig 13 shows.)\n\nWe have also added a new quantitative metric in terms of the number of states visited by BayCRL versus a standard MLE classifier, available in Fig 8. We see that BayCRL avoids getting stuck in local optima and more quickly visits states in the direction of the goal, indicating the exploration benefits suggested by Theorem 4.1. \n\n> \u201cevidence for CNML is doing better at reward shaping.\u201d \n\nEvidence for the reward shaping of CNML is shown qualitatively in Fig 7, as well as empirically through our comparisons in Fig 5 to a prior classifier reward method (VICE, Fu et al 2018) with exploration bonuses. The fact that BayCRL performs better indicates that it is not simply playing the role of an inverse count bonus, but is also providing better shaping towards the goal.\n\n> \u201cevidence to show CNML gives reward that can improve exploration\u201d\n\nWe have added a quantitative plot (Fig 8) that shows the number of unique states visited using BayCRL as opposed to a standard MLE classifier. We can see that even when the actual reward is not observed, BayCRL encourages the agent to explore and visit additional states. The example in Section 4.1 is simply for schematic illustration; empirical evidence is provided in Fig 8 and the improvements in Fig 5 over standard VICE. This is also reinforced theoretically by Theorem 4.1.\n\n> \u201ceach task only adds one sample to the original dataset. Does this difference have influence on learning the model parameter in meta learning?\u201d:\n\nYes! First, we note that the one sample point can be thought of as the actual \"task\", since the goal is to adapt to that point quickly. The rest of the points are included simply so that the meta-training update corresponds to a step of SGD on the overall dataset, allowing us to take additional steps to converge to CNML if desired.\n\nIn practice, the single datapoint does have a big impact on the learned parameters. To illustrate this we visualize pre-update and post-update predictions in Fig 3, which makes it clear that the outputs are significantly different. We also show how the average absolute difference between pre and post-update predictions increases with additional gradient steps in Fig 12 in the Appendix.\n\n> computation complexity of meta-CNML\u201d\n\nWe have added a table of runtimes for standard feedforward inference, meta-NML, and naive CNML in section A.3.3 of the Appendix. These results demonstrate the significant speedup of meta-CNML over naive CNML, which is key to making our method practical.\n\n> \u201ctest the CNML estimation with full NN convergence on entire augmented dataset\u201d \n\nUnfortunately, running an entire RL experiment with CNML trained to convergence would be impractical, as each epoch would take several hours as noted in section A.3.3. However, we hope the table of runtimes along with the visual and quantitative analysis of meta-NML performance with varying numbers of gradient steps in A.3.1 make the tradeoffs clear.\n\n> \u201cconsider the time complexity in each epoch when validating the time complexity.\u201d\n\nAppendix A.3.3 shows the time complexity per epoch for our classifier schemes. We note that while the runtimes of meta-NML are still slower than feedforward inference (i.e. VICE), in our results, VICE requires far more epochs to converge (resulting in more time taken), or does not solve the task at all.\n\n> \u201cexplanation on how BayCRL outperforms in terms of sample complexity\u201d\n\nWe have added a better visualization of performance via success rate in Fig 5, which paints a clearer picture than the plots in the submission.  In general, BayCRL is the only method that solves all of the tasks consistently, and even if other methods converge earlier, they converge to a suboptimal solution or spurious local optima. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "g4dLxUPGPGa", "original": null, "number": 5, "cdate": 1605338063177, "ddate": null, "tcdate": 1605338063177, "tmdate": 1605480874214, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "0mkkGx_usM", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment", "content": {"title": "Response to Reviewer Concerns", "comment": "Thank you for your thoughtful review and suggestions! Please find specific responses below\n\n> \u201cAblation study on impact of single datapoint added in CNML\u201d\n\nAs noted by the literature on CNML, while adding a single datapoint does not have a huge effect in parts of the state space that have a significant density of points, it causes a significant change in the predicted likelihood for low density regions of the space. To visualize this , we present both a qualitative and quantitative assessment. In Fig 3, we show the estimated classifier likelihoods given by the meta-NML model in state space before and after taking one gradient step. The large difference, especially in regions with low density of states shows the impact of the single datapoint. This is also summed up quantitatively in Fig 12, where we can see that the average difference between the MLE and the NML distribution. \n\n> \u201cstatements about BayCRL seem stronger than the results imply.\u201d\n\nWhile the results in Fig 5 of the original submission do show some improvement of the baselines, they are not quite representative of the actual difference in behaviors of the resulting learned policies. To make this more clear, we plot the success rate of learned behaviors in Fig 5 instead of the previous metric of distance to the goal. This metric more clearly shows the reality of the resulting behavior - whereas the baseline algorithms often get stuck in spurious local optima (not actually achieving success, but potentially having a misleadingly small distance to the goal), BayCRL is consistently more successful. \n\n> \u201cWhy is a prior strictly necessary for reward shaping?\u201d\n\nWithout imposing an appropriate prior, the maximum likelihood classifier can converge to  an arbitrarily sharp decision boundary, with no reward shaping. With a naively chosen prior such as L2 regularization, the shaping can be arbitrarily poor and prone to local optima, as indicated by the poor performance of VICE on our experimental evaluations. With a Bayesian classifier trained with NML, the reward adaptively encourages exploration (see Theorem 4.1, Fig 3, and Fig 8) and provides reward shaping towards the goal (see Fig 7). \n\n> \u201c\"[CNML].. is essentially imposing a uniform prior over the space of possible outcomes\". Further explanation is needed.\u201d\n\nAn intuitive way to understand this is by noting that for a query point x, that has no replicas in the dataset the augmented datasets D U (x, 0) and D U (x, 1) can both fit the pseudo-label perfectly, thereby giving a uniform likelihood over outcomes at 0.5. As more instances of x are encountered, the likelihood is more skewed but the additional pseudo-labelled point essentially smooths the predictions towards uniform. Theorem 4.1 formalizes this intuition in the case of tabular representations.\n\n> \u201cif G(s)>0, shouldn't p(e=1|s)=1?\u201d\n\nGood observation! In fact, the way that they algorithm is structured, at convergence p(e=1|s) for successful outcomes would be 0.5 (balanced large numbers of positives and negatives), and every other state will have p(e=1|s) will be 0. So this still encourages the agent to go towards the goal, and provides the agent with the correct gradient. \n\n> \u201cDoes meta-NML need to retrain to convergence at every query point?\u201d\n\nWe are not sure if this is asking about meta-training or taking gradient steps to evaluate a point according to CNML, so we will address both:\n\nMeta-training: while in principle meta-NML would retrain with every new datapoint that is obtained, in practice we retrain meta-NML once every k epochs. We warm-start the meta-learner parameters from the previous iteration of meta-learning, so every instance of meta-training only requires a few steps of fine-tuning.  We have added these details to Appendix A.2. \n\nTaking gradient steps to evaluate a query point: because our model is meta-trained to adapt quickly to arbitrary inputs, at test time we can take just a single gradient step rather than training to convergence and still obtain a good approximation of the NML outputs. This is the key advantage of using our method. We have clarified this in the paper by adding visualizations in Fig 3 and Appendix A.3.1.\n\n\n> Details for reproducibility and environment details: \n\nWe added a clarification to our ablation experiment \u2014 we take 1 gradient step using the same method as meta-NML, just without any meta-training beforehand. We have also added more environment details in Appendix A.4.\n\n> Visualization of idealized NML rewards\n\nWe have updated the visualization in Fig 3, where we can now see the similarity between the idealized NML and meta-NML rewards.\n\n> \u201canalyzes BayCRL on the zigzag maze task, which I assume is a tabular environment\u201d\n\nTo clarify, the maze task is not a tabular environment; the state space is continuous. (described in Appendix A.4.) However, in our visualizations of the maze, we evaluate the reward at discrete intervals of the state space to show the reward contours.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "OZgVHzdKicb", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2833/Authors|ICLR.cc/2021/Conference/Paper2833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923844029, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Comment"}}}, {"id": "mLCde7AjY24", "original": null, "number": 3, "cdate": 1603955087992, "ddate": null, "tcdate": 1603955087992, "tmdate": 1605024122145, "tddate": null, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "invitation": "ICLR.cc/2021/Conference/Paper2833/-/Official_Review", "content": {"title": "This paper aims to better address a known RL problem that learns reward from a set of successful events", "review": "This manuscript aims to solve reinforcement learning problems where the reward is unknown but a set of successful states are available. Iteratively, it trains a classifier using provided successful states as positive and on-policy samples as negative and use its predictions as the reward function to learn RL policy. \n\nIt may be worth clearly explaining the connection and improvement on the work of Fu et al.(2018b, Variational inverse control with events: A general framework for data-driven reward definition) in introduction since both mainly solve the same problems. Fu\u2019s paper first introduced the event framework to generalize inverse RL that also solves the sparse RL problems by iteratively training a classifier to predict the probability on successful states.\n\nCompared to previous work, it seems a major difference in this paper is to change the event classification model, replacing the neural network classifier with CNML classifier. It could enhance the contribution if  good theoretical analysis can be provided. E.g. why the manuscript concludes that CNML performs better than standard neural network clasisifers in terms of uncertainty aware on reward estimation, and how CNML model connects with better exploration and goal-oriented reward shaping.\n\nOther comments are written below.\n\nTheorem 4.1 is defined, but it seems not used in the following sections. It would be helpful if the following can explain whether it is used  to develop the algorithm or explain empirical findings.\n\nIt may need more evidence in Section 4.3 to support the conclusion that the CNML is doing better at reward shaping. It is not easy to make a conclusion based on some specific example in Figure 1. Similarly Section 4.1 also try to show CNML gives reward that can improve exploration and becomes goal oriented using the same example.\n\nSection 5.1 describes how to use meta learning to approximately solve CNML problem so it can reduce computation complexity. It is an interesting idea, but it may need more analysis. Some questions are listed below.\n\n- The distribution of tasks usually has different samples, while the CNML problem has similar data sets for its tasks as each task only adds one sample to the original dataset. Does this difference have influence on learning the model parameter in meta learning?\n\n- It would be interesting to analyze the computation complexity as it is the main reason for applying meta learning in solving CNML.\n\nIn experiments, It may be interesting to test the CNML estimation with full NN convergence on entire augmented dataset every time, to show the trade-off between accuracy and computation complexity when compared with meta learning.\n\nBased on the number of epochs, it seems BayCRL converges slower in some problems, such as zigzag, spiral, sawyer. It may be interesting to give some insights, as good reward may speed the learning process. Moreover, it may be more accurate to consider the time complexity in each epoch when validating the time complexity.\n\nOn page 7, it may need explanation on how to find that BayCRL outputperforms in terms of sample comcplexity as shown in Figure 4.\n\nSome minor comments:\n- On page 5, Appendix Appendix A.5 -> Appendix A.5 \n- It seems Appendix A.2 duplicates Section 5.1, particularly the Eq. 6 and the equations for p_meta-NML and \\theta_y.\n- Is there any reason for line 7 in Algorithm 1 that skips meta learning.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2833/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2833/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples", "authorids": ["kevintli@berkeley.edu", "~Abhishek_Gupta1", "~Vitchyr_H._Pong1", "~Ashwin_Reddy1", "~Aurick_Zhou1", "justinvyu@berkeley.edu", "~Sergey_Levine1"], "authors": ["Kevin Li", "Abhishek Gupta", "Vitchyr H. Pong", "Ashwin Reddy", "Aurick Zhou", "Justin Yu", "Sergey Levine"], "keywords": ["Reinforcement Learning", "Goal Reaching", "Bayesian Classification", "Reward Inference"], "abstract": "Exploration in reinforcement learning is, in general, a challenging problem. In this work, we study a more tractable class of reinforcement learning problems defined by data that provides examples of successful outcome states. In this case, the reward function can be obtained automatically by training a classifier to classify states as successful or not. We argue that, with appropriate representation and regularization, such a classifier can guide a reinforcement learning algorithm to an effective solution. However, as we will show, this requires the classifier to make uncertainty-aware predictions that are very difficult with standard deep networks. To address this, we propose a novel mechanism for obtaining calibrated uncertainty based on an amortized technique for computing the normalized maximum likelihood distribution. We show that the resulting algorithm has a number of intriguing connections to both count-based exploration methods and prior algorithms for learning reward functions from data, while being able to guide algorithms towards the specified goal more effectively. We show how using amortized normalized maximum likelihood for reward inference is able to provide effective reward guidance for solving a number of challenging navigation and robotic manipulation tasks which prove difficult for other algorithms.", "one-sentence_summary": "Bayesian classifiers allow efficient reinforcement learning and reward inference from outcome examples", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|reinforcement_learning_with_bayesian_classifiers_efficient_skill_learning_from_outcome_examples", "pdf": "/pdf/4cb6111b9f472cc00efdfc95670c7987db5cc420.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xdQNZScZRE", "_bibtex": "@misc{\nli2021reinforcement,\ntitle={Reinforcement Learning with Bayesian Classifiers: Efficient Skill Learning from Outcome Examples},\nauthor={Kevin Li and Abhishek Gupta and Vitchyr H. Pong and Ashwin Reddy and Aurick Zhou and Justin Yu and Sergey Levine},\nyear={2021},\nurl={https://openreview.net/forum?id=OZgVHzdKicb}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "OZgVHzdKicb", "replyto": "OZgVHzdKicb", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2833/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087754, "tmdate": 1606915785944, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2833/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2833/-/Official_Review"}}}], "count": 27}