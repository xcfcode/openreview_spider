{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396639756, "tcdate": 1486396639756, "number": 1, "id": "SJO1azLOg", "invitation": "ICLR.cc/2017/conference/-/paper509/acceptance", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper introduces a number of ideas / heuristics for learning and interpreting deep generative models of text (tf-idf weighting, a combination of using an inference networks with direct optimization of the variational parameters, a method for inducing context-sensitive word embeddings). Generally, the last bit is the most novel, interesting and promising one, however, I agree with the reviewers that empirical evaluation of this technique does not seem sufficient. \n \n Positive:\n -- the ideas are sensible \n -- the paper is reasonably well written and clear\n \n Negative\n -- most ideas are not so novel\n -- the word embedding method requires extra analysis / evaluation, comparison to other methods for producing context-sensitive embeddings, etc"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396641837, "id": "ICLR.cc/2017/conference/-/paper509/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1Chut9xl", "replyto": "r1Chut9xl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396641837}}}, {"tddate": null, "tmdate": 1484428506088, "tcdate": 1484428506088, "number": 6, "id": "rkf1Hzu8l", "invitation": "ICLR.cc/2017/conference/-/paper509/public/comment", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["~Rahul_Krishnan1"], "readers": ["everyone"], "writers": ["~Rahul_Krishnan1"], "content": {"title": "Rebuttal ", "comment": "We thank all the reviewers for their careful reading of our work and for their suggestions. \n\nBelow we highlight additional experiments carried out based on suggestions by reviewers and clarify what set out to achieve with this work. Following which, we respond to individual reviewers inline. \n\nA] We have added to the paper additional experiments and insights that we highlight here: \n* We study how the proposed method for learning performs when varying the sparsity and dimensionality of the data. (Figure 2(c))\n* We have added a section that sheds light on how the feature representations are constructed when using gradients with respect to z in linear and non-linear models. (the last subsection of Section 3: Methodology)\n* We have studied the effect of varying hyperparameters in the inference network while continuing the optimize the variational parameters (Appendix A)\n* We make code for our work available here: https://github.com/rahulk90/inference_introspection\n\nB] In response to [R4] regarding the novelty of our work, we outline the contributions: \n\na] Improving learning: Underfitting is a problem in DLGMs and to improve learning on sparse non-negative data, we blend inference networks with more traditional methods from SVI. On two (large and small) standard text benchmarks, we study the effect of this additional optimization in the context of five different axes: \n(i) Depth of the generative model\n(ii) Inputs to the inference network\n(iii) Number of steps of additional optimization of the variational parameters\n(iv) Dimensionality of the data\n(v) Depth and hidden size of the inference network: (*new* added to Appendix A)\nOverall, across almost all these axes, we find that optimizing the variational parameters improves learning in large models on high-dimensional data.  \n\nb] Building Interpretable Models: We propose and study a novel method of introspection into learned DLGMs. We study two distinct uses for the Jacobian matrix: \n(i) Visualizing Latent Space: We show that it may be used as a readily available measure to visualize how much of the latent space is utilized by the generative model. Unlike the statistic used in Burda et. al (which depends on the learned inference network), the singular values of the Jacobian allow one to directly measure and read off the number of active units in a variational autoencoder. In this work, we use them to understand the effect of optimizing the variational parameters. \n\n(ii) Embeddings: Given any learned model, we show that one may obtain embeddings of features using the Jacobian matrix. We evaluated these embeddings on three diverse kinds of data: text, movie ratings (supplementary material) and EHR data and found the method to yield qualitatively sensible results. Text is naturally sequential and the surrounding words present a natural source of context. In domains such as movie ratings, this may not be the case and defining a context in order to train a method such as Word2Vec may be difficult. The Jacobian represents a simple method for obtaining feature representations from any learned VAE. In domains such as medicine, interpretability plays a crucial role in the adoption of methods into the clinical workflow and this paper attempts to bridge the gap between powerful density estimation and interpretability. \n\nC] Concerns brought up by multiple reviewers: \n* Using vectors in a supervised task: This is an excellent suggestion that we are working on. In this work we investigated some of the interesting properties of Jacobian vectors and how they varied as a function of the generative model on diverse domains such as text and medical data. \n\n* Quantitative Results: While our quantitative results are competitive on the medical data, for text data we believe that by augmenting the model with the ability to also take local context into account during learning (as is done in the works of Neelakantan et al and Chen et al brought up by R1), we can hope for further improvements and more comparisons in follow up work.  Indeed, we outperform a comparable method described in Huang et. al that leverages only global information. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287546437, "id": "ICLR.cc/2017/conference/-/paper509/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Chut9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper509/reviewers", "ICLR.cc/2017/conference/paper509/areachairs"], "cdate": 1485287546437}}}, {"tddate": null, "tmdate": 1484428327254, "tcdate": 1484428327254, "number": 5, "id": "SkyVVzdIg", "invitation": "ICLR.cc/2017/conference/-/paper509/public/comment", "forum": "r1Chut9xl", "replyto": "rJOsPmYrl", "signatures": ["~Rahul_Krishnan1"], "readers": ["everyone"], "writers": ["~Rahul_Krishnan1"], "content": {"title": "Response to Review", "comment": "Thank you for your review and your detailed comments!\n\n\u201c(line 5 of Algorithm 1) uses the very first prediction of local parameters (thus ignoring the optimisation in step 3), this is unclear to me.\u201d \nWe apologize for the misunderstanding. Our intention was to point out that line 5 updated the global variational parameters holding theta fixed by ascending in the ELBO. We\u2019ve updated the notation to make this a bit clearer. In our early experiments, we also tried variants of line 5 such as updating phi to minimize the KL divergence with between psi_1 and psi_M but we found taking a gradient step to work comparably. \n\n\u201cPlease, discuss why you expect the initialisation problem to be worse in the case of sparse data.\u201d\nWe hypothesize this is because of rare features in sparse data. The gradient signal for learning the global variational parameters comes (via stochastic backpropagation) from taking derivatives of the ELBO. As the vocabulary size grows, each minibatch is less likely to have every word appearing in it and the gradient signal that tells the inference network how to perform inference for documents with rare words gets weaker. This in turn means the inference network takes longer to learn how to perform inference well, which affects the quality of inference early on in learning. By optimizing the variational parameters, the gradients for the generative model are rendered less susceptible to limitations in the inference network. \n\nWe attempted to probe into this question empirically with the experiment depicted in Figure 2(c). We took data that was originally sparse and we artificially made it less so by restricting the subset of features to the most frequently occurring ones. We found that our method helps most as the data gets sparser and more-high dimensional. \n\n\u201cAre the authors pretty sure that in Figure 2b models with M=1 have reached a plateau (so that longer training would not allow them to catch up with M=100 curves)? As the authors explain in the caption, x-axis is not comparable on running time, thus the question.\u201d\nWhile this is not an experiment we have tried on the wikipedia data, we have tried this on the RCV2 data where we trained for 400 rather than 200 epochs. We did not find that M=1 caught up to M=100. \n\nOn the issue of running time, the training curves presented in Figure 2(a) and (b) suggest that much of the gains in learning from our method are obtained early on in optimization and the number of steps of optimizing the variational parameters may be slowly reduced as a means of reducing the overall training time. \n\n\u201cI think the authors could have walked readers through them.\u201d\nWe have added in more detail in the main text describing the significance of the two plots and why the Jacobian may be appropriate to study the number of active units in the model. \n\n\u201cI wonder whether this construct needs to be carefully designed in order to get Table 2b.\u201d\nThis is a good point. We found that the contextual word vectors varied were depending on the length of the document we used for inference and how well the model was trained. We\u2019re currently looking into methods to quantify this. However, the table exists primarily as a proof of concept that such an operation is indeed possible with this method.  As for the code, the repository will contain finalized the procedure to setup the data for the experiment. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287546437, "id": "ICLR.cc/2017/conference/-/paper509/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Chut9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper509/reviewers", "ICLR.cc/2017/conference/paper509/areachairs"], "cdate": 1485287546437}}}, {"tddate": null, "tmdate": 1484428228182, "tcdate": 1484428228182, "number": 4, "id": "SJnTXGu8x", "invitation": "ICLR.cc/2017/conference/-/paper509/public/comment", "forum": "r1Chut9xl", "replyto": "SJyzFeNHx", "signatures": ["~Rahul_Krishnan1"], "readers": ["everyone"], "writers": ["~Rahul_Krishnan1"], "content": {"title": "Response to Review", "comment": "Thank you for your review! We\u2019ve fixed the typo and addressed your other question above. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287546437, "id": "ICLR.cc/2017/conference/-/paper509/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Chut9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper509/reviewers", "ICLR.cc/2017/conference/paper509/areachairs"], "cdate": 1485287546437}}}, {"tddate": null, "tmdate": 1484428180353, "tcdate": 1484428180353, "number": 3, "id": "SkhcQMuLl", "invitation": "ICLR.cc/2017/conference/-/paper509/public/comment", "forum": "r1Chut9xl", "replyto": "H1Dc01zNe", "signatures": ["~Rahul_Krishnan1"], "readers": ["everyone"], "writers": ["~Rahul_Krishnan1"], "content": {"title": "Response to Review", "comment": "Thank you for your review! We hope you reconsider your score based on some of the additional insights and experiments we have highlighted above. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287546437, "id": "ICLR.cc/2017/conference/-/paper509/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Chut9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper509/reviewers", "ICLR.cc/2017/conference/paper509/areachairs"], "cdate": 1485287546437}}}, {"tddate": null, "tmdate": 1484428147936, "tcdate": 1484428061086, "number": 2, "id": "r1rQXGO8x", "invitation": "ICLR.cc/2017/conference/-/paper509/public/comment", "forum": "r1Chut9xl", "replyto": "HyQznHgVe", "signatures": ["~Rahul_Krishnan1"], "readers": ["everyone"], "writers": ["~Rahul_Krishnan1"], "content": {"title": "Response to Review", "comment": "Thank you for your review and comments! We are currently looking into extensions of the model that incorporate local context during learning that will aid comparison to work by Neelakantan et. al and Chen et. al. Regarding tradeoffs in accuracy, please see Appendix A where we have experimented with learning under different architectures of the inference network. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287546437, "id": "ICLR.cc/2017/conference/-/paper509/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Chut9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper509/reviewers", "ICLR.cc/2017/conference/paper509/areachairs"], "cdate": 1485287546437}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484427929547, "tcdate": 1478297786485, "number": 509, "id": "r1Chut9xl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1Chut9xl", "signatures": ["~Rahul_Krishnan1"], "readers": ["everyone"], "content": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483450272224, "tcdate": 1483450272224, "number": 4, "id": "rJOsPmYrl", "invitation": "ICLR.cc/2017/conference/-/paper509/official/review", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["ICLR.cc/2017/conference/paper509/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper509/AnonReviewer5"], "content": {"title": "Interesting ideas that could have been explored in greater depth", "rating": "6: Marginally above acceptance threshold", "review": "The paper claims improved inference for density estimation of sparse data (here text documents) using deep generative Gaussian models (variational auto-encoders), and a method for deriving word embeddings from the model's generative parameters that allows for a degree of interpretability similar to that of Bayesian generative topic models.\n\nTo discuss the contributions I will quickly review the generative story in the paper: first a K-dimensional latent representation is sampled from a multivariate Gaussian, then an MLP (with parameters \\theta) predicts unnormalised potentials over a vocabulary of V words, the potentials are exponentiated and normalised to make the parameters of a multinomial from where word observations are repeatedly sampled to make a document. Here intractable inference is replaced by the VAE formulation where an inference network (with parameters \\phi) independently predicts for each document the mean and variance of a normal distribution (amenable to reparameterised gradient computation).\n\nThe first, and rather trivial, contribution is to use tf-idf features to inject first order statistics (a global information) into local observations. The authors claim that this is particularly helpful in the case of sparse data such as text.\n\nThe second contribution is more interesting. In optimising generative parameters (\\theta) and variational parameters (\\phi), the authors turn to a treatment which is reminiscent of the original SVI procedure. That is, they see the variational parameters \\phi as *global* variational parameters, and the predicted mean \\mu(x) and covariance \\Sigma(x) of each observation x are treated as *local* variational parameters. In the original VAE, local parameters are not directly optimised, instead they are indirectly optimised via optimisation of the global parameters utilised in their prediction (shared MLP parameters). Here, local parameters are optimised holding generative parameters fixed (line 3 of Algorithm 1). The optimised local parameters are then used in the gradient step of the generative parameters (line 4 of Algorithm 1). Finally, global variational parameters are also updated (line 5). \nWhereas indeed other authors have proposed to optimise local parameters, I think that deriving this procedure from the more familiar SVI makes the contribution less of a trick and easier to relate to.\n\nSome things aren't entirely clear to me. I think it would have been nice if the authors had shown the functional form of the gradient used in step 3 of Algorithm 1. The gradient step for global variational parameters (line 5 of Algorithm 1) uses the very first prediction of local parameters (thus ignoring the optimisation in step 3), this is unclear to me. Perhaps I am missing a fundamental reason why that has to be the case (either way, please clarify).\nThe authors argue that this optimisation turns out helpful to modelling sparse data because there is evidence that the generative model p_\\theta(x|z) suffers from poor initialisation. Please, discuss why you expect the initialisation problem to be worse in the case of sparse data.\n\nThe final contribution is a neat procedure to derive word embeddings from the generative model parameters. These embeddings are then used to interpret what the model has learnt. Interestingly, these word embeddings are context-sensitive once that the latent variable models an entire document.\n\nAbout Figures 2a and 2b: the caption says that solid lines indicate validation perplexity for M=1 (no optimisation of local parameters) and dashed lines indicate M=100 (100 iterations of optimisation of local parameters), but the legends of the figures suggest a different reading. If I interpret the figures based on the caption, then it seems that indeed deeper networks exposed to more data benefit from optimisation of local parameters. Are the authors pretty sure that in Figure 2b models with M=1 have reached a plateau (so that longer training would not allow them to catch up with M=100 curves)? As the authors explain in the caption, x-axis is not comparable on running time, thus the question.\n\nThe analysis of singular values seems like an interesting way to investigate how the model is using its capacity.  However, I can barely interpret Figures 2c and 2d, I think the authors could have walked readers through them.\n\nAs for the word embedding I am missing an evaluation on a predictive task. Also, while illustrative, Table 2b is barely reproducible. The text reads \"we create a document comprising a subset of words in the the context\u2019s Wikipedia page.\" which is rather vague. I wonder whether this construct needs to be carefully designed in order to get Table 2b.\n\nIn sum, I have a feeling that the inference technique and the embedding technique are both useful, but perhaps they should have been presented separately so that each could have been explored in greater depth.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483450272756, "id": "ICLR.cc/2017/conference/-/paper509/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper509/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper509/AnonReviewer1", "ICLR.cc/2017/conference/paper509/AnonReviewer4", "ICLR.cc/2017/conference/paper509/AnonReviewer3", "ICLR.cc/2017/conference/paper509/AnonReviewer5"], "reply": {"forum": "r1Chut9xl", "replyto": "r1Chut9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483450272756}}}, {"tddate": null, "tmdate": 1483110663179, "tcdate": 1483110663179, "number": 3, "id": "SJyzFeNHx", "invitation": "ICLR.cc/2017/conference/-/paper509/official/review", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["ICLR.cc/2017/conference/paper509/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper509/AnonReviewer3"], "content": {"title": "Good ; lacks more decisive experiments", "rating": "7: Good paper, accept", "review": "First I would like to apologize for the delay in reviewing.\n\nSummary : In this paper a variational inference is adapted to deep generative models, showing improvement for non-negative sparse dataset. The authors offer as well a method to interpret the data through the model parameters.\n\nThe writing is generally clear. The methods seem correct. The introspection approach appears to be original. I found very interesting the experiment on the polysemic word embedding. I would however have like to see how the obtained embedding would perform with respect to other more common embeddings in solving a supervised task.\n\nMinor :\nEq. 2: too many closing parentheses", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483450272756, "id": "ICLR.cc/2017/conference/-/paper509/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper509/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper509/AnonReviewer1", "ICLR.cc/2017/conference/paper509/AnonReviewer4", "ICLR.cc/2017/conference/paper509/AnonReviewer3", "ICLR.cc/2017/conference/paper509/AnonReviewer5"], "reply": {"forum": "r1Chut9xl", "replyto": "r1Chut9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483450272756}}}, {"tddate": null, "tmdate": 1481928335040, "tcdate": 1481928335040, "number": 2, "id": "H1Dc01zNe", "invitation": "ICLR.cc/2017/conference/-/paper509/official/review", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["ICLR.cc/2017/conference/paper509/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper509/AnonReviewer4"], "content": {"title": "Decent paper, but lacking novelty", "rating": "5: Marginally below acceptance threshold", "review": "This paper introduces three tricks for training deep latent variable models on sparse discrete data:\n1) tf-idf weighting\n2) Iteratively optimizing variational parameters after initializing them with an inference network\n3) A technique for improving the interpretability of the deep model\n\nThe first idea is sensible but rather trivial as a contribution. The second idea is also sensible, but is conceptually not novel. What is new is the finding that it works well for the dataset used in this paper.\n\nThe third idea is interesting, and seems to give qualitatively reasonable results. The quantitative semantic similarity results don\u2019t seem that convincing, but I am not very familiar with the relevant literature and therefore cannot make a confident judgement on this issue.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483450272756, "id": "ICLR.cc/2017/conference/-/paper509/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper509/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper509/AnonReviewer1", "ICLR.cc/2017/conference/paper509/AnonReviewer4", "ICLR.cc/2017/conference/paper509/AnonReviewer3", "ICLR.cc/2017/conference/paper509/AnonReviewer5"], "reply": {"forum": "r1Chut9xl", "replyto": "r1Chut9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483450272756}}}, {"tddate": null, "tmdate": 1481821195256, "tcdate": 1481821195251, "number": 1, "id": "HyQznHgVe", "invitation": "ICLR.cc/2017/conference/-/paper509/official/review", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["ICLR.cc/2017/conference/paper509/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper509/AnonReviewer1"], "content": {"title": "Weak reject", "rating": "5: Marginally below acceptance threshold", "review": "This paper presents a small trick to improve the model quality of variational autoencoders (further optimizing the ELBO while initializing it from the predictions of the q network, instead of just using those directly) and the idea of using Jacobian vectors to replace simple embeddings when interpreting variational autoencoders.\n\nThe idea of the Jacobian as a natural replacement for embeddings is interesting, as it does seem to cleanly generalize the notion of embeddings from linear models. It'd be interesting to see comparisons with other work seeking to provide context-specific embeddings, either by clustering or by smarter techniques (like Neelakantan et al, Efficient non-parametric estimation of multiple embeddings per word in vector space, or Chen et al A Unified Model for Word Sense Representation and Disambiguation). With the evidence provided in the experimental section of the paper it's hard to be convinced that the Jacobian of VAE-generated embeddings is substantially better at being context-sensitive than prior work.\n\nSimilarly, the idea of further optimizing the ELBO is interesting but not fully explored. It's unclear, for example, what is the tradeoff between the complexity of the q network and steps further optimizing the ELBO, in terms of compute versus accuracy.\n\nOverall the ideas in this paper are good but I'd like to see them a little more fleshed out.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483450272756, "id": "ICLR.cc/2017/conference/-/paper509/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper509/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper509/AnonReviewer1", "ICLR.cc/2017/conference/paper509/AnonReviewer4", "ICLR.cc/2017/conference/paper509/AnonReviewer3", "ICLR.cc/2017/conference/paper509/AnonReviewer5"], "reply": {"forum": "r1Chut9xl", "replyto": "r1Chut9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483450272756}}}, {"tddate": null, "tmdate": 1481738792066, "tcdate": 1481738792060, "number": 1, "id": "HJgEqbJVe", "invitation": "ICLR.cc/2017/conference/-/paper509/public/comment", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["~Rahul_Krishnan1"], "readers": ["everyone"], "writers": ["~Rahul_Krishnan1"], "content": {"title": "Responses to initial comments", "comment": "[R1] Yes. Though our point was to highlight that instead of just conditioning on the input directly, thinking about how to incorporate other characteristics of the data the inference network typically has no access to (such as global first order statistics, which in the case of non-negative data corresponds to tf-idf features) helps learning.  We have updated the algorithm box to make clear the role of the additional optimization of the local variational parameters in the revised version. \n\n[R4] The current implementation is about 15x slower (we have emphasized this in the caption for Fig. 2) since we do 100 steps of optimizing the variational parameter. It is possible that this additional optimization is primarily helpful in the first few epochs after which one could resume training normally, though we did not experiment with such variants. \n\n[R3] x is the same as x_d. We have clarified the notation in the revised version. We haven\u2019t tried our embeddings on prediction tasks for text as yet. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287546437, "id": "ICLR.cc/2017/conference/-/paper509/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1Chut9xl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper509/reviewers", "ICLR.cc/2017/conference/paper509/areachairs"], "cdate": 1485287546437}}}, {"tddate": null, "tmdate": 1480864600642, "tcdate": 1480864600637, "number": 3, "id": "BkZw72-Xe", "invitation": "ICLR.cc/2017/conference/-/paper509/pre-review/question", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["ICLR.cc/2017/conference/paper509/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper509/AnonReviewer3"], "content": {"title": "Notation clarification and experiment", "question": "\n\n- Notation clarification: x_{1:D} is a set of D sparse V-dimensional vectors? x_{dv} is the number of times the v^th word in the dictionary appeared in document d? What is x? The same as x_d?\n\n- Experiment: Did you consider using your embeddings to solve a prediction task (text categorization in RCV2) and see if what they capture yields a better result than using simply bag-of-words tf-idf? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959241503, "id": "ICLR.cc/2017/conference/-/paper509/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper509/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper509/AnonReviewer1", "ICLR.cc/2017/conference/paper509/AnonReviewer4", "ICLR.cc/2017/conference/paper509/AnonReviewer3"], "reply": {"forum": "r1Chut9xl", "replyto": "r1Chut9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959241503}}}, {"tddate": null, "tmdate": 1480703069526, "tcdate": 1480703069523, "number": 2, "id": "B1SPnEyme", "invitation": "ICLR.cc/2017/conference/-/paper509/pre-review/question", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["ICLR.cc/2017/conference/paper509/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper509/AnonReviewer4"], "content": {"title": "Computational cost", "question": "How much slower is it to iteratively optimize the ELBO compared to just using the inference network?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959241503, "id": "ICLR.cc/2017/conference/-/paper509/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper509/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper509/AnonReviewer1", "ICLR.cc/2017/conference/paper509/AnonReviewer4", "ICLR.cc/2017/conference/paper509/AnonReviewer3"], "reply": {"forum": "r1Chut9xl", "replyto": "r1Chut9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959241503}}}, {"tddate": null, "tmdate": 1480530086201, "tcdate": 1480530086198, "number": 1, "id": "ryRi_9hGg", "invitation": "ICLR.cc/2017/conference/-/paper509/pre-review/question", "forum": "r1Chut9xl", "replyto": "r1Chut9xl", "signatures": ["ICLR.cc/2017/conference/paper509/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper509/AnonReviewer1"], "content": {"title": "Questions about the contributions (section 3)", "question": "Am I to understand that using tf-idf weighting is really the first contribution? Seems like a good idea, but weird to highlight to that level.\n\nIn Algorithm 1, step 8, it's not clear how psi is being reestimated. Similarly, step 5 could be clearer about what exactly is being sampled and how this is used to compute the gradient.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inference and Introspection in Deep Generative Models of Sparse Data", "abstract": "Deep generative models such as deep latent Gaussian models (DLGMs) are powerful and popular density estimators. However, they have been applied almost exclusively to dense data such as images; DLGMs are rarely applied to sparse, high-dimensional integer data such as word counts or product ratings. One reason is that the standard training procedures find poor local optima when applied to such data. We propose two techniques that alleviate this problem, significantly improving our ability to fit DLGMs to sparse, high-dimensional data. Having fit these models, we are faced with another challenge: how to use and interpret the representation that we have learned? To that end, we propose a method that extracts distributed representations of features via a simple linearization of the model. ", "pdf": "/pdf/fd7c347ba39489dc35aa40bd6716af6e76c6af68.pdf", "TL;DR": "We study two techniques to improve learning in deep generative models on sparse, high-dimensional text data. We also propose an algorithmic tool to visualize and introspect arbitrarily deep learned models.\u00a0", "paperhash": "krishnan|inference_and_introspection_in_deep_generative_models_of_sparse_data", "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cs.nyu.edu", "adobe.com", "columbia.edu"], "authors": ["Rahul G. Krishnan", "Matthew Hoffman"], "authorids": ["rahul@cs.nyu.edu", "matthoffm@adobe.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959241503, "id": "ICLR.cc/2017/conference/-/paper509/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper509/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper509/AnonReviewer1", "ICLR.cc/2017/conference/paper509/AnonReviewer4", "ICLR.cc/2017/conference/paper509/AnonReviewer3"], "reply": {"forum": "r1Chut9xl", "replyto": "r1Chut9xl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper509/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959241503}}}], "count": 15}