{"notes": [{"id": "BJWfW2C9Y7", "original": "HJxVTDqYtm", "number": 1149, "cdate": 1538087929850, "ddate": null, "tcdate": 1538087929850, "tmdate": 1545355406565, "tddate": null, "forum": "BJWfW2C9Y7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Predictive Local Smoothness for Stochastic Gradient Methods", "abstract": "Stochastic gradient methods are dominant in nonconvex optimization especially for deep models but have low asymptotical convergence due to the fixed smoothness. To address this problem, we propose a simple yet effective method for improving stochastic gradient methods named predictive local smoothness (PLS). First, we create a convergence condition to build a learning rate varied adaptively with local smoothness. Second, the local smoothness can be predicted by the latest gradients. Third, we use the adaptive learning rate to update the stochastic gradients for exploring linear convergence rates. By applying the PLS method, we implement new variants of three popular algorithms: PLS-stochastic gradient descent (PLS-SGD), PLS-accelerated SGD (PLS-AccSGD), and PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their linear convergence. Empirical results show that our variants have better performance gains than the popular algorithms, such as, faster convergence and alleviating explosion and vanish of gradients.", "keywords": ["stochastic gradient method", "local smoothness", "linear system", "AMSGrad"], "authorids": ["junl.mldl@gmail.com", "hongfuliu@brandeis.edu", "bnzhong@gmail.com", "wuyuebupt@gmail.com", "yunfu@ece.neu.edu"], "authors": ["Jun Li", "Hongfu Liu", "Bineng Zhong", "Yue Wu", "Yun Fu"], "pdf": "/pdf/ddb07bb24b7bf0524af557f7195870382d60e310.pdf", "paperhash": "li|predictive_local_smoothness_for_stochastic_gradient_methods", "_bibtex": "@misc{\nli2019predictive,\ntitle={Predictive Local Smoothness for Stochastic Gradient Methods},\nauthor={Jun Li and Hongfu Liu and Bineng Zhong and Yue Wu and Yun Fu},\nyear={2019},\nurl={https://openreview.net/forum?id=BJWfW2C9Y7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1xrg92keN", "original": null, "number": 1, "cdate": 1544698348644, "ddate": null, "tcdate": 1544698348644, "tmdate": 1545354507070, "tddate": null, "forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1149/Meta_Review", "content": {"metareview": "Dear authors,\n\nAll reviewers pointed to severe issues with the analysis, making the paper unsuitable for publication to ICLR. Please take their comments into account should you decide to resubmit this work.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Issues with the analysis"}, "signatures": ["ICLR.cc/2019/Conference/Paper1149/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1149/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predictive Local Smoothness for Stochastic Gradient Methods", "abstract": "Stochastic gradient methods are dominant in nonconvex optimization especially for deep models but have low asymptotical convergence due to the fixed smoothness. To address this problem, we propose a simple yet effective method for improving stochastic gradient methods named predictive local smoothness (PLS). First, we create a convergence condition to build a learning rate varied adaptively with local smoothness. Second, the local smoothness can be predicted by the latest gradients. Third, we use the adaptive learning rate to update the stochastic gradients for exploring linear convergence rates. By applying the PLS method, we implement new variants of three popular algorithms: PLS-stochastic gradient descent (PLS-SGD), PLS-accelerated SGD (PLS-AccSGD), and PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their linear convergence. Empirical results show that our variants have better performance gains than the popular algorithms, such as, faster convergence and alleviating explosion and vanish of gradients.", "keywords": ["stochastic gradient method", "local smoothness", "linear system", "AMSGrad"], "authorids": ["junl.mldl@gmail.com", "hongfuliu@brandeis.edu", "bnzhong@gmail.com", "wuyuebupt@gmail.com", "yunfu@ece.neu.edu"], "authors": ["Jun Li", "Hongfu Liu", "Bineng Zhong", "Yue Wu", "Yun Fu"], "pdf": "/pdf/ddb07bb24b7bf0524af557f7195870382d60e310.pdf", "paperhash": "li|predictive_local_smoothness_for_stochastic_gradient_methods", "_bibtex": "@misc{\nli2019predictive,\ntitle={Predictive Local Smoothness for Stochastic Gradient Methods},\nauthor={Jun Li and Hongfu Liu and Bineng Zhong and Yue Wu and Yun Fu},\nyear={2019},\nurl={https://openreview.net/forum?id=BJWfW2C9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1149/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352948315, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1149/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1149/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1149/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352948315}}}, {"id": "rkgX7TrIp7", "original": null, "number": 4, "cdate": 1541983515393, "ddate": null, "tcdate": 1541983515393, "tmdate": 1541983515393, "tddate": null, "forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1149/Official_Review", "content": {"title": "Unrealistic assumptions, trivial theory", "review": "\n# Unrealistic assumptions and trivial theory\n\nThis papers proposes a method to adjust the learning rate of stochastic gradient methods. The problem is of great importance but the theoretical results and presentation contain many issues that make the paper unfit for publication.\n\nThe main issue that I see is that the assumption made are unrealistic and make the theory trivial. First, for gradient descent, the authors assume that the gradient is of the form L(x_t) (x_t - x*). Under this assumption, gradient descent converges on a single step with step size 1 / L(x_t). In the stochastic setting, they assume that *each* stochastic gradient is of the form L_i(x_t) (x_t - x*), Eq. (11). Again, SGD in this scenario converges in a single iteration with step size 1 / L_i(x_t).\n\nNo wonder in this scenario the authors are able to obtain linear convergence of SGD to arbitrary precision (which is known to be impossible even for quadratics).\n\n\n# Other Issues\n\n* Motivation of Eq. (9) is not discussed in sufficient detail. It is unclear to me how to obtain (9) from (7) as the authors mention. Regarding notation, L(x_t) is a scalar, hence (9) could be written more simply as \\nabla f(x_t) = L(x_t) (x_t - x*). Why the need for the Kronecker product?\n\n* The authors should clearly state what are the assumptions in the theorem statement. For theorem 1 these are not clearly stated, and phrases like \"Theorem 1 provides a simple condition for the linear convergence of SGD\" give the wrong impression that the Theorem is widely applicable.\n\n\n# Minor\n  * Belo Eq. (10): \"where \\epsilon_1 is a parameter to prevent ||x_t - x_{t-1}|| going to zero: . I guess what the authors meant is to prevent *the denominator* going to zero, you do want ||x_t - x_{t-1}|| to go to zero as you approach a stationary point\n", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1149/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predictive Local Smoothness for Stochastic Gradient Methods", "abstract": "Stochastic gradient methods are dominant in nonconvex optimization especially for deep models but have low asymptotical convergence due to the fixed smoothness. To address this problem, we propose a simple yet effective method for improving stochastic gradient methods named predictive local smoothness (PLS). First, we create a convergence condition to build a learning rate varied adaptively with local smoothness. Second, the local smoothness can be predicted by the latest gradients. Third, we use the adaptive learning rate to update the stochastic gradients for exploring linear convergence rates. By applying the PLS method, we implement new variants of three popular algorithms: PLS-stochastic gradient descent (PLS-SGD), PLS-accelerated SGD (PLS-AccSGD), and PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their linear convergence. Empirical results show that our variants have better performance gains than the popular algorithms, such as, faster convergence and alleviating explosion and vanish of gradients.", "keywords": ["stochastic gradient method", "local smoothness", "linear system", "AMSGrad"], "authorids": ["junl.mldl@gmail.com", "hongfuliu@brandeis.edu", "bnzhong@gmail.com", "wuyuebupt@gmail.com", "yunfu@ece.neu.edu"], "authors": ["Jun Li", "Hongfu Liu", "Bineng Zhong", "Yue Wu", "Yun Fu"], "pdf": "/pdf/ddb07bb24b7bf0524af557f7195870382d60e310.pdf", "paperhash": "li|predictive_local_smoothness_for_stochastic_gradient_methods", "_bibtex": "@misc{\nli2019predictive,\ntitle={Predictive Local Smoothness for Stochastic Gradient Methods},\nauthor={Jun Li and Hongfu Liu and Bineng Zhong and Yue Wu and Yun Fu},\nyear={2019},\nurl={https://openreview.net/forum?id=BJWfW2C9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1149/Official_Review", "cdate": 1542234294655, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1149/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335883460, "tmdate": 1552335883460, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1149/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkxhyCoV67", "original": null, "number": 3, "cdate": 1541877220414, "ddate": null, "tcdate": 1541877220414, "tmdate": 1541877220414, "tddate": null, "forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1149/Official_Review", "content": {"title": "The Analysis seems incorrect", "review": "The paper proposes to use an estimate of the 'local' smoothness constructed by taking the difference of the gradients along the previous step. This is a simple idea and has been considered before in literature. The authors seem to take a very simplistic approach to the problem which seems to not work at all in high dimensions. I am reasonable certain that the analysis is incorrect as it is impossible to get linear convergence via SGD or even with GD in general settings. Looking at the proof which is written in a very unreadable way reveals that they make multiple assumptions which holds basically in the case of a quadratic and then further only in one dimension. In which case such a rate with GD is trivial. \n\nSo the theory is blatantly wrong. Regarding the experiments they also look shaky at best and sometimes they diverge. I believe the paper is much below standard for ICLR. ", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1149/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predictive Local Smoothness for Stochastic Gradient Methods", "abstract": "Stochastic gradient methods are dominant in nonconvex optimization especially for deep models but have low asymptotical convergence due to the fixed smoothness. To address this problem, we propose a simple yet effective method for improving stochastic gradient methods named predictive local smoothness (PLS). First, we create a convergence condition to build a learning rate varied adaptively with local smoothness. Second, the local smoothness can be predicted by the latest gradients. Third, we use the adaptive learning rate to update the stochastic gradients for exploring linear convergence rates. By applying the PLS method, we implement new variants of three popular algorithms: PLS-stochastic gradient descent (PLS-SGD), PLS-accelerated SGD (PLS-AccSGD), and PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their linear convergence. Empirical results show that our variants have better performance gains than the popular algorithms, such as, faster convergence and alleviating explosion and vanish of gradients.", "keywords": ["stochastic gradient method", "local smoothness", "linear system", "AMSGrad"], "authorids": ["junl.mldl@gmail.com", "hongfuliu@brandeis.edu", "bnzhong@gmail.com", "wuyuebupt@gmail.com", "yunfu@ece.neu.edu"], "authors": ["Jun Li", "Hongfu Liu", "Bineng Zhong", "Yue Wu", "Yun Fu"], "pdf": "/pdf/ddb07bb24b7bf0524af557f7195870382d60e310.pdf", "paperhash": "li|predictive_local_smoothness_for_stochastic_gradient_methods", "_bibtex": "@misc{\nli2019predictive,\ntitle={Predictive Local Smoothness for Stochastic Gradient Methods},\nauthor={Jun Li and Hongfu Liu and Bineng Zhong and Yue Wu and Yun Fu},\nyear={2019},\nurl={https://openreview.net/forum?id=BJWfW2C9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1149/Official_Review", "cdate": 1542234294655, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1149/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335883460, "tmdate": 1552335883460, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1149/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lsYBrI2m", "original": null, "number": 2, "cdate": 1540932995461, "ddate": null, "tcdate": 1540932995461, "tmdate": 1541533381369, "tddate": null, "forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1149/Official_Review", "content": {"title": "Limitedness of contribution and incorrectness of analysis", "review": "This paper considers the finite-sum optimization problem that is typically seen in machine learning, and proposes methods that adaptively adjust the learning rate by estimating the local Lipschitz constant of the gradient. \n\nThe contributions of the paper seem very limited.  The proposed method which estimates the local Lipschitz constant of the gradient, named local predictive local smoothness (PLS) method in the paper (equation (10)), has been proposed in [1] long ago (see equation (11) in [1]) and is very well-known to the community. It is quite surprising that the authors claim to be the first to propose this while completely ignoring previous works.\n\nI also believe that there are major issues with the analysis for the methods. For example, I do not understand how equation (9) could possibly hold for general functions, and how it could be possible to transform their method into the linear system in (11). Therefore I do not think this paper is technically correct. \n\nIn summary, I believe this paper is limited in its contribution and also has major issues in terms of technical correctness, and is well below the standard for ICLR. \n\nReference: \n\n[1] Magoulas, G. D., Vrahatis, M. N., & Androulakis, G. S. (1997). Effective backpropagation training with variable stepsize. Neural networks, 10(1), 69-82.\n\n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1149/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predictive Local Smoothness for Stochastic Gradient Methods", "abstract": "Stochastic gradient methods are dominant in nonconvex optimization especially for deep models but have low asymptotical convergence due to the fixed smoothness. To address this problem, we propose a simple yet effective method for improving stochastic gradient methods named predictive local smoothness (PLS). First, we create a convergence condition to build a learning rate varied adaptively with local smoothness. Second, the local smoothness can be predicted by the latest gradients. Third, we use the adaptive learning rate to update the stochastic gradients for exploring linear convergence rates. By applying the PLS method, we implement new variants of three popular algorithms: PLS-stochastic gradient descent (PLS-SGD), PLS-accelerated SGD (PLS-AccSGD), and PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their linear convergence. Empirical results show that our variants have better performance gains than the popular algorithms, such as, faster convergence and alleviating explosion and vanish of gradients.", "keywords": ["stochastic gradient method", "local smoothness", "linear system", "AMSGrad"], "authorids": ["junl.mldl@gmail.com", "hongfuliu@brandeis.edu", "bnzhong@gmail.com", "wuyuebupt@gmail.com", "yunfu@ece.neu.edu"], "authors": ["Jun Li", "Hongfu Liu", "Bineng Zhong", "Yue Wu", "Yun Fu"], "pdf": "/pdf/ddb07bb24b7bf0524af557f7195870382d60e310.pdf", "paperhash": "li|predictive_local_smoothness_for_stochastic_gradient_methods", "_bibtex": "@misc{\nli2019predictive,\ntitle={Predictive Local Smoothness for Stochastic Gradient Methods},\nauthor={Jun Li and Hongfu Liu and Bineng Zhong and Yue Wu and Yun Fu},\nyear={2019},\nurl={https://openreview.net/forum?id=BJWfW2C9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1149/Official_Review", "cdate": 1542234294655, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1149/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335883460, "tmdate": 1552335883460, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1149/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkxtxbaDjQ", "original": null, "number": 1, "cdate": 1539981553021, "ddate": null, "tcdate": 1539981553021, "tmdate": 1541533381142, "tddate": null, "forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1149/Official_Review", "content": {"title": "Experimental results are too weak", "review": "In the paper, the authors try to propose an adaptive learning rate method called predictive local smoothness.  They also do some experiments to show the performance. \n\nThe following are my concerns:\n\n1. The definition of the L(x_t) is confusing. In (8), the authors define L(x_t), and in (10), the authors give another definition.  Does the L(x_t) in (10) always guarantee that (8) is satisfied? \n\n2. In theorem 1, \\mu^2 = \\frac{1}{n} \\sum_{i=1}^n L_i^2(x_t) + \\frac{2}{n^2}  \\sum_{i<j}^n L_i(x_t) L_j(x_t) > v. It looks like that \\mu > (1-\\rho^2) v, no matter the selection of \\rho.  Why?\n\n3. How do you compute L_i(x_t)  if x is a multi-layer neural network?\n\n4. The experimental results are too weak. In 2018, you should at least test your algorithm using a deep neural network, e.g. resnet. The results on a two-layer neural network mean nothing. \n\n5. sometimes, you algorithm even diverge. for example, figure 3 second column third row.  \n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1149/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Predictive Local Smoothness for Stochastic Gradient Methods", "abstract": "Stochastic gradient methods are dominant in nonconvex optimization especially for deep models but have low asymptotical convergence due to the fixed smoothness. To address this problem, we propose a simple yet effective method for improving stochastic gradient methods named predictive local smoothness (PLS). First, we create a convergence condition to build a learning rate varied adaptively with local smoothness. Second, the local smoothness can be predicted by the latest gradients. Third, we use the adaptive learning rate to update the stochastic gradients for exploring linear convergence rates. By applying the PLS method, we implement new variants of three popular algorithms: PLS-stochastic gradient descent (PLS-SGD), PLS-accelerated SGD (PLS-AccSGD), and PLS-AMSGrad. Moreover, we provide much simpler proofs to ensure their linear convergence. Empirical results show that our variants have better performance gains than the popular algorithms, such as, faster convergence and alleviating explosion and vanish of gradients.", "keywords": ["stochastic gradient method", "local smoothness", "linear system", "AMSGrad"], "authorids": ["junl.mldl@gmail.com", "hongfuliu@brandeis.edu", "bnzhong@gmail.com", "wuyuebupt@gmail.com", "yunfu@ece.neu.edu"], "authors": ["Jun Li", "Hongfu Liu", "Bineng Zhong", "Yue Wu", "Yun Fu"], "pdf": "/pdf/ddb07bb24b7bf0524af557f7195870382d60e310.pdf", "paperhash": "li|predictive_local_smoothness_for_stochastic_gradient_methods", "_bibtex": "@misc{\nli2019predictive,\ntitle={Predictive Local Smoothness for Stochastic Gradient Methods},\nauthor={Jun Li and Hongfu Liu and Bineng Zhong and Yue Wu and Yun Fu},\nyear={2019},\nurl={https://openreview.net/forum?id=BJWfW2C9Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1149/Official_Review", "cdate": 1542234294655, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJWfW2C9Y7", "replyto": "BJWfW2C9Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1149/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335883460, "tmdate": 1552335883460, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1149/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}