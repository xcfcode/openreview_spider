{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363731300000, "tcdate": 1363731300000, "number": 1, "id": "O3uWBm_J8IOlG", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "ZhGJ9KQlXi9jk", "replyto": "EHF-pZ3qwbnAT", "signatures": ["Alan L. Yuille, Roozbeh Mottaghi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for your comments. The paper is indeed conjectural which is why we are submitting it to this new type of conference. But we have some proof of content from some of our earlier work -- and we are working on developing real world models using these types of ideas."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363730760000, "tcdate": 1363730760000, "number": 1, "id": "eG1mGYviVwE-r", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "ZhGJ9KQlXi9jk", "replyto": "Av10rQ9sBlhsf", "signatures": ["Alan L. Yuille, Roozbeh Mottaghi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Okay, thanks. We understand your viewpoint."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363643940000, "tcdate": 1363643940000, "number": 1, "id": "Av10rQ9sBlhsf", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "ZhGJ9KQlXi9jk", "replyto": "Rny5iXEwhGnYN", "signatures": ["anonymous reviewer c1e8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Sorry: I should have written 'although I do not see it as very surprising' instead of 'novel'.\r\n\r\nThe analogy with convolutional networks is that quantities computed by low-level nodes can be shared by several high level nodes. This is trivial in the case of conv. nets, and not trivial in your case because you have to organize the search algorithm in a manner that leverages this sharing.\r\n\r\nBut I still like your paper because it gives 'a self-contained description of a sophisticated and conceptually sound object recognition system'.  Although my personal vantage point makes the complexity result less surprising, the overall achievement is non trivial and absolutely worth publishing."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363536060000, "tcdate": 1363536060000, "number": 4, "id": "sPw_squDz1sCV", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "ZhGJ9KQlXi9jk", "replyto": "ZhGJ9KQlXi9jk", "signatures": ["Aaron Courville"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Reviewer c1e8,\r\n\r\nPlease read the authors' responses  to your review. Do they change your evaluation of the paper?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362609900000, "tcdate": 1362609900000, "number": 3, "id": "EHF-pZ3qwbnAT", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "ZhGJ9KQlXi9jk", "replyto": "ZhGJ9KQlXi9jk", "signatures": ["anonymous reviewer a9e8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "review": "This paper explores how inference can be done in a part-sharing model and the computational cost of doing so. It relies on 'executive summaries' where each layer only holds approximate information about the layer below. The authors also study the computational complexity of this inference in various settings.\r\n\r\nI must say I very much like this paper. It proposes a model which combines fast and approximate inference (approximate in the sense that the global description of the scene lacks details) with a slower and exact inference (in the sense that it allows exact inference of the parts of the model). Since I am not familiar with the literature, I cannot however judge the novelty of the work.\r\n\r\nPros:\r\n- model which attractively combines inference at the top level with inference at the lower levels\r\n- the analysis of the computational complexity for varying number of parts and objects is interesting\r\n\r\n- the work is very conjectural but I'd rather see it acknowledged than hidden under toy experiments.\r\nCons:"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362352080000, "tcdate": 1362352080000, "number": 1, "id": "zV1YApahdwAIu", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "ZhGJ9KQlXi9jk", "replyto": "oCzZPts6ZYo6d", "signatures": ["Alan L. Yuille, Roozbeh Mottaghi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We hadn't thought of renormalization or image compression. But renormalization does deal with scale (I think B. Gidas had some papers on this in the 90's). There probably is a relation to image compression which we should explore."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362211680000, "tcdate": 1362211680000, "number": 1, "id": "oCzZPts6ZYo6d", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "ZhGJ9KQlXi9jk", "replyto": "ZhGJ9KQlXi9jk", "signatures": ["anonymous reviewer 915e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "review": "This paper presents a complexity analysis of certain inference algorithms for compositional models of images based on part sharing. \r\nThe intuition behind these models is that objects are composed of parts and that each of these parts can appear in many different objects; \r\nwith sensible parallels (not mentioned explicitly by the authors) to typical sampling sets in image compression and to renormalization concepts in physics via  model high-level executive summaries. \r\nThe construction of hierarchical part dictionaries is an important and in my appreciation challenging prerequisite, but this is not the subject of the paper. \r\n\r\nThe authors discuss an approach for object detection and object-position inference exploiting part sharing and dynamic programming, \r\nand evaluate its serial and parallel complexity. The paper gathers interesting concepts and presents intuitively-sound theoretical results that could be of interest to the ICLR community."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362095760000, "tcdate": 1362095760000, "number": 1, "id": "Rny5iXEwhGnYN", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "ZhGJ9KQlXi9jk", "replyto": "p7BE8U1NHl8Tr", "signatures": ["Alan L. Yuille, Roozbeh Mottaghi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "The unsupervised learning will also appear at ICLR. So we didn't describe it in this paper and concentrated instead on the advantages of compositional models for search after the learning has been done.\r\n\r\nThe reviewer says that this result is not very novel and mentions analogies to complexity gain of large convolutional networks. This is an interesting direction to explore, but we are unaware of any mathematical analysis of convolutional networks that addresses these issues (please refer us to any papers that we may have missed).  Since our analysis draws heavily on properties of compositional models -- explicit parts, executive summary, etc -- we are not sure how our analysis can be applied directly to convolutional networks. Certain aspects of our analysis also are novel to us -- e.g., the sharing of parts, the parallelization. \r\n\r\nIn summary, although it is plausible that compositional models and convolutional nets have good scaling properties, we are unaware of any other mathematical results demonstrating this."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361997540000, "tcdate": 1361997540000, "number": 2, "id": "p7BE8U1NHl8Tr", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "ZhGJ9KQlXi9jk", "replyto": "ZhGJ9KQlXi9jk", "signatures": ["anonymous reviewer c1e8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "review": "The paper describe a compositional object models that take the form of a hierarchical generative models. Both object and part models provide (1) a set of part models, and (2) a generative model essentially describing how parts are composed.  A distinctive feature of this model is the ability to support 'part sharing' because the same part model can be used by multiple objects and/or in various points of the object hierarchical description. Recognition is then achieved with a Viterbi search. The central point of the paper is to show how part sharing provides opportunities to reduce the computational complexity of the search because computations can be reused. \r\n\r\nThis is analogous to the complexity gain of a large convolutional network over a sliding window recognizer of similar architecture. Although I am not surprised by this result, and although I do not see it as very novel, this paper gives a self-contained description of a sophisticated and conceptually sound object recognition system. Stressing the complexity reduction associated with part sharing is smart because the search complexity became a central issue in computer vision. On the other hand, the unsupervised learning of the part decomposition is not described in this paper (reference [19]) and could have been relevant to ICLR."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358405100000, "tcdate": 1358405100000, "number": 34, "id": "ZhGJ9KQlXi9jk", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "ZhGJ9KQlXi9jk", "signatures": ["yuille@stat.ucla.edu"], "readers": ["everyone"], "content": {"title": "Complexity of Representation and Inference in Compositional Models with\r\n    Part Sharing", "decision": "conferenceOral-iclr2013-conference", "abstract": "This paper describes serial and parallel compositional models of multiple objects with part sharing. Objects are built by part-subpart compositions and expressed in terms of a hierarchical dictionary of object parts. These parts are represented on lattices of decreasing sizes which yield an executive summary description. We describe inference and learning algorithms for these models. We analyze the complexity of this model in terms of computation time (for serial computers) and numbers of nodes (e.g., 'neurons') for parallel computers. In particular, we compute the complexity gains by part sharing and its dependence on how the dictionary scales with the level of the hierarchy. We explore three regimes of scaling behavior where the dictionary size (i) increases exponentially with the level, (ii) is determined by an unsupervised compositional learning algorithm applied to real data, (iii) decreases exponentially with scale. This analysis shows that in some regimes the use of shared parts enables algorithms which can perform inference in time linear in the number of levels for an exponential number of objects. In other regimes part sharing has little advantage for serial computers but can give linear processing on parallel computers.", "pdf": "https://arxiv.org/abs/1301.3560", "paperhash": "yuille|complexity_of_representation_and_inference_in_compositional_models_with_part_sharing", "keywords": [], "conflicts": [], "authors": ["Alan Yuille", "Roozbeh Mottaghi"], "authorids": ["yuille@stat.ucla.edu", "roozbehm@gmail.com"]}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 10}