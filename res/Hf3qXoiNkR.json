{"notes": [{"id": "Hf3qXoiNkR", "original": "1iYNgH2f52", "number": 3718, "cdate": 1601308413721, "ddate": null, "tcdate": 1601308413721, "tmdate": 1616008582972, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DszXX7DVNB", "original": null, "number": 1, "cdate": 1610040418283, "ddate": null, "tcdate": 1610040418283, "tmdate": 1610474016695, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper considers the problem of learning models for NLP tasks that are less reliant on artifacts and other dataset-specific features that are unlikely to be reliable for new datasets. This is an important problem because these biases limit out-of-distribution generalization. Prior work has considered models that explicitly factor out known biases. This work proposes using an ensemble of weak learners to implicitly identify some of these biases and train a more robust model. The work shows that weak learners can capture some of the same biases that humans identify, and that the resulting trained model is significantly more robust on adversarially designed challenge tasks while sacrificing little accuracy on the test sets of the original data sets.\n\nThe paper's method is useful, straightforward, and intuitively appealing. The experiments are generally well conducted. Some of the reviewers raised questions about evaluating on tasks with unknown biases. The authors addressed these concerns in discussion and we encourage them to include this in the final version of the paper using the additional page."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040418269, "tmdate": 1610474016680, "id": "ICLR.cc/2021/Conference/Paper3718/-/Decision"}}}, {"id": "QPdOXZwflxg", "original": null, "number": 2, "cdate": 1603892938345, "ddate": null, "tcdate": 1603892938345, "tmdate": 1607045247359, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Review", "content": {"title": "Official Blind Review #4", "review": "## Reason for score\n\nThe research problem is critical. The solution is appropriate and novel. The claims are validated. The experiments are interesting.\nHowever, the writing in section 3, 4 et 5 should be improved. If so, I would be willing to raise my score.\n\n## My background\n\nMy research is focused on detecting and avoiding data biases (or spurious correlations) learned by deep neural networks. This is the exact scope of this paper. However, my area of expertise is computer vision and multimodal text-image, not natural language processing.\n\n## Summary\n\nContext:\nThe paper focuses on automatically detecting data biases learned by natural language processing models and overcoming them using a learning strategy.\n\nProblem:\nThe authors identify and tackle issues of state-of-the-art methods:\n- they are required to already know about a certain bias to be overcome.\n\nSolution and novelty:\nThe proposed method consists in 1) training a weak model that aims at detecting biases 2) overcoming these biases by training a main model using a product of experts (Hinton, 2002) with the predictions of the fixed weak model.\n\nClaim:\n- A weak model can be used to discover data biases\n- The proposed method produces a main model that generalize better to out-of-distribution examples\n\n## What I liked the most\n\n- meta-problem of automatically detecting and overcoming biases in neural networks is critical\n- well contextualized\n- relevant issues of state of the art have been identified\n- intro and related work are easy to read and understand\n- novel, simple and interesting method to tackle them\n- interesting figures\n- experiments are  interesting and well chosen\n\n## What could be improved\n\n1. Abstract, introduction and 2. Related work\n- Your research problem and solution are general and can be applied to many fields. Is there a specific reason why you decided to focus on NLP only?\n- You could improve the impact of your approach by citing papers that tackle the same problem with similar solutions from different fields. \"Clark et al. 2019 Don\u2019t take the easy way out: Ensemble-based methods for avoiding known dataset biases\" that you already cite ran some experiments in multiple fields (NLP, VQA, etc.). \"Cadene et al. Rubi: Reducing unimodal biases for visual question answering (NeurIPS2019)\" in VQA could also be cited.\n\n3. Proposed Method\n- Next to Eq1: Why an element wise sum is equivalent to an element wise multiplication after softmax? It seems wrong to me.\n- It could be useful to have a general definition of the PoE loss (instead of just an example of binary cross entropy in Eq2)\n- See 4.3, you should define PoE+CE here.\n\n4. Experiments\n- Overall, I think it is important that you improve the writing for this section and reduce jargon. It is really difficult to understand for readers that are not familiar with the datasets on which you perform your study. Also it is really difficult to understand which dataset is \"in-distribution\" or \"out-of-distribution\".\n- You don't define \"development matched accuracy\" before using it.\n4.1\n- You use too many footnotes that could be included in the text.\n4.2\n- You don't define \"CE\" (even in the caption of Figure2).\n- In Table 2, you could reduce jargon by using Weak and Main instead of \"W\" and \"M\".\n- In Table 2, you don't define \"An.\" even in the caption.\n4.3\n- I don't understand why \"PoE+CE\" is better on \"Hard\" \n- I don't like that you propose to use \"PoE+CE\" as your method of choice \"to counteract these effects\" without defining it in section 3. To be clear, I still don't understand what is the learning method that you propose PoE or PoE+CE?\n\n5. Analysis\n5.2\n- Title is on two lines instead of one\n- I don't understand \"When trained jointly with the larger MediumBERT weak learner......\" How many parameters? Don't expect your reader to look at Figure 4 to obtain this information.\n\n6. Conclusion\n- Could you add a discussion about the limitations of your approach. In particular: How to choose the number of parameters of your weak learner? What to choose between PoE and PoE+CE? And most critically, if you don't assess the type of biases and the amount of biases included in the dataset, how to be sure that your method will have a beneficial impact? Then, if you need to assess the type of biases, using another method that specifically targets them could be more efficient.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070913, "tmdate": 1606915777716, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3718/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Review"}}}, {"id": "lDmhAEKBpM", "original": null, "number": 1, "cdate": 1603640857559, "ddate": null, "tcdate": 1603640857559, "tmdate": 1606254996990, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "EM6nENB9ut8", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment", "content": {"title": "Clarification on ICLR policy", "comment": "Thank you for your review! I wanted to clarify that ICLR policy is that not citing/comparing with unpublished work, particularly recent work, may be excused. See https://iclr.cc/Conferences/2021/ReviewerGuide\n\nQ: Are authors expected to cite and compare with very recent work? What about non peer-reviewed (e.g., ArXiv) papers?\n\nA: We consider papers contemporaneous if they are published within the last two months. That means, since our full paper deadline is Oct 2, if a paper was published on or after Aug 2, 2020, authors are not required to compare their own work to that paper. Authors are encouraged to cite and discuss all relevant papers, but they may be excused for not knowing about papers not published in peer-reviewed conference proceedings or journals.\n\n\n\nSince [2] was submitted to EMNLP by June 3, and notifications of acceptance were Sept. 14, this work can be considered contemporaneous. Of course, it is helpful to point authors to this work and ask them to cite it, but please consider reviewing the paper on its own merits."}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Hf3qXoiNkR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3718/Authors|ICLR.cc/2021/Conference/Paper3718/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment"}}}, {"id": "PRaAGtr8T5", "original": null, "number": 9, "cdate": 1606196295928, "ddate": null, "tcdate": 1606196295928, "tmdate": 1606196295928, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "EM6nENB9ut8", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment", "content": {"title": "Response to Reviewer1", "comment": "\nWe want to sincerely thank you for your comments. We are encouraged to see that this area of research is very active and that multiple concurrent works are proposed independently.\n\nWe want to address to your comments:\n\n1. **The authors argue they have shown the model with limited capacity capture biases. However, this has been shown already**\n\nIn [1], the authors also look at models with low capacity to discover examples that support the heuristics. However, they use a different definition of \"hard examples\": forgotten examples. From [1], \"an example is forgotten if it goes from being correctly to incorrectly classified (because of multiple gradient updates performed on other examples).\" It is similar to the definition used in Data Cartography [3] for which we show in Appendix A6 that the groups they identified strongly overlap with our groups whose definition of hardness is based on the pair (final loss, final uncertainty). Our results show that we do not need to track the whole fine-tuning of our low-capacity weak learner and yet can produce similar improvements. \n\nFrom a practical point of view, we can imagine a setup where one could use a publicly released fine-tuned checkpoint (such as the ones found on huggingface.co/models) and use it as (an already trained) weak learner.\n\n2. **The main method proposed in this paper, is exactly the same method proposed in [2].**\n\nWe can now see that [2] was available on OpenReview mid-July 2020 as an anonymous pre-print. We became aware of it when it was posted to arxiv September 25th, 2020 (it was finally published in EMNLP2020\u2019s proceedings on November 9th, 2020). Upon becoming aware of this work as preparing for this submission (October 2nd, 2020), we mentioned it explicitly in our manuscript and added its results. We believe this was following proper research protocol, and think that our work should be evaluated independently from the concurrent work of [2]. \n\nWe also want to highlight a core difference with this work. In [2], the authors use the same high-capacity model (BERT-base) for both the weak model and the main model. They \u201ccontrol\u201d the weak learner by only presenting a tiny fraction of the data (for instance, 2\u2019000 examples for MNLI randomly sampled among the 392\u2019000 training examples). In contrast, we \u201ccontrol\u201d the weak learner by limiting its capacity (number of parameters) but fine-tune it on the whole training dataset. We argue that the method used to \u201ccontrol\u201d the weak learner in [2] can have drawbacks. Namely, it fails to leverage a significant proportion of the signal present in the training set.\n\nTo fairly compare the two setups, we design an extreme experiment: we adversarially sample the 2K examples fed to the weak learner by training a hypothesis-only classifier and selecting the examples this classifier can\u2019t correctly classify. Intuitively, these examples are harder to classify because the bias is not (or less) present.\nWe observe that the generalization decreases compared to a randomly sampled set of 2k examples at no cost of performance on in-domain inputs. This suggests that there are unfortunate small subsamples of (hard) examples that provide less debiasing ability to the product of experts setup. Intuitively, by training on a set of hard and less biased examples, the weak model learns a stronger explanation for the data which does not rely solely on superficial biases, which characterizes the uncertain/incorrect group, which play a crucial role in debiasing the main model.\n\n| Main model=BERT-base, Weak model=BERT-base |    |    |\n|--|--|--|\n| Weak model's training set                     | MNLI matched (acc.) | HANS Ent (acc.)| HANS Non-Ent (acc.) |\n| Random 2K examples                            |   84.32 | 98.21 |   18.51 |\n| Adv 2K examples (highest loss for hypothesis-only classifier)|   84.41 | 98.85 |   14.42 |\n\n3. **Though the method in [1] is different, the discussion in that paper still would apply here as well.**\n\nWe have made it more clear that our results in Section 5.2 corroborate and complement insights from [1].\n\n[3] Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\\\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, Yejin Choi\\\nEMNLP 2020"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Hf3qXoiNkR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3718/Authors|ICLR.cc/2021/Conference/Paper3718/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment"}}}, {"id": "FHW0HGdP_ug", "original": null, "number": 8, "cdate": 1606195933773, "ddate": null, "tcdate": 1606195933773, "tmdate": 1606195933773, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "yjaciOjMThu", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment", "content": {"title": "Response to Reviewer4 (2/2)", "comment": "Regarding the amount of bias, our intuition is the following: in the worst-case scenario, no bias is picked up by the weak learner (it can happen for instance when the bias is present in a tiny fraction of the data). However, Swayamdipta et al. [2020] shows on a range of different dataset that the category \u201chard-to-learn\u201d examples is always populated. In Appendix A6, we detail the connection between the data maps and our categories and highlight that the \u201chard-to-learn\u201d examples strongly overlap with our \u201ccertain/incorrect\u201d category. It means that even though the weak learner is not picking up biases, there are still examples in the \u201ccertain/incorrect\u201d examples which correspond to the hard examples which are upsampled in our PoE training.\n\n[Swayamdipta et al., 2020]\\\nDataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics\\\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith, Yejin Choi\\\nEMNLP 2020"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Hf3qXoiNkR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3718/Authors|ICLR.cc/2021/Conference/Paper3718/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment"}}}, {"id": "yjaciOjMThu", "original": null, "number": 7, "cdate": 1606195894339, "ddate": null, "tcdate": 1606195894339, "tmdate": 1606195894339, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "QPdOXZwflxg", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment", "content": {"title": "Response to Reviewer4 (1/2)", "comment": "We want to sincerely thank you for taking the time to carefully read our submission and providing such detailed suggestions. We have updated our submission by addressing your comments.\n\n**Is there a specific reason why you decided to focus on NLP only?**\n\nOur group expertise is in NLP, and it is an area where these biases have been of particular concern. In theory our method is general, but we make no claims to its empirical impact in other domains. \n\n**You could improve the impact of your approach by citing papers that tackle the same problem with similar solutions from different fields.**\n\nThis is a good point. We will add a reference to other works, particularly Cadene et al [2019].\n\n**Next to Eq1: Why an element wise sum is equivalent to an element wise multiplication after softmax? It seems wrong to me.**\n\nWe apologize for the typo. The corrected equation is $softmax(e) \\propto softmax(w) \\odot softmax(m)$     you have noted.\n\n**How to choose the number of parameters of your weak learner?**\n\nWe took the smallest BERT model publicly available (TinyBERT with 4.4 million parameters) and did not tune that aspect of the work. Our experiments show that the weaker the pretrained model is, the more common it is to produce \u201ccertain but incorrect\u201d responses (Section 5.2). This category gives most of the signals for debiased training.\n\nOther works [Clark et al., 2019; He et al., 2019] suggest that shallow classifiers on top FastText/Glove representations may also lead to good results.\n\n**I still don't understand what is the learning method that you propose PoE or PoE+CE? What to choose between PoE and PoE+CE?**\n\nWe found that PoE+CE loss controls the balance between the features from the dataset (superficial cues or not) and the signal from the weak learner. This is similar to how in Distillation it is common to use a mixture of Distill+CE. This indicates that not all the information picked up by the weak model should be discarded.\n\n**And most critically, if you don't assess the type of biases and the amount of biases included in the dataset, how to be sure that your method will have a beneficial impact? Then, if you need to assess the type of biases, using another method that specifically targets them could be more efficient.**\n\n*Start - Copying part of the response to Reviewer3*\n\nThis is an interesting point. While it is difficult to enumerate all sources of bias, we focus on \u201csuperficial cues\u201d that correlate with the label in the training set but do not transfer. These superficial cues are sufficiently apparent to be captured by a shallow neural network. In a sense, the biases we are targeting are defined by the weakness of the model. For instance, Conneau et al., [2018] suggest that word presence can be detected with very shallow networks (linear classifier on top of FastText bag of words): Table 2 shows very high accuracy for \u201cWord Content\u201d, the probing task of detecting which of the 1\u2019000 target words is present in a given sentence.\n\nTo verify that a weak model is still effective with \u201chard to detect\u201d biases, we consider an example where the bias is only present in a small portion of the training (while the rest of the examples either contradict the bias or are neutral towards this bias). This remaining bias is difficult to spot but should be able to be captured by the weak model. We remove from the MNLI training set all the examples that exhibit one of the two biases detailed in Section 4.1 (high word overlap between premise and hypothesis & entailment; and negation in the hypothesis & contradiction). We end up with 268K examples which present a \u201clow-amount\u201d of bias. \n\nWe apply our debiasing method with these 268K examples as our training set. For comparison, we train a main model with standard cross-entropy on a set of 268K randomly selected examples. Our results confirm on HANS that our debiasing method is still effective even when the bias is hard to detect.\n\n\n| Training data | Main Model | Weak Model | Loss | MNLI matched (acc.) | HANS Ent (acc.) | HANS Non-Ent (acc.) |\n| --- | --- | --- | --- | --- | --- | --- |\n| Adversarially selected 268K examples | BERT-base | TinyBERT | PoE  |    83.3 | 92.41 |   38.74 |\n| Randomly selected 268K examples      | BERT-base | \u2205               |CE   |   84.01 | 98.15 |    16.50 |\n\n*End - Copying part of the response to Reviewer3*"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Hf3qXoiNkR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3718/Authors|ICLR.cc/2021/Conference/Paper3718/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment"}}}, {"id": "aQtUGAxgGt9", "original": null, "number": 4, "cdate": 1606195408872, "ddate": null, "tcdate": 1606195408872, "tmdate": 1606195633785, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "vr_Y5sIqJFU", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment", "content": {"title": "Response to to Reviewer5 (1/2)", "comment": "1. **Comparison to [Clark et al, 2019] - BiDAF**\n\nThe reported results from Clark et al, [2019] do use a modified BiDAF model while we use a BERT model. Our goal in Table 3 was to highlight the drop of performance when evaluating on the adversarial sets as opposed to in-domain sets. Results from Clark et al [2019] give us a different comparative trade-off in adversarial set performance for a near-state-of-the-art class of model.\n\n2. **Is it possible to better quantify what useful information is being learned (and subsequently thrown out) by the weak learner?**\n\nSince in-domain performance of the weak learner is an imperfect proxy for learned information, we use a transfer benchmark (see Appendix A.4) to evaluate the generalization capabilities of the weak model. We train the weak model on SNLI using standard cross-entropy and evaluate the transfer capabilities on a range of other natural language inference datasets. We found the following results:\n\n| Transfer Benchmark | Main Model | Weak Model |\n| --- | --- | --- |\n| Test set           | BERT-base (acc.) |  TinyBERT (acc.) |\n| AddOne             |            86.82 |            72.35 |\n| DPR                |            50.29 |            50.01 |\n| SPR                |            58.27 |            42.89 |\n| FN+                |            54.35 |            48.18 |\n| Scitail            |            69.71 |            59.27 |\n| GLUE               |            55.34 |            48.55 |\n\nThese numbers suggest that the weak model (TinyBERT) is able to learn some useful information but greatly lags behind the main model (BERT-base).\n\nIn general, our experiments suggest that the less generalizable the weak model, the more often it is  \u201ccertain but incorrect\u201d. These misclassifications are most beneficial for training the main model. We, therefore, want a weak learner that focuses on less useful information. \n\n3. **it\u2019s somewhat unclear to me how well this method would translate to a setting with unknown biases [...] I\u2019d like to see whether this method applies well to tasks where it isn\u2019t immediately obvious that the bias is easy to learn**\n\n*Start - Copying part of the response to Reviewer3*\n\nThis is an interesting point. While it is difficult to enumerate all sources of bias, we focus on \u201csuperficial cues\u201d that correlate with the label in the training set but do not transfer. These superficial cues are sufficiently apparent to be captured by a shallow neural network. In a sense, the biases we are targeting are defined by the weakness of the model. For instance, Conneau et al., [2018] suggest that word presence can be detected with very shallow networks (linear classifier on top of FastText bag of words): Table 2 shows very high accuracy for \u201cWord Content\u201d, the probing task of detecting which of the 1\u2019000 target words is present in a given sentence.\n\nTo verify that a weak model is still effective with \u201chard to detect\u201d biases, we consider an example where the bias is only present in a small portion of the training (while the rest of the examples either contradict the bias or are neutral towards this bias). This remaining bias is difficult to spot but should be able to be captured by the weak model. We remove from the MNLI training set all the examples that exhibit one of the two biases detailed in Section 4.1 (high word overlap between premise and hypothesis & entailment; and negation in the hypothesis & contradiction). We end up with 268K examples which present a \u201clow-amount\u201d of bias. \n\nWe apply our debiasing method with these 268K examples as our training set. For comparison, we train a main model with standard cross-entropy on a set of 268K randomly selected examples. Our results confirm on HANS that our debiasing method is still effective even when the bias is hard to detect.\n\n\n| Training data | Main Model | Weak Model | Loss | MNLI matched (acc.) | HANS Ent (acc.) | HANS Non-Ent (acc.) |\n| --- | --- | --- | --- | --- | --- | --- |\n| Adversarially selected 268K examples | BERT-base | TinyBERT | PoE  |    83.3 | 92.41 |   38.74 |\n| Randomly selected 268K examples      | BERT-base | \u2205               |CE   |   84.01 | 98.15 |    16.50 |\n\n*End - Copying part of the response to Reviewer3*"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Hf3qXoiNkR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3718/Authors|ICLR.cc/2021/Conference/Paper3718/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment"}}}, {"id": "ByqhpaJM6LK", "original": null, "number": 6, "cdate": 1606195615839, "ddate": null, "tcdate": 1606195615839, "tmdate": 1606195615839, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "MemGGWWCt_", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment", "content": {"title": "Response to Reviewer3", "comment": "We thank you for your sincere comments. \n\n**I'd imagine knowing how weak the weak learner needs to be requires some intuition about which biases you are trying to remove**\n\nThis is an interesting point. While it is difficult to enumerate all sources of bias, we focus on \u201csuperficial cues\u201d that correlate with the label in the training set but do not transfer. These superficial cues are sufficiently apparent to be captured by a shallow neural network. In a sense, the biases we are targeting are defined by the weakness of the model. For instance, Conneau et al., [2018] suggest that word presence can be detected with very shallow networks (linear classifier on top of FastText bag of words): Table 2 shows very high accuracy for \u201cWord Content\u201d, the probing task of detecting which of the 1\u2019000 target words is present in a given sentence.\n\nTo verify that a weak model is still effective with \u201chard to detect\u201d biases, we consider an example where the bias is only present in a small portion of the training (while the rest of the examples either contradict the bias or are neutral towards this bias). This remaining bias is difficult to spot but should be able to be captured by the weak model. We remove from the MNLI training set all the examples that exhibit one of the two biases detailed in Section 4.1 (high word overlap between premise and hypothesis & entailment; and negation in the hypothesis & contradiction). We end up with 268K examples which present a \u201clow-amount\u201d of bias. \nWe apply our debiasing method with these 268K examples as our training set. For comparison, we train a main model with standard cross-entropy on a set of 268K randomly selected examples. Our results confirm on HANS that our debiasing method is still effective even when the bias is hard to detect.\n\n| Training data | Main Model | Weak Model | Loss | MNLI matched (acc.) | HANS Ent (acc.) | HANS Non-Ent (acc.) |\n| --- | --- | --- | --- | --- | --- | --- |\n| Adversarially selected 268K examples | BERT-base | TinyBERT | PoE  |    83.3 | 92.41 |   38.74 |\n| Randomly selected 268K examples      | BERT-base | \u2205               |CE   |   84.01 | 98.15 |    16.50 |\n\n**For research questions such as this (\"is the model using the heuristic?\") I always find it unsatisfying to think about performance gains that are in between 0 and 100.**\n\nThat is a fair point.\n\nFor context, McCoy et al [2019] show that the model can \u201cget it\u201d by simply augmenting the training set with heuristic-non-entailed examples (i.e. examples similar to the HANS non-entailed part). With standard fine-tuning, the model gets an almost-perfect accuracy on the HANS non-entailed examples. So we know that for the fixed capacity of BERT-base, we can get to a better minimum (i.e. a minimum that leads to better generalization). Therefore, the question is whether we can optimize the model to reach this minimum without the extra curated data. Intuitively, even though we do not have formal evidence, we can expect that an 80% accuracy reflects the fact the optimization ended in a better generalizing state than when it is at 20% accuracy. Thus, thinking about the 0-100 range as a continuous scale can be helpful (at least maybe outside the ~40-60 range).\n\n[Conneau et al., 2018]\\\nWhat you can cram into a single vector: Probing sentence embeddings for linguistic properties\\\nAlexis Conneau, German Kruszewski, Guillaume Lample, Lo\u00efc Barrault, Marco Baroni\\\nACL 2018\n\n[McCoy et al, 2019]\\\nRight for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference\\\nTom McCoy, Ellie Pavlick, Tal Linzen\\\nACL 2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Hf3qXoiNkR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3718/Authors|ICLR.cc/2021/Conference/Paper3718/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment"}}}, {"id": "KD9FetJ_7U", "original": null, "number": 5, "cdate": 1606195436655, "ddate": null, "tcdate": 1606195436655, "tmdate": 1606195436655, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "aQtUGAxgGt9", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment", "content": {"title": "Response to Reviewer5 (2/2)", "comment": "\n4. **I think more experiments would be useful**\n\nAdditionally, we have added experiments on one other textual dataset: FEVER [Thorne et al, 2018] using the symmetric challenge set [Schuster et al, 2019]. Our method is again effective at removing potential biases present in training sets. The results are:\n\n| FEVER (avg on 6 seeds) |                        | Loss     | Dev set (acc.)    | Symmetric Test set (acc.) |\n|------------------------|------------------------|----------|-------------------|---------------------------|\n| Reported               | Schuster et al. (2019) | CE       | **85.85** +/- 0.5 | 57.46 +/- 1.6             |\n|                        | Mahabadi et al (2020)  | CE       | 85.99             | 56.49                     |\n|                        | Mahabadi et al (2020)  | PoE      | 84.46             | **66.25**                 |\n|                        |                        |          |                   |                           |\n| Ours                   | Bert-base-uncased      | CE       | 85.61 +/- 0.3     | 55.13 +/- 1.5             |\n|                        | TinyBERT - W           | CE       | 69.43 +/- 0.2     | 43.10 +/- 0.2             |\n|                        |                        |          |                   |                           |\n| Ours                   | Bert-base-uncased - M  | PoE      | 81.97 +/- 0.5     | **59.95** +/- 3.3         |\n|                        | Bert-base-uncased - M  | PoE + CE | **85.29** +/- 0.6 | 57.86 +/- 1.4             |\n\n[Thorne et al, 2018]\\\nFEVER: a large-scale dataset for Fact Extraction and VERification]\\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, Arpit Mittal\\\nNAACL 2018\n\n[Schuster et al, 2019]\\\nTowards Debiasing Fact Verification Models\\\nTal Schuster, Darsh J Shah, Yun Jie Serene Yeo, Daniel Filizzola, Enrico Santus, Regina Barzilay\\\nEMNLP 2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Hf3qXoiNkR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3718/Authors|ICLR.cc/2021/Conference/Paper3718/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834559, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Comment"}}}, {"id": "EM6nENB9ut8", "original": null, "number": 1, "cdate": 1603015971636, "ddate": null, "tcdate": 1603015971636, "tmdate": 1605023950109, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Review", "content": {"title": "review ", "review": "Paper summary:\nThe authors argue that they have proposed a method to train robust models to biases without having prior knowledge of the biases. They argue also to provide analysis on how weak learner capacity impacts the in-domain/out-of-domain performance.\n\nReasons to reject:\n1) The authors argue they have shown the model with limited capacity capture biases. However, this has been shown already in [1] in 2019 and therefore is not a contribution of the authors.\n2) The main method proposed in this paper, is exactly the same method proposed in [2]. Please note that [2] was already available in early July 2020, and on top of existing work, the paper does not provide other contributions. \n3) About the third argued contribution on showing how the performance of the debiasing method change based on the capacity of weak learners, in [1], the authors included the discussion between the choice of weak learners on their impact. Though the method in [1] is different, the discussion in that paper still would apply here as well.  Please refer to table 1-3 and Figure 1 in [1]. \n\nGiven the points above, and since the main method in the paper is proposed in [2], the paper does not provide enough contributions to be suitable for the ICLR venue. \n\n[1] Robust Natural Language Inference Models with Example Forgetting, Yaghoobzadeh et al, https://arxiv.org/pdf/1911.03861.pdf, 2019 \n[2] Towards Debiasing NLU Models from Unknown Biases, Utama et al, 13 July 2020, https://openreview.net/forum?id=UHpxm2K-jHE, EMNLP 2020 ", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070913, "tmdate": 1606915777716, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3718/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Review"}}}, {"id": "MemGGWWCt_", "original": null, "number": 3, "cdate": 1603982935090, "ddate": null, "tcdate": 1603982935090, "tmdate": 1605023949983, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Review", "content": {"title": "Straightforward method for reducing model's reliance on spurious features", "review": "Summary:\n\nThis paper focuses on the known problem that current NLP models tend to solve tasks by exploiting superficial properties of the training data that do not generalize. For example, in the NLI task, models learn that negation words are indicative of the label \"contradiction\" and high word overlap is indicative of the label \"entailment\". There have been many recent solutions proposed for mitigating such behavior, but existing methods have tended to assume knowledge of the specific dataset biases a priori. In this paper, the authors propose a method based on product of experts that doesn't assume particular knowledge of specific dataset biases. The method works by first training a weak model and then training a \"main\" model using a loss that upweights examples on which the weak model performs poorly (namely, predicts the wrong answer with high confidence). The assumption is that weak models will exploit heuristics, and so this method will deincentivize the main model to use those same heuristics. The authors evaluate on a range of tasks, including a simulated bias setting, and NLI setting, and a QA setting, and offer a fair amount of analysis of their results. In particular, the analysis showing that the weak learners do in fact adopt the biases which have been documented elsewhere in the literature is interesting, and the discussion of \"how weak does the weak learner need to be\" is appreciated (a few questions on this below).\n\nStrengths:\n* Straightforward method for addressing an important known problem with neural NLP models\n* Thorough analysis, not just a \"method and results\" paper\n\nWeaknesses:\n* Novelty might be somewhat limited, method is not wildly creative (but I don't necessarily think \"wild creativity\" is a prerequisite for scientific value). The authors do a good job of directly contending with the similar contemporaneous work in their paper\n\nAdditional Comments/Questions:\n\nJust a few thoughts that came up while reading...\n* The weakness-of-weak-learner analysis is interesting. I imagine this is not something that can be understood in absolute terms, i.e., I would not expect there to be some level of weakness that is sufficient for all biases and all datasets. E.g., surely the lexical overlap bias is \"harder\" to learn than a lexical bias like the presence of negation words, since recognizing lexical overlap presupposes recognizing lexical identity. Therefore, I'd imagine knowing how weak the weak learner needs to be requires some intuition about which biases you are trying to remove, which runs counter to the primary thrust of the paper, namely, removing bias without knowing what the bias is. Thoughts?\n* Its interesting that even with this the performance on hans non-entailed is still only 56%, which is better but still not exactly good, and doesn't suggest the model has learned the \"right\" thing so much as its has learned not to use that particular wrong thing. For research questions such as this (\"is the model using the heuristic?\") I always find it unsatisfying to think about performance gains that are in between 0 and 100. E.g., when we talk about human learning, we usually see an abrupt shift when the learner \"gets it\", and our hope in removing the spurious features with methods like yours would be that we'd help the neural models similarly \"get it\" and reach 100% at least on examples that isolate the effect of this spurious feature. I don't expect you to have an answer for this, but just raising to hear your thoughts. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070913, "tmdate": 1606915777716, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3718/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Review"}}}, {"id": "vr_Y5sIqJFU", "original": null, "number": 4, "cdate": 1604644128873, "ddate": null, "tcdate": 1604644128873, "tmdate": 1605023949919, "tddate": null, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "invitation": "ICLR.cc/2021/Conference/Paper3718/-/Official_Review", "content": {"title": "Potentially useful extension of prior work", "review": "*Summary*: This paper proposes a method for training model that are robust to spurious correlations, building upon prior work that uses product-of-experts and a model explicitly trained on a dataset bias (e.g., a hypothesis-only model). Instead of using a model explicitly trained to learn the dataset bias, the authors use a \u201cweak learner\u201d with limited capacity. Then, this model is used in the PoE setting as in past work. The advantage of this method is that a model developer doesn\u2019t need to know that a bias exists, since the hope is that the weak learner will implicitly learn the bias.\n\n*Strengths*: A thorough study of using a limited-capacity auxiliary model to train more robust models, which helps a final model ignore spurious correlations that are easy to learn.\n\n*Weaknesses*: The work is a rather straightforward extension of prior work. Furthermore, the authors only evaluate on 2 textual tasks---I would have liked to see more experiments with spurious correlations in vision (e.g., VQA or the datasets used in https://openreview.net/forum?id=ryxGuJrFvS), and other experiments on text (e.g., the TriviaQA-CP dataset in the Clark paper). As is, it\u2019s hard to glean how broadly applicable this method actually is. I would have also liked to see more of a comparison with methods that use known bias (e.g., Clark et al or He et al)---it seems like some of the comparisons in the table aren\u2019t completely fair.\n\n*Recommendation*: 6 . I think this paper is a potentially-useful extension of a prior method, but I\u2019m still somewhat unconvinced that this method is applicable in settings where the bias is hard to detect, which is what we really care about (since, if the bias is easy to detect, we can use Clark et al and other methods).\n\nComments and Questions:\n\n1. The comparisons to Clark et al aren\u2019t fair comparisons for adversarial SQuAD, since the Clark et al paper uses a different base model for adversarial SQuAD (modifed BIDAF).\n\n2. The weak learner is a rather blunt instrument. It picks up dataset biases, but it also likely picks up features that are actually useful---not all robust features have to be difficult to learn. Is it possible to better quantify what useful information is being learned (and subsequently thrown out) by the weak learner? This would make it easier to determine if using it is worthwhile.\n\n3. While it\u2019s true that the weak model empirically learns to re-learn the same dataset biases targeted in prior work (e.g., negation correlates with contradiction), it\u2019s somewhat unclear to me how well this method would translate to a setting with unknown biases. The MNLI / SQuAD examples are a bit artificial since we already have knowledge of the bias---it\u2019s possible that weak learners can pick up on spurious features that are \u201ceasy to learn\u201d, which are the same ones that humans notice. I\u2019d like to see whether this method applies well to tasks where it isn\u2019t immediately obvious that the bias is easy to learn; perhaps a synthetic experiment would be useful here. Is it possible to modulate the learnability of the bias? The synthetic experiments in the paper suggest that for cases the bias is hard to learn, this method isn\u2019t very effective, which makes sense---in how many of the cases in the literature is the bias hard to learn? This is another reason why I think more experiments would be useful.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3718/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3718/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning from others' mistakes: Avoiding dataset biases without modeling them", "authorids": ["~Victor_Sanh1", "~Thomas_Wolf1", "~Yonatan_Belinkov1", "~Alexander_M_Rush1"], "authors": ["Victor Sanh", "Thomas Wolf", "Yonatan Belinkov", "Alexander M Rush"], "keywords": ["dataset bias", "product of experts", "natural language processing"], "abstract": "State-of-the-art natural language processing (NLP) models often learn to model dataset biases and surface form correlations instead of features that target the intended underlying task. Previous work has demonstrated effective methods to circumvent these issues when knowledge of the bias is available. We consider cases where the bias issues may not be explicitly identified, and show a method for training models that learn to ignore these problematic correlations. Our approach relies on the observation that models with limited capacity primarily learn to exploit biases in the dataset. We can leverage the errors of such limited capacity models to train a more robust model in a product of experts, thus bypassing the need to hand-craft a biased model. We show the effectiveness of this method to retain improvements in out-of-distribution settings even if no particular bias is targeted by the biased model.", "one-sentence_summary": "Reducing a model's reliance on dataset biases by encouraging a robust model to learn from a weak learner's mistakes.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "sanh|learning_from_others_mistakes_avoiding_dataset_biases_without_modeling_them", "pdf": "/pdf/85a08fe172298a34963413165b7313bc68c4656a.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nsanh2021learning,\ntitle={Learning from others' mistakes: Avoiding dataset biases without modeling them},\nauthor={Victor Sanh and Thomas Wolf and Yonatan Belinkov and Alexander M Rush},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Hf3qXoiNkR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Hf3qXoiNkR", "replyto": "Hf3qXoiNkR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3718/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070913, "tmdate": 1606915777716, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3718/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3718/-/Official_Review"}}}], "count": 13}