{"notes": [{"id": "HkxlcnVFwB", "original": "HkgurdeRUS", "number": 102, "cdate": 1569438855785, "ddate": null, "tcdate": 1569438855785, "tmdate": 1583912039042, "tddate": null, "forum": "HkxlcnVFwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "wPTYJz9HH0", "original": null, "number": 1, "cdate": 1576798687436, "ddate": null, "tcdate": 1576798687436, "tmdate": 1576800947662, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "The authors develop a framework for off-policy value estimation for infinite horizon RL tasks, for estimating the stationary distribution of a Markov chain. Reviewers were uniformly impressed by the work, and satisfied by the author response. Congratulations! \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795710953, "tmdate": 1576800260050, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper102/-/Decision"}}}, {"id": "BkgNEfo2ir", "original": null, "number": 6, "cdate": 1573855788174, "ddate": null, "tcdate": 1573855788174, "tmdate": 1573855788174, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "BkeIkxtijH", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment", "content": {"title": "Thank you for the Reponse", "comment": "Thank the author for the detailed response. The ablation study answers all my questions and the current paper is quite solid.  It would be great if the author can open source the code, which will definitely benefit the OPE community."}, "signatures": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxlcnVFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper102/Authors|ICLR.cc/2020/Conference/Paper102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176370, "tmdate": 1576860536804, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment"}}}, {"id": "SJxp7c5hjr", "original": null, "number": 5, "cdate": 1573853733179, "ddate": null, "tcdate": 1573853733179, "tmdate": 1573853733179, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "HklHP-tjjH", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment", "content": {"title": "Thank you for the response", "comment": "Thank you for the response, the ablation study is clear and provide important guidance in empirical experiments.\n\nFor the comparison to Liu et al. normalized method, it seems that is only used in continuous setting. For tabular setting since we can constraint the summation of density function to be 1, a constraint optimization (like mirror descend or quadratic programming is enough). The normalized trick in their equation (11) (divided by $z_w$) I believe is mainly used in continuous setting.\n\nI am still wondering compared to the regularizer you use in equation (11), which one is more stable in optimization process with neural network? "}, "signatures": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxlcnVFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper102/Authors|ICLR.cc/2020/Conference/Paper102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176370, "tmdate": 1576860536804, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment"}}}, {"id": "S1gt9jdjjH", "original": null, "number": 2, "cdate": 1573780369139, "ddate": null, "tcdate": 1573780369139, "tmdate": 1573787104360, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "H1l3zHj6Yr", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment", "content": {"title": "Reply to reviewer #2", "comment": "Thanks for the encouraging comments.  We will keep improving the draft. We have refined the paper as listed above in the summary of revisions.\n\nBest,\nAuthors"}, "signatures": ["ICLR.cc/2020/Conference/Paper102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxlcnVFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper102/Authors|ICLR.cc/2020/Conference/Paper102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176370, "tmdate": 1576860536804, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment"}}}, {"id": "BkeIkxtijH", "original": null, "number": 3, "cdate": 1573781470067, "ddate": null, "tcdate": 1573781470067, "tmdate": 1573787054795, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "Hyluc57aFB", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment", "content": {"title": "Reply to reviewer #1", "comment": "Thanks for the insightful comments and constructive suggestions. In the revision, we have added the suggested discussion and an ablation study that investigates the choice of divergences. The summary is listed below: \n\n1. The generalized integral probability metrics (IPM) \n\nThe proposed estimator is compatible with IPM, e.g., MMD, Wasserstein-$1$ distance, and Dudley metric, as discussed in the Remark above section 3.4. We have further elaborated on this point in the revision. \n\nBy definition, the IPM naturally leads to a similar min-max optimization with identity function as $\\phi^*(\\cdot)$ and different feasible sets of the dual functions $f$:\n\n    i) MMD requires $f$ to be in an RKHS and its RKHS norm should be smaller than $1$, i.e., $\\|f\\|_{\\mathcal{H}}\\le 1$; \n    ii) Wasserstein-$1$ distance requires $f$ to be $1$-Lipschitz, i.e., $\\|\\nabla f\\|_2\\le 1$;\n    iii) Dudley metric requires $\\|f\\|_{BL}\\le 1$ with $\\|f\\|_{BL}:= \\|f\\|_\\infty + \\|\\nabla f\\|_2$.\n\nThese requirements on the dual function may incur some additional difficulty in practice. For the Wasserstein-$1$ distance and Dudley metric, we might need the extra gradient penalty, which requires extra computation to take the gradient through a gradient. Meanwhile, the consistency of the surrogate loss under regularization is not clear. For MMD, we can obtain the closed-form solution for the dual function, which saves the cost of the inner optimization with the trade-off of requiring two independent samples in each update for outer optimization. Moreover, it relies on the condition that the dual function must be in some RKHS, which introduces extra important kernel parameters to be tuned, and in practice might not be sufficiently flexible compared to neural networks.\n\nThanks for pointing out the concurrent submission. Both aim to solve for the stationary distribution. However,  i) we explicitly handle the degeneracy issue of the naive estimator in a principled way, leading to an easier optimization problem;  and ii) the proposed estimator is more general, which is compatible with different divergences.  We will discuss this concurrent submission in our final version. \n\n\n2. The empirical performance using different divergences\n\nWe tested GenDICE with several alternative divergences on the offline PageRank task in Appendix E.3, including Wasserstein-$1$ distance, Jensen-Shannon divergence, $KL$-divergence, Hellinger divergence, and MMD. To ensure that the dual function is $1$-Lipchitz, we added the gradient penalty. We used a learned Gaussian kernel for MMD. \n\nAs we can see in Fig. 13(a), with these different divergences, the proposed GenDICE estimator can always achieve significantly better performance compared to the model-based estimator, showing that the GenDICE estimator is compatible with many different divergences. Most of the divergences, with appropriate techniques added to handle the optimization difficulties and careful tuning of the additional parameters, can achieve similar performance, consistent with the phenomena observed with variants of GANs [1]. However, $KL$-divergence is an outlier, performing noticeably worse, which might be caused by the ill-behaved $\\exp(\\cdot)$ in its conjugate function. The $\\chi^2$-divergence and JS-divergence are better, both achieving good performance with fewer parameters to tune.\n\n\n[1] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier Bousquet. Are GANs created equal? a large-scale study. In Advances in neural information processing systems, pp. 700\u2013709, 2018.\n\n\nBest,\nAuthors"}, "signatures": ["ICLR.cc/2020/Conference/Paper102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxlcnVFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper102/Authors|ICLR.cc/2020/Conference/Paper102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176370, "tmdate": 1576860536804, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment"}}}, {"id": "HklHP-tjjH", "original": null, "number": 4, "cdate": 1573781852583, "ddate": null, "tcdate": 1573781852583, "tmdate": 1573786060995, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "r1geCyJTKB", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment", "content": {"title": "Reply to reviewer #3", "comment": "Thanks for the generally positive comments and constructive suggestions.  Below you can find the detailed response to the comments:\n\n1. Effect of penalty weights in the empirical study: \n\nIn our ablation study, we compared GenDICE in off-policy policy evaluation w/o the regularization term.  We have also added another ablation study to investigate the effect of the penalty weight in the offline PageRank task in Appendix E.3.  We vary $\\lambda \\in [0.1, 5]$ with the $\\chi^2$-divergence in the GenDICE estimator. \n\nWithin a large range of $\\lambda$, the performance of GenDICE is quite consistent, which justifies Theorem 1. The penalty multiplies with $\\lambda$. Therefore, with increasing $\\lambda$, the variance of the stochastic gradient estimator also increases, which explains the increasing variance for a large $\\lambda$ in Fig.13(b). In practice, we found that $\\lambda = 1$ is a reasonable choice for general cases.\n\n2. Comparison to Liu et al. (2018):\n\nWe added a comparison to Liu et al. (2018) in the Taxi domain (both discounted and average cases) using their implementation with a learned behavior policy. As we can see in Fig. 2, the proposed GenDICE performs the best among all the competitors. \n\nBest,\nAuthors"}, "signatures": ["ICLR.cc/2020/Conference/Paper102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxlcnVFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper102/Authors|ICLR.cc/2020/Conference/Paper102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176370, "tmdate": 1576860536804, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment"}}}, {"id": "Skli-sussr", "original": null, "number": 1, "cdate": 1573780226621, "ddate": null, "tcdate": 1573780226621, "tmdate": 1573780226621, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment", "content": {"title": "Summary of Revisions ", "comment": "We would like to thank all the reviewers for taking the time to contribute their insightful comments, which helped us improve the paper. Our detailed point-to-point responses can be found below, and we have also carefully updated the manuscript to follow the constructive suggestions from the reviewers. \n\nHere is a brief summary of major updates made to the revised manuscript:\n1. Added an ablation study on the choice of the divergence: Wasserstein-$1$ distance, Jensen-Shannon divergence, $KL$-divergence, Hellinger divergence, and MMD.\n2. Added an ablation study on the effect of the constraint weight in practice.\n3. Added more discussion about the choice of divergence.\n\nWe will further improve the paper based on the reviewers' comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper102/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxlcnVFwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper102/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper102/Authors|ICLR.cc/2020/Conference/Paper102/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176370, "tmdate": 1576860536804, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper102/Authors", "ICLR.cc/2020/Conference/Paper102/Reviewers", "ICLR.cc/2020/Conference/Paper102/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Comment"}}}, {"id": "r1geCyJTKB", "original": null, "number": 1, "cdate": 1571774407539, "ddate": null, "tcdate": 1571774407539, "tmdate": 1572972638519, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Main contributions:\nThis paper generalizes the recent state-of-the-art behavior agnostic off-policy evaluation DualDice into a more general optimization framework: GenDice. Similar to DualDice, GenDice considers distribution correction over state, action pairs rather than state in Liu et al. (2018), which can handle behavior-agnostic settings. The optimization framework (in equation (9)) is novel and neat, and the practical algorithm seems more powerful than the previous DualDice. As a side product, it can also use to solve offline page rank problem.\n\nClarity:\nThis paper is well established and written. \n\nConnection of theory and experiment:\nI have a major concern for the theory 1 about the choice of regularizer $\\lambda$. For infinite samples case, the derivation of theory 1 is reasonable since both term is nonnegative. However, in practice we will have empirical gap for the divergence term, thus picking a suitable $\\lambda$ seems crucial for the experiment. I think a discussion on $\\lambda$  for average case in experiment part should be added. And compared to Liu et al. (2018) which normalized the weight of $\\tau$ in average case, which one is better in practice?\n\nOverall I think this paper is good enough to be accepted by ICLR. The optimization framework can also inspire future algorithm using different divergence."}, "signatures": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575517659998, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper102/Reviewers"], "noninvitees": [], "tcdate": 1570237757046, "tmdate": 1575517660010, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Review"}}}, {"id": "H1l3zHj6Yr", "original": null, "number": 3, "cdate": 1571824916457, "ddate": null, "tcdate": 1571824916457, "tmdate": 1572972638474, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new estimator to infer the stationary distribution of a Markov chain, with data from another Markov chain. The method estimates the ratio between stationary distribution of target MC and the empirical data distribution.  It is based on the observation that the ratio is a fixed point solution to certain operators. The proposed method could work in the behavior-agnostic and undiscounted case, which is unsolved by the previous method.\n\nThis paper tackles an interesting problem with an increasing number of studies in the reinforcement learning community and gives a practical algorithm with strong empirical justification, as well as theoretical justification. I think this paper should be accepted.\n\nDetailed comments:\n1) This paper provides experiment results in multiple domains, including two continuous control domains which are more complex than experiments in previous OPE methods. The paper also provides many details about the learning dynamics and ablation studies, which is very useful for the reader to understand the result of the paper.\n2) The theoretical result is as same strong as previous work DICE and DualDICE, under similar assumptions.\n3) I appreciate this paper formalizes the two difficulties of degeneration and intractability, and then explain how those are addressed in a principled way. Degeneration is important and is at least ignored in two similar works on this topic."}, "signatures": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575517659998, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper102/Reviewers"], "noninvitees": [], "tcdate": 1570237757046, "tmdate": 1575517660010, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Review"}}}, {"id": "Hyluc57aFB", "original": null, "number": 2, "cdate": 1571793551559, "ddate": null, "tcdate": 1571793551559, "tmdate": 1572972638429, "tddate": null, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "invitation": "ICLR.cc/2020/Conference/Paper102/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper the authors proposed a framework for off-policy value estimation under the scenario of infinite horizon RL tasks. The new proposed method utilize the variational representation of $f$-divergence, which quantifies the difference between $\\mathcal{T}\\tau$ and $\\tau p$, where $\\tau$ is the parametric density ratio between the unknown behavior policy data and the target policy. If only if $\\tau$ is the true density ratio, the loss $\\mathcal{D}_{f}(\\mathcal{T}\\tau || \\tau p) = 0$. \n\nCompared with prior work (Nachum et. al 2019), the new proposed framework can generalize the undiscounted case $\\gamma = 1$, and the derivation for the new algorithm is quite simple and easy to follow. The experimental results show the advantage of the proposed methods over baseline methods such as model-based, DualDice etc, for both discrete and continuous cases. Moreover, I have two specific questions:\n\n- The choice of $f$-divergence. Although the author mentioned the difficulty of using the dual representation of KL divergence, it would be nice to have an ablation study that shows the effectiveness of various $f$-divergence (Personally I think Jensen-Shannon Divergence may be also a good choice).\n\n-The authors should also have a discussion that similar idea can be generalized to more general  distribution metrics such as Integral Probability Metrics, specifically wasserstein-1 distance (similar to wasserstein-gan) or maximum mean discrepancy (Maybe it is unnecessary to conduct experiments, some discussion should be enough to clarify the relationship. I think there is a concurrent submission using MMD metrics).\n \n\nOverall I think this is a good paper and I recommend for acceptance. \n\nReference Papers:\n- Nowozin, Sebastian, Botond Cseke, and Ryota Tomioka. \"f-gan: Training generative neural samplers using variational divergence minimization.\" Advances in neural information processing systems. 2016.\n- Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein gan.\" arXiv preprint arXiv:1701.07875 (2017).\n- Nachum, Ofir, et al. \"DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections.\" arXiv preprint arXiv:1906.04733 (2019).\n- Anonymous, \u201cBlack-box Off-policy Estimation for Infinite-Horizon Reinforcement Learning\u201d, submitted to ICLR 2020.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper102/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ryzhang@cs.duke.edu", "bodai@google.com", "lihongli.cs@gmail.com", "schuurmans@google.com"], "title": "GenDICE: Generalized Offline Estimation of Stationary Values", "authors": ["Ruiyi Zhang*", "Bo Dai*", "Lihong Li", "Dale Schuurmans"], "pdf": "/pdf/e359c9f4a7a14094671ffc723863544111ede3e2.pdf", "TL;DR": "In this paper, we proposed a novel algorithm, GenDICE, for general stationary distribution correction estimation, which can handle both discounted and average off-policy evaluation on multiple behavior-agnostic samples.", "abstract": "An important problem that arises in reinforcement learning and Monte Carlo methods is estimating quantities defined by the stationary distribution of a Markov chain. In many real-world applications, access to the underlying transition operator is limited to a fixed set of data that has already been collected, without additional interaction with the environment being available. We show that consistent estimation remains possible in this scenario, and that effective estimation can still be achieved in important applications. Our approach is based on estimating a ratio that corrects for the discrepancy between the stationary and empirical distributions, derived from fundamental properties of the stationary distribution, and exploiting constraint reformulations based on variational divergence minimization. The resulting algorithm, GenDICE, is straightforward and effective. We prove the consistency of the method under general conditions, provide a detailed error analysis, and demonstrate strong empirical performance on benchmark tasks, including off-line PageRank and off-policy policy evaluation.", "keywords": ["Off-policy Policy Evaluation", "Reinforcement Learning", "Stationary Distribution Correction Estimation", "Fenchel Dual"], "paperhash": "zhang|gendice_generalized_offline_estimation_of_stationary_values", "_bibtex": "@inproceedings{\nZhang*2020GenDICE:,\ntitle={GenDICE: Generalized Offline Estimation of Stationary Values},\nauthor={Ruiyi Zhang* and Bo Dai* and Lihong Li and Dale Schuurmans},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxlcnVFwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/5f082074178a300dfd61b8ed9b0ebf50e0a21b7f.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxlcnVFwB", "replyto": "HkxlcnVFwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper102/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575517659998, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper102/Reviewers"], "noninvitees": [], "tcdate": 1570237757046, "tmdate": 1575517660010, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper102/-/Official_Review"}}}], "count": 11}