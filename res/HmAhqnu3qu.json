{"notes": [{"id": "HmAhqnu3qu", "original": "zS6cKpKoPYf", "number": 849, "cdate": 1601308097800, "ddate": null, "tcdate": 1601308097800, "tmdate": 1614985656677, "tddate": null, "forum": "HmAhqnu3qu", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "hJUH5tfXYJc", "original": null, "number": 1, "cdate": 1610040505782, "ddate": null, "tcdate": 1610040505782, "tmdate": 1610474113007, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper experimentally observes the negative transfer in Multi-task Graph Representation Learning and proposes to solve the negative transfer with a novel Meta-Learning based training procedure. However, the proposed methods seems not technically sound. There are some concerns about this paper\uff1a1. The technique contribution of this paper is limited. The method proposed in this paper is just an application of MAML in Graph Representation Learning with a little variation. 2. This paper only compares SAME with the vanilla MTL method, which adopts the uniform weights. However, the vanilla MTL method commonly performs poorly. The state-of-the-art MTL methods should be taken into comparison, for example MGDA [1]. 3. The traditional Meta-Learning framework introduced in Algorithm 4 is misleading.  4. The experimental analysis of this paper is not sufficient. For example, the paper has not analyzed whether the improvement comes from the meta updating or comes from the singularly training strategy.\n\n[1]. Sener, Ozan, and Vladlen Koltun. \"Multi-task learning as multi-objective optimization.\" NIPS 2018.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040505768, "tmdate": 1610474112991, "id": "ICLR.cc/2021/Conference/Paper849/-/Decision"}}}, {"id": "ufWPIRwVzNk", "original": null, "number": 8, "cdate": 1605428757250, "ddate": null, "tcdate": 1605428757250, "tmdate": 1606153632194, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "4nMX3y5g8J", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment", "content": {"title": "Answer to Reviewer 2", "comment": "We thank the reviewer for the positive and insightful comments and we provide answers to his questions below.\n\n### Q1:\nThe initial version of the Related Work section was forced by space limitations, we now provide a revised version of the paper with an extended Related Work section.\n\n### Q2:\nWe had to limit our presentation of the datasets for space limitations, but the new version of the paper that we uploaded follows the comments of the reviewer and provides more information on the datasets.\n\n### Q3:\nThe new version of our paper adds a fine-tuning baseline, where a multi-task model is first trained on the three considered tasks, and then it is fine-tuned on specific task(s) of interest.  The reasoning behind this is that the initial training on all tasks should lead the model towards the extraction of features that it would otherwise not consider (by only seeing 2 tasks), and the fine-tuning procedure then allows it to use these features to target the specific tasks of interest. Results in Table 2 and Table 3 show that this method is outperformed by SAME and actually performs worse than classically trained models. This shows that, in order to extract general information that can be effectively exploited for multiple tasks, we need new representation learning procedures like SAME, as traditional training methods are not enough.\n\n### Minor Comments\nWe thank again the reviewer for the comments. We fixed the legend in Figure 1 and in Figure 3. We believe Section 3 can be useful to people that are not familiar with Graph Neural Networks, MAML, and ANIL, and acts as a refresher (for the more experienced readers) that increases the readability of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper849/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Paper849/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "HmAhqnu3qu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper849/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper849/Authors|ICLR.cc/2021/Conference/Paper849/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866565, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment"}}}, {"id": "24Z0S87lO-D", "original": null, "number": 9, "cdate": 1605428794057, "ddate": null, "tcdate": 1605428794057, "tmdate": 1606153611079, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "th96Uu1O5h6", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment", "content": {"title": "Answer to Reviewer 3", "comment": "We thank the reviewer for his insights and we answer below to his questions and comments.\n### Q1\nWe are not aware of works that study the use of Graph Neural Networks for graph classification, node classification, and link prediction concurrently in a multi-task setting. We kindly ask the reviewer if he can provide the references to such works, if we have not mentioned them in this answer. We are in fact aware of some papers that, at a high level, may seem related to ours, and we provide below the motivations as to why they are not. We further add these comments and references in the new version of the paper.\n- Graph Star Net for Generalized Multi-Task Learning, Haonan et al., arXiv 2019: the authors propose a model that can be trained for multiple tasks, however it can be trained only on one task at a time and it can not perform multiple tasks concurrently. It is in fact a single-task model, as, once it is trained, it can only perform one task (the one it was trained for), and it can be trained only on one task at a time.\n- Multi-Task Network Representation Learning, Xie et al., Frontiers in Neuroscience 2020: this paper proposes a multi-head model that can perform graph classification and node classification. Not only we already cover multi-head models in our experiments, but the proposed method is also not capable of performing link prediction (we already referenced this paper in our initial submission).\n- Multi-Task Learning Based Network Embedding Wang et al., Frontiers in Neuroscience 2020: the authors propose a method that does not use Graph Neural Networks, does not consider node attributes, and does not perform the three tasks we perform. Furthermore, it makes use of \"handcrafted\" features to extract structural information, and in fact the authors do not provide comparisons to end-to-end models like Graph Neural Networks.\n\n### Q2\nThere are several important differences between MAML and SAME. We provide a summary of such differences below:\n- In most existing applications of MAML (like the ones for few-shot learning), episodes are all instantiations of the same task, i.e. in each episode the same loss function is used and the same number of support examples is used (e.g., the k-shot n-way framework). In fact, in previous uses of meta-learning (specially the few-shot learning ones), the difference between episodes is only given by the different labels of the examples involved. In our method, not only we have multiple tasks in each episode, but each task can have a different loss function and a different number of training examples. It is actually possible to view MAML as a special case of SAME, where we only have one task with a fixed structure.\n- MAML and other variations of MAML perform a single adaptation on the support set in the inner loop. In SAME, we perform multiple adaptations (one per task), each time starting from the same initial parameter configuration. Furthermore, in each adaptation, different subsets of parameters are involved, which again is a property that differentiates us from previous approaches, and actually poses SAME as a generalization of MAML. \n\n### Q3\nWe thank the reviewer for the comment. For space motivations we only discussed this aspect in the Appendix, but the new version of the paper has been updated to contain comments about the \\lambda parameters. In our experiments we saw that the \\lambdas were not very important for SAME. In fact we set them all to 1 and did not optimize them. We did introduce them in our presentation as we wanted to provide a general framework that could be instantiated in many different ways."}, "signatures": ["ICLR.cc/2021/Conference/Paper849/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Paper849/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "HmAhqnu3qu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper849/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper849/Authors|ICLR.cc/2021/Conference/Paper849/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866565, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment"}}}, {"id": "fvJ26-lAB6D", "original": null, "number": 6, "cdate": 1605428682257, "ddate": null, "tcdate": 1605428682257, "tmdate": 1606153594087, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "x52RDBfnnLP", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment", "content": {"title": "Answer to Question 1", "comment": "We thank the reviewer for the insightful comments and we provide answers to his comments below. (For character limitations we answer to question 1 below, and to question 2 and 3 in a separate comment).\n\n### Q1:\nOur work does fit into the meta-learning framework composed of a nested loop optimization, but presents several important aspects that differentiates it from previously proposed methods. We first provide more information on how our work positions itself in the Meta-Learning literature and what are the differences from existing methods, and,  successively, we highlight several interesting directions for future work.\n\n#### Novelty of SAME\n- Meta-learning has been used in many scenarios such as few-shot learning, continual learning, domain adaptation, and autoML. However, the use of meta-learning as a technique for representation learning (as we do) was entirely unexplored before our work, and, further, we do it for the multi-task setting.\n- In meta-learning the task design is a very important factor. We substantially differentiate ourself from most previous works, as we design target and support sets to contain multiple tasks and to resemble training and validation sets.\n- In most existing applications of meta-learning (like the ones for few-shot learning), episodes are all instantiations of the same task, i.e. in each episode the same loss function is used and the same number of support examples is used (e.g., in the k-shot n-way framework). In fact, in previous uses of meta-learning (specially the few-shot learning ones), the difference between episodes is only given by the different labels of the examples involved. In our method, not only we have multiple tasks in each episode, but each task can have a different loss function and a different number of training examples. It is actually possible to view MAML as a special case of SAME, where we only have one task with a fixed structure in both support and target sets.\n- MAML and its variations proposed in previous work perform a single adaptation on the support set in the inner loop. In SAME, we perform multiple adaptations (one per task), each time starting from the same initial parameter configuration. Furthermore, in each adaptation, different subsets of parameters are involved, which again is a property that differentiates us from previous approaches, and actually poses SAME as a generalization of MAML. \n- As pointed out by the reviewer, the GNN model we use is quite simple, and we believe this is a strong positive point for our work. If our method could work only with sophisticated architecture, then it would be of limited use. The fact that it works very well with such a simple model is a promising signal for the future applications of the method proposed in our work.\n\n#### Future Works\nThe generality of the SAME framework offers many directions for future work. Some of the most interesting ones are presented below.\n- As written above, we are the first to view meta-learning as a tool for representation learning. This new view of meta-learning allows us to design new ways to extract information from data by properly engineering the meta-learning episodes. We believe this new view can be of great interest to the community.\n- It is possible to explore the use of more sophisticated architectures, and the combinations of other tasks for multi-task learning. This also includes applications outside of the Graph domain.\n- Another area for exploration is in the design of more sophisticated episodes, where the task(s) for adaptation in the inner loop are different from the task(s) in the outer loop. E.g., the inner loop could perform multiple low-level tasks, while the outer loop could perform higher-level tasks. This way the learner has to extract information from the low-level tasks that can be used to better perform higher level tasks. And this could lead to very powerful new ways to learn representations.\n- We follow ANIL and its results by only using the last layers for adaptation in the inner loop (in iSAME). However, it is interesting to explore more sophisticated choices for selecting the parameters used for adaptation. There could in fact be some multi-task combinations, and some sophisticated architectures, where it is actually best to subdivide the parameters in different ways.\n- Finally, we want to highlight the direction of applications to few-shot learning (as suggested by the reviewer), and other areas where meta-learning has proven itself useful (e.g. continual learning)."}, "signatures": ["ICLR.cc/2021/Conference/Paper849/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Paper849/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "HmAhqnu3qu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper849/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper849/Authors|ICLR.cc/2021/Conference/Paper849/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866565, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment"}}}, {"id": "V3kyFSvBDZo", "original": null, "number": 7, "cdate": 1605428710099, "ddate": null, "tcdate": 1605428710099, "tmdate": 1606153574855, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "x52RDBfnnLP", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment", "content": {"title": "Answer to Questions 2 & 3", "comment": "### Q2:\nMeta-learning has proven to be useful in many scenarios: few-shot learning, continual learning, domain adaptation, autoML. In this paper we focus on the unexplored representation learning scenario. In fact, we are the first to view meta-learning as a technique for representation learning, and we focus our work on the specific task of graph representation learning. On this aspect, we added another baseline (a fine-tuned model) in our experiments to further confirm that standard training techniques are not enough to properly extract general features in multi-task scenarios. While the few-shot learning setting is definitely an interesting one, we believe it is a promising direction for future work that necessitates a paper of its own to be properly explored.\n\n### Q3:\nWe thank the reviewer for pointing this out. We provide an updated version of our paper with a revised reference format."}, "signatures": ["ICLR.cc/2021/Conference/Paper849/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Paper849/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "HmAhqnu3qu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper849/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper849/Authors|ICLR.cc/2021/Conference/Paper849/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866565, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment"}}}, {"id": "Rmf1QNOq5S", "original": null, "number": 10, "cdate": 1605428838815, "ddate": null, "tcdate": 1605428838815, "tmdate": 1606153252939, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment", "content": {"title": "Summary of changes made to paper", "comment": "We thank the reviewers for their insightful reviews. We did our best to answer all the concerns and we are available for further questions. We have uploaded a new version of the paper implementing the reviewers' comments. We summarizes the changes as follows:\n- We reformulated the Related Work section, and we added more references. (Section 2)\n- We revised the reference format. (References)\n- We provide a brief description of each dataset. (Section 5)\n- We added results for a Fine-Tuning baseline. (Section 5)\n- In our experimental section we added a brief discussion on the lambda hyperparamters. (Secction 5)"}, "signatures": ["ICLR.cc/2021/Conference/Paper849/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Paper849/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "HmAhqnu3qu", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper849/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper849/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper849/Authors|ICLR.cc/2021/Conference/Paper849/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866565, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper849/-/Official_Comment"}}}, {"id": "th96Uu1O5h6", "original": null, "number": 2, "cdate": 1604042719583, "ddate": null, "tcdate": 1604042719583, "tmdate": 1605024592044, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Official_Review", "content": {"title": "Direct extension of MAML", "review": "This paper formulates the learning of three tasks, including graph classification, node classification and link prediction, as a multi-task learning problem and adopts a meta learning approach to learn the three tasks together in the spirit of the Model-Agnostic Meta Learning (MAML) method.\n\nActually, there have been some works to study the three tasks (i.e., graph classification, node classification and link prediction) as a multi-task learning problem. Authors need to discuss differences with those works and compare with them in experiments.\n\nThe proposed meta learning approach seems a direct application of the MAML method. I cannot see much difference with the MAML method.\n\nIn the meta-objective, how to set different \\lambda\u2019s? This is more important to the performance.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper849/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper849/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133634, "tmdate": 1606915801694, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper849/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper849/-/Official_Review"}}}, {"id": "x52RDBfnnLP", "original": null, "number": 3, "cdate": 1604154514321, "ddate": null, "tcdate": 1604154514321, "tmdate": 1605024591986, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Official_Review", "content": {"title": " interesting  problem ", "review": "The manuscript proposes SAME, a model based on GNN and meta-learning for learning multi-task node embeddings. Unlike multi-task learning setting, SAME aims at learning to quickly adapt to multiple tasks. Two model variants iSAME and eSAME are proposed base on different settings in inner/outer loop of parameter update. Experiments on several datasets demonstrate the good performance of SAME. \n\nPros\n1. The problem is new and interesting. It is the first work to study single set of node embeddings for multi-tasks in graph. \n2. The presentation is overall good. The content is clear for me. \n3. Introduce a new model for learning multi-task node embeddings through meta-learning way. The model is simple yet interesting for new problem. \n\nCons/Questions\n1. The novelty of this work incremental. Despite the new problem and different task settings, the model framework adopts the similar procedure as general meta-learning procedure. The model is quite simple. I would like to see more discussion about the contribution and novelty of this work as well as the potential future study. \n\n2. This work follows the meta-learning setting. Besides studying different graph learning tasks, it is better to provide content and add experiment for the scenario of few-shot labeled data. If I did not miss it, there is no discussion and experiment about this. I would suggest the authors to add comparison experiments for different tasks where only few-shot supervised data are available. \n\n3. Reference format is not consistent, typos, etc. ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper849/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper849/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133634, "tmdate": 1606915801694, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper849/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper849/-/Official_Review"}}}, {"id": "4nMX3y5g8J", "original": null, "number": 1, "cdate": 1603846636948, "ddate": null, "tcdate": 1603846636948, "tmdate": 1605024591918, "tddate": null, "forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "invitation": "ICLR.cc/2021/Conference/Paper849/-/Official_Review", "content": {"title": "Graph Representation Multi-Task Learning.", "review": "This paper presents a multi-task framework to represent the node embedding for transferred knowledge. The methodology is based on the meta-learning, which is capable of producing multi-task node embedding. This paper is well-motivated and well-written. The experimental results illustrate the effectiveness of the model. I would like to recommend to accept this paper.\n\nMajor Concerns:\n1. The related work can be strengthened. In the current version, the related work seems to stack all the papers in sequence, which makes it tough to understand the development of this task. I suggest the authors to reformate this subsection to Mind Graph.\n2. Can you introduce the dataset more specifically?\n3. There shall be more baselines for the experiments.\n\nMinor Concern:\n1. Fig.1: the legend shall be outside the box.\n2. Sec 3 seems redundant to most related readers. ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper849/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper849/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach", "authorids": ["~Davide_Buffelli1", "~Fabio_Vandin2"], "authors": ["Davide Buffelli", "Fabio Vandin"], "keywords": ["Graph Representation Learning", "Multi-Task Learning", "Meta-Learning", "Graph Neural Networks"], "abstract": "Graph Neural Networks (GNNs) have become the state-of-the-art method for many applications on graph structured data. GNNs are a framework for graph representation learning, where a model learns to generate low dimensional node embeddings that encapsulate structural and feature-related information. GNNs are usually trained in an end-to-end fashion, leading to highly specialized node embeddings. While this approach achieves great results in the single-task setting, generating node embeddings that can be used to perform multiple tasks (with performance comparable to single-task models) is an open problem. We propose a novel representation learning strategy, based on meta-learning, capable of producing multi-task node embeddings. Our method avoids the difficulties arising when learning to perform multiple tasks concurrently by, instead, learning to quickly (i.e. with a few steps of gradient descent) adapt to multiple tasks singularly.  We show that the embeddings produced by our method can be used to perform multiple tasks with comparable or higher performance than both single-task and multi-task end-to-end models. Our method is model-agnostic and task-agnostic and can hence be applied to a wide variety of multi-task domains.", "one-sentence_summary": "A novel representation learning strategy, based on meta-learning, for multi-task graph representation learning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "buffelli|graph_representation_learning_for_multitask_settings_a_metalearning_approach", "supplementary_material": "", "pdf": "/pdf/424c3dabaf8db502f7ce3f8c1fe81be2f84e837b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=L-ie0KaGHO", "_bibtex": "@misc{\nbuffelli2021graph,\ntitle={Graph Representation Learning for Multi-Task Settings: a Meta-Learning Approach},\nauthor={Davide Buffelli and Fabio Vandin},\nyear={2021},\nurl={https://openreview.net/forum?id=HmAhqnu3qu}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "HmAhqnu3qu", "replyto": "HmAhqnu3qu", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper849/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133634, "tmdate": 1606915801694, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper849/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper849/-/Official_Review"}}}], "count": 10}