{"notes": [{"id": "yKYiyoHG4N3", "original": "AeDLQg5-5Fb", "number": 2702, "cdate": 1601308299453, "ddate": null, "tcdate": 1601308299453, "tmdate": 1614985756472, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "wFoKcfPslx", "original": null, "number": 1, "cdate": 1610040378249, "ddate": null, "tcdate": 1610040378249, "tmdate": 1610473970840, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes a new multimodal neuro-symbolic technique for synthesizing programs. The specification is given in natural language (soft constraints) and input-output examples (hard constraints). The multimodal program synthesis is formulated as a constrained maximization problem where the goal is to find a program maximizing the conditional probability w.r.t. the natural language specification while satisfying the input-output examples. The proposed technique is evaluated on a multimodal synthesis dataset of regular expressions, and significant performance gains are shown w.r.t. the state-of-the-art synthesis methods. Overall this is an important direction of research, and the paper presents significant results in the space of multimodel program synthesis.\n\nI want to thank the authors for actively engaging with the reviewers during the discussion phase. The reviewers generally appreciated the paper's ideas; however, there was quite a bit of spread in the reviewers' assessment of the paper (scores: 4, 5, 6, 7). In summary, this is a borderline paper, and unfortunately, the final decision is a rejection. The reviewers have provided detailed and constructive feedback for improving the paper. In particular, the authors should more clearly describe the paper's primary contributions, compare their technique with related work that combines neural generation approaches with deductive methods, and simplify the presentation of technical sections. This is exciting and potentially impactful work, and we encourage the authors to incorporate the reviewers' feedback when preparing future revisions of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040378236, "tmdate": 1610473970822, "id": "ICLR.cc/2021/Conference/Paper2702/-/Decision"}}}, {"id": "-X-ne7Z9l_", "original": null, "number": 9, "cdate": 1606169958262, "ddate": null, "tcdate": 1606169958262, "tmdate": 1606169958262, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "KYz3TYqXG9c", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 ", "comment": "Thanks for the detailed reply.\n\n> I tried to came up with a simple example and a prefix that could be pruned by your proposed method, but not by the prefix-matching trick. \n\nOur approach can prune those partial regexes that do not have deterministic prefixes, e.g., contain(concat(<let>,?)), and(endwith(?),?), whereas simple prefix-matching is not effective in these cases.\n\n> So maybe it would be helpful for the reader to illustrate the difference by an example, to make it easier to understand the benefit of your proposed method? \n\n> I'd strongly recommend adapt your presentation to the venue and move the inference rules into the appendix and use the additional space for an example instead\n\nThanks for the feedback about the presentation. While we will not attempt to rush a major update to the presentation of the paper during the review period, we will explore how to rewrite this in any future version.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yKYiyoHG4N3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2702/Authors|ICLR.cc/2021/Conference/Paper2702/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845348, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment"}}}, {"id": "KYz3TYqXG9c", "original": null, "number": 8, "cdate": 1606123970732, "ddate": null, "tcdate": 1606123970732, "tmdate": 1606123997679, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "LMHqIoXwpI4", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment", "content": {"title": "Respone^2 to AnonReviewer2", "comment": "Thank you for your reply and the paper update. Let me note that you seem to have missed my comments on the typos in Fig. 9, which was not updated.\n\n> Our ASN+P model is basically the same as execution-guided approaches (Wang et al, 2019), \n> where we prune out infeasible partial regexes in the decoding procedure using the proposed\n> pruning technique that is more powerful than prefix matching.\n\nNow my interest is piqued: I tried to came up with a simple example and a prefix that could be pruned by your proposed method, but not by the prefix-matching trick. The latter is a bit tricky in the presence of \"or(...)\" but doable, and so the only examples I can come up with are either using non-standard regular expression primitives (contain/endwith/repeat/...) or fail because the functional representation of \"star(R)\" is not amenable to the prefix trick (but writing it as \"R*\" would work well).\n\nSo maybe it would be helpful for the reader to illustrate the difference by an example, to make it easier to understand the benefit of your proposed method?\n\n> > For the ICLR audience, Sect. 3.2 may be at a too formal level (and especially, familiarity \n> > with the inference rule notation in Fig. 5 should not be assumed).\n> We can include a worked example in appendices. We tried to strike balance between\n> rigorously defining our procedure and making the paper approachable given the space limit.\n\nUsing a formalism that the audience is not familiar with is not a \"rigorous definition\", it just becomes visual noise that people ignore. This is not POPL/PLDI and hence I'd strongly recommend adapt your presentation to the venue and move the inference rules into the appendix and use the additional space for an example instead."}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yKYiyoHG4N3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2702/Authors|ICLR.cc/2021/Conference/Paper2702/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845348, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment"}}}, {"id": "4DZYyTAXC9z", "original": null, "number": 7, "cdate": 1605317469962, "ddate": null, "tcdate": 1605317469962, "tmdate": 1605317469962, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment", "content": {"title": "Updates in the Result Section", "comment": "We would like to thank all the reviewers for the valuable comments and suggestions. We\u2019ve updated our paper based on the feedback, particularly the results section. Specifically, we now include the fraction of problems for which we can find an I/O-consistent program and the average wall-clock time used for each of the approaches. We believe the newly added statistics better distinguish the reasons for failures and demonstrate the advantages of our approach, particularly in terms of efficiency."}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yKYiyoHG4N3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2702/Authors|ICLR.cc/2021/Conference/Paper2702/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845348, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment"}}}, {"id": "LMHqIoXwpI4", "original": null, "number": 6, "cdate": 1605312589211, "ddate": null, "tcdate": 1605312589211, "tmdate": 1605312589211, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "sa13c2I9Bx9", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thanks for the detailed feedback. We address the main points of concern below.\n\n*Q: Lack of comparison with highly related work that combines neural generation approaches with deductive methods to simplify the search space. While most of these methods have not been developed for the multi-modal setting. Handling multi-modal specifications would be similarly*\n\nA: Extending such approaches to work in our setting requires substantial effort and is outside the scope of the paper. As a comparison to these approaches:\n\nThe scoring of a production rule in Kalyan et al. (2018) only depends on the specification but not on the partial program enumerated so far. Also, the search procedure in this paper has no optimality guarantee, which is the reason they propose two types of controller to handle the performance and  generalizability tradeoff.\n\nFeng et al. (2018) rely on using an SMT solver, which can be computationally costly and is not always easy to apply to a domain such as regex. This work also doesn\u2019t guarantee optimality in its search procedure with respect to the neural model.\n\nOur ASN+P model is basically the same as execution-guided approaches (Wang et al, 2019), where we prune out infeasible partial regexes in the decoding procedure using the proposed pruning technique that is more powerful than prefix matching.\n\n*Q: For the ICLR audience, Sect. 3.2 may be at a too formal level (and especially, familiarity with the inference rule notation in Fig. 5 should not be assumed).*\n\nA: We can include a worked example in appendices. We tried to strike balance between rigorously defining our procedure and making the paper approachable given the space limit.\n\n*Q: If pre-order works best anyway, building on top of TranX (or \"Generative Code Modeling with Graphs\" (Brockschmit et al)) would be possible as well, which have shown better results than ASN in generative code modelling. Did you experiment with such models?*\n\nA: Yes. The TreeSearch model performs tree search using a neural model built on top of TranX, which is not as good as OpSyth-P.\n\nConceptually, we believe this happens for the following reason (repeating our answer to reviewer #4): Our work uses a functional DSL, whereas Brockschmidt et al. generate C# programs. Functional programs are more likely to be built out of semi-independent pieces, whereas the expression completion task from Brockschmidt et al. has stronger dependence between sibling nodes when variables are reused across expressions in the more procedural style. We believe this difference is important when considering what kind of factorization in the model is likely to be most effective.\n\nWe also note that models like TranX are not universally better than ASN: on the HearthStone dataset, ASN reports a significant advantage over the action-sequence based model (Yin & Neubig, 2017). \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yKYiyoHG4N3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2702/Authors|ICLR.cc/2021/Conference/Paper2702/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845348, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment"}}}, {"id": "3X2jFvLU9vj", "original": null, "number": 5, "cdate": 1605312182518, "ddate": null, "tcdate": 1605312182518, "tmdate": 1605312207905, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "YrvQG5sIqMp", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "We would like to thank the reviewer for constructive comments and clarify the following concern:\n\n*Q: One interesting question is in what cases Sketch found a solution compatible with the input/output examples, but it was not the desired solution for the task.*\n\nA: In some cases, Sketch finds some programs that are very similar but not equivalent to the target program. For instance, Sketch may fail to include some subpart in the regex surrounded by optional (e.g., optional(<let>)) when the examples do not cover enough corner cases.\n\n*Q: There was only one dataset and one task for which it improved the results over the baselines.*\n\nA: To the best of our knowledge, the StructuredRegex dataset is the only existing multimodal program synthesis dataset that contains natural language written by real humans. The other relevant dataset, AlgoLisp, only contains synthetic natural language, which is the reason that existing systems can already achieve near-perfect performance on this task.\n\n*Q:  Was there any improvement in terms of wall-time say in comparison to Sketch?*\n\nA: We\u2019ve added wall clock time in our revision. It takes around 20-25s for OpSynth to search for 2500 states, which improves the efficiency by 3X on wall-clock time compared to Sketch. \n\n*Q: Definition 3.1 seems inaccurate and does not imply that a path must start at the root.*\n\nA: All paths must start at the root -- we will clarify this.\n\n*Q: Why does Definition 3.2 define assigning rules to nodes, which is not part of a tree or a grammar. It looks like it tries to make applying a production rule in two steps.*\n\nA: We always start from a root node and choose the production rule to apply on it. Everytime a production rule is applied, it creates new undetermined nodes (its child fields). So the new nodes are instantiated as a rule is applied, not as a separate step in the process.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yKYiyoHG4N3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2702/Authors|ICLR.cc/2021/Conference/Paper2702/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845348, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment"}}}, {"id": "n1K5e_asDjr", "original": null, "number": 3, "cdate": 1605311413171, "ddate": null, "tcdate": 1605311413171, "tmdate": 1605311902673, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "Jg3PTr_KmU8", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you very much for your detailed review and suggestions! We have included missing results and we would like to explain the following points:\n\n**[Contributions]**\n\n*Q: What are the main contributions of the paper?*\n\nA: The main contribution is an **optimal** synthesis technique for **multi-modal** program synthesis (combining examples and natural language). While the ideas of optimality and multi-modality have been independently explored in prior work, this is the first paper that guarantees optimality with respect to an NLP model in the multi-modal context. The prior work that you list for optimal synthesis is all in the pure PBE setting. Multimodality imposes the additional challenge of needing a complex neural model to handle open-ended natural language inputs; standard autoregressive seq2seq models do not typically admit efficient inference. Our techniques are novel in that we show how to do efficient inference in this setting. \n\n**[Abstract Interpretation]**\n\n*Q: Discuss the difficulties of applying abstract interpretation to other domains.*\n\nA: We agree with the reviewer that the choice of abstract domains is crucial: if the abstract domain is too imprecise, it may not have good pruning power; on the other hand, if it is too precise, its overhead may outweigh the benefits of pruning. This is why our algorithm is parametrized by over- and under-approximating abstract semantics. For the regex domain, we provide a concrete instantiation of these semantics and show that it is helpful in practice. Prior work in the context of pure PBE has proposed useful abstract domains in other settings, such as table transformations [1] and tensor and string manipulations [2]. Furthermore, prior work also proposes automated techniques for learning useful abstract semantics for a given DSL and synthesis engine [3]. Thus, choosing a suitable abstract domain for a specific DSL is orthogonal to the contributions of this work.\n\n[1] Feng, Yu, et al. \u201cComponent-based synthesis of table consolidation and transformation tasks from examples\u201d\n[2] Wang, Xinyu, et al. \u201cProgram synthesis using abstraction refinement\u201d\n[3] Wnag, Xinyu, et al. \u201cLearning Abstractions for Program Synthesis\u201d\n\n**[Results]**\n\n*Q: Table 1 should present data for OpSynth-P with a routine that does check for consistency, such as the beam search approaches.*\n\nA: Such results are already there.  In particular, Seq2Seq+P and TranX+P perform model-guided beam search with pruning in exactly the same way as done in OpSynth. As we show in Table 1, augmenting beam search with pruning increases the number of benchmarks solved by 7 to 11 percent absolute.  In addition, OpSynth-P does check for consistency with examples, but only of concrete programs.\n\n*Q: When comparing against different baselines, it\u2019s impossible to tell whether failures are due to timeouts or failure to generalize?*\n\nA: We have included such results in Table 2 for OpSynth and the ablation ASN+P. OpSynth finds 75.5% of the optimal programs with respect to the neural model. The remaining 24.5% of the errors are due to timeouts. 28.6% of the errors are due to finding spurious programs (consistent with examples but not the user-specified program).\nBy contrast, ASN+P finds roughly the same number of consistent programs if allowed to explore many more states, but the programs are not optimal and thus solve fewer problems.  We have also provided the updated results for the other baselines in the revision.\n\n*Q: Wall clock time evaluation is missing.*\n\nFor all the neural models in the last two sections of Table 1, the wall clock time follows the number of states very closely, because the neural computation and pruning costs are similar. We include the average time used in our revision. The baselines are the same order of magnitude as these methods in terms of wall clock, and are much less effective.\n\n\n*Q: The advantages in Table 2 of OpSynth may be very much diminished with a greater beam size/threshold.*\n\nA: Table 2 shows that the optimal synthesis approach does a better job of finding the optimal program than beam search even when beam search explores more states. Of course, as the beam size grows larger and larger, the results will get closer, but beam search will take much longer.\n\n*Q: What exactly is the \u201cfraction of solved problems\u201d? How could it be less than the percentage of optimal programs found? Is this performance on a test set?*\n\nA: In the context of multimodal program synthesis, a program that is consistent with examples may not be the desired program. (In contrast, most papers in the  PBE setting consider a solution to be valid as long as it satisfies all the examples.) Thus, the fraction of solved problems reports the percentage of cases where we find the ground truth program. By contrast, an optimal program is optimal with respect to the model, which may not be correct.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yKYiyoHG4N3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2702/Authors|ICLR.cc/2021/Conference/Paper2702/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845348, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment"}}}, {"id": "R7NON_X0QKa", "original": null, "number": 4, "cdate": 1605311778552, "ddate": null, "tcdate": 1605311778552, "tmdate": 1605311778552, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "15LtDozPUVL", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thanks for the valuable comments and suggestions!\n\n*Q: The main gains in the approach seem to come from the choice of ASN as a semantic parsing model and the feasibility pruning, not from the best-first formulation.*\n\nA: Most importantly, the best-first formulation enables a large improvement in the search speed, cutting the search cost by roughly 2X. It does also improve the accuracy by 1.8%.\n\n*Q: Prior works (Yin & Neubig, 2019; Brockschmidt et al., 2019) report an opposite ordering [between ASN and TranX]. Do you have any intuition why the performance of these two baselines so drastically flips on your task?*\n\nA: Our work uses a functional DSL, whereas Brockschmidt et al. generate C# programs. Functional programs are more likely to be built out of semi-independent pieces, whereas the expression completion task from Brockschmidt et al. has stronger dependence between sibling nodes when variables are reused across expressions in the more procedural style. We believe this difference is important when considering what kind of factorization in the model is likely to be most effective.\n\nWe also note that models like TranX are not universally better than ASN: on the Hearthstone dataset, ASN reports a significant advantage over the action-sequence based model (Yin & Neubig, 2017). \n\n*Q: Could you formally state and prove the statement of optimality wrt the semantic parsing model as a theorem?*\n\nA: We now provide the proof in the appendix.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "yKYiyoHG4N3", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2702/Authors|ICLR.cc/2021/Conference/Paper2702/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845348, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Comment"}}}, {"id": "sa13c2I9Bx9", "original": null, "number": 1, "cdate": 1603723769389, "ddate": null, "tcdate": 1603723769389, "tmdate": 1605024150297, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "Summary\n-------------\nA method for synthesizing programs from a combination of natural language specifications and formal specificatios (or, more concretely, input/output examples) is presented. It combines a top-down, grammar-based expansion strategy with a deduction component that can help to rule out partial programs that cannot be completed into a correct program. Experiments show small but noticeable improvements over baselines on a dataset of regular expression synthesis benchmarks.\n\nStrong/Weak Points\n-------------\n* (+) Paper attempts to integrate neural and symbolic methods more deeply than just using one as filter for the other.\n* (+) Experiments with relevant baselines seem clean and answer the most important questions.\n* (-) Lack of comparison with highly related work that combines neural generation approaches with deductive methods to simplify the search space. While most of these methods have not been developed for the multi-modal setting. Handling multi-modal specifications would be similarly straightforward as the extension from OpSynth to OpSynth+R described in Appendix E. Examples of relevant work:\n   * \"Neural-guided deductive search for real-time program synthesis from examples\" (Kalyan et al.) uses version space algebras to determine if a partial program can be completed to a correct program and a neural network conditioned on the partial program to select the next most-likely expansion step.\n   * \"Program Synthesis using Conflict-Driven Learning\" (Feng et al) uses a CDCL-like procedure to rule out infeasible parts of the search space with a ML-based heuristic for chosing next expansion steps.\n   * Execution-guided approaches (\"Execution-guided neural program decoding\" (Wang et al.), \"Execution-guided neural program synthesis\" (Chen et al.) which check if the partial program generated so far is compatible with the examples. In the regex space, this is naturally applicable to left-to-right generation approaches by checking if the partial regex generated so far matches a prefix of the input example.\n* (-) For the ICLR audience, Sect. 3.2 may be at a too formal level (and especially, familiarity with the inference rule notation in Fig. 5 should not be assumed). I have doubts that this _helps_ readers to understand and due to the space limitations, it doesn't make things exact either (e.g., it took me a while to understand that $\\Phi^+$/$\\Phi^-$ implicitly consume $\\mathbf{z}$; and the \"definitions\" for $\\Phi^{+,-}$ in Appendix B/Fig. 9 are not precise as they just map to some undefined underlying regular expression language)\n\nRecommendation\n-------------\nI think this is a borderline paper. Overall, it is in a crowded space with many competing approaches and does not do a great job at differentiating itself (see above), which makes it hard to judge its contribution clearly. The experimental results are an indication that the method works, but some questions (e.g., does using ASN instead of a stronger neural generator model make things worse) remain.\n\nQuestions\n-------------\n* [See above re related work that could be compared to]\n* Appendix D states that implementing SelectLeaf using a pre-order traversal works best, which somewhat negates the argument for an approach that is invariant to derivation order (in Sect 3 / 3.1). If pre-order works best anyway, building on top of TranX (or \"Generative Code Modeling with Graphs\" (Brockschmit et al)) would be possible as well, which have shown better results than ASN in generative code modelling. Did you experiment with such models?\n\nDetail Feedback\n-------------\n* Page 3: \"Wwe\" -> \"We\"\n* Fig. 9:  second/third equation are lacking a closing \")\"; \"or\" appears twice in the third equation.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090383, "tmdate": 1606915766349, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2702/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Review"}}}, {"id": "YrvQG5sIqMp", "original": null, "number": 2, "cdate": 1603902889684, "ddate": null, "tcdate": 1603902889684, "tmdate": 1605024150227, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Review", "content": {"title": "Interesting results on a specific task. Possible writing improvements", "review": "The paper shows an interesting and novel combination of neural and constraint-based synthesis that despite using a relatively simple neural model, manages to improve over more involved neural synthesis techniques. This is a significant point in the space of such algorithms and an important research result.\n\nSeveral important systems were compared, including non-neural results. One interesting question is in what cases Sketch found a solution compatible with the input/output examples, but it was not the desired solution for the task.\n\nThere are a few areas for improvement of the paper, but overall it is well written and shows interesting results:\n\n- The technique is described very generally, but there was only one dataset and one task for which it improved the results over the baselines. This task has the advantage that it contains an elaborate \u201cInfeasible\u201d procedure that is usually not present in general DSLs, but it does not seem that this is the main reason why the technique works well. Judging from the ablated results, it seems that the main reason seems to be that the other neural baselines are baselines are a poor fit for this specific task.\n\n- While the approach talks about speed, it only includes the number of steps in its results. Was there any improvement in terms of wall-time say in comparison to Sketch? You seem to be giving 90 seconds to it. How long does it take for OpSynth to run?\n\n- Some improvements in the definitions and the writing are possible:\n\npage 2: In f( s_0 .. s_n) where as later it talks s_1 to s_n\npage 3: Wwe\n\nDefinition 3.1 seems inaccurate and does not imply that a path must start at the root. If it does not, then the function pi cannot be defined. Also n_k is not the same as n_k in the examples.\nDefinition 3.2 defines assigning rules to nodes, but this is not part of a tree or a grammar. It looks like it tries to make applying a production rule in two steps, but it is not clear why this is needed.\nSupp is not defined in the algorithm.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090383, "tmdate": 1606915766349, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2702/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Review"}}}, {"id": "15LtDozPUVL", "original": null, "number": 3, "cdate": 1604021827315, "ddate": null, "tcdate": 1604021827315, "tmdate": 1605024150164, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Review", "content": {"title": "Limited technical novelty but a strong end-to-end contribution with provable optimality", "review": "The work presents a technique for synthesizing program from a combination of NL and input-output examples. It is guaranteed to find the best-possible program under a trained NL->AST model that satisfies the given examples. The technique assumes a particular kind of NL->AST model from the literature and a pruning mechanism for infeasible partial programs, integrated into best-first search. On a recent dataset of multi-modal regular expression problems, the technique significantly outperforms both purely-neural, purely-symbolic, and prior multi-modal baselines, although it only slightly outperforms its own ablations.\n\n## Strengths & Weaknesses\n\nThe main contribution is using best-first search to guarantee optimality wrt the semantic parsing model, which, in turn, required fixing the model class to ASN. The infeasibility pruning seems to repeat the procedure of Chen et al. (PLDI 2020). Nevertheless, the end-to-end formulation is a valuable contribution to the program synthesis community even if its technical novelty is limited.\n\nAs far as I'm aware, this is the first multi-modal program synthesis method that has an optimality guarantee under a neural model. Other approaches to optimal program synthesis exist, but they focus on manual cost functions. In theory, best-first search could be integrated into other methods, but this is the first work that presents such a method end to end.\n\nThe paper is written clearly and with plethora of examples. One exception are the inference rules for pruning in Figure 5, which introduce a significant abstraction that does not become clearer until Appendix B. However, space constraints are understandable and unavoidable. In absence of space, I would suggest swapping the two and presenting a concrete example of infeasibility (plus perhaps pseudocode) in Section 3 and domain-independent formalism in the Appendix.\nGiven that the paper only evaluates itself on a single application domain, a domain-independent general abstraction of pruning without a regex-related example in the main paper body seems excessive.\n\nThe method clearly outperforms both single-modality and prior multi-modal baselines. The ablation experiments are also quite insightful. Interestingly, best-first search only adds 0.9% of accuracy on Test-E over beam search. The main gains in the approach seem to come from the choice of ASN as a semantic parsing model and the feasibility pruning, not from the best-first formulation.\n\n## Questions/Suggestions\n\n- As stated in the Appendix D, the authors expand the nodes using the pre-order traversal in practice. This is a fixed order of expansion, the same one as TranX. The models, of course, differ - ASN conditions its prediction on the AST path whereas TranX conditions on the expansion path and the parent node. In Table 1, ASN+P outperforms TranX+P by 7.7 points. However, prior works (Yin & Neubig, 2019; Brockschmidt et al., 2019) report an opposite ordering, with TranX-style models outperforming ASN-style models on many different datasets. Do you have any intuition why the performance of these two baselines so drastically flips on your task?\n\n- Could you formally state and prove the statement of optimality wrt the semantic parsing model as a theorem? It is intuitively true, but the paper (otherwise impressively rigorous) currently only posits optimality at a very high level in a single sentence on page 5.\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090383, "tmdate": 1606915766349, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2702/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Review"}}}, {"id": "Jg3PTr_KmU8", "original": null, "number": 4, "cdate": 1604436299054, "ddate": null, "tcdate": 1604436299054, "tmdate": 1605024150102, "tddate": null, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "invitation": "ICLR.cc/2021/Conference/Paper2702/-/Official_Review", "content": {"title": "Promising approach, but contributions unclear, limitations under explored, and experiments don\u2019t demonstrate what they should.", "review": "## Summary\nThis paper proposes an approach to program synthesis that aims to incorporate information from different modalities, focusing on combining input-output examples with natural language specifications. They formulate it as a form of constrained optimal program synthesis.  The constraints are in the form of positive and negative examples, which the program must satisfy.  The optimality criteria is in the form of maximizing the conditional probability of a program $P$ given a natural language description, as specified by a conditional distribution that is learned from data.\n\n## Overview\n\nOverall:\n- The use of program analysis methods within neural program synthesis is promising.  \n- I found no technical flaws with this paper.\n- The paper needlessly combines different concepts, and doesn\u2019t clearly disambiguate what is it\u2019s contribution.   The main contribution of this paper is in pruning partial programs, but that is obscured under the framing of multimodal learning.\n- Both the benefits and limitations of an abstract interpretation approach are under explored.\n\nIt is difficult to tell exactly what the claims of the paper are, as they are not clearly specified.  Nevertheless, there are three different concepts in this paper that are worth separating:\n\n- The representation of the distribution over expressions that supports enumerating programs in order of decreasing probability.  Consequently the first program in this enumeration that satisfies the positive and negative examples is optimal.\n-  The use of abstract interpretation to prune infesabile partial programs.  This means that conditioning $p_\\theta$ on the fact that $P$ must satisfy positive and negative examples can be done more efficiently than simply rejecting complete programs.\n- The formulation of optimal multimodal program synthesis in these terms.\n\n(please correct me if I am missing something or mischaracterizing the contributions)\n\nThe representation used (Abstract Syntax Networks) is not a contribution of this work.  It is not entirely clear to me whether the authors are claiming the enumeration strategy that produces the most probable program first as a contribution.  If so, there is existing work both in conventional program synthesis (e.g. [1, 2]) and neural program synthesis (e.g.[3]) that enumerates programs in order of probability.  Does this approach present anything new over these?\n\nThe formulation is fine.\n\nGiven all of this, it seems that the primary contribution of this paper is the use of a form of abstract interpretation to prune partial programs.  There is much merit to this idea and approach.  In particular, there are potentially very large gains to be had in efficiency.  I have a number of concerns though:\n\n**Writing:** Given the importance of this section, the abstract interpretation is very poorly explained.  Terms in the derivation tree (e.g. Root, SubTree, Children) are not specified.  Several notations (such as expression substitution) which are likely unfamiliar to a machine learning audience are not explained.\n\nThe authors have also not cited any of the work that uses abstract interpretation for program synthesis (e.g. [4,5,6])\n\n**Imprecision:** one of the major problems in abstract interpretation is imprecision \u2014 the over approximation can include too many in feasible solutions, and the under approximation can exclude too many.  Familiarity with abstract interpretation would lead most to have a healthy dose of skepticism that the approach can be used naively for program synthesis, without vary careful selection of the abstract domains.  This is because for most abstract domains in most non-trivial examples, imprecision would lead to an inability to prune virtually any programs.\n\nCarefully choosing abstractions for specific domains can be done in some situations, which is precisely what the regex example has done.  They present an abstract interpretation framework that is parameterized using the $\\psi$ function, but the only example is in terms of regular expressions which permit precise abstractions.\n\nThis is not to say that focusing on restricted applications isn\u2019t worthwhile, but the authors should take a lot more care to both explore and explain the limitations of abstract interpretation and the difficulties that one would face in actually applying their framework to other DSLs.  \n\n**Results** The results are a little hard to interpret.  $\\text{OpSynth}$ solves 5% more (of the total number of) solutions than the ablated $\\text{OpSynth}^{-\\mathcal{P}}$.  However, this ablated version has nothing checking for consistency with the actual data.  In addition to this, the authors should present in Table 1 data for OpSynth with a routine that does check for consistency, such as the beam search approaches.\n\nWhen comparing against different approaches, it is impossible to tell from the data if failures are due to timing out or due to finding a program that doesn\u2019t generalize to the test data.  If it is the former, then the results may look very different with different timeout thresholds.\n\nWall clock time is missing from evaluation, which is as important since neurosymbolic methods can be orders of magnitude slower in real time than conventional methods.\n\nTaking a step back, the primary problem is that the bulk of the work done is in the ASN network, and importance of the pruning cannot be discerned from this.  The advantages in Table 2 of OpSynth may be very much diminished with a greater beam size / threshold.\n\nThis is all to say that I cannot tell from this data that OpSynth has real advantages over ASN combined with simple enumeration.\n\n## Questions\n\n- OpSynth finds 75.5% of the optimal programs.  Is this optimality with respect to the trained model $p_\\theta$?  If so, according to your approach, the remaining 25.5% of failures must be due to timeouts.  Is this correct?\n- What exactly is the \u201cfraction of solved problems\u201d?  How could it be less than the percentage of optimal programs found?  Is this performance on a test set? \n\n## Typo\nWwe -> We\n\n## References\n[1] Lee, Woosuk, et al. \"Accelerating search-based program synthesis using learned probabilistic models.\" ACM SIGPLAN Notices 53.4 (2018): 436-449.\n[2] John K Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations from input-output examples. In PLDI, 201\n[3] Ellis, Kevin, et al. \"Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning.\" arXiv preprint arXiv:2006.08381 (2020).\n[4] Rishabh Singh, Armando Solar-Lezama, Synthesizing data structure manipulations from storyboards, 2011(bibtex)\n[5] Martin T. Vechev, Eran Yahav, Greta Yorsh, Abstraction-guided synthesis of synchronization\n[6] Wang, Xinyu, Isil Dillig, and Rishabh Singh. \"Program synthesis using abstraction refinement.\" Proceedings of the ACM on Programming Languages 2.POPL (2017): 1-30.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2702/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2702/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Optimal Neural Program Synthesis from Multimodal Specifications", "authorids": ["~Xi_Ye2", "qchen@cs.utexas.edu", "~Isil_Dillig1", "~Greg_Durrett1"], "authors": ["Xi Ye", "Qiaochu Chen", "Isil Dillig", "Greg Durrett"], "keywords": ["program synthesis"], "abstract": "Multimodal program synthesis, which leverages different types of user input to synthesize a desired program, is an attractive way to scale program synthesis to challenging settings; however, it requires integrating noisy signals from the user (like natural language) with hard constraints on the program's behavior. This paper proposes an optimal neural synthesis approach where the goal is to find a program that satisfies user-provided constraints while also maximizing the program's score with respect to a neural model. Specifically, we focus on multimodal synthesis tasks in which the user intent is expressed using combination of natural language (NL) and input-output examples. At the core of our method is a top-down recurrent neural model that places distributions over abstract syntax trees conditioned on the NL input. This model not only allows for efficient search over the space of syntactically valid programs, but it allows us to leverage automated program analysis techniques for pruning the search space based on infeasibility of partial programs with respect to the user's constraints. The experimental results on a multimodal synthesis dataset (StructuredRegex) show that our method substantially outperforms prior state-of-the-art techniques in terms of accuracy and explores fewer states during search.", "one-sentence_summary": "An optimal neural synthesis approach that finds an I/O-statisfying program while also maximizing the program's score with respect to a neural model. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ye|optimal_neural_program_synthesis_from_multimodal_specifications", "supplementary_material": "/attachment/6dab308551a3cb4536ce467e270c815967531a2e.zip", "pdf": "/pdf/aeb97e5c3054c9b20f91cf8e4e8bd66a929ecb44.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3dTbGpg5zB", "_bibtex": "@misc{\nye2021optimal,\ntitle={Optimal Neural Program Synthesis from Multimodal Specifications},\nauthor={Xi Ye and Qiaochu Chen and Isil Dillig and Greg Durrett},\nyear={2021},\nurl={https://openreview.net/forum?id=yKYiyoHG4N3}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "yKYiyoHG4N3", "replyto": "yKYiyoHG4N3", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2702/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090383, "tmdate": 1606915766349, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2702/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2702/-/Official_Review"}}}], "count": 13}