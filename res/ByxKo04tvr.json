{"notes": [{"id": "1ftxiemQpY", "original": null, "number": 8, "cdate": 1582483533608, "ddate": null, "tcdate": 1582483533608, "tmdate": 1582483533608, "tddate": null, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment", "content": {"title": "Review and Decision Process", "comment": "Like other authors, we devoted a significant amount of time and effort during the feedback phase to address the reviews--running additional experiments as requested by the reviewers and drafting a thorough response. However, neither the reviews nor the meta-review suggest that our response was taken into account as part of the decision-making process. No changes were made to the official reviews, while the meta-review does not acknowledge any of the clarifications that we offered in our response. This is inconsistent with the ICLR 2020 meta-review guidelines (https://iclr.cc/Conferences/2020/MetareviewGuide), which state that the AC is expected to \"clearly and thoroughly convey this recommendation and reasoning behind it to the authors\" with a justification that reflects discussion among the AC and reviewers. Shortly after meta-reviews were released, we raised our concerns with the program chairs. We explicitly stated that we were not asking the PCs to reconsider the decision, but were raising awareness about the quality of the review process. We have yet to receive a response.\n\nRegards,\nMatthew Walter, Michael Maire, and Tri Huynh"}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxKo04tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1328/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1328/Authors|ICLR.cc/2020/Conference/Paper1328/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157679, "tmdate": 1576860544861, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment"}}}, {"id": "ByxKo04tvr", "original": "S1gW-QFOwH", "number": 1328, "cdate": 1569439393389, "ddate": null, "tcdate": 1569439393389, "tmdate": 1577168256218, "tddate": null, "forum": "ByxKo04tvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "3-P6WGQ-0b", "original": null, "number": 1, "cdate": 1576798720660, "ddate": null, "tcdate": 1576798720660, "tmdate": 1576800915932, "tddate": null, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Decision", "content": {"decision": "Reject", "comment": "This paper investigates convolutional LSTMs with a multi-grid structure. This idea in itself has very little innovation and the experimental results are not entirely convincing.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712271, "tmdate": 1576800261626, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Decision"}}}, {"id": "H1g4E9joor", "original": null, "number": 5, "cdate": 1573792300015, "ddate": null, "tcdate": 1573792300015, "tmdate": 1573792300015, "tddate": null, "forum": "ByxKo04tvr", "replyto": "B1lPj6V3tr", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "\n\"comparison with more recent memory-augmented models [1,2]\"\n\nDNC is the only state-of-the-art memory architecture that has an official code release.  Other methods mentioned by reviewers do not provide source code, which drastically increases the difficulty of comparison.\n\nWe were interested to compare against the Neural Map paper [1], which also has 2D structured memory.  During the course of research, we asked the authors of Neural Map for their implementation, but did not receive a response.\n\nA notable difference from [1] is that we force our memory network to solve a more difficult task.  While [1] also explores spatial mapping, it anchors (hand-codes) memory updates to the current spatial location of the agent.  In contrast, our multigrid memory network must learn which memory cells to update.\n\nThe memory component from [2] is directly taken from DNC without any additional modification, so DNC is a good proxy for comparison.\n\n----\n\n\"DNC baseline has memory sizes that are much smaller\"\n\nThis is a fundamental limit of the official DNC implementation, which has a hidden cost of maintaining the Temporal Linkage matrix (https://github.com/deepmind/dnc/blob/master/dnc/addressing.py#L163).  For N memory slots, this matrix either incurs O(N^2) cost in space or requires an approximation.  We used the largest DNC that would fit in GPU memory.\n\nWe have now trained a smaller Multigrid Memory model on the spatial mapping task for the 15x15 world map, spiral motion pattern, 3x3 FoV and Query (new result in Table 1).  Comparing with DNC on the same task:\n\nDNC (0.75M Params, 8.00K Memory): [Prec = 91.09, Recall = 87.67, F = 89.35]\nMG  (0.12M Params, 7.99K Memory): [Prec = 99.79, Recall = 99.88, F = 99.83]\n\nThe strictly smaller (in both params and memory) multigrid model perfectly masters the task, while the DNC does not.\n\nHere, the DNC's 8K memory is organized as 500 slots of 16 channels, and 500 slots is more than adequate to store a 15x15 world map (225 locations).  To the Appendix, we added a visualization of DNC memory contents for this task.  The learned memory access strategy, rather than memory size, appears to be the stumbling block for the DNC.\n\nWe are also training a multigrid model with 8K memory on the 25x25 world map.  As of now, it is partially trained, but already significantly outperforms the DNC.  On the 25x25 map, spiral motion, 3x3 FoV and Query, we have:\n\nDNC (0.68M Params, 8.00K Memory): [Prec = 77.63, Recall = 14.50, F = 24.44]\nMG* (0.17M Params, 7.99K Memory): [Prec = 96.48, Recall = 96.19, F = 96.33]\n\n(*) partially trained\n\nWe will add results for this multigrid model, fully-trained, to the final paper version.\n\n----\n\n\"How are the input tensors upsampled in the multigrid memory layers?\"\n\nWe use nearest-neighbor upsampling.\n\n----\n\n\"How is the visualization on the right of Figure 3 generated?\"\n\nWe show the mean across channels of the hidden states in the deepest layer, highest resolution grid.  For comparison, we added Figure 10 (in Appendix) to visualize DNC memory.\n\n----\n\n\"Clarity of some parts, especially Section 3.1 & 3.2\"\n\nWe have added a formal description of multigrid memory layers to Section 3.1.  To the Appendix, we have added detailed architectures used in the experiments, complementing the more abstract descriptions in Section 3.2.\n\n----\n\n\"How would multigrid memory networks generalize to other tasks that do not involve input images?\"\n\n[Quoting our reply to Reviewer #1 regarding a similar question]\n\nOur experiments already cover non-visual sequential data.  There is a distinction between the input having visual structure (i.e., structure over which convolutional operations are helpful), and the network having convolutional layers.\n\nOur spatial mapping task, and a version of each of our algorithmic tasks, all utilize 3x3 input item size.  Here, the first network layer is fully connected: a 3x3 conv filter on a 3x3 input is exactly equivalent to a fully connected layer on a 1x9 input vector.  Moreover, for priority sort and associative recall, the 3x3 items are random - they have no internal visual structure.\n\nPriority sort or associative recall of random 9-element vectors has nothing to do with spatial reasoning.  It is actually a testament to generality that Multigrid Memory, which is internally biased to a 2D spatial memory layout, accomplishes these tasks just as well as the DNC.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxKo04tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1328/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1328/Authors|ICLR.cc/2020/Conference/Paper1328/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157679, "tmdate": 1576860544861, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment"}}}, {"id": "H1xcoKjsor", "original": null, "number": 4, "cdate": 1573792161788, "ddate": null, "tcdate": 1573792161788, "tmdate": 1573792161788, "tddate": null, "forum": "ByxKo04tvr", "replyto": "rJlrkMmpYH", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment", "content": {"title": "Response to Reviewer #1 [1/2]", "comment": "\n\"details on how [1] and [2] are integrated\", \"formulas describing the combination\"\n\nWe revised Section 3.1 to include formulas specifying the exact functionality of our multigrid memory layer.  This presentation now fully details the integration of [1] and [2], with equations elaborating the diagram in Figure 1.\n\n----\n\n\"Section 3.2\", \"what does the Writer do to the memory?\"\n\nSubnetworks are distinguished as \"reader\" and \"writer\" solely based on whether they contain parameters that govern the update of LSTM state.  The \"writer\" subnet is a Multigrid convolutional-LSTM; the \"reader\" subnet is a Multigrid CNN.  The \"writer\" subnet contains memory distributed alongside computation, whereas the \"reader\" subnets contain only computation.\n\nThere is feedforward communication from the writer to the readers: as an additional input, the convolution layers in the readers receive the current hidden state of the corresponding LSTMs in the writer.  But, readers cannot effect changes to that state.\n\n----\n\nOn: \"overall contribution\", \"addressable memory space\", and \"information routing\"\n\nOur contribution is to obtain emergent behavior (a memory subsystem) by training a network consisting of basic building blocks connected via a superior wiring pattern (multigrid connectivity).  We achieve with merely multigrid wiring all that the DNC does with extensive custom-designed components (controllers, addressing modes).  To summarize:\n\n(1) A multigrid wiring architecture endows a neural network with exponentially more efficient internal information routing pathways, compared to standard designs.  With multigrid wiring, a few successive layers of a convolutional network can approximate the connectivity of a fully-connected network.\n\n(2) Network parameters determine how to utilize these pathways in a dynamic input-dependent manner.  Ke et al. demonstrate such networks (Multigrid CNNs) can learn to accomplish tasks requiring attentional behavior.\n\n(3) Distributing memory units (LSTMs) throughout the network, the intrinsic capacity to implement attention translates into a capacity to attend to specific memory locations.  Our experiments demonstrate that Multigrid convolutional-LSTMs learn to behave like a large, addressable memory space.\n\n(4) This behavior emerges as a result of training for tasks that benefit from having memory.  Trained multigrid memory networks modify their memory in a localized and context-appropriate manner, directing reads and writes to specific memory cells.  Visualizations clearly show that our maze-exploring agent writes local observations into a coherent map of the environment contained within its memory.\n\nFor more detail on these points, please see our overview response above.\n\n----\n\n\"model is a straightforward replacement of the vanilla CNN with another CNN (multigrid CNN) in the convolutional LSTM architecture\"\n\nMore accurately, we replace convolutional layers in the multigrid architecture with convolutional LSTM layers.  Multigrid wiring weaves together many distinct convolutional LSTMs (one for each pyramid level in each layer), so that their collective behavior can emulate a large-scale memory store.\n\nThis straightforward strategy has huge practical consequences: we achieve all the capabilities of the DNC (and more*) from a network composed of basic components.  A powerful, yet simple, design is a virtue.\n\n(*) As the DNC fails to master the spatial navigation tasks, multigrid memory is strictly more capable.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxKo04tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1328/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1328/Authors|ICLR.cc/2020/Conference/Paper1328/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157679, "tmdate": 1576860544861, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment"}}}, {"id": "SklBMFijoB", "original": null, "number": 3, "cdate": 1573792012967, "ddate": null, "tcdate": 1573792012967, "tmdate": 1573792012967, "tddate": null, "forum": "ByxKo04tvr", "replyto": "rJlrkMmpYH", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment", "content": {"title": "Response to Reviewer #1 [2/2]", "comment": "\"experiments are not very satisfactory for the 'generality'\" and \"visual inputs\" vs \"sequential data\"\n\nOn the contrary, our experiments already cover non-visual sequential data.  There is a distinction between the input having visual structure (i.e., structure over which convolutional operations are helpful), and the network having convolutional layers.\n\nOur spatial mapping task, and a version of each of our algorithmic tasks, all utilize 3x3 input item size.  Here, the first network layer is fully connected: a 3x3 conv filter on a 3x3 input is exactly equivalent to a fully connected layer on a 1x9 input vector.  Moreover, for priority sort and associative recall, the 3x3 items are random - they have no internal visual structure.\n\nPriority sort or associative recall of random 9-element vectors has nothing to do with spatial reasoning.  It is actually a testament to generality that Multigrid Memory, which is internally biased to a 2D spatial memory layout, accomplishes these tasks just as well as the DNC.\n\nFinally, on the algorithmic tasks that operate on 28x28 image input and mix in classification, we are more general than the DNC (which does poorly here).  Multigrid Memory can learn to behave as a flexible combination of CNN-like and DNC-like abilities, as demanded by the task.\n\nWe agree that applying Multigrid Memory to NLP tasks is a promising avenue for future work.  Our experiments already cover two domains: reinforcement learning with spatial reasoning (Table 1) and algorithmic tasks (Table 2), comparable in breadth to the experiments introducing NTMs.  Additionally, Multigrid CNNs are a special case (a strict subnet) of our design; Ke et al. already demonstrated their superior performance on vision tasks, including ImageNet classification.\n\n----\n\nOn: \"information from source grid\" being \"preserved in higher levels/layers\"\n\nWhat is stored on each grid is entirely learned.  There is no constraint that any grid duplicate or preserve the information on another.  Upsampling/downsampling of grids occurs when preparing the input to a subsequent convolutional LSTM layer, acting as mechanism for information flow across pyramid scales.\n\n----\n\n\"how did you feed the images to DNC?\"\n\nWe feed all data to the DNC as vectors, reshaping if necessary.  As explained above, most of our experimental tasks involve 3x3 inputs, which should not be regarded as \"images\"; initial layers of both Multigrid Memory and DNC are fully-connected in those cases.\n\n----\n\n\"other solutions to increase memory capacity [3,4]\"\n\nSource code is not available for these methods, making comparison difficult.  In contrast, we will release code to foster open research.\n\nFor fair comparison at equal memory capacity, we trained a smaller Multigrid Memory model; see new results in Table 1 and our response to Reviewer #3.\n\n----\n\n\"ConvLSTM as a baseline?\"\n\nFor the spatial mapping task, ConvLSTM baselines are provided and perform poorly (Table 1).\n\nWe are now training ConvLSTM baselines on the algorithmic tasks, and will include those results in the final version of the paper.  Training is only partially complete, at 400,000 iterations, as of this response.  These partially trained baseline ConvLSTMs have error rates much higher than those for multigrid or DNC (Table 2):\n\nConvLSTM-deep error rates:\nAssociative Recall: [Standard variant: 0.500896,  MNIST variant: 0.887097]\nSorting: [Standard variant: 0.381804, MNIST variant: 0.565312]\n\nConvLSTM-thick error rates:\nAssociative Recall: [Standard variant: 0.108871, MNIST variant: 0.891129]\nSorting: [Standard variant: 0.502153, MNIST variant: 0.887500]\n\n----\n\n\"NTM maybe a better baseline than DNC\"\n\nDNC is the improved version of NTM with better addressing and memory allocation.  Furthermore, DNC has an official code release (which we use), while NTM does not.\n\n----\n\n\"model size and computational complexity\"\n\nTables list parameter counts and memory capacity.  We have added several Appendix sections, providing more architectural details and wall-clock inference runtime.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxKo04tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1328/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1328/Authors|ICLR.cc/2020/Conference/Paper1328/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157679, "tmdate": 1576860544861, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment"}}}, {"id": "HJgkHOooiH", "original": null, "number": 2, "cdate": 1573791798769, "ddate": null, "tcdate": 1573791798769, "tmdate": 1573791798769, "tddate": null, "forum": "ByxKo04tvr", "replyto": "SJgP4SU0YB", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "\"work could be improved by providing more detail\"\n\nWe have added a new section (Experiment Details) to the Appendix.  This section diagrams the precise reader-writer architecture used in the navigation (spatial mapping) task, as well as the precise encoder-decoder architecture used in the MNIST sorting task.  Included are details on all network inputs, outputs, and losses applied during training.\n\n----\n\n\"the only explicit discussion of the loss used in the navigation task is in figure 3\"\n\nSection 4.1 explicitly stated the form of the loss: \"We used a pixel-wise cross-entropy loss over predicted and true locations.\"\n\nPlease also see our answer to the previous question and the new section (Experiment Details) we have added to the Appendix.\n\n----\n\n\"runtime and memory requirements\"\n\nFigure 6 concerns model training, not inference time.  The x-axis is training iterations, while the y-axis is loss.  These plots show that for all three tasks (spatial mapping, priority sort, associative recall), the multigrid memory model learns faster (reaches lower loss in fewer steps) than competing models.  For associative recall (rightmost plot), we stopped training the multigrid model at 500,000 steps because it already achieved low loss; in contrast, the DNC, trained for longer (2,000,000 steps) still has much higher loss.\n\nFor wall-clock inference time, we have added an Appendix section comparing the runtime of a single forward pass through Multigrid Memory and DNC.  Here, both models have 8K memory cells (a configuration added to Table 1 in response to another reviewer request).  Multigrid Memory takes 18ms (+- 3ms) for inference, whereas DNC takes 17ms (+- 1ms), averaged over 10 runs.\n\nOverall, Multigrid Memory is CNN-like in resource usage.  To roughly estimate the resource requirements of Multigrid Memory, one can take a Multigrid CNN with equivalent depth, grid scales, and channel counts, and multiply GPU compute and GPU memory usage by a constant overhead factor to account for LSTM cell and hidden states.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxKo04tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1328/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1328/Authors|ICLR.cc/2020/Conference/Paper1328/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157679, "tmdate": 1576860544861, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment"}}}, {"id": "HyeTtPsjoS", "original": null, "number": 1, "cdate": 1573791620858, "ddate": null, "tcdate": 1573791620858, "tmdate": 1573791620858, "tddate": null, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment", "content": {"title": "Response to Reviews and Updates to Paper", "comment": "We thank the reviewers and address comments in individual replies.  Acting on reviewer suggestions, we have updated the paper with more technical details (Section 3.1 and Appendix) and new results.  Notably, at equal memory capacity (8K), Multigrid Mem+CNN still significantly outperforms the DNC on the spatial mapping task (Table 1).\n\nWe also want to clarify our view of the motivations, contributions, and (high-level) technical insight of the work; this perspective follows.\n\n----\n\nMultigrid memory is a radical new approach to endowing neural networks with access to long-term memory.  It is a drastic simplification compared to all prior memory designs: instead of custom controllers, addressing modes, and/or external storage cells, we have wires.  We take well-known basic building blocks (convolution, LSTMs), wrap them in a multigrid wiring pattern, and achieve all the capabilities of far more complex designs.  Replacing the complexity of the DNC with a wiring pattern is exactly the kind of simplification that advances science.  The results we present here are cause for drastically rethinking the prevailing strategies for designing neural network architectures.\n\nOur networks, once trained, behave like memory subsystems - this is an emergent phenomenon.  Reviews ask about \"addressable memory space\"; our design contains no explicit address calculation unit, no controller, no attention mask computation.  Yet, our trained networks modify their memory in a localized and context-appropriate manner, directing reads and writes to specific memory cells.  Our maze-exploring agent writes local observations into a coherent map of the environment contained within its memory.  This is possible because learned parameters govern how information flows through its multigrid network, what (if any) memory cells to update, and how to update them.\n\nMultigrid wiring provides an exponentially more efficient information routing topology.  Individual layers (convolutional LSTMs), connected within this topology, learn to coordinate in a manner that allows the system as a whole to behave as a useful memory store.  Multigrid topology provides short neuron-to-neuron communication pathways, which are absent in traditional architectures.\n\nSpecifically, bi-directional connections (both coarse-to-fine and fine-to-coarse) between pyramids in subsequent layers allow a signal to hop up pyramid levels and back down again (and vice-versa).  As a consequence, pathways connect any neuron in a given layer with every neuron located only O(log(S)) layers deeper, where S is the spatial extent of the highest-resolution grid (that grid has width and height equal to S).  In a traditional network, this takes O(S) layers to occur.  Equivalently stated, using fixed-size convolutional filters, receptive fields grow exponentially faster in multigrid networks than in standard networks.\n\nQuickly (in few layers of depth) gathering, scattering, broadcasting, or re-directing data across the spatial axes of activation tensors, is thus possible with appropriately chosen network parameters.  Notably, attentional behavior can be implemented in terms of such operations and [Ke et al., Section 4.3] show multigrid CNNs actually learn tasks requiring attention over images.\n\nSprinkling memory cells throughout the network, the capacity for attention translates to a capacity for specifying which memory cells to read and write.  Our results demonstrate this too can be learned.  A simple wiring design thus enables us to build an entire neural memory subsystem out of basic components.  Simplicity is a virtue; our approach stands in stark contrast to those, like the DNC, that try to craft \"neural\" versions of conventional CPU components.  Thinking carefully about a core property - efficient communication pathways - obviates the need for such complicated heavy-handed designs.\n\nRecently, one wiring design change has had enormous impact: residual connections [He et al., 2016] provide shortcut pathways across depth, facilitating gradient propagation and allowing very deep networks to be trained.  Multigrid wiring is complementary, improving connectivity across an orthogonal aspect of the network: the spatial dimension.  Its impact is also groundbreaking: multigrid wiring exponentially improves internal data routing efficiency, allowing complex behaviors (attention) and subsystems (memory stores) to emerge from training simple components."}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByxKo04tvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1328/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1328/Authors|ICLR.cc/2020/Conference/Paper1328/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157679, "tmdate": 1576860544861, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Authors", "ICLR.cc/2020/Conference/Paper1328/Reviewers", "ICLR.cc/2020/Conference/Paper1328/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Comment"}}}, {"id": "B1lPj6V3tr", "original": null, "number": 1, "cdate": 1571732895501, "ddate": null, "tcdate": 1571732895501, "tmdate": 1572972483118, "tddate": null, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes the multigrid memory networks which combine multigrid convolutional layers with LSTMs and evaluates its performance on a reinforcement learning-based navigation task and two algorithmic tasks of priority sorting and associative recall. \n\nThe authors claim that by integrating LSTM within the layers of the network, it affords larger memory size while remaining parameter efficient compared to other memory-augmented networks like the DNC, which abstract its memory module as a separate unit from its computational units. \n\nThe authors show with their experiments that multigrid memory networks outperform DNC and other models that lack its multigrid inference property.\n\n\nWhile the experiments show the proposed networks\u2019 superior performance over baselines, my main concern about this paper is the lack of comparison with more recent memory-augmented models [1,2]. \n\nMoreover, in all the experiments, the DNC baseline has memory sizes that are much smaller than the multigrid memory networks. To my understanding, the memory module of DNC can be scaled up without increasing the number of computational parameters. Why is the DNC\u2019s memory not scaled to match that of the multigrid memory networks? It would seem unfair to compare against a baseline with much smaller memory in memory-intensive tasks.\n\n\nOther comments:\n\n\n1) How are the input tensors upsampled in the multigrid memory layers?\n\n2) How is the visualization on the right of Figure 3 generated? It\nwould be more convincing to compare it with that of DNC.\n\n3) Clarity of some parts, especially Section 3.1 & 3.2, could be\nimproved with more formal mathematical statements about the multigrid\nmemory networks.\n\n4) How would multigrid memory networks generalize to other tasks that\ndo not involve input images?\n\n\n\n[1] Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured\nmemory for deep reinforcement learning. ICLR, 2018.\n\n\n[2] Arbaaz Khan, Clark Zhang, Nikolay Atanasov, Konstantinos Karydis,\nVijay Kumar, Daniel D. Lee. Memory augmented control networks. ICLR,\n2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575831129212, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Reviewers"], "noninvitees": [], "tcdate": 1570237738973, "tmdate": 1575831129224, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Review"}}}, {"id": "SJgP4SU0YB", "original": null, "number": 3, "cdate": 1571869999509, "ddate": null, "tcdate": 1571869999509, "tmdate": 1572972483082, "tddate": null, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Recurrent neural networks that can grow their memory capacity independent of the number of training parameters are an interesting topic. DNC, memory networks and NTM (all cited in this work) are some examples.\n\nThis work proposes an architecture inspired by an approach used in the computer vision literature, a multi-scale CNN. However, each cell of the CNN here is a convolutional LSTM.\n\nThis approach allows the memory capacity of the architecture to be increased (by increasing the number of cells) while maintaining a fixed number of parameters. The multi-scale nature of it allows memory operations across multiple scales the 2D grid in an efficient manner.\n\nThey test this architecture on a mapping and localization task (a natural fit for the multi-scale architecture) and find it outperforms other architectures including a single scale version of the same architecture.\n\nThey also compare against the DNC on tasks similar to that used in the original paper (priority sort) and associative recall and again find it learns in fewer iterations and achieves good performance.\n\nOverall, this architecture, while not groundbreaking, is novel in this context and the results show empirical gains. The paper is fairly well written.\n\nThis work could be improved by providing more detail (e.g. in the appendix) on the losses and approach used in the navigation task (the only explicit discussion of the loss used in the navigation task is in figure 3). It would also be helpful to provide more detail on the other tasks in the appendix.\n\nFinally, there is little analysis (either theoretical or empirical) on the runtime and memory requirements of this model. For example, figure 6 would seem to imply this model is running slower than the DNC (already quite a slow model) since it has completed less iterations? At a minimum, some empirical numbers of run time speed and memory usage compared with the DNC would be helpful."}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575831129212, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Reviewers"], "noninvitees": [], "tcdate": 1570237738973, "tmdate": 1575831129224, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Review"}}}, {"id": "rJlrkMmpYH", "original": null, "number": 2, "cdate": 1571791324931, "ddate": null, "tcdate": 1571791324931, "tmdate": 1572972483034, "tddate": null, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "invitation": "ICLR.cc/2020/Conference/Paper1328/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a multigrid memory architecture by introducing multigrid CNN [1] into convolutional LSTM network [2]. The method extends the convolutional LSTM with bigger memory capacity in forms of multigrid CNNs. Some specific designs such as multiple threads and encoder-decoder are also proposed. The model is validated with synthetic tasks such as spatial mapping, associative recall and priority sort. \n\nPros: \n* The method is well-motivated. Utilizing multigrid hierarchy enables convolutional LSTM to operate across scale space and thus may achieve richer representation for the hidden state memory.\n* The experiments are well designed (especially RL tasks), demonstrating the advantage of the proposed model.\n\nCons:\n* The proposed model is not presented clearly. The paper does not show details on how [1] and [2] are integrated, which requires the reader to refer back to the old works and make inference on the integration. Besides graphic illustration, the authors should include a brief review on [1, 2] and introduce some basic formulas describing the combination between the two.\n* Section 3.2 is supposed to contain the most important design considerations, but details seem missing. For example, what does the Writer do to the memory?\n* The contribution is rather incremental. Without detailed description, it seems that the proposed model is a straightforward replacement of the vanilla CNN with another CNN (multigrid CNN) in the convolutional LSTM architecture.\n* The experiments are not very satisfactory for the \u201cgenerality\u201d claim that the paper makes. \n\nQuestions and concerns:\n* Could you explain the term \u201caddressable memory space\u201d? It seems that your network\u2019s memory comes from the internal states of LSTM. How is the memory addressable? \n* The analysis on information routing seems interesting. However, how does it relate to the memorization capacity? Is there any guarantee that the information from source grid is preserved in higher levels/layers? How does it differ from using vanilla multiple-layer neural networks?\n* As DNC is not originally designed for image inputs, how did you feed the images to DNC? Did you tune DNC carefully by adjusting the number of elements per memory slot? Also, DNC seems not a really strong baseline. Other solutions to increase memory capacity of MANNs exist [3, 4]\n* For algorithmic tasks, why don\u2019t you include ConvLSTM as a baseline? Also, NTM maybe a better baseline than DNC for these tasks. \n* What is the model size and computational complexity compared to other MANNs?\n* The model is naturally fit for visual inputs. Is there any advantage when applying it to other sequential data (NLP, time-series)? \n\nReference\n[1] Ke, Tsung-Wei, Michael Maire, and Stella X. Yu. \"Multigrid neural architectures.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6665-6673. 2017.\n[2] Xingjian, S. H. I., Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. \"Convolutional LSTM network: A machine learning approach for precipitation nowcasting.\" In Advances in neural information processing systems, pp. 802-810. 2015.\n[3] Rae, Jack, Jonathan J. Hunt, Ivo Danihelka, Timothy Harley, Andrew W. Senior, Gregory Wayne, Alex Graves, and Timothy Lillicrap. \"Scaling memory-augmented neural networks with sparse reads and writes.\" In Advances in Neural Information Processing Systems, pp. 3621-3629. 2016.\n[4] Hung Le, Truyen Tran, and Svetha Venkatesh. Learning to remember more with less memorization. In International Conference on Learning Representations, 2019. URL\nhttps://openreview.net/forum?id=r1xlvi0qYm.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1328/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1328/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["trihuynh@uchicago.edu", "mmaire@uchicago.edu", "mwalter@ttic.edu"], "title": "Multigrid Neural Memory", "authors": ["Tri Huynh", "Michael Maire", "Matthew R. Walter"], "pdf": "/pdf/106690bd64ea9a501b0e546f2cbe55ecac4deb8f.pdf", "TL;DR": "A novel neural memory architecture that co-locates memory and computation throughout the network structure, providing addressable, scalable, long-term and large capacity neural memory.", "abstract": "We introduce a novel architecture that integrates a large addressable memory space into the core functionality of a deep neural network.  Our design distributes both memory addressing operations and storage capacity over many network layers.  Distinct from strategies that connect neural networks to external memory banks, our approach co-locates memory with computation throughout the network structure.  Mirroring recent architectural innovations in convolutional networks, we organize memory into a multiresolution hierarchy, whose internal connectivity enables learning of dynamic information routing strategies and data-dependent read/write operations.  This multigrid spatial layout permits parameter-efficient scaling of memory size, allowing us to experiment with memories substantially larger than those in prior work.  We demonstrate this capability on synthetic exploration and mapping tasks, where the network is able to self-organize and retain long-term memory for trajectories of thousands of time steps.  On tasks decoupled from any notion of spatial geometry, such as sorting or associative recall, our design functions as a truly generic memory and yields results competitive with those of the recently proposed Differentiable Neural Computer.", "keywords": ["multigrid architecture", "memory network", "convolutional neural network"], "paperhash": "huynh|multigrid_neural_memory", "original_pdf": "/attachment/0e3d6b1e4a15c9f2618f20b4562842122132281a.pdf", "_bibtex": "@misc{\nhuynh2020multigrid,\ntitle={Multigrid Neural Memory},\nauthor={Tri Huynh and Michael Maire and Matthew R. Walter},\nyear={2020},\nurl={https://openreview.net/forum?id=ByxKo04tvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByxKo04tvr", "replyto": "ByxKo04tvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1328/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575831129212, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1328/Reviewers"], "noninvitees": [], "tcdate": 1570237738973, "tmdate": 1575831129224, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1328/-/Official_Review"}}}], "count": 11}