{"notes": [{"id": "e3KNSdWFOfT", "original": "fPFaDWbxtNV", "number": 2595, "cdate": 1601308287326, "ddate": null, "tcdate": 1601308287326, "tmdate": 1614985640948, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 26, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "S6nhA7nFE2", "original": null, "number": 1, "cdate": 1610040520713, "ddate": null, "tcdate": 1610040520713, "tmdate": 1610474129394, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper studies the convergence of gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games that the authors call \"hidden zero-sum games\". Unlike general min-max games, these games have a well-defined notion of a \"von Neumann solution\". The authors show that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to this von Neumann solution.\n\nThe paper received four high quality reviews and was discussed extensively during the author rebuttal phase. From an application angle, the authors' replies did not convince the reviewers on the relevance of this paper to GANs, and one of the original \"accept\" recommendations was downgraded to a \"reject\" because of this. On the theory side, the novelty over Vlatakis-Gkaragkounis et al. (2019) is not clear and the reviewers found the writing often confusing or hard to connect with practice. The reviewer with the most positive recommendation did not champion the paper post-rebuttal. In the end, the consensus was that the work shows significant promise, but it requires refocusing before appearing at a top-tier conference."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040520699, "tmdate": 1610474129376, "id": "ICLR.cc/2021/Conference/Paper2595/-/Decision"}}}, {"id": "_kcY_wgGOF4", "original": null, "number": 22, "cdate": 1606272597992, "ddate": null, "tcdate": 1606272597992, "tmdate": 1606272597992, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "T8adNVo_n3c", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "An additional application of our framework", "comment": "Given the restrictions of maximum 8 pages, in the main paper we focused our discussion on simple GAN applications that our framework can capture. However, our paper is not written with a sole focus on GANs (no explicit mention of GANs on the title and abstract) but on this new class non-convex non-concave zero sum games and our non-local convergence results, that based on our discussion the reviewer clearly appreciates. In our Appendix 9.3, we already cover a different non-GAN application that provably resolves the cycling issues in the case of the model of Vlatakis et. al (Neurips 2019). We show this problem can be effectively resolved by using regularization. We believe that it is straightforward to add a paragraph showcasing this non-GAN application of our framework since the technical analysis is already included in the appendix. We believe that an application that resolves a key problem in a Spotlight Neurips 2019 paper suffices as a contribution. We hope that in light of this, the reviewer would revert back to their original evaluation. We look forward to hearing back from the reviewer and we thank them for the engaging discussion.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "MQEFB2F0BHU", "original": null, "number": 4, "cdate": 1603912926604, "ddate": null, "tcdate": 1603912926604, "tmdate": 1606245075959, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Review", "content": {"title": "Interesting progress beyond convex concave games. Application to GANs questionable?", "review": "Summary:\nWhile single-agent optimization is quite well understood and even convergence results in the nonconvex setting, the study of non-convex-concave saddle point problems is still in its infancy. In particular, recent work by Letcher (2020) and Hsieh et al (2020) suggests that even many recently proposed modifications of simultaneous gradient descent are not guaranteed to converge in the non-convex-concave case.\nThe present work makes significant progress on this problem by introducing hidden convex-concavity, a class of structured problems nonlinear functions $F(\\theta)$ and $G(\\phi)$ that depend on only one of the agents $\\theta$ and $\\phi$, are plugged into a convex-concave problem $L(\\cdot, \\cdot)$, resulting in $\\min_{\\theta} \\max_{\\phi} L(F(\\theta), G(\\phi))$.\nHere, the components $F_i$ and $G_j$ of $F$ and $G$ are multivariate, real-valued functions $f_i$, $g_i$ of disjoint sets of components of $\\theta$ and $\\phi$.\nBy cleverly relating the dynamics of the $f_i$,$g_i$ to those of the $\\theta_j$, $\\phi_j$, the authors derive a Lyapunov function for the underlying dynamics and use it to prove asymptotic stability and convergence under very minor additional assumptions.\nThey furthermore show a notion of \"hidden convergence\" of the $f_i$, $g_i$ that is particularly relevant to the overparametrized regime. \nIn the last section, the authors relate their findings to GANs.\n\nDecision:\nI believe that this is good work that will be interesting to a wide range of readers. My only concern is the following:\n\nIn the definition of hidden convex-concavity, the functions $f_i, g_j$ are multivariate and real valued, but depend on **disjoint** sets of parameters. In particular, it does not seem to cover the case of  $L(f(\\phi), g(\\theta))$ where $f$ and $g$ are multivariate vector valued functions. \nHowever, this is arguably the case in GANs, where the discriminator and generator can be thought of high/infinite-dimensional \"vectors\" that depend nonlinearly on all parameters. \nTherefore, the results of the authors do not seem to apply to GANs even formally? Please let me know if there is something that I overlooked.\n\nI believe that the results are interesting and relevant even if they would not apply to GANs. However, in this case the application section and motivation of the paper might need some restructuring.\n\nOther suggestions/questions to authors\n- I would suggest citing https://arxiv.org/abs/2005.12649 and https://arxiv.org/abs/2006.09065 as they provide additional motivation for the necessity to consider more structured classes of nonconvex games. \n- A popular extension beyond convexity is pseudo-monotonicity (see for instance https://arxiv.org/abs/1807.02629). How does strict hidden convexity relate to strict pseudo-monotonicity? Does the former imply the latter? Are there counterexamples?\n- Do the authors expect that methods that converge in the bilinear case such as extragradient, symplectic gradient adjustment, or competitive gradient descent can be guaranteed to converge even for weak hidden convexity?\n\nminor comments: \n- Statement of Lemma 1: I think the statement would be more clear if \\Sigma_1 and \\Sigma_2 were defined before the conclusion.\n- Theorem 2: \"is a safe\" -> \"is safe\" \n- \"Hidden convex-concave games & Regularizaiton\" -> \"...Regularization\"\n\n=============================================================================================================\nAfter author discussion: After discussion with the authors, I am now convinced that any applicability of the theory proposed in this work to GANs is fundamentally tied to univariate latent space or generator output since the \"hidden convexity assumption\" does not allow for a multivariate set of latent variables to be combined to a multivariate set of outputs.\nI still find the theoretical findings and method interesting, but I think that the work requires substantial refocusing and the identification of more examples of \"hidden strong convexity\" before being published at a top-tier conference. I therefore change my rating from 7 to 5 and recommend rejection, for now.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538092769, "tmdate": 1606915805939, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2595/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Review"}}}, {"id": "T8adNVo_n3c", "original": null, "number": 21, "cdate": 1606158793687, "ddate": null, "tcdate": 1606158793687, "tmdate": 1606158793687, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "vuEkyZLiB-N", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "But in this example the loss depends only on expectation. GAN examples seem to be tied to one-dimensionality", "comment": "You are right that in this example the distribution can have multiple parameters, but this is only true because the loss function only depends on a single parameter, as you said the operator $F$ has one-dimensional output. \n\nBased on our discussion, I am not convinced that the method has applications to GANs beyond settings that have been restricted to be essentially one-dimensional.\n\nAs I said in my review I think that the theoretical results and techniques are interesting, but I believe that the paper should be rewritten to provide more convincing examples/motivation for the setting considered by the theory. This revision is too substantial to be done in a single round of reviews, which is why I lowered my score to 5 and recommend rejection at this time"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "vuEkyZLiB-N", "original": null, "number": 20, "cdate": 1606005226137, "ddate": null, "tcdate": 1606005226137, "tmdate": 1606005226137, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "ET5m9hwijWk", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Clarification #2", "comment": "First, we would like to note that we changed the notation in our previous responses in order to avoid the overloading of the letter $G$ referring to both the distribution family of the generator and the operator of the discriminator. We are sorry if this caused any confusion.\n\nIn this new notation, we have:\n\n$\\min_{\\theta}\\max_{u} (E_{X\\sim\\Phi(\\mathcal{D}(\\theta))}[X]-E_{X\\sim\\Phi(\\mathcal{D}(\\theta^\\star))}[X])u -u^2/2$\n\n$\\bullet$ The hidden strictly convex-concave game is $ L(x,y)=xy -y^2/2 $.\n\n$\\bullet$ The non-convex operators $F,G$ are:\n $F(\\theta)=(E_{X\\sim\\Phi(\\mathcal{D}(\\theta))}[X]-E_{X\\sim\\Phi(\\mathcal{D}(\\theta^\\star))}[X])$\nand $G(\\phi)=\\phi$\n\nNotice that the distribution $\\mathcal{D}(\\theta)$ can have an arbitrary number of parameters \n(e.g moments, mean,variance,scale,etc) that are controlled by an arbitrary combination of parameters in $\\theta=(\\theta_1,\\cdots,\\theta_K)$.\n\nFor instance, $\\mathcal{D}(\\theta)$ could be the Gaussian family of $\\mathcal{N}(\\mu,\\sigma^2)$.\nWe can parametrize the mean and the variance of the variables of the vector $\\theta$, i.e \nthere exists two functions $\\mu=\\mu(\\theta),\\sigma^2=\\sigma^2(\\theta)$. Notice that we don't impose any restriction on these functions to depend on separate variables.\n\nIndeed, in this example the operator $F$ is one dimensional so $F(\\theta)=f_1(\\theta)$. The aim of this WGAN example\neven with one dimensional $F$ we can formulate games with practical applications that involve continuous distributions. If we consider\nas ''latent-dimension'' the dimension of $F$ then the above HCC indeed corresponds to a latent-dimension one.\n\nOn the other hand as in many machine learning applications, \nif we consider the parameters of the distribution $\\mathcal{D}$ as the latent variables that need to be determined through \nmoment matching, then the latent-dimension can be arbitrarily large. \n\nWe would like to add that in the example above, we chose $G(\\phi)=\\phi$ for simplicity.\nOur results cover the cases of the discriminator using, in general, any smooth $G$ operator.\n\nWe hope that we resolved any ambiguity.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "5w-kYqyv_wQ", "original": null, "number": 18, "cdate": 1606001371347, "ddate": null, "tcdate": 1606001371347, "tmdate": 1606003609448, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "9Ubgp4PeiR", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Clarification #1", "comment": "The hidden strictly convex-concave game is $ L(x,y)=xy -y^2/2 $.\n\nThe non-convex operators $F,G$ are:\n\n $F(\\theta)=(E_{X\\sim\\Phi(\\mathcal{D}(\\theta))}[X]-E_{X\\sim\\Phi(\\mathcal{D}(\\theta^\\star))}[X])$\nand $G(\\phi)=\\phi$\n\nNotice that the distribution $G(\\theta)$ can have an arbitrary number of parameters \n\n(e.g moments, mean,variance,scale,etc) that are controlled by an arbitrary combination of parameters in $\\theta=(\\theta_1,\\cdots,\\theta_K)$.\n\nThus semantically, this is not a latent-dimension one example."}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "CGKgC3peNDr", "original": null, "number": 13, "cdate": 1605987186515, "ddate": null, "tcdate": 1605987186515, "tmdate": 1606003541659, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "yzvuqHEe8QU", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Comments on GAN application", "comment": "Firstly, we would like to thank the reviewer for his valuable feedback.\n\nWhen we view GAN applications as a generator outputting a distribution and a discriminator outputting a classifier, then we agree that our results directly capture the case of \u201cone latent dimension\u201d for both the generator and discriminator in its full generality.\n\nIn our work, we also highlight an application of WGANs that goes beyond the scope outlined above, namely parameter matching over transformed distributions. In this setting the generator outputs the parameters of a distribution instead of the distribution itself as a pdf. For the case of linear discriminators our results fully capture this case regardless of the number of parameters that need to be learned. More concretely, the min-max objective in this case is:\n\n$\\min_{\\theta}\\max_{u} (E_{X\\sim\\Phi(\\mathcal{D}(\\theta))}[X]-E_{X\\sim\\Phi(\\mathcal{D}(\\theta^\\star))}[X])u -u^2/2$\n\nHere $\\Phi$ is the nonlinear smooth transformation. $\\mathcal{D}(\\theta)$ is the distribution of the output of the generator and can have any number of parameters that in turn can depend on any combinations of variables in $\\theta$. $\\mathcal{D}(\\theta^*)$ is the target distribution which is realized for some setting of parameters $\\theta^*$.\n\nThis clearly goes beyond the \u201clatent dimension one\u201d case in the reviewer\u2019s terminology. Notice that this is still a HCC with the disjoint parameters property since the generator only needs to output the difference of expectations directly and not the individual parameters. In our opinion, we find highly encouraging the fact that the subset of HCC games studied in this work can accurately predict the behaviour of WGANs proposed in prior, independent work. We hope that this addresses your question satisfactorily.\n\nFinally, we do want to point out that our goal is not to capture all aspects of GANs but to create a theoretical model that significantly pushes the boundary of CC games in the direction of GANs that still allows for theoretical analysis. As you point out HCC will make for a nice test bed for algorithms such as extra-gradient whereas previous works only explored their performance in (pseudo)monotone cases. Maybe and unlike CC they might allow for a more fine grained comparison between techniques that are convergent in all CC games but eg may require distinct types of safety conditions in HCC.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "ET5m9hwijWk", "original": null, "number": 19, "cdate": 1606002264825, "ddate": null, "tcdate": 1606002264825, "tmdate": 1606002264825, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "5w-kYqyv_wQ", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Thanks for the clarification, one follow-up question", "comment": "Thanks for the clarification, but it is still not clear what the $f_i$ are in this case? "}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "9Ubgp4PeiR", "original": null, "number": 17, "cdate": 1605999324209, "ddate": null, "tcdate": 1605999324209, "tmdate": 1605999357576, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "CGKgC3peNDr", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Comments on GAN application", "comment": "It is not obvious to me what the $L, F, G, f_i, g_i$ are in the example you just gave, could you please define them explicitly? "}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "nW8aSilGDTW", "original": null, "number": 16, "cdate": 1605988867101, "ddate": null, "tcdate": 1605988867101, "tmdate": 1605988894886, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "3tluY7uc4PJ", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Other assumptions & Safety Condition", "comment": "We would like to thank the reviewer again for his willingness to give us feedback to improve the readability of our work to non-continuous time experts. We commit to include the crucial part of our discussion about the various clarifications in the first sections of the camera-ready version. \n\nAnonReviewer 1 asked about a comparison between our safety assumption and the \u2018\u2018other\u201d assumptions in the literature. Roughly speaking, our assumptions are totally orthogonal to the typical assumption about convergence in min-max settings. Critically, our results are non-local, whereas prior results in the literature are local and hence the related assumptions reflect sufficient conditions for applying variants of the Stable Manifold Theorem (SMT) which themselves relate to system eigenvalues. \n\nOn the other hand, our safety conditions address mainly the issue of the implementability of the equilibrium of $L(x,y)$ when it is composed with the non-convex operators $F(x), G(y)$. Clearly these two types of assumptions lie in orthogonal directions (the curvature around the equilibrium VS the range of the operators). \n\nFinally, the reviewer asked about extensions of our result to non--continuous methods and possible discretizations. There are multiple promising approaches to finding Nash Equilibria or Von-Neumann solutions with discrete dynamics. One methodology comes from the seminal work of Benaim 1990 ([Dynamics of Stochastic Approximation Algorithms](http://www.numdam.org/article/SPS_1999__33__1_0.pdf)) and another recent one is of  [Haihao Lu](https://arxiv.org/abs/2001.08826) about the $O(s^r)$-Resolution ODE Framework for Discrete-Time Optimization Algorithms and Applications to the Linear Convergence of Minimax Problems, which can give us a promising path for a discrete-time analysis. Leveraging those frameworks introduces extra technical hurdles and concepts (e.g. pseudotrajectories or resolution ODEs), which lie beyond the scope of this work. Nevertheless, we believe that our continuous-time results will serve as an important building block of any argument in that direction. \n\nWe will be happy to incorporate this discussion in our manuscript and we look forward to any other suggestions that you may have.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "GdyGLqLT7gx", "original": null, "number": 15, "cdate": 1605988702141, "ddate": null, "tcdate": 1605988702141, "tmdate": 1605988702141, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "fMLXlIbwzX4", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Infinite-dimensional & Extra-Gradient", "comment": "We thank the reviewer for his valuable comments and thorough review and additional references that we will happily incorporate.\n\nWe certainly agree that our result does not apply in the infinite-dimensional case. We definitely did not mean to suggest that this is the case and we believe that a reader even after a cursory pass over the paper should have little trouble telling that this is not so (ie Banach spaces are never mentioned). On the other hand, as we pointed out in our very recent response to **AnonReviewer 3** our framework covers applications of WGANs that go beyond the scope outlined above, namely parameter matching over transformed distributions.\n\nIn any case, we agree that it is a good idea to explicitly point out the case of the infinitely dimensional HCC games as the ultimate frontier of this type of work. Naturally, this goal is well beyond scope of this work. Our contribution is a modest step towards this very ambitious direction, which sets up some early foundational work including the first non-local asymptotic stability result for any class of HCC games. \nAs we pointed out in our initial response to **AnonReviewer 3** these settings lie truly beyond pseudo-monotone games and to our knowledge represent the most advanced version of global convergence result in min-max games. We will be happy if the reviewer could point out any other global convergence result in a class of nonconvex non-concave games that was a closer representative of GANs than our setting. For example, https://arxiv.org/abs/2002.05820 despite its somewhat similar themes that the reviewer nicely points out offers no such analysis.\n\n**AnonReviewer 2** also inquired about the existence of different ODE for the extra gradient. A recent work of  [Haihao Lu](https://arxiv.org/abs/2001.08826) describes a $O(s^r)$-Resolution ODE framework, where different discrete-time algorithms (DTA) correspond to different continuous dynamics. Thus by choosing a small enough resolution parameter, each DTA shares different local stability-convergence conditions. For the case of extra gradient check Corollary 1 (item iii) and v)) and Proposition 1.\n\nWe hope that these address the questions of the reviewer satisfactorily.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "2tDXKPsnByF", "original": null, "number": 6, "cdate": 1605562313134, "ddate": null, "tcdate": 1605562313134, "tmdate": 1605987500958, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "uDmFioIsY_G", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Response to AnonReviewer2 (1/1) ", "comment": "We thank the reviewer for his valuable comments and thorough review. \n\nRegarding the comment of $\\textit{AnonReviewer2}$ about a potential overclaim in our work, we would like to highlight that the restriction on HCC where $f_i$ and $g_j$ have all disjoint inputs is already clearly mentioned at the second line of the $\\textbf{Our results}$ paragraph (page 2). Since other reviewers did not make a similar comment, it is possible that $\\textit{AnonReviewer2}$ might have missed this.\n\n$\\textit{AnonReviewer2}$ expressed some confusion about how training GANs fit the HCC framework. We would like first to remind the reviewer that although we argued in the Applications section that some instances of training GANs fit the HCC framework, we did not claim that all GAN training can be viewed as an HCC game. Despite this, we believe that the tools developed here to transfer results from the operator space of $F$ and $G$ to the parameter space, which are the first of their kind to the best of our knowledge, are a major stepping stone. \n\nThat being said, we would like to point out that GANs can also be applied in settings that are not captured by either cases brought forth by the reviewer. Specifically, for some settings, the generator does not output a measure directly but outputs the parameters of a distribution. The application of [Lei et al](https://arxiv.org/pdf/1910.07030.pdf). on WGANs is an instance of such an application. The generator does not learn from individual samples of the distribution directly but instead tries to match the moments of the target distribution. The setting is more general than traditional moment matching since the observed samples may be transformed by non-linear activations. HCC games capture this class of GAN applications directly.   \n\nFinally, $\\textit{AnonReviewer2}$ also inquired about promising approaches to finding Nash Equilibria or Von-Neumann solutions with discrete dynamics. Focusing on safe initializations, $\\textbf{Theorem 5}$ & $\\textbf{Corollary 1}$ in combination with the seminal work of Benaim 1990 ([Dynamics of Stochastic Approximation Algorithms]( http://www.numdam.org/article/SPS_1999__33__1_0.pdf )) can give us a promising path for a discrete-time analysis. Leveraging Benaim\u2019s framework introduces extra technical hurdles and concepts (e.g. pseudotrajectories), which lie beyond the scope of this work. Nevertheless, we believe that our continuous-time results will serve as a fundamental building block of any argument in that direction. \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "QdWcodpQ1iP", "original": null, "number": 14, "cdate": 1605987374454, "ddate": null, "tcdate": 1605987374454, "tmdate": 1605987461004, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "Avtg_wz0dtB", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Response about the novelty concerns", "comment": "It is unclear what the reviewer means when he says that the requirement of invertibility can be read off from  [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)]. The reason why  [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)] cannot transfer the recurrence results from the output space to the input space is because the Poincare recurrence theorem guarantees recurrence in the output space for almost all initializations but not all of them. If the Poincare recurrence theorem had a guarantee for all initializations then the transfer would be immediate. The distinction between almost all and all initializations for the Poincare recurrence theorem is a very complex subject that is not yet formally understood in the literature. We thus believe that conclusions about when invertibility is required or not cannot just be \u201cread off\u201d from a paper or be resolved by intuitive arguments directly.\n\nThe situation above directly mirrors our discussion about contribution in $\\textbf{Theorem 5}$, where $\\textbf{Lemma 3}$ only provides convergence guarantees for some initializations of the $\\textbf{Eq. (3)}$ dynamical systems and not all of them. We avoid going for black-box style reductions through global invertibility by providing a novel analysis for the structure of the sublevel sets of $H$. Our work broadens the toolbox of parameter space - output space transfer results significantly.\n\nRegarding $\\textbf{Theorem 14}$, the sigmoid activations are used as a concrete example to match the experiment we did in $\\textbf{Figure 3}$. Essentially, what is required by $\\textbf{Theorem 14}$ is that the $f_i$ and $g_j$ of the generator and discriminator to map in (0,1) and the initialization of the dynamical system to be safe for the equilibrium of the hidden game. Sigmoid activations are chosen as examples because they satisfy the restriction for all potential initializations and target distributions.  \n\nTo sum up, our aim in this work is to make the first steps towards constructing a theoretically tractable framework that captures many of the complexities of GAN training. We firmly believe that leveraging the special structure of GAN training objectives is a necessary step to avoid reaching blanket negative conclusions that do not agree with the empirical success of GANs.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "Avtg_wz0dtB", "original": null, "number": 12, "cdate": 1605871397797, "ddate": null, "tcdate": 1605871397797, "tmdate": 1605871397797, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "xx6nfbpv6Rs", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Some remarks", "comment": "I thank the authors for the thorough reply.\n\n-  \"In [VGFP19], the main claim of Poincar\u00e9 recurrence in hidden bi-linear games can be transferred to the dynamical system of  only when (1) each $f_i$ and $g_i$ are globally invertible.... (our results) do not have this requirement.\"\n\nThis is again something that can be readily read off from [VGFP19]: to study recurrence you naturally need global invertibility, whereas in the strictly convex-concave case you only want the solutions in the output space to be \"covered\" by the hidden functions (F and G). For instance, you can have $F(\\theta)=F(\\theta') = $ solution, thus invertibility is not a concern.\n\nI understand this is a difference between this work and [VGFP19], and I also understand the contributions of Theorem 4, 5, 7. My contention is that this work does not bring much new beyond [VGFP19], since the \"safe initialization\" and the H function ideas are already present in [VGFP19]. Generalizing them from bilinear to strictly convex-concave seems straightforward and does not bring much novelty.\n\n\n- On the other hand, we were very fastidious by providing proofs tuned for the experiment of Figure 3...\n\nIndeed, I missed Theorem 14; my apology. \n\nTheorem 14 applies only to sigmoid activation which is quite restrictive and uncommon in practice. Hence, the practical value is still limited.\n\n- Regarding our WGAN experiment, we feel that the reviewer has misinterpreted the intent of the discussion.\n\nI think my original comment was misleading: I meant to say that I cannot see the practical value of this work based on the provided experiments, including the WGAN example (which is more of a theoretical nature)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "3tluY7uc4PJ", "original": null, "number": 11, "cdate": 1605861504356, "ddate": null, "tcdate": 1605861504356, "tmdate": 1605861504356, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "R1-vTid89oW", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Some quick comments about the response", "comment": "Thank you for the response and the clarifications. I was indeed confused about some points such as the discretization of the continuous time dynamics and the relation between Von Neumann solutions and Nash Equilibria. However, I'm afraid the same may happen to non-specialists reading the paper so I would appreciate a bit more pedagogy in the first parts of the paper.\n\nI agree that we have to lay out some assumptions to work, I was mainly wondering if this \"safe initialization\" could be related to other \"assumptions\" of the literature in some special cases (either from the study of GDA in continuous or discrete-time). Typically, the paragraph \"It is important to highlight\" in your response kind of clarifies it (and could be added in some form in the paper), but I still fail to see if this connects with other assumptions of the literature.\n\nCould you also comment on discretization ? \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "yzvuqHEe8QU", "original": null, "number": 10, "cdate": 1605825672361, "ddate": null, "tcdate": 1605825672361, "tmdate": 1605825672361, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "ykMg_0584Bl", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Does it cover GANs, or not? ", "comment": "*The main concern of review was the disjointness of the parameter vectors in functions \n, and its connection with the Generative Adversarial Networks. As our well-tailored and detailed work shows, even this simple case includes multiple new challenges that should be tackled in order to aim at further generalizations. We believe that our theoretical techniques may extend to more general settings, and although these questions are beyond our current scope, we hope that our work will enable this kind of interesting follow-ups.*\n\nI reiterate my question from above: In what sense is the theory applicable to at least an idealized version of GAN with latent dimension larger than 1? Section 4 creates the impression that GANs fall into the hidden convex-concave setting but I have my doubts about that.\nAre there other interesting classes of games that have a hidden convex-concave structure?"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "fMLXlIbwzX4", "original": null, "number": 9, "cdate": 1605806420999, "ddate": null, "tcdate": 1605806420999, "tmdate": 1605806420999, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "2tDXKPsnByF", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Some remarks", "comment": "Thank you for the reply.\n\nThe first two points that I mentioned were equally brought up by Reviewer 3 and my main concern is indeed on how things are presented in its current form (I acknowledge that I probably overlooked the word \"disjoint\" on page 2 so it may worth emphasizing it a bit more; in the following I will rather focus on the GAN part).\n\nIn my opinion, even the generator is outputting the parameter of a distribution it is still learning a distribution, but this problem is not relevant here. What is more important is that the problems highlighted in the Applications sections (main and appendix) concern only toy GAN models (Gaussian distribution + linear discriminator or probability on a finite set) and this is not so clear from the text. \nThat is why I am suggesting distinguishing clearly between the two cases: the general GAN with a HCC in infinite-dimensional space and some toy GAN models such as the examples mentioned in the paper that actually fit the analysis of the work. Stating this way, it would be clear that the high-level concept indeed applies to very general problems (as briefly discussed by the authors in the last paragraph of section 4, see also https://arxiv.org/abs/2002.05820), and this work is addressing a simplified version of the problem which encompasses several toy GAN models.\n\nTo finish, I agree with Reviewer 1 that different discretization of a same continuous dynamic may lead to different behaviors. Indeed, optimistic methods can be understood as deriving from a different continuous dynamic as also highlighted in https://arxiv.org/abs/1905.10899. Nonetheless, as shown in https://arxiv.org/pdf/2006.09065.pdf, these very different methods can also come from discretization of a single continuous dynamic system. In particular, I am curious to know if the authors can provide any reference for a different continuous dynamic of extragradient. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "dTpjJswBCu", "original": null, "number": 8, "cdate": 1605563278996, "ddate": null, "tcdate": 1605563278996, "tmdate": 1605563278996, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "EvdW-pqg8_", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (1/2) ", "comment": "\nWe would like to thank the reviewer for the insightful comments on how to make our work more approachable and understandable by a wider audience.\n\nThe reviewer feels that we implied that alternative optimization algorithms like extragradient dynamics are non-standard or secondary. This was not our intention when we mentioned that GDA is a standard optimization algorithm for min-max optimization. We were merely referring to the fact that GDA is a foundational algorithm from which other gradient-based dynamics can be derived. Thus studying GDA for HCC is an important stepping stone towards non-local arguments about other dynamics in non-convex non-concave games.\n\nWe also feel that the reviewer has made some inaccurate statements about the connections of discrete-time EG and GDA to the continuous-time GDA. It is not the case that standard and \u201coptimistic\u201d/extra-gradient techniques correspond to the same continuous-time dynamical system. \nFor example, [Cheung et al.](https://papers.nips.cc/paper/2020/file/66de6afdfb5fb3c21d0e3b5c3226bf00-Paper.pdf) recently studied both Multiplicative Weights Update (MWU) and Optimistic (MWU), which are known to be divergent and convergent respectively in bilinear zero-sum games via their connections to their continuous-time analogues and showed that the positive convergent behavior OMWU can be understood by the properties of its continuous-time method, which is a new ODE and not the well-known continuous-form of MWU (replicator dynamics). Thus, it should be clear that continuous-time dynamics bear very valuable information about the behavior of algorithms in saddle point problems.\n\nThe reviewer is also confused about the relation of von Neumann solutions and Nash equilibria in the context of HCC games. For the convex-concave $L$, von Neumann solutions correspond to inputs $p$ and $q$ such that $L(p,q) = \\min\\max L(x, y)$. Notice that since this corresponds to a zero-sum competition, they coincide with the Nash equilibria of $L$. Our solution concept for HCC games are $(\\theta^*, \\phi^*)$ such that $F(\\theta^*)=p$ and $G(\\phi^*)=q$ for some von Neumann solution $(p,q)$ of $L$. The chosen solution concept is not arbitrary since clearly           \n$(\\theta^*, \\phi^*)$ is a global Nash equilibrium of the HCC. We will try to make this more clear in our discussion.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "R1-vTid89oW", "original": null, "number": 7, "cdate": 1605562906867, "ddate": null, "tcdate": 1605562906867, "tmdate": 1605562906867, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "EvdW-pqg8_", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (2/2) ", "comment": "\nThe reviewer also has some questions about how one can check if an initialization of an HCC game is safe based on $\\textbf{Definition 3}$, which is a sufficient condition for convergence to a global Nash equilibrium of the game. Even in non-convex optimization, a significantly simpler problem compared to non-convex non-concave min-max optimization, understanding which initializations lead to a global minimum of the loss function under gradient descent is a very hard problem. Despite the special structure of the HCC game, understanding which initial conditions are safe still remains at least just as hard. For example, if $f_i$ is a deep neural net and the $p_i$ of the target von Neumann solution happens to be a value close to the global minimum of $f_i$, then we cannot efficiently decide a-priori (e.g. without running GDA) if an initialization is safe. \n\nHaving some form of assumptions on the initialization of the game is completely necessary to avoid negative blanket conclusions for most established classes of non-convex non-concave games. Instead of reiterating negative results based on unfortunate initializations that are not in agreement with the empirical success of GANs, we choose to study the dynamics of HCC games under safety. \n\nIt is important to highlight though that regions of attractions that are possible under safety are not similar to standard results of local convergence and this is why we describe the convergence type as \u201cnon-local\u201d.  Indeed, in a local convergence result the algorithm needs to be initialized in a small basin of the equilibrium to converge to it. This limitation does not apply to our results where depending on the choice of $F$ and $G$, the points that converge to an equilibrium can be arbitrarily far away from it. For example, observe that for the function of $\\textbf{Figure 1}$, at least with respect to $f_i$, all negative initializations except the local minima and maxima of $f_i$ are safe. Clearly, this is a non-local convergence result.\n\nThe reviewer is also unsure about the differences of our work with setups studied in prior work. The class of HCC games is a generalization of both convex-concave games and hidden bi-linear games. Convex-concave games correspond to $F$ and $G$ being the identity operators whereas hidden bi-linear games correspond to $L$ being a bi-linear map. We refer the reviewer to our response to $\\textit{AnonReviewer4}$ for a comprehensive analysis of the technical challenges of HCC as compared to the work of [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)] on hidden bi-linear games as well as compared to the convex-concave games.\n\nWe understand the reviewer\u2019s concern that a significant portion of technical details remains in the Appendix. The combination of the dynamics of $\\textbf{Eq. (1)}$, the reparametrization theorem as well as the potential function are necessary tools to even establish the expected behavior of $\\textbf{Eq. (3)}$. But transferring the results to the dynamics of $\\textbf{Eq. (1)}$ still requires some additional tools that are found in the corresponding proofs in the Appendix. \n\nTo guide the reader we have tried to provide summaries of what is being proven in each theorem in the Appendix as well as the basic idea of the arguments used. Unfortunately moving this discussion to the main paper is not straightforward as one can observe based on the extensive background material contained in $\\textbf{Section 7}$. In response to the reviewer\u2019s comment, we will do our best to include additional intuition about the technical tools required for each proof\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "xx6nfbpv6Rs", "original": null, "number": 5, "cdate": 1605561926139, "ddate": null, "tcdate": 1605561926139, "tmdate": 1605561926139, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "cFWDSonGCcX", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 (1/2) ", "comment": "$\\textit{AnonReviewer4}$ has concerns about the novelty of our work. Indeed, the main predecessor of [our work](https://openreview.net/pdf?id=e3KNSdWFOfT) is [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)], which first introduced the framework of hidden games. While we use tools regarding reparametrization and safety from their work, the rest of our analysis and the technical ideas behind them are qualitatively different.\n\n\nAs we highlighted in the introduction, the challenges of HCC games are twofold. First, the game dynamics in the operator space of $F$ and $G$ do not correspond to a simple convex concave game. Additionally, it is still not clear how to transfer stability/convergence properties from the operator space of $F$ and $G$ to the parameter space of $\\theta,\\phi$. More concretely, it is unclear how to transfer results from the dynamical system of $\\textbf{Eq. (3)}$ to the one of $\\textbf{Eq. (1)}$.\n\nAny similarities with [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)], that the reviewer highlighted have to do with the first challenge. In contrast, in terms of the second challenge, there is a substantial gap between our work and the results of [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)]. In [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)], the main claim of Poincar\u00e9 recurrence in hidden bi-linear games can be transferred to the dynamical system of $\\textbf{Eq. (1)}$ only when each $f_i$ and $g_j$ are globally invertible. To understand how limiting this requirement is, all $n_i$ and $m_j$ need to be one for this to hold. In contrast, observe that all of our results, except $\\textbf{Theorem 3}$ which is shown purely for exposition purposes, do not have this requirement. \n\nTo the best of our knowledge, there has not been any related work studying the transfer of stability results between dynamical systems in different spaces that do not rely on the existence of one to one maps between the spaces. \n\nLet us show through a concrete example of why the transfer of stability statements given a Lyapunov function in the output space is non-trivial. $\\textbf{Lemma 2}$ provides an example of a non-increasing function in the operator space that is not sufficient to prove any stability property for convex-concave games in the parameter space. Indeed, observe that $\\theta_i$ can move on a level set of $f_i$ arbitrarily away from an equilibrium without ever-increasing the value of $H$. \n\n\nAlthough $\\textbf{Theorem 4}$ does not prove stability in the parameter space, it still provides guarantees about the stability of the outputs $F$ and $G$. Once again, observe that $\\textbf{Theorem 4}$ and $\\textbf{Theorem 2}$ provide very different guarantees since $\\textbf{Theorem 4}$ talks about sets of initializations of $\\textbf{Eq. (1)}$ that map to a potentially infinite amount of different systems of the form of $\\textbf{Eq. (3)}$. $\\textbf{Theorem 4}$ provides novel sufficient conditions for constructing neighborhoods of safe initializations that did not appear in [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)]. It also leverages a potentially infinite amount of Lyapunov functions of many coupled dynamical systems in the $F$ and $G$ space to provide its hidden stability guarantees. To the best of our knowledge, this result is the first of its kind.  \n\nSimilar challenges need to be addressed when going from $\\textbf{Lemma 3}$ to $\\textbf{Theorem 5}$. Each initialization of $\\textbf{Eq. (1)}$ corresponds to a single initialization of the dynamical system of $\\textbf{Eq. (3)}$. It is clear once again that  $\\textbf{Lemma 3}$ is not by itself sufficient to guarantee convergence of any initial condition of $\\textbf{Eq. (1)}$. Our novel analysis of the region of attraction based on the study of the sublevel sets of $H$ is thus an integral part of the proof of $\\textbf{Theorem 5}$. Analyzing the boundedness of sublevel sets of $H$ is challenging since $X_{\\theta_i(0)}$ and $X_{\\phi_j(0)}$ are based on existential results. \n\nFinally, for $\\textbf{Theorem 7}$, it is clear that the standard strong Lyapunov function argument of strongly convex-concave games does not directly apply here. To sidestep this we provide novel initialization dependent upper and lower bounds of $H$ that did not appear in [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)],.\n\nTo the best of our knowledge, our work is the first one to provide sufficient conditions for non-local convergence to a game theoretically meaningful solution for a wide family of non-convex non-concave min-max problems exploiting these tools in such a way. We will happily incorporate part of this discussion to illustrate the differences between our work and the initial work of [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)],.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "ykMg_0584Bl", "original": null, "number": 3, "cdate": 1605560479169, "ddate": null, "tcdate": 1605560479169, "tmdate": 1605561041813, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "MQEFB2F0BHU", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 (1/1)", "comment": "We thank the reviewer for pointing us to the interesting prior work of (Letcher 2020) and (Hsieh et al. 2020). We commit to adding a discussion in our literature review and the proposed citations.\n \n$\\textit{AnonReviewer3}$ also wanted to know the connection between the pseudo-monotonicity framework of Mertiikopoulos et al. based on a variational inequality (VI) encoding of solutions to saddle point problems and the hidden convexity structural condition in our paper. The VI approach does not suffice to encode hidden convex-concave games.\n\nE.g. let\u2019s consider a simple HCC game $\\displaystyle\\min_{x_1, x_2} \\max_y $ $x_1x_2 y$ . \n\nIn this case, it is easy to see that its saddle points are exactly the points such that at least two of $x_1,x_2, y$ are equal to zero. However, it is easy to see that e.g. the all-zero solution does not satisfy the corresponding VI which would be $x_1 x_2 y \\ge 0$ for all $x_1, x_2, y$. Moreover, that framework and the analysis in that paper do not allow for equilibrium selection arguments, which is a key contribution of our work showing that a specific subset of all saddle points, i.e., the von Neumann solutions have desirable stability properties.\n\nThe main concern of review was the disjointness of the parameter vectors in functions $f_i,g_j$, and its connection with the Generative Adversarial Networks. As our well-tailored and detailed work shows, even this simple case includes multiple new challenges that should be tackled in order to aim at further generalizations. We believe that our theoretical techniques may extend to more general settings, and although these questions are beyond our current scope, we hope that our work will enable this kind of interesting follow-ups.\n\nFinally, $\\textit{AnonReviewer3}$ asked us how other optimistic methods in min-max optimization perform in the case of the structured hidden convex-concave games. We would like to refer the reviewer to $\\textbf{Section 8.2.2}$ where we actually show how our proof technique can prove that a variation of Hamiltonian Gradient Descent achieves convergence to the von-Neumann solution problem. This already shows that the fundamental tools presented in this work are potentially applicable to other optimization dynamics as well. We believe that this is an interesting direction for future work.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "Mlo1ksHi1f9", "original": null, "number": 4, "cdate": 1605561023634, "ddate": null, "tcdate": 1605561023634, "tmdate": 1605561023634, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "cFWDSonGCcX", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Response to AnonReviewer4 (2/2) ", "comment": "About the constrained optimization, we would like to clarify that we never claimed that our theorems, as stated, cover the general case of constrained min-max optimization. On the other hand, we were very fastidious by providing proofs tuned for the experiment of $\\textbf{Figure 3}$ which includes a sum-to-1 constraint. You can find the proof in $\\textbf{Theorem 14}$. More broadly, an HCC game with smooth convex equality constraints in the output space can be turned to an unconstrained HCC game through the use of Lagrange multipliers.\n\nRegarding our WGAN experiment, we feel that the reviewer has misinterpreted the intent of the discussion. We are highlighting how our results on HCC games can readily provide direct insights on the results of prior work on GANs. \n[Lei et al.](https://arxiv.org/pdf/1910.07030.pdf) proposed to add a quadratic regularizer to the WGAN objective, arguing that this was merely a replacement for the Lipschitz constraints of the standard GAN formulations. Even for this toy 1D setting, that is chosen for visualization purposes, our results in conjunction with the ones of [[VGFP19](https://arxiv.org/pdf/1910.13010.pdf)], directly predict that the quadratic regularizer is far from irrelevant and is actually necessary to ensure convergence. Observe that our results directly apply to any differentiable activation function. The quadratic activation was chosen because we can compute the expectations and $H$ analytically to produce the left plot of $\\textbf{Figure 4}$. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "4VF3laynZ_A", "original": null, "number": 2, "cdate": 1605560005921, "ddate": null, "tcdate": 1605560005921, "tmdate": 1605560005921, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment", "content": {"title": "Thank you for your reviews", "comment": "We wholeheartedly thank the reviewers for the positive and encouraging feedback, as well as for the insightful comments to improve our submitted work. We continue by addressing the comments of each reviewer separately.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "e3KNSdWFOfT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2595/Authors|ICLR.cc/2021/Conference/Paper2595/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923846518, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Comment"}}}, {"id": "EvdW-pqg8_", "original": null, "number": 1, "cdate": 1603197275919, "ddate": null, "tcdate": 1603197275919, "tmdate": 1605024173459, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Review", "content": {"title": "Review", "review": "In this paper, the authors introduce a class of games called Hidden Convex-Concave where a (stricly) convex-concave potential is composed with smooth maps. On this class of problems, they show that the continuous gradient dynamics converge to (a neighbordhood of) the minimax solutions of the problem. This is an exploratory theoretical paper which aims at better capturing the behaviors that can be observed e.g. in the training of GANs.\n\n\nWhile the paper has merits, it fails in my opinion to clearly explain its theoretical grounds and findings to a non-specialist of continuous dynamics (like me). I thus cannot recommend acceptance for this version based on my comprehension.\n\n\n* The authors consider the gradient descent/ascent as a standard optimization dynamics. However, in min-max optimization (and even in practical GAN training see e.g. \"Reducing Noise in GAN Training with Variance Reduced Extragradient\" by Tatjana Chavdarova, Gauthier Gidel, Fran\u00e7ois Fleuret, Simon Lacoste-Julien), extragradient techniques are actually more often used that gradient descent-ascent due to they better theoretical properties (see the first paragraph of the introduction). Notably, EG can converge to a saddle point in bilinear games where gradient fails (see Chavdarova et al. above or Hsieh et al. 2020) even though they globally come from the same continuous-time dynamics. Thus, it is natural to wonder why/if the continuous-time gradient dynamics bears valuable information about the saddle-point problem and associated behaviors. I would like to see the authors address this in the introduction.\n\n* The paragraph \"Our class of games\" and \"Our solution concept\" quite verbose and hard to understand (maybe adding in equations the definitions of Nash Eq. and Von Neumann solutions would help). Since a goal of the paper is to provide intuition on the dynamics of certain games with respect to certain solution, these should be made very clear for the reader, this is not the case at the moment.\n\n* I am concerned with the definition of \"safe initialization\" (Def 3) and notably how it can be checked (which is unfortunately not discussed). Typically, most results assume a safe initialization which looks like a local basin of attraction. This kind of contradicts the statement that the results are \"non-local\" in the abstract.\n\n* Concerning the difference between the considered setup and previous ones: i) Apart from the \"reparametrization\" of Section 2.3, what changes between this setup and the convex-concave saddle point gradient dynamics?  ii) and Compared to hidden bilinear games?\n\n* The authors claim in the conclusion that the \"modular structure\" of their proofs make HCC games suitable \"theoretical testbeds\". However, it is a bit hard while reading the paper to pinpoint the mathematical tools that can actually be used for further studies. While looking at the 23 pages appendix, I feel that Eq. (1) -- the dynamics; Theorem 1 -- the reparametrization; and Lemma 2 -- derivation of the non-increasing potential, are the most important results (or rather their instantiation for GDA dynamics). In th present version, they are a bit lost in the very long appendix. Maybe dropping down the general case and the regularization parts would enable to better present these points in the paper.\n\n\nMinor Comments:\n* In the first paragraph of the introduction, the authors cite 16 references at once. I am not sure that this is actually informative. Either the authors could break down these refs into smaller chunks/units that relate to one particular kind of results (as id done in the second paragraph) or drastically reduce the number of papers cited there.\n* The term \"hidden convexity\" may be a bit confusing since it may refer to e.g. local strong convexity around the solution. In the literature, the term convex composite problem is sometimes used to denote minimization problems of the form \\min_x h(c(x)) with h a convex function and c a smooth mappings (see e.g. \"Efficiency of minimizing compositions of convex functions and smooth maps\" by Drusvyatskiy and Paquette), this may be mentioned as well.\n* In Lemma 2 and Theorem 2, \"is a safe for\" probably misses the word \"initialization\".\n* In Theorem 2, the sense of \"stable\" here should be defined.\n* Lemma 3: \"is the non-empy\" should be \"be the non-empty\".\n* bottom of p7: \"regularization\" is spelled wrong, \"it can posed\" -> \"be posed\"\n* In Figure 3, how is the continuous dynamics discretized?", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538092769, "tmdate": 1606915805939, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2595/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Review"}}}, {"id": "uDmFioIsY_G", "original": null, "number": 2, "cdate": 1603728087374, "ddate": null, "tcdate": 1603728087374, "tmdate": 1605024173395, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Review", "content": {"title": "A paper with potential impact despite several important limitations", "review": "### Summary\n\nWith the aim of improving our understanding of GAN training, the paper studies the behavior of gradient descent ascent (GDA) dynamics in a subclass of the so-called _hidden convex-concave games_. In this family of problems, the players control the input variables of smooth functions whose outputs are taken as inputs of a convex-concave game. \nFor the problems covered by this paper, different types of stability around nash equilibria of the hidden game are proved under different assumptions. In particular, the concept of _safe_ initializations is introduced, which clearly distinguishes between the case where the trajectory can and cannot converge to a hidden game solution. As an example, the authors prove that when the hidden game is strictly convex-concave, with safe initialization GDA dynamics converge to a point that solves the hidden minimax game.\n\n### Pros\n\nThis paper can have an impact on the analysis of GAN optimization by examining a different line of attack on the problem. \nThe analysis and the presentation are clean and the appendix is extremely well-organized. \nThe importance of the initialization is further highlighted by the concept of safety, reflecting the additional complexity caused by the nonlinear transformation. \nOverall, this paper presents some beautiful results for the problem under study, and provide concise explanations on how these results are obtained.\n\n### Cons / Limitations of the paper\n\nThere are however several important limitations of the work which make it difficult to evaluate the significance of the results.\n\n#### Slight overclaim on the problems covered by the paper\nUp to page 2, one may believe that the paper addresses the general hidden convex-concave game as defined by (HCC). Then, on page 3, it turns out actually the paper only focuses on a special type of HCC for which every coordinate of the function output is independently determined by its input variables. This seems to be rather restricted and it is unclear how the results of the paper can be applied to general HCC. Moreover, this limitation is mentioned nowhere in the abstract or the introduction of the paper.\n\n#### Ambiguities in how GANs fit the framework\nWhile the authors claim that GAN training is a specific case of HCC, it is not straightforward from the text that this in fact indicates either of the following\n1. The probability distribution has finite discrete support.\n2. The discriminator outputs a function and the generator outputs a measure.\n\nThe paper (and Section 9 in particular) centers on case 1 which is quite far from real GAN problems, and this message is not so clear in the paper (with only the keyword \"discrete\" mentioned discreetly). On the other hand, for the second case, we need to study minimax games in a Banach space which is not covered by the paper.\n\n#### The potential of the approach is unclear\n\nFinally, in addition to the above two limitations which weaken the link between the problems studied by the paper and real GAN training, the discretization from continuous ODE to practical algorithms is also non-trivial. The study of the dynamics itself is surely of interest. Nonetheless, taking all these into account, I am just wondering if it would really be possible/suitable to study GAN optimization through the lens of hidden convex-concave games.\n\n### Score justification\n\nI appreciate the efforts that the authors have made to come out with all the results and to present them in such a succinct manner. I however do not put a higher score due to the concerns that I raised above.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538092769, "tmdate": 1606915805939, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2595/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Review"}}}, {"id": "cFWDSonGCcX", "original": null, "number": 3, "cdate": 1603884453701, "ddate": null, "tcdate": 1603884453701, "tmdate": 1605024173260, "tddate": null, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "invitation": "ICLR.cc/2021/Conference/Paper2595/-/Official_Review", "content": {"title": "This work is a strictly convex-concave extension of (Vlatakis-Gkaragkounis et al. 2019).", "review": "This paper studies min-max problems of the form L(F(x),G(y)), where L is a strictly convex-concave loss. The authors prove that, under certain assumptions, the GDA dynamics converges to von Neumann solutions. Toy examples are provided to illustrate the usage of the framework.\n\nI see this paper as a rather incremental extension of the work (Vlatakis-Gkaragkounis et al. 2019). Specifically, Lemma 1 appeared in (Lemma 1, Vlatakis-Gkaragkounis et al. 2019), Theorem 1 appeared in (Lemma 2, Vlatakis-Gkaragkounis et al. 2019), Definition 3 makes a strong assumption that basically gets rid of the challenges of non-convexity, and Lemma 2 appeared in (Theorem 2, Vlatakis-Gkaragkounis et al. 2019).\n\nThe key difference between this work and (Vlatakis-Gkaragkounis et al. 2019) is the strict convex-concave losses (present work) vs. bilinear losses (Vlatakis-Gkaragkounis et al. 2019). Due to this difference, the function (4) in Lemma 2 is time-invariant (or acts as a Hamiltonian) in (Vlatakis-Gkaragkounis et al. 2019), whereas in this submission it is non-increasing in the presence of strict convexity, and hence enabling Lyapunov-type analysis. This is an almost immediate conclusion one can draw from studying (Vlatakis-Gkaragkounis et al. 2019), and hence the contribution does not seem to meet the standard of top ML venues.\n\nIn addition, the authors studied unconstrained min-max problems with hidden structures, while in Section 4 they claimed that GANs can be viewed as **constrained** min-max problems. I would like to point out that the presence of constraints is a significant difference as it would completely invalidate all the analysis in this paper. The toy example of WGAN is also not convincing enough as it is tied to one dimensionality with a (practically irrelevant) quadratic regularizer.\n\nIn conclusion, I did not see much novelty of this submission in terms of theory, and there is no practical implication. I hence suggest rejection for the Area Chair.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2595/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2595/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent", "authorids": ["~Emmanouil-Vasileios_Vlatakis-Gkaragkounis1", "~Lampros_Flokas1", "~Georgios_Piliouras1"], "authors": ["Emmanouil-Vasileios Vlatakis-Gkaragkounis", "Lampros Flokas", "Georgios Piliouras"], "keywords": ["Min-max optimization", "Lyapunov functions", "Stability Analysis", "Generative Adversarial Networks", "Non-convex optimization"], "abstract": "Many recent AI architectures are inspired\u00a0by zero-sum games, however, the behavior of their dynamics is still not well understood. Inspired by this, we study standard gradient descent ascent (GDA) dynamics in a specific class of non-convex non-concave zero-sum games, that we call hidden zero-sum games. In this class, players control the inputs of smooth but possibly non-linear functions whose outputs are being applied as inputs to a convex-concave game. Unlike general min-max games, these games have a well-defined notion of solution; outcomes that implement the von-Neumann equilibrium of the ``hidden convex-concave game. We prove that if the hidden game is strictly convex-concave then vanilla GDA converges not merely to local Nash, but typically to the von-Neumann solution. If the game lacks strict convexity properties, GDA may fail to converge to any equilibrium, however, by applying standard regularization techniques we can prove convergence to a von-Neumann solution of a slightly perturbed min-max game.\u00a0Our convergence\u00a0guarantees are non-local, which as far as we know is a first-of-its-kind type of result in non-convex non-concave games. Finally, we discuss connections of our framework with generative adversarial networks. \n", "one-sentence_summary": "We prove non-local asymptotic convergence guarantees in a class of non-convex non-concave zero-sum.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "vlatakisgkaragkounis|solving_minmax_optimization_with_hidden_structure_via_gradient_descent_ascent", "pdf": "/pdf/809a8d03b53bf40e39ed01ad02b80a7684777567.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=u6DuDmVD1c", "_bibtex": "@misc{\nvlatakis-gkaragkounis2021solving,\ntitle={Solving Min-Max Optimization with Hidden Structure via Gradient Descent Ascent},\nauthor={Emmanouil-Vasileios Vlatakis-Gkaragkounis and Lampros Flokas and Georgios Piliouras},\nyear={2021},\nurl={https://openreview.net/forum?id=e3KNSdWFOfT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "e3KNSdWFOfT", "replyto": "e3KNSdWFOfT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2595/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538092769, "tmdate": 1606915805939, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2595/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2595/-/Official_Review"}}}], "count": 27}