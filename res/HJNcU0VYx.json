{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028611632, "tcdate": 1490028611632, "number": 1, "id": "ryhrdY6ix", "invitation": "ICLR.cc/2017/workshop/-/paper120/acceptance", "forum": "HJNcU0VYx", "replyto": "HJNcU0VYx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multimodal Noise and Covering Initializations for GANs", "abstract": "This note describes two simple techniques to stabilize the training of Generative Adversarial Networks (GANs) on multimodal data. First, we propose a covering initialization for the generator. This initialization pre-trains the generator to match the empirical mean and covariance of its samples with those of the real training data. Second, we propose using multimodal input noise distributions. Our experiments reveal that the joint use of these two simple techniques stabilizes GAN training, and produces generators with a richer diversity of samples. Our code is available at http://pastebin.com/GmHxL0e8.", "pdf": "/pdf/e91e57594c3d5343a46325c7c964a382b818e62e.pdf", "TL;DR": "Using multimodal noise distributions and initializing the generator to cover all the data distribution stabilizes GAN training.", "paperhash": "lopezpaz|multimodal_noise_and_covering_initializations_for_gans", "conflicts": ["fb.com", "inria.fr"], "keywords": ["Deep learning", "Unsupervised Learning", "Optimization"], "authors": ["David Lopez-Paz", "Maxime Oquab"], "authorids": ["dlp@fb.com", "maxime.oquab@inria.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028612170, "id": "ICLR.cc/2017/workshop/-/paper120/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HJNcU0VYx", "replyto": "HJNcU0VYx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028612170}}}, {"tddate": null, "tmdate": 1489361108237, "tcdate": 1489361108237, "number": 2, "id": "r1hROL7jl", "invitation": "ICLR.cc/2017/workshop/-/paper120/official/review", "forum": "HJNcU0VYx", "replyto": "HJNcU0VYx", "signatures": ["ICLR.cc/2017/workshop/paper120/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper120/AnonReviewer2"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "Summary: This paper proposed two techniques to help train GANs, in particular, by improving their stability. Multimodal noise essentially replaces the typically Standard Normal distributions with a multimodal distribution. Although theoretically unimodal noise can allow modeling any data distribution, but it's believed that it would be easier to model real-world data using multimodal noise distributions since real-world data also resembles islands separated by low-probability regions. Covering initialization pretrains the generator such that the mean and covariance of the model distribution and data distribution matches. This avoids mode collapse by avoiding initializing the generator as a many-to-one function (which is harmful). The experiments are shown on a low-dimensional toy dataset.\n\nNovelty: The ideas described have not been described in prior work, AFAIK. But, the ideas have been well-known by practitioners of GANs, but never studied properly, or found to be significantly useful in large-dimensional datasets and tasks.\n\nClarity: The paper is written clearly.\nSignificance: Improving techniques for training GANs is a significant problem.\nQuality: Overall, the lack of experimentation on a high-dimensional dataset (even MNIST) hurts the quality of the paper.\n\nPros: The main reason for accepting the paper would be to have a written description of these two techniques, so that these ideas can be discussed and cited.\nCons: The paper provides little to no evidence on whether these techniques are actually useful in real high-dimensional tasks.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multimodal Noise and Covering Initializations for GANs", "abstract": "This note describes two simple techniques to stabilize the training of Generative Adversarial Networks (GANs) on multimodal data. First, we propose a covering initialization for the generator. This initialization pre-trains the generator to match the empirical mean and covariance of its samples with those of the real training data. Second, we propose using multimodal input noise distributions. Our experiments reveal that the joint use of these two simple techniques stabilizes GAN training, and produces generators with a richer diversity of samples. Our code is available at http://pastebin.com/GmHxL0e8.", "pdf": "/pdf/e91e57594c3d5343a46325c7c964a382b818e62e.pdf", "TL;DR": "Using multimodal noise distributions and initializing the generator to cover all the data distribution stabilizes GAN training.", "paperhash": "lopezpaz|multimodal_noise_and_covering_initializations_for_gans", "conflicts": ["fb.com", "inria.fr"], "keywords": ["Deep learning", "Unsupervised Learning", "Optimization"], "authors": ["David Lopez-Paz", "Maxime Oquab"], "authorids": ["dlp@fb.com", "maxime.oquab@inria.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489361108973, "id": "ICLR.cc/2017/workshop/-/paper120/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper120/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper120/AnonReviewer1", "ICLR.cc/2017/workshop/paper120/AnonReviewer2"], "reply": {"forum": "HJNcU0VYx", "replyto": "HJNcU0VYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper120/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper120/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489361108973}}}, {"tddate": null, "tmdate": 1489355461357, "tcdate": 1489355461357, "number": 1, "id": "SyT6fBXjg", "invitation": "ICLR.cc/2017/workshop/-/paper120/official/review", "forum": "HJNcU0VYx", "replyto": "HJNcU0VYx", "signatures": ["ICLR.cc/2017/workshop/paper120/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper120/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents two tricks to prevent the mode-missing behavior of GANs. The first trick is to use a multimodal noise at the input in order to make it easier for the generator to generate multi-modal data. The second trick is to initialize the generator with a \"covering initialization\" so that the mean and covariance of the generated and real data match. One of the reasons that GAN training fails is that typically the support of the real and generated distributions are disjoint, in which case there will be a perfect discriminator between them which makes the training dynamic unstable. It is argued in the paper that the second trick addresses this problem. It is further shown that both of these tricks are necessary for covering all the modes of a mixture of Gaussian toy dataset with a GAN.\n\nWhile both of these tricks do make sense, I am not convinced that these trick will actually resolve the mode-missing behavior of GANs in high-dimensional data. The GAN training dynamic of a 2D toy dataset is very different from that of a 1000 dimensional dataset, and it is likely that heuristic tricks like these do not generalize to high dimensional datasets. I think the quality of this paper can be substantially improved, if the authors show that these trick will actually help on a more realistic dataset.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multimodal Noise and Covering Initializations for GANs", "abstract": "This note describes two simple techniques to stabilize the training of Generative Adversarial Networks (GANs) on multimodal data. First, we propose a covering initialization for the generator. This initialization pre-trains the generator to match the empirical mean and covariance of its samples with those of the real training data. Second, we propose using multimodal input noise distributions. Our experiments reveal that the joint use of these two simple techniques stabilizes GAN training, and produces generators with a richer diversity of samples. Our code is available at http://pastebin.com/GmHxL0e8.", "pdf": "/pdf/e91e57594c3d5343a46325c7c964a382b818e62e.pdf", "TL;DR": "Using multimodal noise distributions and initializing the generator to cover all the data distribution stabilizes GAN training.", "paperhash": "lopezpaz|multimodal_noise_and_covering_initializations_for_gans", "conflicts": ["fb.com", "inria.fr"], "keywords": ["Deep learning", "Unsupervised Learning", "Optimization"], "authors": ["David Lopez-Paz", "Maxime Oquab"], "authorids": ["dlp@fb.com", "maxime.oquab@inria.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489361108973, "id": "ICLR.cc/2017/workshop/-/paper120/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper120/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper120/AnonReviewer1", "ICLR.cc/2017/workshop/paper120/AnonReviewer2"], "reply": {"forum": "HJNcU0VYx", "replyto": "HJNcU0VYx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper120/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper120/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489361108973}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487509151089, "tcdate": 1487361676522, "number": 120, "id": "HJNcU0VYx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "HJNcU0VYx", "signatures": ["~David_Lopez-Paz2"], "readers": ["everyone"], "content": {"title": "Multimodal Noise and Covering Initializations for GANs", "abstract": "This note describes two simple techniques to stabilize the training of Generative Adversarial Networks (GANs) on multimodal data. First, we propose a covering initialization for the generator. This initialization pre-trains the generator to match the empirical mean and covariance of its samples with those of the real training data. Second, we propose using multimodal input noise distributions. Our experiments reveal that the joint use of these two simple techniques stabilizes GAN training, and produces generators with a richer diversity of samples. Our code is available at http://pastebin.com/GmHxL0e8.", "pdf": "/pdf/e91e57594c3d5343a46325c7c964a382b818e62e.pdf", "TL;DR": "Using multimodal noise distributions and initializing the generator to cover all the data distribution stabilizes GAN training.", "paperhash": "lopezpaz|multimodal_noise_and_covering_initializations_for_gans", "conflicts": ["fb.com", "inria.fr"], "keywords": ["Deep learning", "Unsupervised Learning", "Optimization"], "authors": ["David Lopez-Paz", "Maxime Oquab"], "authorids": ["dlp@fb.com", "maxime.oquab@inria.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 4}