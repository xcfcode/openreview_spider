{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396432913, "tcdate": 1486396432913, "number": 1, "id": "SyFf3z8Og", "invitation": "ICLR.cc/2017/conference/-/paper208/acceptance", "forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper received borderline reviews. All reviewers as well as AC agree that the authors pursue a very interesting and less explored direction. The paper essentially addresses the problem of double grounding; visual information helping to group acoustic signal into words, and words helping to localize object-like regions in images. While somewhat hidden under the rug, this is what makes the paper different from the authors' previous work. The reviewers mentioned this to be a minor contribution. The AC agrees with the authors that this is an interesting and novel problem worth studying. However, the AC also agrees with the reviewers that this major novelty over the previous work is missing technical depth. The AC strongly encourages the authors to improve on this aspect of the paper. A simple intuition would be to look at https://arxiv.org/abs/1609.01704 in order to discover words, and use something along the lines of attention (eg https://arxiv.org/abs/1502.03044) to link with image regions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396433391, "id": "ICLR.cc/2017/conference/-/paper208/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396433391}}}, {"tddate": null, "tmdate": 1484861396505, "tcdate": 1484861396505, "number": 1, "id": "ryn01hR8e", "invitation": "ICLR.cc/2017/conference/-/paper208/official/comment", "forum": "Bkbc-Vqeg", "replyto": "HJ7ppfvLg", "signatures": ["ICLR.cc/2017/conference/paper208/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper208/AnonReviewer1"], "content": {"title": "response to comments to reviewer1:", "comment": "\"We chose not to use an off-the-shelf object detection system for the same reason that we chose not to give the system oracle word boundaries derived from a speech recognizer.\" These are not comparable. An oracle gives ground truth, what an object detection makes a probabilistic prediction. The authors correctly state: \"We wanted the model to learn to localize, identify, and associate spoken words and visual objects\", however, they \"don't want to explicitly train for it\". This is a contradiction. If the goal is to solve the problem at hand, then using all possible tools seems like a good idea. At the very least the authors should should the effect of employing an off-the-shelf object detector in their pipeline."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287684990, "id": "ICLR.cc/2017/conference/-/paper208/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Bkbc-Vqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper208/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper208/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper208/reviewers", "ICLR.cc/2017/conference/paper208/areachairs"], "cdate": 1485287684990}}}, {"tddate": null, "tmdate": 1484365363244, "tcdate": 1484365242981, "number": 2, "id": "HJ7ppfvLg", "invitation": "ICLR.cc/2017/conference/-/paper208/public/comment", "forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "signatures": ["~David_Harwath1"], "readers": ["everyone"], "writers": ["~David_Harwath1"], "content": {"title": "Rebuttal to reviews", "comment": "We'd like to first thank the reviewers for taking the time to read our submission and offer their thoughtful opinions and encouragement. Since the three reviewers raise some similar questions and concerns, I'll try to address them in one post instead of multiple replies.\n\nI'll first address the issue of novelty, which is the main criticism raised by all three reviewers. When thinking about research, I think it's important to consider the problem to be solved separately from the tool(s) used to solve it. While I agree that the deep neural network architecture used in this submission is not a significant departure from the one used in our NIPS 2016 paper, the way we use it is very different. I believe that the problem we address in this submission - that is, joint localization/isolation, clustering, and association of word-like speech patterns and object-like visual patterns - is significantly novel and wasn't studied in our NIPS paper. I don't believe that this submission qualifies as just an analysis paper, because it actually attempts to solve a specific and novel problem, and does so surprisingly well.\n\nThere exists an active sub-field of the speech/linguistics/cogsci communities which studies the problem of acquiring language from untranscribed speech audio alone (see relevant citations in the submission), and most of the techniques used in that problem space rely on segmentation and clustering of the speech signal into linguistically meaningful units (often called unsupervised term discovery or UTD when the desired granularity of the units is at the word or phrase level). The most widely-used and successful techniques for UTD are based on segmental dynamic time warping, which is inherently O(N^2) complexity. I believe that one of the most significant contributions of this submission is the demonstration that the addition of contextually relevant visual information is sufficient to reduce the computational complexity of UTD to O(N), allowing it to scale to much larger datasets.\n\nI'd also like to respond to a few specific points:\n\nFrom reviewer 2:\n\"As correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\"\n\nI respectfully disagree that the move from text to speech audio constitutes an incremental step. The field of automatic speech recognition research has been grappling with the problem of mapping speech audio to symbolic strings for over 65 years, so it's not a trivial problem. By marrying language and vision at the raw signal level as we are here, we're not just creating models that learn the associations between words and images, but actually forcing the models to simultaneously learn how to perform speech recognition in their own, non-symbolic way.\n\nFrom reviewer 1:\n\"Clustering and grouping in section 4, is hacky. Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions. In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.\"\n\nI think that these optimizations, while good suggestions if one's goal is to squeeze every last bit of performance out of the system, are not crucial to the central theme of the paper. We chose not to use an off-the-shelf object detection system for the same reason that we chose not to give the system oracle word boundaries derived from a speech recognizer. We wanted the model to learn to localize, identify, and associate spoken words and visual objects without being explicitly trained to do so. Spectral clustering is an O(N^3) complexity algorithm, and our problem deals with clustering on the order of a million points; a full similarity matrix at floating point precision would require on the order of 4 terabytes of memory, and that's just an O(N^2) subroutine in the spectral clustering algorithm. I realize that various approximate algorithms could possibly be made to work, but in our case I think that the fact that a classic algorithm like k-means works so well is actually a testament to the separability of the embeddings that are being learned by the multimodal CNN."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287685118, "id": "ICLR.cc/2017/conference/-/paper208/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkbc-Vqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper208/reviewers", "ICLR.cc/2017/conference/paper208/areachairs"], "cdate": 1485287685118}}}, {"tddate": null, "tmdate": 1482183108224, "tcdate": 1482183108224, "number": 3, "id": "ByhTZ0rNx", "invitation": "ICLR.cc/2017/conference/-/paper208/official/review", "forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "signatures": ["ICLR.cc/2017/conference/paper208/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper208/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "This paper is a follow-up on the NIPS 2016 paper \"Unsupervised learning of spoken language with visual context\", and does exactly what that paper proposes in its future work section: \"to perform acoustic segmentation and clustering, effectively learning a lexicon of word-like units\" using the embeddings that their system learns. The analysis is very interesting and I really like where the authors are going with this.\n\nMy main concern is novelty. It feels like this work is a rather trivial follow-up on an existing model, which is fine, but then the analysis should be more satisfying: currently, it feels like the authors are just illustrating some of the things that the NIPS model (with some minor improvements) learns. For a more interesting analysis, I would have liked things like a comparison of different segmentation approaches (both in audio and in images), i.e., suppose we have access to the perfect segmentation in both modalities, what happens? It would also be interesting to look at what is learned with the grounded representation, and evaluate e.g. on multi-modal semantics tasks.\n\nApart from that, the paper is well written and I really like this research direction. It is very important to analyze what models learn, and this is a good example of the types of questions one should ask. I am afraid, however, that the model is not novel enough, nor the questions deep enough, to make this paper better than borderline for ICLR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512663820, "id": "ICLR.cc/2017/conference/-/paper208/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper208/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper208/AnonReviewer2", "ICLR.cc/2017/conference/paper208/AnonReviewer1", "ICLR.cc/2017/conference/paper208/AnonReviewer3"], "reply": {"forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper208/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper208/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512663820}}}, {"tddate": null, "tmdate": 1481917547653, "tcdate": 1481917547653, "number": 2, "id": "BJN_Eab4e", "invitation": "ICLR.cc/2017/conference/-/paper208/official/review", "forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "signatures": ["ICLR.cc/2017/conference/paper208/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper208/AnonReviewer1"], "content": {"title": "Learning word-like units from joint audio-visual analysis", "rating": "5: Marginally below acceptance threshold", "review": "This work proposes a joint classification of images and audio captions for the task of word like discovery of acoustic units that correlate to semantically visual objects. The general this is a very interesting direction of research as it allows for a richer representation of data: regularizing visual signal with audio and visa versa. This allows for training of visual models from video, etc. \n\nA major concern is the amount of novelty between this work and the author's previous publication at NIPs 2016. The authors claim a more sophisticated architecture and indeed show an improvement in recall. However, the improvements are marginal, and the added complexity to the architecture is a bit ad hoc. Clustering and grouping in section 4, is hacky. Instead of gridding the image, the authors could actually use an object detector (SSD, Yolo, FasterRCNN, etc.) to estimate accurate object proposals; rather than using k-means, a spectral clustering approach would alleviate the gaussian assumption of the distributions. In assigning visual hypotheses with acoustic segments, some form of bi-partite matching should be used.\n\nOverall, I really like this direction of research, and encourage the authors to continue developing algorithms that can train from such multimodal datasets. However, the work isn't quite novel enough from NIPs 2016.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512663820, "id": "ICLR.cc/2017/conference/-/paper208/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper208/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper208/AnonReviewer2", "ICLR.cc/2017/conference/paper208/AnonReviewer1", "ICLR.cc/2017/conference/paper208/AnonReviewer3"], "reply": {"forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper208/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper208/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512663820}}}, {"tddate": null, "tmdate": 1481874218369, "tcdate": 1481874218369, "number": 1, "id": "HkMNiGbNe", "invitation": "ICLR.cc/2017/conference/-/paper208/official/review", "forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "signatures": ["ICLR.cc/2017/conference/paper208/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper208/AnonReviewer2"], "content": {"title": "Review: Learning Word-Like Units from Joint Audio-Visual Analysis", "rating": "5: Marginally below acceptance threshold", "review": "CONTRIBUTIONS \nThis paper introduces a method for learning semantic \"word-like\" units jointly from audio and visual data. The authors use a multimodal neural network architecture which accepts both image and audio (as spectrograms) inputs. Joint training allows one to embed both image and spoken language captions into a shared representation space. Audio-visual groundings are generated by measuring affinity between image patches and audio clips. This allows the model to relate specific visual regions to specific audio segments. Experiments cover image search (audio to image) and annotation (image to audio) tasks and acoustic word discovery.\n\n\nNOVELTY+SIGNIFICANCE\nAs correctly mentioned in Section 1.2, the computer vision and natural language communities have studied multimodal learning for use in image captioning and retrieval. With regards to multimodal learning, this paper offers incremental advancements since it primarily uses a novel combination of input modalities (audio and images).\n\nHowever, bidirectional image/audio retrieval has already been explored by the authors in prior work (Harwath et al, NIPS 2016). Apart from minor differences in data and CNN architecture, the training procedure in this submission is identical to this prior work. The novelty in this submission is therefore the procedure for using the trained model for associating image regions with audio subsequences.\n\nThe methods employed for this association are relatively straightforward combination of standard techniques with limited novelty. The trained model is used to compute alignment scores between densely sampled image regions and audio subsequences; from these alignment scores a number of heuristics are applied to associate clusters of image regions with clusters of audio subsequences.\n\n\nMISSING CITATION\nThere is a lot of work in this area spanning computer vision, natural language, and speech recognition. One key missing reference:\n\nNgiam, et al. \"Multimodal deep learning.\" ICML 2011\n\n\nPOSITIVE POINTS\n- Using more data and an improved CNN architecture, this paper improves on prior work for bidirectional image/audio retrieval\n- The presented method performs efficient acoustic pattern discovery\n- The audio-visual grounding combined with the image and acoustic cluster analysis is successful at discovering audio-visual cluster pairs\n\nNEGATIVE POINTS\n- Limited novelty, especially compared with Harwath et al, NIPS 2016\n- Although it gives good results, the clustering method has limited novelty and feels heuristic\n- The proposed method includes many hyperparameters (patch size, acoustic duration, VAD threshold, IoU threshold, number of k-means clusters, etc) and there is no discussion of how these were set or the sensitivity of the method to these choices\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512663820, "id": "ICLR.cc/2017/conference/-/paper208/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper208/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper208/AnonReviewer2", "ICLR.cc/2017/conference/paper208/AnonReviewer1", "ICLR.cc/2017/conference/paper208/AnonReviewer3"], "reply": {"forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper208/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper208/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512663820}}}, {"tddate": null, "tmdate": 1480841461965, "tcdate": 1480841393442, "number": 1, "id": "Bktnd8-mg", "invitation": "ICLR.cc/2017/conference/-/paper208/public/comment", "forum": "Bkbc-Vqeg", "replyto": "S1Rrqtk7l", "signatures": ["~David_Harwath1"], "readers": ["everyone"], "writers": ["~David_Harwath1"], "content": {"title": "re:Difference from prior work", "comment": "The pre-print of the NIPS paper was recently made available here:\n\nhttp://papers.nips.cc/paper/6186-unsupervised-learning-of-spoken-language-with-visual-context.pdf\n\nIn the NIPS paper, we trained our models to associate entire audio captions with entire image frames. We did this by compressing the entire audio caption into its own embedding vector, and the image into its own embedding vector. The models were able to do a good job of associating the audio captions with semantically related images (and vice versa), but they weren't yet capable of automatically segmenting the audio caption into words, or picking out individual objects in the image.\n\nThe idea of introducing an automatic segmentation/clustering mechanism for learning individual words and objects was the key area of future work we talked about in the NIPS paper, and that's exactly what we did in this ICLR submission. In that way, this submission picks up where the NIPS paper left off.\n\nThis submission uses the same dataset as our NIPS paper, albeit with twice the amount of training data available (described in section 2), and uses a similar, but deeper, CNN architecture in the initial training phase (described in section 3). Everything from section 4 onward in this submission is completely novel.\n\nIn this submission, we introduce a grounding algorithm for associating sub-regions of an image to semantically related acoustic segments within its associated caption. This algorithm utilizes a multimodal CNN trained in the same way as our models from the NIPS paper, but modified to be applied locally to sub-regions of the image/caption. We demonstrate that in practice, this algorithm tends to pick out individual words or short phrases in the caption which reference specific objects in the images, e.g. in a spoken audio caption containing the words \"this is a lighthouse on a rocky shore during a sunny day\" the algorithm is able to localize the word \"lighthouse\" within the continuous speech signal and associate it with a bounding box around the lighthouse in the image.\n\nIn addition to producing a joint segmentation and alignment of an image/caption pair, the grounding algorithm outputs a local embedding vector for each discovered sub-region. We demonstrate that the different types of discovered acoustic patterns (\"words\") and visual patterns (\"objects\") are easily separated within this embedding space into highly pure clusters using simple clustering algorithms (k-means). We show that the semantic associations between the acoustic and visual clusters are preserved (e.g. the cluster of spoken instances of the word \"grass\" is highly associated with the cluster of image patches containing grass), and finally that the embedding space seems to be capturing semantic similarity between different acoustic pattern clusters (e.g. the \"river\" cluster is semantically similar to the \"lake\" cluster)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287685118, "id": "ICLR.cc/2017/conference/-/paper208/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkbc-Vqeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper208/reviewers", "ICLR.cc/2017/conference/paper208/areachairs"], "cdate": 1485287685118}}}, {"tddate": null, "tmdate": 1480723014322, "tcdate": 1480723014317, "number": 1, "id": "S1Rrqtk7l", "invitation": "ICLR.cc/2017/conference/-/paper208/pre-review/question", "forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "signatures": ["ICLR.cc/2017/conference/paper208/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper208/AnonReviewer2"], "content": {"title": "Difference from prior work", "question": "Can you comment on the specific technical differences between this paper and Harwath et al, NIPS 2016?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959405981, "id": "ICLR.cc/2017/conference/-/paper208/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper208/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper208/AnonReviewer2"], "reply": {"forum": "Bkbc-Vqeg", "replyto": "Bkbc-Vqeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper208/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper208/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959405981}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478288375401, "tcdate": 1478275464726, "number": 208, "id": "Bkbc-Vqeg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Bkbc-Vqeg", "signatures": ["~David_Harwath1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Learning Word-Like Units from Joint Audio-Visual Analylsis", "abstract": "Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions. For example, our model is able to detect spoken instances of the words ``lighthouse'' within an utterance and associate them with image regions containing lighthouses. We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations. Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.", "pdf": "/pdf/21daf351a51f71241db0425689ae98e74f491509.pdf", "paperhash": "harwath|learning_wordlike_units_from_joint_audiovisual_analylsis", "conflicts": ["mit.edu"], "keywords": ["Speech", "Computer vision", "Deep learning", "Multi-modal learning", "Unsupervised Learning", "Semi-Supervised Learning"], "authors": ["David Harwath", "James R. Glass"], "authorids": ["dharwath@mit.edu", "glass@mit.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}