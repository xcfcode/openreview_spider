{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396337575, "tcdate": 1486396337575, "number": 1, "id": "r1chozUul", "invitation": "ICLR.cc/2017/conference/-/paper70/acceptance", "forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396338083, "id": "ICLR.cc/2017/conference/-/paper70/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396338083}}}, {"tddate": null, "tmdate": 1484230669432, "tcdate": 1484230669432, "number": 8, "id": "r1BMxGB8l", "invitation": "ICLR.cc/2017/conference/-/paper70/public/comment", "forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Update", "comment": "Updated PDF with suggested references."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741981, "id": "ICLR.cc/2017/conference/-/paper70/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkKCdAdgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper70/reviewers", "ICLR.cc/2017/conference/paper70/areachairs"], "cdate": 1485287741981}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484230577196, "tcdate": 1478187217575, "number": 70, "id": "rkKCdAdgx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkKCdAdgx", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "content": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": ["rySCp-1Yg"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483952268958, "tcdate": 1483952268958, "number": 7, "id": "SyrcxReUg", "invitation": "ICLR.cc/2017/conference/-/paper70/public/comment", "forum": "rkKCdAdgx", "replyto": "Bk1fdnjHe", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Answer to \"previous work\"", "comment": "Correct. That's the reference pointed out below by us and by Reviewer 3."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741981, "id": "ICLR.cc/2017/conference/-/paper70/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkKCdAdgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper70/reviewers", "ICLR.cc/2017/conference/paper70/areachairs"], "cdate": 1485287741981}}}, {"tddate": null, "tmdate": 1483618311239, "tcdate": 1483618311239, "number": 6, "id": "Bk1fdnjHe", "invitation": "ICLR.cc/2017/conference/-/paper70/public/comment", "forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "signatures": ["~Moustapha_Cisse1"], "readers": ["everyone"], "writers": ["~Moustapha_Cisse1"], "content": {"title": "previous work", "comment": "Bloom Filters have been previously used to reduce training and prediction complexity when facing large output spaces:\nhttps://papers.nips.cc/paper/5083-robust-bloom-filters-for-large-multilabel-classification-tasks.pdf\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741981, "id": "ICLR.cc/2017/conference/-/paper70/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkKCdAdgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper70/reviewers", "ICLR.cc/2017/conference/paper70/areachairs"], "cdate": 1485287741981}}}, {"tddate": null, "tmdate": 1483462354148, "tcdate": 1483462354148, "number": 5, "id": "By9A88YHx", "invitation": "ICLR.cc/2017/conference/-/paper70/public/comment", "forum": "rkKCdAdgx", "replyto": "HkSUrgFBx", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Answer to Reviewer 1", "comment": "Thanks for the (somewhat delayed) review. The main critic seems to be summarized in the phrase: \"It is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here.\" That is a surprising way to criticize the work, since many advances in deep learning come about through the application of a relatively simple method to a network or model. For instance, batch-normalization is the application of z-score normalization to each layer in the network, dropout is the application of sampling to the units of each layer in the network, embedding is adding a dense layer between the network and the input, residual learning is adding the input to the layer directly to the next layer, and much more. In that respect, we are somewhat surprised to see that the simplicity of our approach is seen as its main weakness.\n\nOur approach is simple but yields very good results and we are not aware of any \"off-the-shelf existing embedding\" that provides all the benefits of Bloom embeddings (such as they can be applied to both inputs outputs, minimal computation overhead, increase in performance compared to alternatives or uncompressed networks in many cases, etc.). We would be grateful if the reviewer could point out any such \"off-the-shelf existing embedding\" with similar properties.\n\nRegarding studying more embedding dimensionalities for competing approaches, we do not do so because the majority of such approaches do not scale in terms of time and space. Consider, for instance, the time required to perform an SVD decomposition from a 70K-by-70K sparse matrix (MSD data set) into 30K dimensions and the space to store the resultant 30K-by-30K dense projection matrix. Moreover, considering different input and output projections would result in an unattainable number of experiments (InputDim \\times OutputDim \\times #Datasets (\\times #Approaches)), which is somewhat prohibitive given our GPU resources. Note though that, even without optimizing for the 'right' input/output embedding dimensionality, our method still performs very well.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741981, "id": "ICLR.cc/2017/conference/-/paper70/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkKCdAdgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper70/reviewers", "ICLR.cc/2017/conference/paper70/areachairs"], "cdate": 1485287741981}}}, {"tddate": null, "tmdate": 1483462251677, "tcdate": 1483462251677, "number": 4, "id": "B1VuI8YSl", "invitation": "ICLR.cc/2017/conference/-/paper70/public/comment", "forum": "rkKCdAdgx", "replyto": "HkwLhQrEg", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Answer to Reviewer 3", "comment": "Thank you for your review. In the next iteration of the paper we will include and comment the Shi et al. (JMLR 2009) and Cisse et al. (NIPS 2013) references. In addition, we will add the suggested \"Tensorflow\" and \"integer/binary weights\" references to our related work section. Regarding the \"model compression approach\" reference, we'd like to note that we already cite it in Section 2, together with https://arxiv.org/abs/1511.06530."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741981, "id": "ICLR.cc/2017/conference/-/paper70/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkKCdAdgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper70/reviewers", "ICLR.cc/2017/conference/paper70/areachairs"], "cdate": 1485287741981}}}, {"tddate": null, "tmdate": 1483462193621, "tcdate": 1483462193621, "number": 3, "id": "S1cVUUtSl", "invitation": "ICLR.cc/2017/conference/-/paper70/public/comment", "forum": "rkKCdAdgx", "replyto": "rkYspezNl", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Answer to Reviewer 1", "comment": "Thank you for your review. We'll try to simplify the description of the method in Section 3."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741981, "id": "ICLR.cc/2017/conference/-/paper70/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkKCdAdgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper70/reviewers", "ICLR.cc/2017/conference/paper70/areachairs"], "cdate": 1485287741981}}}, {"tddate": null, "tmdate": 1483437388679, "tcdate": 1483437388679, "number": 3, "id": "HkSUrgFBx", "invitation": "ICLR.cc/2017/conference/-/paper70/official/review", "forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "signatures": ["ICLR.cc/2017/conference/paper70/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper70/AnonReviewer2"], "content": {"title": "Straightforward application of Bloom filter embedding and membership checking, not a lot of novelty; experiments should test more dimensionalities", "rating": "3: Clear rejection", "review": "Description:\n\nThis paper aims at compressing binary inputs and outputs of neural network models with unsupervised \"Bloom embeddings\".\n\nThe embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.\n\nInputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs.\n\nDesired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class.\n\nThe paper simply applies this idea, testing it on seven data sets. Scores and training times are compared to the baseline networks without embeddings. Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach.\n\n\nEvaluation:\n\nIt is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here. The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test.\n\nThe way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step.\n\nIn the comparisons of Table 3, only two embedding dimensionalities are used for each data set. This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method. (In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?)\n\nOverall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable.\n\n\nMinor points:\n\nAs the paper notes, dimensionality reduction of inputs by various techniques is common. The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned.\n\nThe experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension. This seems unnecessary.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483459807725, "id": "ICLR.cc/2017/conference/-/paper70/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper70/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper70/AnonReviewer1", "ICLR.cc/2017/conference/paper70/AnonReviewer3", "ICLR.cc/2017/conference/paper70/AnonReviewer2"], "reply": {"forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483459807725}}}, {"tddate": null, "tmdate": 1482140751021, "tcdate": 1482140751021, "number": 2, "id": "HkwLhQrEg", "invitation": "ICLR.cc/2017/conference/-/paper70/official/review", "forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "signatures": ["ICLR.cc/2017/conference/paper70/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper70/AnonReviewer3"], "content": {"title": "a lot of interesting experiments but limited novelty", "rating": "6: Marginally above acceptance threshold", "review": "The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \"Hash Kernels for Structured Data\" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \"Robust Bloom Filters for Large MultiLabel Classification Tasks\". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.\n\nThe main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization). More recent methods are e.g.,\n\n1) the model compression approach of https://arxiv.org/abs/1510.00149\n2) training with integer/binary weights https://arxiv.org/abs/1511.00363\n\nOverall, the advantage of the Bloom filter approach is its simplicity and its wide applicability -- it could, as well, be used in conjunction with weight quantization and other compression methods. The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483459807725, "id": "ICLR.cc/2017/conference/-/paper70/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper70/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper70/AnonReviewer1", "ICLR.cc/2017/conference/paper70/AnonReviewer3", "ICLR.cc/2017/conference/paper70/AnonReviewer2"], "reply": {"forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483459807725}}}, {"tddate": null, "tmdate": 1481932192919, "tcdate": 1481932192919, "number": 1, "id": "rkYspezNl", "invitation": "ICLR.cc/2017/conference/-/paper70/official/review", "forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "signatures": ["ICLR.cc/2017/conference/paper70/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper70/AnonReviewer1"], "content": {"title": "Straightforward but marginal impact", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.\n\nPros:\n\n- Can be applied to practically any model, either at the input or hte output.\n- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. \n\nCons:\n\n- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.\n- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.\n\nThis seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483459807725, "id": "ICLR.cc/2017/conference/-/paper70/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper70/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper70/AnonReviewer1", "ICLR.cc/2017/conference/paper70/AnonReviewer3", "ICLR.cc/2017/conference/paper70/AnonReviewer2"], "reply": {"forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483459807725}}}, {"tddate": null, "tmdate": 1480970216922, "tcdate": 1480970216918, "number": 2, "id": "BJbgx8m7e", "invitation": "ICLR.cc/2017/conference/-/paper70/public/comment", "forum": "rkKCdAdgx", "replyto": "rJ4xT_1Xx", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Answer to \"Can you discuss the implications for representation learning?\"", "comment": "Thanks for your questions. \n\n1) Yes, the produced vector has k times more ones than the original one. The crucial point here is that the embedding dimension m is smaller than the original d, thus considerably lowering the space requirements of the model.\n\n2) The computational complexity of decoding the output is O(dk), k<<d, typically k={3,4,5} as used in the paper. Notice however that the output of the models after the softmax is not sparse, and one has to iterate over all the outputs, even in the uncompressed case (for instance, to sort or to find the maximum, thus leading to an O(d) process). Moreover, in the case of Bloom Embeddings, such decoding is only needed at prediction time, but not at training time. Finally, the driving motivation of our paper is space efficiency, with affordable timings as a byproduct.\n\n3) Yes. As embeddings can be viewed as an intermediate layer between the encoding of the items and the actual model, our approach yields to embeddings that are distributed across k vectors. Moreover, the total space of the embedding is compressed by a factor of d/m. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741981, "id": "ICLR.cc/2017/conference/-/paper70/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkKCdAdgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper70/reviewers", "ICLR.cc/2017/conference/paper70/areachairs"], "cdate": 1485287741981}}}, {"tddate": null, "tmdate": 1480970102885, "tcdate": 1480970102877, "number": 1, "id": "B1yFyIXml", "invitation": "ICLR.cc/2017/conference/-/paper70/public/comment", "forum": "rkKCdAdgx", "replyto": "B1xZ8sl7g", "signatures": ["~Joan_Serr\u00e01"], "readers": ["everyone"], "writers": ["~Joan_Serr\u00e01"], "content": {"title": "Answer to \"Questions on experiments and baselines\"", "comment": "Thank you for the questions and the interest in the paper. And thank you for pointing out the multilabel classification case. Certainly we missed to include it in the related work section. After some search, we found the work by Cisse et al (NIPS 2013). Was that the work being referred to?\n\n1) Input compression is applied to all data sets without exception. Output compression is applied to all data sets except CADE, which is a 12-class data set. The compression ratios (m/d) reported in the paper are both applied to inputs and outputs (except the CADE case, where it is only applied to the input).\n\n2) The only considered data sets that originally had count data were the CADE and the MSD data sets, which we reduced to a binary indicator of present/not present and like/not like, respectively. As it is reported, we obtain state-of-the-art (or slightly better) results using this trick. In general, many recommendation/collaborative filtering (CF) methods discard count data. Of course, using the actual count is a signal that can improve performance, as it imposes a type of more fine-grained relevance. Count data can be incorporated in CF models in several ways, e.g., using the counts for ranking in the loss function, using them as weights in a weighted least squares model (Wu et al, 2016), or modeling the data with poisson models (Gopalan et al, 2014). In our work, we focused on the most common setting of binary relevance (with no grades), as mostly all the data we used was of binary relevance (or ratings that were binarized), but it is definitely possible to extend the Bloom compression to count data. One option would be to use some kind of counting Bloom filters, which would most likely help in improving the recommendation accuracy in these scenarios.\n\n3) In the experiments we used Keras and Theano. We did not use any sparse layers, and the type of implementation was the same for both compressed and uncompressed networks. We believe that the timing experiments will be quite similar even if sparse layers are used at the input. Basically, in the case of sparse layers, we would have O(c) complexity for the input which, using BE, becomes O(ck), with k typically equal to 3 or 4 and, in many cases, k<c. In that case, the timings would be still dominated by the output layer (O(d) or O(m), d>m>c>=k). Moreover, our main motivation in the paper is fundamentally to reduce the space of the models. Good timings are a byproduct of our approach, but not the driving idea of the paper.\n\n4) We did not. Although we plan to do so in an extension of this work, we expect though to get quite similar results in terms of relative performance (between compressed and uncompressed models). One can expect a 1-3% performance improvement overall by using a ranking loss function (see Hidasi et al, 2016). One could also use a similar subsampling trick on a softmax-type loss as shown in the paper by Ji et al (ICLR 2016). Note though that using an uncompressed model with the subsampling trick, one would still have to store the full output matrix. Interestingly, after the reconstruction of the original space through the proposed Bloom Embedding, we can still use a ranking-based subsampling loss.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287741981, "id": "ICLR.cc/2017/conference/-/paper70/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkKCdAdgx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper70/reviewers", "ICLR.cc/2017/conference/paper70/areachairs"], "cdate": 1485287741981}}}, {"tddate": null, "tmdate": 1480795639580, "tcdate": 1480795639576, "number": 2, "id": "B1xZ8sl7g", "invitation": "ICLR.cc/2017/conference/-/paper70/pre-review/question", "forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "signatures": ["ICLR.cc/2017/conference/paper70/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper70/AnonReviewer3"], "content": {"title": "questions on experiments and baselines", "question": "Thank you for this very interesting paper. While Bloom filters have already been used to encode outputs in multilabel classification, the paper makes a more general contribution by using it for inputs and outputs, and in particular gives a method to apply bloom filters for ranking tasks.\n\nSome questions regarding the experiments:\n\n1) As far as I understood the \"dimensionality ratio\" is the compression applied to both inputs and outputs, as far as this is applicable (e.g., I suppose input compression is not performed for PTB). In case compression was applied to both (e.g., recommendation), did you try different compression ratios in the input ?\n\n2) regarding the use of bloom filters for input encoding: bloom filters do not encode counts of objets, but some extensions do. In particular for recommendation tasks with count data, to what extent the suppression of the counts affects performance ? \n\n3) Some Deep Learning libraries have layers specialized for sparse inputs (e.g., SparseLinear in torch, which behaves exactly like a Linear layer). When timings are reported, do they use such implementations ? In particular, the Bloom filter approach for the inputs leads to inputs that are *less* sparse (but in smaller dimension), so I think it is important on the exact implementation when reporting timings.\n\n4) When using bloom filters on the output, I suppose the baseline uses a sofmax activation layer. However, since all your evaluations are in terms of ranking and not in terms of perplexity/log-likelihood, a pairwise ranking loss could be used instead of softmax+cross-entropy, making it possible to severely subsample the negative classes during training (each epoch would essentially cost about the number of positive labels, even though this doesn't say anything about the overall training time nor on the final accuracy). Did you experiment with such losses ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959481125, "id": "ICLR.cc/2017/conference/-/paper70/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper70/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper70/AnonReviewer1", "ICLR.cc/2017/conference/paper70/AnonReviewer3"], "reply": {"forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959481125}}}, {"tddate": null, "tmdate": 1480719596186, "tcdate": 1480719596182, "number": 1, "id": "rJ4xT_1Xx", "invitation": "ICLR.cc/2017/conference/-/paper70/pre-review/question", "forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "signatures": ["ICLR.cc/2017/conference/paper70/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper70/AnonReviewer1"], "content": {"title": "Can you discuss the implications for representation learning?", "question": "As far as I can tell, this paper is proposing to apply multiple hashes and take the union, as opposed to taking a single hash of input or output values. While the intuition about bloom filters is nice, at the end of the day, this technique is just producing a smaller, but more dense, binary vector, is that correct? \n\nAlso, what is the computational complexity of decoding the output? The bloom filter can tell you quickly if a given element is in the set. But for producing a sparse output it looks like the suggested procedure is just to iterate over all possible outputs...which defeats the purpose of a fast sparse coding of the output.\n\nCan you discuss in more detail also how this will affect neural models that learn embeddings of the input? A straightforward application of your method would suggest that you represent each input as a sum of embeddings indexed by random hashes. Is that correct?  Is there anything further going on here?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compact Embedding of Binary-coded Inputs and Outputs using Bloom Filters", "abstract": "The size of neural network models that deal with sparse inputs and outputs is often dominated by the dimensionality of those inputs and outputs. Large models with high-dimensional inputs and outputs are difficult to train due to the limited memory of graphical processing units, and difficult to deploy on mobile devices with limited hardware. To address these difficulties, we propose Bloom embeddings, a compression technique that can be applied to the input and output of neural network models dealing with sparse high-dimensional binary-coded instances. Bloom embeddings are computationally efficient, and do not seriously compromise the accuracy of the model up to 1/5 compression ratios. In some cases, they even improve over the original accuracy, with relative increases up to 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4 alternative methods, obtaining favorable results. We also discuss a number of further advantages of Bloom embeddings, such as 'on-the-fly' constant-time operation, zero or marginal space requirements, training time speedups, or the fact that they do not require any change to the core model architecture or training configuration.", "pdf": "/pdf/0b678b2fadd02759acb9cf2d3a9bf6e83538f4ae.pdf", "TL;DR": "Bloom embeddings compactly represent sparse high-dimensional binary-coded instances without compromising accuracy", "paperhash": "serr\u00e0|compact_embedding_of_binarycoded_inputs_and_outputs_using_bloom_filters", "conflicts": ["telefonica.com"], "keywords": ["Applications", "Deep learning", "Unsupervised Learning"], "authors": ["Joan Serr\u00e0", "Alexandros Karatzoglou"], "authorids": ["joan.serra@telefonica.com", "alexandros.karatzoglou@telefonica.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959481125, "id": "ICLR.cc/2017/conference/-/paper70/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper70/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper70/AnonReviewer1", "ICLR.cc/2017/conference/paper70/AnonReviewer3"], "reply": {"forum": "rkKCdAdgx", "replyto": "rkKCdAdgx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper70/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959481125}}}], "count": 15}