{"notes": [{"id": "u9LrVLDu5i", "original": "a09p3LZoQ6", "number": 39, "cdate": 1582750165107, "ddate": null, "tcdate": 1582750165107, "tmdate": 1587925116793, "tddate": null, "forum": "u9LrVLDu5i", "replyto": null, "invitation": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Blind_Submission", "content": {"keywords": ["SGD", "Splitting", "ODE"], "authors": ["Daniil Merkulov", "Ivan Oseledets"], "title": "Stochastic gradient algorithms from ODE splitting perspective", "pdf": "/pdf/340596f3d49a0a3c2bba78378c02ec585d342dbd.pdf", "TL;DR": "We treat SGD as a splitting scheme for continious gradient flow equation and show, that more accurate solution of each local problem leads to the great robustness in stepsize choosing.", "abstract": "We present a different view on stochastic optimization, which goes back to the splitting schemes for approximate solutions of ODE. In this work, we provide a connection between stochastic gradient descent approach and first-order splitting scheme for ODE. We consider the special case of splitting, which is inspired by machine learning applications and derive a new upper bound on the global splitting error for it. We present, that the Kaczmarz method is the limit case of the splitting scheme for the unit batch SGD for linear least squares problem. We support our findings with systematic empirical studies, which demonstrates, that a more accurate solution of local problems leads to the stepsize robustness and provides better convergence in time and iterations on the softmax regression problem.", "authorids": ["daniil.merkulov@skolkovotech.ru", "i.oseledets@skoltech.ru"], "paperhash": "merkulov|stochastic_gradient_algorithms_from_ode_splitting_perspective", "_bibtex": "@inproceedings{\nmerkulov2020stochastic,\ntitle={Stochastic gradient algorithms from {\\{}ODE{\\}} splitting perspective},\nauthor={Daniil Merkulov and Ivan Oseledets},\nbooktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},\nyear={2020},\nurl={https://openreview.net/forum?id=u9LrVLDu5i}\n}"}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq"]}, "signatures": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "readers": ["everyone"], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "invitees": ["~"], "tcdate": 1582750147213, "tmdate": 1587924718420, "id": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "sCdA-nF5_", "original": null, "number": 1, "cdate": 1582774458517, "ddate": null, "tcdate": 1582774458517, "tmdate": 1582774458517, "tddate": null, "forum": "u9LrVLDu5i", "replyto": "u9LrVLDu5i", "invitation": "ICLR.cc/2020/Workshop/DeepDiffEq/Paper39/-/Decision", "content": {"decision": "Accept (Poster)", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"keywords": ["SGD", "Splitting", "ODE"], "authors": ["Daniil Merkulov", "Ivan Oseledets"], "title": "Stochastic gradient algorithms from ODE splitting perspective", "pdf": "/pdf/340596f3d49a0a3c2bba78378c02ec585d342dbd.pdf", "TL;DR": "We treat SGD as a splitting scheme for continious gradient flow equation and show, that more accurate solution of each local problem leads to the great robustness in stepsize choosing.", "abstract": "We present a different view on stochastic optimization, which goes back to the splitting schemes for approximate solutions of ODE. In this work, we provide a connection between stochastic gradient descent approach and first-order splitting scheme for ODE. We consider the special case of splitting, which is inspired by machine learning applications and derive a new upper bound on the global splitting error for it. We present, that the Kaczmarz method is the limit case of the splitting scheme for the unit batch SGD for linear least squares problem. We support our findings with systematic empirical studies, which demonstrates, that a more accurate solution of local problems leads to the stepsize robustness and provides better convergence in time and iterations on the softmax regression problem.", "authorids": ["daniil.merkulov@skolkovotech.ru", "i.oseledets@skoltech.ru"], "paperhash": "merkulov|stochastic_gradient_algorithms_from_ode_splitting_perspective", "_bibtex": "@inproceedings{\nmerkulov2020stochastic,\ntitle={Stochastic gradient algorithms from {\\{}ODE{\\}} splitting perspective},\nauthor={Daniil Merkulov and Ivan Oseledets},\nbooktitle={ICLR 2020 Workshop on Integration of Deep Neural Models and Differential Equations},\nyear={2020},\nurl={https://openreview.net/forum?id=u9LrVLDu5i}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Paper Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject"], "description": "Decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}, "forum": "u9LrVLDu5i", "replyto": "u9LrVLDu5i", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}}, "cdate": 1582156800000, "expdate": 1589155200000, "duedate": 1588291200000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Workshop/DeepDiffEq/Program_Chairs"], "tcdate": 1582771073748, "tmdate": 1587925029938, "super": "ICLR.cc/2020/Workshop/DeepDiffEq/-/Decision", "signatures": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "writers": ["ICLR.cc/2020/Workshop/DeepDiffEq"], "id": "ICLR.cc/2020/Workshop/DeepDiffEq/Paper39/-/Decision"}}}], "count": 2}