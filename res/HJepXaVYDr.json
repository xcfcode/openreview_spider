{"notes": [{"id": "HJepXaVYDr", "original": "rkexsk-DDS", "number": 466, "cdate": 1569439012862, "ddate": null, "tcdate": 1569439012862, "tmdate": 1593487295998, "tddate": null, "forum": "HJepXaVYDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "tl6hE5oLCD", "original": null, "number": 1, "cdate": 1576798697267, "ddate": null, "tcdate": 1576798697267, "tmdate": 1576800938470, "tddate": null, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper proposed using stochastic AUC for dealing with imbalanced data. This paper provides useful insights and experiments on this important problem. I recommend acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707269, "tmdate": 1576800255462, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper466/-/Decision"}}}, {"id": "Syxs8KUnoH", "original": null, "number": 6, "cdate": 1573837139189, "ddate": null, "tcdate": 1573837139189, "tmdate": 1573841749374, "tddate": null, "forum": "HJepXaVYDr", "replyto": "SkgauQLnjH", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment", "content": {"title": "Thank you for your question.", "comment": "Thanks for the insightful question. \n\nPlease note that there is a multiplicative constant $\\mu$ in the definition of PL condition. Recall that the definition of PL condition of function $\\phi$ is $\\phi(v)-\\phi(v_*)\\leq\\frac{1}{2\\mu}\\|\\nabla\\phi(v)\\|^2$, where $\\mu>0$ and $v_*$ is the global minima. If we find a point $v$ that is close to local minima such that $\\|\\nabla\\phi(v)\\|\\leq \\epsilon$, then the objective gap satisfies $\\phi(v)-\\phi(v_*)\\leq\\frac{\\epsilon^2}{2\\mu}$. If $\\phi$ is the loss of a deep neural network, $\\mu$ is usually very small, so the solution may not be very close to the global minima.\n\nFinally we would like to mention the following two points.\n1.  PL condition is proved and utilized in some recent neural network theory papers [r1, r2].\n2.  In the setting of deep neural network, the fact that the parameter $\\mu$ in PL condition is very small is observed in [r3].\n\n[r1] Du et al. Gradient descent provably optimizes over-parameterized neural networks. ICLR 2019.\n[r2] Allen-Zhu et al. A convergence theory for deep learning via overparameterization. ICML 2019. \n[r3] Yuan et al. Stagewise training accelerates convergence of testing error over SGD. NeurIPS 2019.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJepXaVYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper466/Authors|ICLR.cc/2020/Conference/Paper466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171070, "tmdate": 1576860541558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment"}}}, {"id": "SkgauQLnjH", "original": null, "number": 5, "cdate": 1573835636713, "ddate": null, "tcdate": 1573835636713, "tmdate": 1573835636713, "tddate": null, "forum": "HJepXaVYDr", "replyto": "H1xgTHnssH", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment", "content": {"title": "thanks for the clarification", "comment": "Thanks for the clarification. I am happy to recommend acceptance. \n\nThere is one more question though, would the PL assumption a bit too restricted since it seems to imply all local optima is not too far away from the global optimum?"}, "signatures": ["ICLR.cc/2020/Conference/Paper466/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJepXaVYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper466/Authors|ICLR.cc/2020/Conference/Paper466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171070, "tmdate": 1576860541558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment"}}}, {"id": "H1xgTHnssH", "original": null, "number": 3, "cdate": 1573795255584, "ddate": null, "tcdate": 1573795255584, "tmdate": 1573795974861, "tddate": null, "forum": "HJepXaVYDr", "replyto": "rkximJxaKH", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment", "content": {"title": "Thank you for your constructive feedback. We have updated the paper accordingly.", "comment": "Thank you for your constructive comments.\n\nQ1: I think some comparisons with AdaGrad and related methods should be performed in experiments. Since PPD-Adagrad is \u201cAdagrad style\u201d.\n\nA: To the best of our knowledge, AdaGrad can not be directly applied to solving stochastic AUC maximization problem. So we have included the results when applying AdaGrad for minimizing cross-entropy loss. These results are included in Figure 3 on page 22.\n\n\nQ2: The assumptions seem a bit unclear. What does the first assumption in Assumption 1 imply?\n\nA: The first assumption in Assumption 1 is PL condition on the function $\\phi$. It implies that when the gradient of $\\phi$ is small, then the objective value is close to the optimal value up to a multiplicative constant $\\mu$. \n\n\nQ3: Is it possible to provide results on experiments that label in the opposite way (or randomly label 5/50 classes) and add these results in the paper/appendix? Just to make results more convincing and reduce some potential dataset influences. \n\nA: For CIFAR10 and STL10 dataset, we randomly partition the 10 classes into two labels (i.e., randomly select 5 classes as positive label and other 5 classes as negative label). For CIFAR100 dataset, we randomly partition the 100 classes into two labels (i.e., randomly select 50 classes as positive label and other 50 classes as negative label). We have added a description at Appendix A.10 on page 21 and also included the corresponding results in Figure 3 on page 22. \n\n\nQ4: Is it possible to provide some results on more imbalanced positive-negative ratio like 20:1? Is it possible to provide some comparison in terms of actual time, like learning curves with time as x-axis?\n\nA: We have done more experiments for more imbalanced ratios, i.e., 20:1 and 10:1, and the results are plotted in Figure 3 in the supplement. In particular, in order to create the imbalanced data with 20:1 positive-negative ratio (top four plots), we remove 95% examples with negative label from the original data. The four plots on the bottom are the results with 10:1 positive-negative ratio, for which the data is created by removing 90% examples from the original data. The plot about AUC curve versus actual time is also provided.\n\n\nQ5: In the multi-class problems, why are the lower layers shared while last layer separated?\n\nA: What we meant is that the last layer denotes the classifier and each individual class has a corresponding classifier $h(w_c, x)$. All of these classifiers are built on the same feature induced by the same lower layers. \n\n\nQ6: Since the extension to multi-classes problems are mentioned in the paper. I like to see some experimental results on this setting.\n\nA: Due to time constraint, we are not able to finish this experiment during the rebuttal. We expect to include the results in the final version. \n\n\nQ7: How do the proposed methods perform on models other than NN?\n\nA: We tried linear model but it did not work very well for complex image datasets as used in the experiments. \n\n\nQ8: Typo on Page 4, the definition of AUC definition: the latter y should be -1.\n\nA: Thank you for carefully reading our paper! You are absolutely right and we have fixed this typo in the revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJepXaVYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper466/Authors|ICLR.cc/2020/Conference/Paper466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171070, "tmdate": 1576860541558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment"}}}, {"id": "HJlRg82sir", "original": null, "number": 4, "cdate": 1573795318513, "ddate": null, "tcdate": 1573795318513, "tmdate": 1573795318513, "tddate": null, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment", "content": {"title": "General Comments", "comment": "Dear reviewers,\n\nThank you all for your positive ratings and insightful comments. We have updated the paper according to your suggestions. All updates are marked in red. The main summary of the updates are:\n\n1. We have added a description of how we choose training/validation data in our experiments in Section 5 on page 9, as suggested by R2.\n\n2. Per R3\u2019s suggestions, we conducted more experiments on datasets whose classes are randomly partitioned to positive and negative labels with equal size. At the same time, we studied a case with more imbalanced positive-negative ratios (e.g., 20:1, 10:1). The experimental setup and results (both AUC curve versus number of iterations and actual time) are reported in Appendix A.10 and Figure 3.\n\n3. As suggested by R3, we have included AdaGrad as a new baseline in Figure 3.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJepXaVYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper466/Authors|ICLR.cc/2020/Conference/Paper466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171070, "tmdate": 1576860541558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment"}}}, {"id": "SJxEOH3sjS", "original": null, "number": 2, "cdate": 1573795180503, "ddate": null, "tcdate": 1573795180503, "tmdate": 1573795180503, "tddate": null, "forum": "HJepXaVYDr", "replyto": "HklnD5Ke9r", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment", "content": {"title": "Thank you for your valuable comments. We have included our way of choosing validation data.", "comment": "Thank you for your insightful comments. We have included the description of the way we chose validation dataset. The revision has been highlighted in red.\n\nQ: How the validation data for tuning parameters are chosen in the experiments? This is absent in the descriptions for experiments.\n\nA: We use 19k/1k, 45k/5k, 45k/5k, 4k/1k training/validation split on C2, C10, C100, and STL10 respectively. We have included this description in Section 5 on page 9.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJepXaVYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper466/Authors|ICLR.cc/2020/Conference/Paper466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171070, "tmdate": 1576860541558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment"}}}, {"id": "H1lKWB2ooH", "original": null, "number": 1, "cdate": 1573795073037, "ddate": null, "tcdate": 1573795073037, "tmdate": 1573795073037, "tddate": null, "forum": "HJepXaVYDr", "replyto": "rJxAPtR45H", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment", "content": {"title": "Thank you for your review.", "comment": "Thank you for your valuable comments and constructive feedback.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper466/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJepXaVYDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper466/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper466/Authors|ICLR.cc/2020/Conference/Paper466/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504171070, "tmdate": 1576860541558, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper466/Authors", "ICLR.cc/2020/Conference/Paper466/Reviewers", "ICLR.cc/2020/Conference/Paper466/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Comment"}}}, {"id": "rkximJxaKH", "original": null, "number": 1, "cdate": 1571778339090, "ddate": null, "tcdate": 1571778339090, "tmdate": 1572972591900, "tddate": null, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes two algorithms for the non-convex concave AUC maximization problem, along with theoretical analysis. Experiments show the proposed methods are effective, especially in data imbalanced scenarios.\n\nStrengths:\n\nThis paper might be useful and interesting to related research, which overcomes some limitations in previous works such as: 1. the convex assumptions; 2. only considering simple models like linear models; 3. the need of extra memory to store/maintain samples. The proposed method extends existing works to a non-convex setting, which can be applied to deep neural networks, and is applicable for batch-learning and online learning.\n\nThe proposed methods achieve better experimental results, especially in the data imbalanced scenarios, which is a real problem that may arise in many scenarios. The paper provides theoretical analysis on the proposed methods, based on Assumption 1, and inspired by the PL condition.\n\nWeaknesses:\n\nI think some comparisons with AdaGrad and related methods should be performed in experiments. Since PPD-AdaGrad is \u201cAdaGrad style\u201d.\n\nThe assumptions seem a bit unclear. What does the first assumption in Assumption 1 imply?\n\nMinor Comments:\n1. Since the experiments label the first 5/50 classes as negative, and the last 5/50 classes as positive for CIFAR10/CIFAR100, is it possible to provide results on experiments that label in the opposite way (or randomly label 5/50 classes) and add these results in the paper/appendix? Just to make results more convincing and reduce some potential dataset influences.\n\n2. Is it possible to provide some results on more imbalanced positive-negative ratio like 20:1?\n\n3. Is it possible to provide some comparison in terms of actual time, like learning curves with time as x-axis?\n\n4. In the multi-class problems, why are the lower layers shared while last layer separated? \n\n5. Since the extension to multi-classes problems are mentioned in the paper. I like to see some experimental results on this setting.\n\n6. How do the proposed methods perform on models other than NN?\n\n7. I think there is a typo on Page 4, the definition of AUC definition: the latter y should be -1.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper466/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833355247, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper466/Reviewers"], "noninvitees": [], "tcdate": 1570237751717, "tmdate": 1575833355293, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Review"}}}, {"id": "HklnD5Ke9r", "original": null, "number": 2, "cdate": 1572014691818, "ddate": null, "tcdate": 1572014691818, "tmdate": 1572972591790, "tddate": null, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary: \nThe authors propose stochastic algorithms for AUC maximization using a deep neural network. Under the assumption that the underlying function satisfies the PL condition, they prove convergence rates of the proposed algorithms. The key insight is to use the equivalence between AUC maximization and some min-max function. Experiments results show the proposed algorithms works better than some baselines. \n\nComments: \nThe technical contribution is to show stochastic optimization algorithms for some kind of min-max functions converge to the optimum under the PL condition. The proposed algorithms have better convergence rates than a na\u00efve application of Rafique et al. The technical results rely on previous work on the PL condition and stochastic optimization of min-max functions. The techniques are not straightforward but not seem to be highly innovative, either. \n\nAs a summary, non-trivial algorithms for AUC maximization with neural networks are presented, which could be useful in practice.\n\nMinor Comments:\n\n-How the validation data for tuning parameter are chosen in the experiments? This is absent in the descriptions for experiments. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper466/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833355247, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper466/Reviewers"], "noninvitees": [], "tcdate": 1570237751717, "tmdate": 1575833355293, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Review"}}}, {"id": "rJxAPtR45H", "original": null, "number": 3, "cdate": 1572297062253, "ddate": null, "tcdate": 1572297062253, "tmdate": 1572972591749, "tddate": null, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "invitation": "ICLR.cc/2020/Conference/Paper466/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors propose two modifications to an algorithm from [Rafique et al 2018] for optimizing AUC under a min-max formulation, prove bounds for the two modifications, and experimentally compare the modifications against SGD and the original algorithm by varying class ratios of four datasets.\n\nThe proposal builds on [Rafique et al 2018], so it may be considered incremental. However, the algorithm is carefully analyzed and resulting bounds are stronger. The experimental analysis is fairly minimal, with the proposed modifications performing similarly to the original algorithm from [Rafique et al 2018]."}, "signatures": ["ICLR.cc/2020/Conference/Paper466/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper466/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic AUC Maximization with Deep Neural Networks", "authors": ["Mingrui Liu", "Zhuoning Yuan", "Yiming Ying", "Tianbao Yang"], "authorids": ["mingrui-liu@uiowa.edu", "zhuoning-yuan@uiowa.edu", "yying@albany.edu", "tianbao-yang@uiowa.edu"], "keywords": ["Stochastic AUC Maximization", "Deep Neural Networks"], "TL;DR": "The paper designs two algorithms for the stochastic AUC maximization problem with state-of-the-art complexities when using deep neural network as predictive model, which are also verified by empirical studies.", "abstract": "Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.", "pdf": "/pdf/d125ab0e25bf87bb12c7a81dda747d9a50fe78e4.pdf", "code": "https://drive.google.com/drive/folders/1nPM6fmvN5fTsSaWsOcGFbhMVW7Fxso-Y", "paperhash": "liu|stochastic_auc_maximization_with_deep_neural_networks", "_bibtex": "@inproceedings{\nLiu2020Stochastic,\ntitle={Stochastic AUC Maximization with Deep Neural Networks},\nauthor={Mingrui Liu and Zhuoning Yuan and Yiming Ying and Tianbao Yang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HJepXaVYDr}\n}", "original_pdf": "/attachment/252c7b341737d1315dc3823ffddda3071689192d.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJepXaVYDr", "replyto": "HJepXaVYDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper466/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575833355247, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper466/Reviewers"], "noninvitees": [], "tcdate": 1570237751717, "tmdate": 1575833355293, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper466/-/Official_Review"}}}], "count": 11}