{"notes": [{"id": "SJl47yBYPS", "original": "BygRBI3uwH", "number": 1614, "cdate": 1569439516477, "ddate": null, "tcdate": 1569439516477, "tmdate": 1577168263238, "tddate": null, "forum": "SJl47yBYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8Q2T8yT1vr", "original": null, "number": 1, "cdate": 1576798727918, "ddate": null, "tcdate": 1576798727918, "tmdate": 1576800908596, "tddate": null, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Decision", "content": {"decision": "Reject", "comment": "The paper studies the role of entropy in maximum entropy RL, particularly in soft actor-critic, and proposes an action normalization scheme that leads to a new algorithm, called Streamlined Off-Policy (SOP), that does not maximize entropy, but retains or exceeds the performance of SAC. Independently from SOP, the paper also introduces Emphasizing Recent Experience (ERE) that samples minibatches from the replay buffer by prioritizing the most recent samples. After rounds of discussion and a revised version with added experiments, the reviewers viewed ERE as the main contribution, while had doubts regarding the claimed benefits of SOP. However, the paper is currently structured around SOP, and the effectiveness of ERE, which can be applied to any off-policy algorithm, is not properly studied. Therefore, I recommend rejection, but encourage the authors to revisit the work with an emphasis on ERE.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795708366, "tmdate": 1576800256772, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Decision"}}}, {"id": "H1x1YN36Kr", "original": null, "number": 2, "cdate": 1571828855174, "ddate": null, "tcdate": 1571828855174, "tmdate": 1574256347053, "tddate": null, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "\n# Summary\nThe paper identifies a problem with TD3 related to action clipping. The authors notice that SAC alleviates this problem by means of entropy regularization. Given the insight that action clipping is crucial, the authors propose an alternative approach to avoid action clipping in TD3, which is empirically shown to yield the same results as SAC. Surprisingly, with this improvement, even several parts from TD3 can be removed, such as delayed policy updates and the target policy parameters. In addition, a straightforward-to-implement experience replay scheme is proposed, that emphasizes recent experiences, which propels the proposed algorithm to achieve state-of-the-art results on MuJoCo.\n\n# Decision\nThis is a great paper: accept. The proposed Streamlined Off Policy (SOP) algorithm is thoroughly evaluated, ablation studies performed, code made available. Nevertheless, there are a few suggestions below that may further improve the paper.\n\n# Suggestions\n1) It is said that entropy regularization leads to action not being saturated in SAC. I feel that this causal relation is very indirect. Maybe SAC with entropy just discovers better policies that do not go crazy between extremes? For example, if you would leave the entropy term but remove tanh saturation from SAC, don't you think you would also get a bang-bang policy? Adding such an ablation study could further strengthen the argument that entropy leads to no constraint violation, if it turns out true.\n\n2) The Emphasizing Recent Experience (ERE) replay scheme seems reminiscent of sampling according to a distribution exponentially decaying into the past. It is said that physically shrinking the allowed sampling range by dropping old experiences is better because then very old experiences cannot be used at all. It would be interesting to see a comparison to sampling according to exponential distribution from the replay queue.\n\n# AFTER REBUTTAL\nTaking into account the concerns of other reviewers and the newly added evaluations, I lower my score to weak accept. Since now it seems that ERE is quite crucial, the argument of SPO outperforming SAC becomes weaker. Therefore, the authors should tone down the claims of outperforming SAC. Nevertheless, I still find the contribution of the paper valuable and think that it should be accepted, albeit with the aforementioned modifications in the camera-ready version.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652760435, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Reviewers"], "noninvitees": [], "tcdate": 1570237734818, "tmdate": 1575652760453, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Review"}}}, {"id": "B1gpjwjOOS", "original": null, "number": 1, "cdate": 1570449316649, "ddate": null, "tcdate": 1570449316649, "tmdate": 1574251015111, "tddate": null, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The authors investigate the role of entropy maximization in SAC and show that entropy regularization does not do what is usually thought: in the examples they investigate, where the output of the policy network needs to be squashed to fit in the action space domain, squashing would result in having only action at the boundaries, but entropy regularization maintains some intermediate values, hence exploration. From this insight, the authors replace entropy regularization by a simpler normalization process and show equivalent performance with their simpler Streamlined Off-Policy (SOP) algorithm. Then they introduce a second \"Emphasizing Recent Experience\" mechanism and show that SOP+ERE performs better than SAC.\n\nA good point for the paper is that the entropy regularization  study is very nice, more papers in the field should show similar detailed analyses of internal processes. But the paper suffers from a few serious weaknesses:\n\n- The TD3 mechanism goes beyond the Double Q-learning (or DDQN) mechanism of Van Hasselt et al: it takes the min over two critics. This should be explained properly.\n- the title, abstract and introduction insist more on SOP, but performance improvement seem to result more from ERE. If this is possible, studying the performance of SAC + ERE would disambiguate the relative contribution of both mechanisms.\n\nAbout gradient squashing issues, the authors main mention de gradient inverter idea from this paper:\n\n@article{hausknecht2015deep,\n  title={Deep reinforcement learning in parameterized action space},\n  author={Hausknecht, Matthew and Stone, Peter},\n  journal={arXiv preprint arXiv:1511.04143},\n  year={2015}\n}\n\nThe authors should also probably also cite (and read the latest arxiv version of):\n@inproceedings{ahmed2019understanding,\n  title={Understanding the impact of entropy on policy optimization},\n  author={Ahmed, Zafarali and Le Roux, Nicolas and Norouzi, Mohammad and Schuurmans, Dale},\n  booktitle={International Conference on Machine Learning},\n  pages={151--160},\n  year={2019}\n}\n\n\nMore local points:\n- \"without performing a careful hyper-parameter search\": so how did you choose these hyper-parameters? I see what you mean, but this is a very vague and slippery statement.\n- I do not find the 23*4 images in Appendix B much useful\n- Fig 3 seems to be repeated in Fig 4. Can't you just remove Fig 3?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652760435, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Reviewers"], "noninvitees": [], "tcdate": 1570237734818, "tmdate": 1575652760453, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Review"}}}, {"id": "HyeO-W-0YB", "original": null, "number": 3, "cdate": 1571848447851, "ddate": null, "tcdate": 1571848447851, "tmdate": 1574227094540, "tddate": null, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "The main contribution of this paper is a normalization scheme to avoid saturating the squashing function typically used to constrain actions within a bounded range in continuous control problems. It is argued that algorithms like DDPG and TD3 suffer from such saturation, which prevents proper exploration during training, while maximum entropy algorithms like Soft Actor-Critic (SAC) avoid it thanks to their entropy bonus. The main reason behind the success of SAC would thus be its ability to keep exploring throughout training, by avoiding saturation. A second contribution is a new experience replay sampling scheme, named Emphasizing Recent Experience (ERE), based on the idea that most recently added transitions should be given higher weights when sampling mini-batches from the replay bufffer. Combining both ideas leads to the SOP (Streamlined Off-Policy)+ERE algorithm, which is shown to consistently outperform SAC on Mujoco tasks.\n\nAlthough this paper presents interesting insights and very good empirical results, I am currently leaning towards rejection mostly due to missing some important empirical comparisons, which hopefully can be added in a revised version.\n\nThe first key missing comparison (IMO) is to the Inverting Gradients approach from Hausknecht & Stone (2016), which the authors know about since it is cited in the related work section. Note that in that paper, the problem of saturating squashing functions preventing proper exploration was already mentioned, although not investigated in as much depth as in this submission (\u00ab(\u2026) squashing functions quickly became saturated. The resulting agents take the same discrete action with the same maximum/minimum parameters each timestep \u00bb). Their proposed Inverting Gradients technique was found to work significantly better than squashing functions, which is why I believe it should be an obvious baseline to compare to.\n\nThe other important experiments which I think need to be added are simply to implement the proposed normalization scheme within DDPG & TD3 to demonstrate its usefulness as a standalone improvement over existing algorithms. This would strengthen the claim that \u00ab algorithms such as DDPG and TD3 based on the standard objective with additive noise exploration can be greatly impaired by squashing exploration \u00bb. Without this comparison on the same benchmark, it is difficult to fully grasp the impact of this normalization.\n\nFinally, regarding the ERE sampling scheme, I would appreciate to see SAC+ERE as well, to (hopefully) show that it can benefit SAC too (since this second contribution is orthogonal to the SOP algorithm).\n\nMinor points:\n\u2022\tI would tone down a bit the claims for \u00ab the need to revisit the benefits of entropy maximization in DRL \u00bb, since better exploration has always been put forward as a major benefit (\u00ab the maximum entropy formulation provides a substantial improvement in exploration and robustness \u00bb, as written in \u00ab Soft Actor-Critic Algorithms and Applications \u00bb). To me, what this submission shows is essentially that naive implementation of additive noise exploration in e.g. DDPG is very bad for exploration, more than uncovering some novel properties of SAC.\n\u2022\tBelow eq. 1: \u00ab the optimal policy is deterministic \u00bb => should be replaced with \u00ab there exists an optimal policy that is deterministic \u00bb\n\u2022\t\u00ab principle contribution \u00bb => principal\n\u2022\tThe normalization scheme does not appear in Alg. 1\n\u2022\tIn Alg. 1 there are a Q_phi,i and a Q_phi,1 that should probably be Q_phi_i and Q_phi_1\n\u2022\tThe results from section E in the Appendix should be mentioned in the main text\n\u2022\tIn Fig. 4f the y axis\u2019 label is a bit clipped\n\nUpdate based on new revision: thank you for adding more results. From what I can see, it is difficult to conclude on the benefits of SOP over IG, which I find really problematic. It seems to me that the most impactful result is related to the improvements brought by the ERE sampling scheme, which could probably be worth a paper on its own (by showing its benefits over a wider range of algorithms, e.g. TD3 & DQN+variants), but this would be a different paper. As a result I am sticking to \"Weak Reject\".", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575652760435, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Reviewers"], "noninvitees": [], "tcdate": 1570237734818, "tmdate": 1575652760453, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Review"}}}, {"id": "B1xrT5V2oB", "original": null, "number": 11, "cdate": 1573829308783, "ddate": null, "tcdate": 1573829308783, "tmdate": 1573829308783, "tddate": null, "forum": "SJl47yBYPS", "replyto": "HygLsdD_oH", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Final revision finished", "comment": "We have just finished the final revision. A summary of the changes has been posted separately for all reviewers. We would like to thank you again for your helpful feedback, they really helped us improve the quality of the paper. Thank you! "}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "BJgCauVnsB", "original": null, "number": 10, "cdate": 1573828806097, "ddate": null, "tcdate": 1573828806097, "tmdate": 1573828806097, "tddate": null, "forum": "SJl47yBYPS", "replyto": "B1gpjwjOOS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Final revision finished", "comment": "We have just finished uploading a final revision of the paper. We have addressed all the \"serious weaknesses\" you mentioned in your comments, as well as the minor points. We have also added some other new experiments on Inverting Gradients and an exponential sampling scheme due to the request of other reviewers. A summary of the changes has been posted to all reviewers. \nWe would like to thank you again for your helpful feedback, we feel that they helped us greatly in making this paper more refined. Thanks! "}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "SJeIf-ejjH", "original": null, "number": 6, "cdate": 1573744909960, "ddate": null, "tcdate": 1573744909960, "tmdate": 1573828069282, "tddate": null, "forum": "SJl47yBYPS", "replyto": "HygLsdD_oH", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re: Reply to your review", "comment": "We have just uploaded a new version of our paper and will continue working on it.  \nFor the experiments you suggested,  below is a summary of what we did so far:\n\nIn the revision, we implemented the Inverting Gradients(IG) technique and TD3+normalization, and compared their performance with SAC and SOP in figure 3. The IG experiments for Humanoid and Ant have not finished running yet. We expect them to finish tomorrow and we will update the figures once we have them. Our results show that IG has similar overall performance compared to SOP and SAC. It learns faster in the beginning in Hopper and Ant, but is slightly weaker in HalfCheetah, and did not do as well in Humanoid. TD3+normalization also has good performance, although not quite as good as the other schemes.\n\nWe also compare the performance of TD3 with TD3+normalization. The results are shown in Appendix G (figure 9). Our results indicate that for humanoid, normalization boosts the performance of TD3 significantly, but does not bring the performance to the level of SOP.\n\nWe also include our results for SAC+ERE in figure 4, in comparison to the performance of SAC, SOP, and SOP+ERE. From the results, we can see that with ERE, both SAC and SOP gain a significant performance improvement in all environments. Moreover, we also include results for IG+ERE and compare it with IG, SOP and SOP+ERE in Appendix G (figure 8). Some IG+ERE experiments are still running. We will update the figure once we have them. \n\nWe have also fixed a list of typos and formatting issues. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "Hkl7dBE2jr", "original": null, "number": 9, "cdate": 1573827946616, "ddate": null, "tcdate": 1573827946616, "tmdate": 1573827946616, "tddate": null, "forum": "SJl47yBYPS", "replyto": "SJl47yBYPS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Summary of changes", "comment": "We want to thank the reviewers again for their feedback, we have conducted a set of new experiments and now made a final revision. Here is a list of changes. We have also made some small fixes and reorganized some of the text. \n\nSection 1 Introduction:\n- Emphasize contributions of TD3: clipped double Q-learning, delayed policy update and target policy smoothing. \n- Mention additional experiments with Inverting Gradients(IG).\n\nSection 4 (Streamlined Off-Policy Algorithm)\n- Remove the extra hyper-parameter beta.\n- Include results comparing SAC, SOP, TD3+(TD3 plus normalization), and IG in figure 3.\n- Add a description on the core idea of Inverting Gradients. \n\nSection 5 (Non-uniform Sampling)\n- Include results for SAC+ERE, comparing to SAC, SOP, and SOP+ERE in figure 4.\n\nRelated Work:\n- Cite paper \u201cUnderstanding the impact of entropy on policy optimization\u201d.\n- Cite several critique and reproducibility papers.\n\nAppendix B:\n- Add a concrete and thorough discussion of how we conducted hyper-parameter search.\n- Provide hyper-parameters of additional experiments.\n\nAppendix D:\n- Add algorithmic and implementation details for Inverting Gradients.\n\nAppendix E:\n- Include algorithmic details and results for SOP+Exponential Sampling\n\nAppendix G: \n- Add discussion of implementation and computation complexity of new experiments\n\nAppendix H:\n- Include results for IG+ERE, and compare it with SOP+ERE, SOP and IG.\n- Include results comparing TD3 with TD3+.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "Bye7Tbeiir", "original": null, "number": 8, "cdate": 1573745082664, "ddate": null, "tcdate": 1573745082664, "tmdate": 1573745082664, "tddate": null, "forum": "SJl47yBYPS", "replyto": "B1gpjwjOOS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Reply to your review", "comment": "Thank you for your comments. We have uploaded a new version of our paper and will continue working on it. Below is a summary of the changes we made so far:\n\nWe now have a more detailed explanation of how SAC and TD3 are different. We explained that TD3 takes the min of 2 target critic values when computing update targets, and also added that TD3 uses delayed policy update and target policy smoothing. \n\nIn the revision, we discuss the paper \u201cDeep reinforcement learning in parameterized action space\u201d in more detail and we also implemented the Inverting Gradients (IG) technique mentioned in the paper and compare its performance with SAC and SOP in figure 3. The IG experiments for Humanoid and Ant have not finished running yet. We expect them to finish tomorrow and we will update the figure once we have them. that IG has similar overall performance compared to SOP and SAC. It learns faster in the beginning in Hopper and Ant, but is slightly weaker in HalfCheetah, and did not do as well in Humanoid. \n\nWe also include our results for SAC+ERE in figure 4, in comparison to the performance of SAC, SOP, and SOP+ERE. From the results, we can see that with ERE, both SAC and SOP gain a significant performance improvement in all environments. Since figure 3 and 4 now contain entirely different information, there is no redundancy. \n\nMoreover, we also include results for IG+ERE and compare it with IG, SOP and SOP+ERE in Appendix G (figure 8). Some IG+ERE experiments are still running. We will update the figure once we have them. \n\nWe have read and cited the paper \u201cUnderstanding the impact of entropy on policy optimization\u201d. Thank you again for recommending this very insightful paper.\n\nInstead of a rather vague claim on hyper-parameters, we have now added a thorough discussion of exactly how we performed hyper-parameter search and why we chose those values in Appendix B Hyper-parameters section. \n\nThe 23*4 images in the Appendix do take a lot of space, however, we are worried that other people may want to see the mu and action values for all action dimensions instead of just one action dimension we picked out in the main body. Thus, we decide to keep the 23*4 images but removed them to the end of the Appendix. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "H1xldZejoH", "original": null, "number": 7, "cdate": 1573744999808, "ddate": null, "tcdate": 1573744999808, "tmdate": 1573744999808, "tddate": null, "forum": "SJl47yBYPS", "replyto": "HkgCz0IdjS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re:Reply to your review", "comment": "Thank you for the clarification! We have just uploaded a revision, here are some of the changes we made:\n\nWe made an efficient implementation of the exponential sampling scheme (EXP) for SOP and performed experiments, the results are now reported in section D of the appendix, where we compare the performance of SOP with three sampling schemes, ERE, PER and EXP. The results show that EXP improves the performance of SOP consistently, and does well especially in the HalfCheetah and Humanoid environments, although the performance is not as strong as ERE. \n\nWe have also included details on implementation and hyper-parameter search in the appendix. It turns a naive EXP sampling implementation will incur a significant computation overhead, but we avoided it by first sample segments of size 100 from the buffer, then sample a data uniformly from each segment. This modification does not really change the sampling scheme, is relatively easy to implement and has a negligible computation overhead. \n\nWe also performed experiments on removing the tanh from SAC. However, our results show that if we simply remove tanh, it causes SAC performance to drop significantly. We suspect that some other parts of SAC will have to be modified in order for it to work correctly without tanh, but currently, it is unclear what is the missing part. We will try to continue work on this and run more experiments. And we can later add our findings to the camera-ready version. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "HygLsdD_oH", "original": null, "number": 5, "cdate": 1573578910434, "ddate": null, "tcdate": 1573578910434, "tmdate": 1573578910434, "tddate": null, "forum": "SJl47yBYPS", "replyto": "rygGVGFzoS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Re: Reply to your review", "comment": "Thank you for running these additional experiments. If you can share their results before the end of the discussion period, that would be great.\n\nRegarding my first minor point, I guess this is mostly a matter of interpretation of your words. I understood them myself as \"the reasons why SAC works so well are not those most people think\", and this is something I tend to disagree with since what you show is essentially that SAC is better at exploring, and better exploration is a key motivation for the entropy maximization in SAC."}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "HkgCz0IdjS", "original": null, "number": 4, "cdate": 1573576214067, "ddate": null, "tcdate": 1573576214067, "tmdate": 1573576214067, "tddate": null, "forum": "SJl47yBYPS", "replyto": "Bylzx9uGiH", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Remove tanh but keep the entropy penalty", "comment": "Thanks for agreeing to do these additional experiments.\n\nI meant keeping the entropy term but remove tanh."}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "rygGVGFzoS", "original": null, "number": 3, "cdate": 1573192233977, "ddate": null, "tcdate": 1573192233977, "tmdate": 1573192233977, "tddate": null, "forum": "SJl47yBYPS", "replyto": "HyeO-W-0YB", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Reply to your review", "comment": "Thank you for your careful reading of the paper. You have made many good suggestions, most of which we will address in the revision. \n\nAs you suggest, we will implement the inverted gradient technique and compare the results with SOP and SAC. We will also be sure to acknowledge more fully the insights that paper made regarding saturating squashing functions. \n\nAs you suggest, we are also running experiments for TD3+normalization. We agree in hindsight that this is a very natural combination to consider. Our results so far seem to indicate that for humanoid, normalization does improve the performance of TD3, but does not bring the performance to the level of SOP. We will provide the experimental results for all five environments in the revision. \n\nReviewer 3 also pointed out that it would be good to see experimental results for SAC+ERE. We are currently running these experiments and will provide the results in the revision. \n\nWe will also take care of your minor points in the revision. However, we do not fully agree with the first minor point. We feel that an important contribution of the paper is to show that entropy maximization is not a major benefit for the Mujoco environments; in fact by introducing a simple normalization of the outputs, we can achieve equivalent performance without entropy maximization. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "Hkg8xaOMiH", "original": null, "number": 2, "cdate": 1573190893773, "ddate": null, "tcdate": 1573190893773, "tmdate": 1573190893773, "tddate": null, "forum": "SJl47yBYPS", "replyto": "B1gpjwjOOS", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Reply to your comments", "comment": "Thank you for your careful review of the paper. Your comments and suggestions are very useful. Below we respond to your various points. \n\n\"The TD3 mechanism goes beyond the Double Q-learning (or DDQN) mechanism of Van Hasselt et al: it takes the min over two critics. This should be explained properly.\" Yes, thank you for bringing this to our attention. We will make this clear in the revision. \n\n\"The title, abstract and introduction insist more on SOP, but performance improvement seem to result more from ERE. If this is possible, studying the performance of SAC + ERE would disambiguate the relative contribution of both mechanisms.\"  This is a good point. We are now running experiments for SAC+ERE, and we will include the results in the revision.\n\n\"About gradient squashing issues, the authors main mention de gradient inverter idea from this paper. \" We will discuss this paper in more detail in the revision. We will also cite the \"Understanding the impact of entropy...\" paper in the revision. Thank you for bringing it to our attention. \n\nWe will respond to your \"local points\" in the revision, including a more thorough discussion of how the hyper-parameters were determined. \n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}, {"id": "Bylzx9uGiH", "original": null, "number": 1, "cdate": 1573190122187, "ddate": null, "tcdate": 1573190122187, "tmdate": 1573190122187, "tddate": null, "forum": "SJl47yBYPS", "replyto": "H1x1YN36Kr", "invitation": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment", "content": {"title": "Reply to your comments.", "comment": "Thank you  for your careful review of the paper. We are happy that you think the paper is \"great\". We also feel that it makes an important contribution both in terms in bringing insight to off-policy DRL and achieving the state-of-art performance. \n\nWe like your suggestion of comparing ERE with exponential sampling. We are currently running experiments, and we will include the results of these new experiments in the paper. \n\nConcerning your first suggestion, we would like to ask for a clarification. If we remove the entropy term from SAC, we found that the pre-tanh values become huge in magnitude, which leads to tanh saturation and poor exploration.  For the additional ablation study, do you mean to keep the entropy term, but then to remove tanh entirely (so that in some cases, the chosen action will be outside the bound), or do you mean keep the term and adding L1 penalty to the pre-tanh value? "}, "signatures": ["ICLR.cc/2020/Conference/Paper1614/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning", "authors": ["Che Wang", "Yanqiu Wu", "Quan Vuong", "Keith Ross"], "authorids": ["cw1681@nyu.edu", "yanqiu.wu@nyu.edu", "quan.hovuong@gmail.com", "keithwross@nyu.edu"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency", "Off-Policy Algorithms"], "TL;DR": "We propose a new DRL off-policy algorithm achieving state-of-the-art performance. ", "abstract": "The field of Deep Reinforcement Learning (DRL) has recently seen a surge in the popularity of maximum entropy reinforcement learning algorithms.  Their popularity stems from the intuitive interpretation of the maximum entropy objective and their superior sample efficiency on standard benchmarks. In this paper, we seek to understand the primary contribution  of the entropy term to the performance of maximum entropy algorithms. For the Mujoco benchmark, we demonstrate that the entropy term in Soft Actor Critic (SAC) principally addresses the bounded nature of the action spaces. With this insight, we propose a simple normalization scheme which allows a streamlined algorithm without entropy maximization match the performance of SAC. Our experimental results demonstrate a need to revisit the benefits of entropy regularization in DRL. We also propose a simple non-uniform sampling method for selecting transitions from the replay buffer during training.  We further show that the streamlined algorithm with the simple non-uniform sampling scheme outperforms SAC and achieves state-of-the-art performance on challenging continuous control tasks.", "pdf": "/pdf/e5f23c639da3d2aa14eb8540800034dcf07f230c.pdf", "code": "https://anonymous.4open.science/r/e484a8c7-268a-4a66-a001-1e7676540237/", "paperhash": "wang|towards_simplicity_in_deep_reinforcement_learning_streamlined_offpolicy_learning", "original_pdf": "/attachment/053c349e656fa90393c1b95ed2f8ce01c806f767.pdf", "_bibtex": "@misc{\nwang2020towards,\ntitle={Towards Simplicity in Deep Reinforcement Learning: Streamlined Off-Policy Learning},\nauthor={Che Wang and Yanqiu Wu and Quan Vuong and Keith Ross},\nyear={2020},\nurl={https://openreview.net/forum?id=SJl47yBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJl47yBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1614/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1614/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1614/Authors|ICLR.cc/2020/Conference/Paper1614/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504153413, "tmdate": 1576860541899, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1614/Authors", "ICLR.cc/2020/Conference/Paper1614/Reviewers", "ICLR.cc/2020/Conference/Paper1614/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1614/-/Official_Comment"}}}], "count": 16}