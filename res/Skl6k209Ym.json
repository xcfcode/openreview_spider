{"notes": [{"id": "Skl6k209Ym", "original": "rkgYGO6tY7", "number": 1029, "cdate": 1538087909426, "ddate": null, "tcdate": 1538087909426, "tmdate": 1545355440933, "tddate": null, "forum": "Skl6k209Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1gGK2wxlE", "original": null, "number": 1, "cdate": 1544744058130, "ddate": null, "tcdate": 1544744058130, "tmdate": 1545354476500, "tddate": null, "forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1029/Meta_Review", "content": {"metareview": "The reviewers are polarized on this paper and the overall feeling is that it is not quite ready for publication. There is also an interesting interpretability aspect that, while given as a motivation for the approach, is never really explored beyond showing some figures of alignments. One of the main concerns of the method\u2019s effectiveness in practice is the computational cost. There is also concern from one of the reviewers that the formulation could result in creating sparse matching maps where only a few pixels get matched. The authors provide some justification for why this wouldn\u2019t happen, and this should be put in a future draft. Even better would be to show statistics to demonstrate empirically that this doesn\u2019t happen.\n\nThere were a number of clarifications that were brought up during the discussion, and the authors should go over this carefully and update the draft to resolve these issues. There is also a typo in the title that should be fixed.\n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Potentially interpretable few-shot learning algorithm."}, "signatures": ["ICLR.cc/2019/Conference/Paper1029/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1029/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1029/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352993071, "tddate": null, "super": null, "final": null, "reply": {"forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1029/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1029/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1029/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352993071}}}, {"id": "S1e9sNWygE", "original": null, "number": 4, "cdate": 1544651937972, "ddate": null, "tcdate": 1544651937972, "tmdate": 1544651937972, "tddate": null, "forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1029/Official_Review", "content": {"title": "point wise embedding (+) with greedy matching (-)", "review": "Authors argue that using average (independent) greedy matching of pixel embedding (based on 4-6 layer cnn hypercolumns) is a better metric for one-shot learning than just using final layer embedding of a 4-6 layer cnn for the whole image.  Their argument is backed by outperforming their baseline and getting competitive results on few shot learning tasks. Their method is much more computationally heavy than the baseline matching networks. In order to make training feasible, in practice they train with 90% dropout of test pixels embedding & 80% dropout of reference pixels embedding. \n\n\nThe caveats: \n-> Using hyper-columns is related to adding residual connections. The question remains how much performance can be gained by just adding residual connections (with dropout) to the matching networks and letting the network automatically (or with a probability) choose to embed higher layers or lower ones. Adding the residual connection and just comparing the final layer embeddings is a cleaner method than ABM which  provides a richer embedding than baseline and could potentially close the performance gap between ABM and final layer matching.\n\n->It is strictly designed for one-shot learning. It does not benefit from few shots (extra shots) and the fact that these different shots are getting classified as the same label. Vinyal et al mitigates this shortcoming by adding the FCE. However FCE is not directly applicable anymore. Author\u2019s don\u2019t suggest any alternatives either. Their smaller gains (or even worse than baseline without self-regularization) in the 5-shot cases is an evidence of this shortcoming. \n\nThe fact that SNAIL (TCML Mishra et al. (2017)) consistently outperforms this method puts a question mark on the significance of this work. If it was computationally feasible, authors could have used SNAIL and replaced the 64 dimensional embedding of each picture with the 10-20% hypercolumns. Essentially due to computational costs authors are sacrificing a more thorough matching system (non-greedy) for a richer embedding and they don\u2019t get better results. \n\n\nOn the other hand, authors may argue that the hyper-column matching is not just about performance, whereas it also adds interpretability to why two images are categorized the same. Illustrations like fig. 3 for example shows that the model is not matching semantically similar points and can be used to debug & improve the model. While understanding why a blackbox matching network is making a mistake and improving, is  harder. \nIt would have been nice if authors used this added interpretability in some manner. Such as getting an idea about a regularizer, a prior, a mask, etc. and improved the performance.\n\nI would argue for accepting this paper for two reasons.\n-> Given that they beat their baseline and  they get comparable performance to sota even with a greedy matching (min-pooling followed by average pooling), is impressive. Furthermore, it is orthogonal to methods like SNAIL if the computational cost could be resolved.\n\n-> They not only provide which image is a match but how they are matched, which could be interesting for one-shot detection as well as classification. \n\n\nQuestion: At test/validation: do you still only categorize with 10,20% samples or do you average the full attention map for all test pixels?\n\n\nNit: The manuscript needs several passes of proofreading, spell & grammar checking. A few examples in the first couple of pages:\n-> The citing format needs to be fixed (like: LSTMsRavi, there should be () around citations). \n-> are not incompatible: are compatible\n->incomprehensible sentence with two whiles: ABM networks outperforms these other state-of-the-art models on the open-set recognition task while in the one-shot setting by achieving a high accuracy of matching the non-open-set classes while maintaining a high F1 score for identifying samples in the open-set. \n-> add dots to the end of contribution list items.\n-> we first look pairwise matchings: we first look at the pairwise matchings\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1029/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1029/Official_Review", "cdate": 1542234322204, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1029/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335857162, "tmdate": 1552335857162, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1029/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1eVQB6FTQ", "original": null, "number": 3, "cdate": 1542210844392, "ddate": null, "tcdate": 1542210844392, "tmdate": 1542210844392, "tddate": null, "forum": "Skl6k209Ym", "replyto": "SylnZdjD37", "invitation": "ICLR.cc/2019/Conference/-/Paper1029/Public_Comment", "content": {"comment": "Thank you for the detailed comments. \n\n1) C is not a binary map, but a continuous cost matrix where C_{i,j,k} tells us how well pixel i of the target image aligns to pixel j of reference image k. If you are viewing this as a binary matching problem, then M is the binary matching matrix and C (when normalized row-wise) is the continuous relaxation of M.\n\nTo construct C_{i,j,k}, we take the hyper-column descriptor of pixel i in the target image \\phi_i(I) and the hyper-column descriptor of pixel j in reference image k \\phi_j(S_k) and compute the similarity between them using cosine similarity as the normalized dot product (below Equation 2). P(M_{i,j,k} | I_t, S) is inverse-exponentially related to C_{i,j,k} (Equation 2)\n\n2) The contextual feature encoding of a point is the hyper-column representation of the point and not just the pixel colors. Hence, the system will not align two red points unless the contextual information is also in agreement.\n\n3) Our approach is motivated by finding correspondences. We express the solution as an alignment between sampled points in the two images which we force the system to perform, aided by self-regularization. The label of the target image is the reference image that best aligns with the various parts of the target image.\n\n4) [same response as to AnonReviewer3] Our method computes an approximate, sampled matching between two images, not an exact matching as computing an exact matching while enforcing locality constraints is computationally difficult. We hence make the independent point matching assumption. Given the full image and a point, let us assume there is an oracle that performs the matching. In this case, the independent point matching assumption is reasonable as the oracle will assign P_{i,j,k}=1 when the correct matching is done for a given pixel i and 0 otherwise. In our method, points are given increasing context as we use the features of higher layers, so given enough filters at each layer, an oracle would essentially have access to the full image using the hyper-column description of a particular point. Self-regularization trains the system to learn to perform the matching task of the oracle. Hence, we expect the approximate solution to be good enough to be able to perform the original task - one-shot learning, and still learning a reasonable matching.\n\n5) Visual attention corresponds to learning a mask over the input where the model should pay attention and then expressing the feature descriptor of the image as a combination of the feature descriptors across the image according to the attention weight. If one were to directly augment Matching Networks [Vinyals et al., 2016] with attention, a comparison between two images would be performed in the feature embedding space corresponding to a single similarity score between the resulting image feature vectors. On the other hand, alignment corresponds to the matching of individual points in a manner that incorporates the context around the points. Thus the comparison between two images is computed as the sum of similarity scores of individual pixels (hyper-column representations). Feature vectors are not compressed to a single feature descriptor. \n\n6) a) By bandwidth, we mean that only a single label is provided per target image for training in the traditional one-shot learning case (low bandwidth). With alignment, self-regularization provides a label per point corresponding to the index of each matched pixel, thus leading to rich set of labels (feedback) in addition to the original one-shot label (hence high-bandwidth). We will revise the terminology to make it more understandable.\n\nb)  \\phi and \\psi are discussed below equation 2 in Section 3.1. \\psi is the hyper-column descriptor of the pixel using the encoding network while \\phi is an additional embedding of \\psi. When we are looking at symmetric encoding of the target and reference images, then \\phi = \\psi. However, we note that the reference and target images need not be the same (for example, we could try to align a 1-channel gray-scale image to a 3-channel color image). In this case, we need to project the two hyper-column pixel descriptors into the same space, which corresponds to \\phi being an embedding of \\psi.\n\nc) We meant to say M_{i,correspondingMatch(i)} = 1. M_{i, \\cdot, \\cdot} was a typo. ", "title": "Author Response to Reviewer 2"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1029/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311695619, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skl6k209Ym", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1029/Authors", "ICLR.cc/2019/Conference/Paper1029/Reviewers", "ICLR.cc/2019/Conference/Paper1029/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1029/Authors", "ICLR.cc/2019/Conference/Paper1029/Reviewers", "ICLR.cc/2019/Conference/Paper1029/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311695619}}}, {"id": "S1gHbfVFaQ", "original": null, "number": 2, "cdate": 1542173180712, "ddate": null, "tcdate": 1542173180712, "tmdate": 1542173180712, "tddate": null, "forum": "Skl6k209Ym", "replyto": "BkxJlR5ahX", "invitation": "ICLR.cc/2019/Conference/-/Paper1029/Public_Comment", "content": {"comment": "Thank you for the review. Your summary captures the essence of our paper. \n", "title": "Author Response to Reviewer 1"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1029/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311695619, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skl6k209Ym", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1029/Authors", "ICLR.cc/2019/Conference/Paper1029/Reviewers", "ICLR.cc/2019/Conference/Paper1029/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1029/Authors", "ICLR.cc/2019/Conference/Paper1029/Reviewers", "ICLR.cc/2019/Conference/Paper1029/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311695619}}}, {"id": "Hye4nbNtaX", "original": null, "number": 1, "cdate": 1542173100504, "ddate": null, "tcdate": 1542173100504, "tmdate": 1542173100504, "tddate": null, "forum": "Skl6k209Ym", "replyto": "Bye9YWnphm", "invitation": "ICLR.cc/2019/Conference/-/Paper1029/Public_Comment", "content": {"comment": "Thank you for the comments. \n\n1) (i) Our method computes an approximate, sampled matching between two images, not an exact matching as computing an exact matching while enforcing locality constraints is computationally difficult. We hence make the independent point matching assumption. Given the full image and a point, let us assume there is an oracle that performs the matching. In this case, the independent point matching assumption is reasonable as the oracle will assign P_{i,j,k}=1 when the correct matching is done for a given pixel i and 0 otherwise. In our method, points are given increasing context as we use the features of higher layers, so given enough filters at each layer, an oracle would essentially have access to the full image using the hyper-column description of a particular point. Self-regularization trains the system to learn to perform the matching task of the oracle. Hence, we expect the approximate solution to be good enough to be able to perform the original task - one-shot learning, and still learning a reasonable matching.\n\n\n(ii) If we assume that (i) gets us close to oracle level performance, then log P_{i,j,k} -> 0 when the correct matching is found for pixel i and log P_{i,j,k} << 0 otherwise. The similarity between the hyper-column feature vectors is expected to be high (and is trained to be high with self-regularization). With the independence assumption, since the image alignment score = \\sum_i max_{j,k} log P_{i,j,k}, we expect that score -> 0 when the matching is correct and score << 0 already when there are a few misalignments. We expect most of the probability mass to be concentrated around the correct alignment due to the hyper-column representation of points. \n\n2) When it comes to self-regularization, it does not matter as much whether 10-20 sampling is used or if the full alignment is computed, as long as the target pixels are also selected in the reference set (for convenience of determining the self-alignment matching matrix). As the regularization is selected as the categorical loss, we have for an image I, S = {I}\n\nself-regularization loss = CrossEntropyLoss(P(M | I) , Identity) = \\sum_i CrossEntropyLoss(P(M_{i,*,1} |I), 1_i)\u00a0 where 1_i is a vector with 1 at position i and 0 everywhere else. In other words, we want P(M_{i,j,1}|I) = 1 if i=j and 0 otherwise. Here, P(M_{i,j,1} | I, S) is proportional to C_{i,j}(I,I) as in Equation (2).\n\nIn our experiments, we used the 10% sampling during self-regularization as well. We could have sampled an additional 10% for the \"reference\" portion of the self-alignment, but this has marginal benefit as we are already contrasting the possible locations for a pixel across 10% of the image.\n\n3) The justification for the sampling is that as we are performing independent alignment (ie the net alignment cost is the sum of the alignment costs of the individual pixels), a reasonable pixel sampling will well approximate the alignment score of the full image. If the images are misaligned (or can't be aligned), we expect a reasonable fraction of the pixels to have poor alignment scores. ", "title": "Author Response to Reviewer 3"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1029/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311695619, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "Skl6k209Ym", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1029/Authors", "ICLR.cc/2019/Conference/Paper1029/Reviewers", "ICLR.cc/2019/Conference/Paper1029/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1029/Authors", "ICLR.cc/2019/Conference/Paper1029/Reviewers", "ICLR.cc/2019/Conference/Paper1029/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311695619}}}, {"id": "Bye9YWnphm", "original": null, "number": 3, "cdate": 1541419393597, "ddate": null, "tcdate": 1541419393597, "tmdate": 1541533482192, "tddate": null, "forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1029/Official_Review", "content": {"title": "Sound empirical study", "review": "The authors propose a deep learning method based on image alignment to perform one-shot classification and open-set recognition. The proposed model is an extension of Matching Networks [Vinyals et al., 2016] where a different image embedding is adopted and a pixel-wise alignment step between test and reference image is added to the architecture. \n\nThe work relies on two strong assumptions: (i) to consider each point mapping as independent, and (ii) to consider the correct alignment much more likely than the incorrect ones. The manuscript doesn\u2019t report arguments in favour of these assumptions. The motivation is partially covered by your statement \u201cmarginalizing over all possible matching is intractable\u201d, nevertheless an explanation of why it is reasonable to introduce these assumptions is not clearly stated.\n\nThe self-regularization allows the model to have a performance improvement, and it is considered one of the contribution of this work. Nevertheless the manuscript doesn\u2019t provide a detailed explanation on how the self regularization is designed. For example it is not clear whether the 10% and 20% pixel sampling is applied also during self regularization.\n\nThe model is computationally very expensive and force the use of only 10% of the target image pixels and 20% of the reference images\u2019 pixels. The complexity is intrinsic of the pixel-wise alignment formulation, but in any case this approximation is a relevant approximation that is never justified. The use of hyper column descriptors is an effective workaround to achieve good performance even though this approximation. The discussion is neglecting to argue this aspect.\n\nOne motivation for proposing an alignment-based matching is a better explanation of results. The tacit assumption of the authors is that a classifier driven by a point-wise alignment may improve the interpretation. The random uniformly distributed subsampling of pixels makes the model less interpretable.It may occur for example as shown in figure 3 where the model finds some points that for human interpretation are not relevant and at the same time these points are matched with points that have some semantic meaning.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1029/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1029/Official_Review", "cdate": 1542234322204, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1029/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335857162, "tmdate": 1552335857162, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1029/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkxJlR5ahX", "original": null, "number": 2, "cdate": 1541414374899, "ddate": null, "tcdate": 1541414374899, "tmdate": 1541533481984, "tddate": null, "forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1029/Official_Review", "content": {"title": "A new way of learning key point correspondence which can reflect visual concept", "review": "This paper proposed a new way of learning point-wise alignment of two images, and based on this idea, one-shot classification and open-set recognition can be further improved. The idea is interesting. As a human, when we say two images are similar, we may compare them locally and globally in our mind. However, traditional CNN models do not make direct comparisons. And this work give a good direction to further improve this motivation.\n\nThe paper is well written and easy to understand. \n\nFor the experiments, MNIST, Omniglot and MiniImageNet are used to demonstrate the effectiveness of the proposed method. From Figure 2. we can see many interesting correspondences.  ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1029/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1029/Official_Review", "cdate": 1542234322204, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1029/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335857162, "tmdate": 1552335857162, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1029/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SylnZdjD37", "original": null, "number": 1, "cdate": 1541023748246, "ddate": null, "tcdate": 1541023748246, "tmdate": 1541533481781, "tddate": null, "forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1029/Official_Review", "content": {"title": "Review for ABM-Nets", "review": "In this work, the authors tackle the problem of few-shot learning and open-set classification using a new type of NNs which they call alignment-based matching networks or ABM-Nets for short. They main idea is to benefit from binary maps between the query image and the support set (for the case of few-shot learning for the sake of discussion here) to guide the similarity measure. \n\nI have quite a few concerns;\n\n- After reading the paper two times, I still couldn't find a clear explanation as how the binary map C is constructed. The paper says the cost of M,i,j,k = 1 is C. So what exactly happens given I_t and I_k. My understanding is that a vector representation of each image is obtained and then from those representations the matrix C is constructed (maybe an outer product or something). This does not come out clearly. \n\n- Nevertheless, I suspect if such a construction (based on my understanding) is the right approach. Firstly, I guess the algorithm should somehow encourage to match more points between the images. Right now the loss  does not have such a term so hypothetically you can match two images because they just share a red pixel which is obviously not right. \n\n- Aside from the above (so basically regularizing norm(C) somehow), one wonders why matching a point to several others (as done according to matrix C) is the right choice. \n\n- Related to the issues mentioned before, I may also complain that matching far away points might not be ideal. Currently I do not see how this can be avoided nor a solid statement as why this should not be a problem.  \n\n\n- Another comment is how the alignment here differs from attention models? They surely resemble each-other though the alignment seems not that rich.\n\n\n-  last but not least, I have found the language confusing. Some examples,\n   -p2 bandwidth signal than the traditional label-only signal : I am very confused by how bandwidth comes to the picture and how this can be measured/justified\n\n  - fig.1, what are \\phi and \\psi. paper never discussed these.\n\n  - the authors say M is a tensor with 3dimensions. Then the marginalization before eq.1 uses M_{i,\\cdot,\\cdot} = 1 . What does this really mean?\n\n    ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1029/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition", "abstract": "Deep learning for object classification relies heavily on convolutional models. While effective, CNNs are rarely interpretable after the fact. An attention mechanism can be used to highlight the area of the image that the model focuses on thus offering a narrow view into the mechanism of classification. We expand on this idea by forcing the method to explicitly align images to be classified to reference images representing the classes. The mechanism of alignment is learned and therefore does not require that the reference objects are anything like those being classified. Beyond explanation, our exemplar based cross-alignment method enables classification with only a single example per category (one-shot). Our model cuts the 5-way, 1-shot error rate in Omniglot from 2.1\\% to 1.4\\% and in MiniImageNet from 53.5\\% to 46.5\\% while simultaneously providing point-wise alignment information providing some understanding on what the network is capturing. This method of alignment also enables the recognition of an unsupported class (open-set) in the one-shot setting while maintaining an F1-score of above 0.5 for Omniglot even with 19 other distracting classes while baselines completely fail to separate the open-set class in the one-shot setting.", "keywords": [], "authorids": ["pareshmg@csail.mit.edu", "tommi@csail.mit.edu"], "authors": ["Paresh Malalur", "Tommi Jaakkola"], "pdf": "/pdf/b2aff749b17fb4801b3c2c8e8546b79f74c3b1d7.pdf", "paperhash": "malalur|alignment_based_mathching_networks_for_oneshot_classification_and_openset_recognition", "_bibtex": "@misc{\nmalalur2019alignment,\ntitle={Alignment Based Mathching Networks for One-Shot Classification and Open-Set Recognition},\nauthor={Paresh Malalur and Tommi Jaakkola},\nyear={2019},\nurl={https://openreview.net/forum?id=Skl6k209Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1029/Official_Review", "cdate": 1542234322204, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Skl6k209Ym", "replyto": "Skl6k209Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1029/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335857162, "tmdate": 1552335857162, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1029/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}