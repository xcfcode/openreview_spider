{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124445681, "tcdate": 1518471875268, "number": 305, "cdate": 1518471875268, "id": "ByoT9Fkvz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "ByoT9Fkvz", "signatures": ["~Luke_Metz1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Learning to Learn Without Labels", "abstract": "A major goal of unsupervised learning is for algorithms to learn representations of data, useful for subsequent tasks, without access to supervised labels or other high-level attributes. Typically, these algorithms minimize a surrogate objective, such as reconstruction error or likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect (e.g. semi-supervised classification). In this work, we propose using meta-learning to learn an unsupervised learning rule, and meta-optimize the learning rule directly to produce good representations for a desired task. Here, our desired task (meta-objective) is the performance of the representation on semi-supervised classification, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations that perform well under this meta-objective. We examine the performance of the learned algorithm on several datasets and show that it learns useful features, generalizes across both network architectures and a wide array of datasets, and outperforms existing unsupervised learning techniques.\n", "paperhash": "metz|learning_to_learn_without_labels", "keywords": ["Meta-Learning", "unsupervised learning", "learning to learn"], "_bibtex": "@misc{\n  metz2018learning,\n  title={Learning to Learn Without Labels},\n  author={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\n  year={2018},\n  url={https://openreview.net/forum?id=ByoT9Fkvz}\n}", "authorids": ["lmetz@google.com", "nirum@google.com", "nirum@google.com", "bcheung@berkeley.edu"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We present a method to meta-learn an algorithm for unsupervised learning.", "pdf": "/pdf/d6eeda73a41ab273bcfdb5bcca40299568fcac65.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582725348, "tcdate": 1520677753319, "number": 1, "cdate": 1520677753319, "id": "rkWKQ4btz", "invitation": "ICLR.cc/2018/Workshop/-/Paper305/Official_Review", "forum": "ByoT9Fkvz", "replyto": "ByoT9Fkvz", "signatures": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer2"], "content": {"title": "Well-written paper, good to accept.  ", "rating": "7: Good paper, accept", "review": "This paper introduces an unsupervised meta learning approach by learning the update rules in the unsupervised phase and transfer them to the target tasks. The framework consists of 3 elements: a meta objective, a base model and a set of learned update rules, each of which is also a neural network. In the unsupervised learning phase, the set of update rules is trained to optimize the base model by minimizing the meta objective. Then, the learned update rules are used to optimize new models during supervised learning.\n\nIn the experiments, the update rules and the base model are trained on a variant of Imagenet and CIFAR 10. Then, the base model is used to generate representation to perform low shot learning on various datasets. The results show that classifiers trained on such embedding are consistently better than random initialization and slightly better than VAE initialization. \n\nIn addition, the set of update rules are applied to optimize networks with various architectures. The results show that the update rules can optimize networks that are bigger and deeper than the base model and exploiting the additional capacity although they are only trained to optimize the base model during the meta training phase.\n\nThe paper also shows that their method can transfer knowledge from image to text domain, but the accuracy tends to drop with more training iteration due to domain mismatch.  Authors can conduct more extensive analysis, such as why the base model can generalize from image to text, what are the common features that the base model has learned to accomplish this, and how to avoid domain mismatch. Finally, it will be important to explain the MetaObjective in A.3. It is a bit unclear how to obtain the target label y given it is an unsupervised learning approach. \n\nOverall, this is a well-written paper with interesting idea and positive results for acceptance. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn Without Labels", "abstract": "A major goal of unsupervised learning is for algorithms to learn representations of data, useful for subsequent tasks, without access to supervised labels or other high-level attributes. Typically, these algorithms minimize a surrogate objective, such as reconstruction error or likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect (e.g. semi-supervised classification). In this work, we propose using meta-learning to learn an unsupervised learning rule, and meta-optimize the learning rule directly to produce good representations for a desired task. Here, our desired task (meta-objective) is the performance of the representation on semi-supervised classification, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations that perform well under this meta-objective. We examine the performance of the learned algorithm on several datasets and show that it learns useful features, generalizes across both network architectures and a wide array of datasets, and outperforms existing unsupervised learning techniques.\n", "paperhash": "metz|learning_to_learn_without_labels", "keywords": ["Meta-Learning", "unsupervised learning", "learning to learn"], "_bibtex": "@misc{\n  metz2018learning,\n  title={Learning to Learn Without Labels},\n  author={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\n  year={2018},\n  url={https://openreview.net/forum?id=ByoT9Fkvz}\n}", "authorids": ["lmetz@google.com", "nirum@google.com", "nirum@google.com", "bcheung@berkeley.edu"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We present a method to meta-learn an algorithm for unsupervised learning.", "pdf": "/pdf/d6eeda73a41ab273bcfdb5bcca40299568fcac65.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582725121, "id": "ICLR.cc/2018/Workshop/-/Paper305/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper305/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper305/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper305/AnonReviewer1"], "reply": {"forum": "ByoT9Fkvz", "replyto": "ByoT9Fkvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper305/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582725121}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582698224, "tcdate": 1520710990656, "number": 2, "cdate": 1520710990656, "id": "SywLShbtM", "invitation": "ICLR.cc/2018/Workshop/-/Paper305/Official_Review", "forum": "ByoT9Fkvz", "replyto": "ByoT9Fkvz", "signatures": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer3"], "content": {"title": "Promising and original idea ", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This work presents a new approach to unsupervised representation learning with neural nets. The unsupervised learning problem is seen as a meta-learning problem where an update rule for the base model is learnt through a supervised meta-objective (e.g. linear regression fit on mini-batches in the experiments). Results obtained on few-shot classification shows competitive or better performance compared to the VAEs based approach. The proposed approach appears promising and original. \n\nQuestions and comments: \n\nThe paper is dense and relatively difficult to follow, though it might be hard to do better with the space constraints. \n\nIn equation (3), why consider the sum over time steps instead of the last step? It would be good to spell out what theta exactly in this case.\n\nIt would also be good to define what neutron local means.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn Without Labels", "abstract": "A major goal of unsupervised learning is for algorithms to learn representations of data, useful for subsequent tasks, without access to supervised labels or other high-level attributes. Typically, these algorithms minimize a surrogate objective, such as reconstruction error or likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect (e.g. semi-supervised classification). In this work, we propose using meta-learning to learn an unsupervised learning rule, and meta-optimize the learning rule directly to produce good representations for a desired task. Here, our desired task (meta-objective) is the performance of the representation on semi-supervised classification, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations that perform well under this meta-objective. We examine the performance of the learned algorithm on several datasets and show that it learns useful features, generalizes across both network architectures and a wide array of datasets, and outperforms existing unsupervised learning techniques.\n", "paperhash": "metz|learning_to_learn_without_labels", "keywords": ["Meta-Learning", "unsupervised learning", "learning to learn"], "_bibtex": "@misc{\n  metz2018learning,\n  title={Learning to Learn Without Labels},\n  author={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\n  year={2018},\n  url={https://openreview.net/forum?id=ByoT9Fkvz}\n}", "authorids": ["lmetz@google.com", "nirum@google.com", "nirum@google.com", "bcheung@berkeley.edu"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We present a method to meta-learn an algorithm for unsupervised learning.", "pdf": "/pdf/d6eeda73a41ab273bcfdb5bcca40299568fcac65.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582725121, "id": "ICLR.cc/2018/Workshop/-/Paper305/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper305/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper305/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper305/AnonReviewer1"], "reply": {"forum": "ByoT9Fkvz", "replyto": "ByoT9Fkvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper305/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582725121}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582644246, "tcdate": 1520803202831, "number": 3, "cdate": 1520803202831, "id": "HkiKpM7FM", "invitation": "ICLR.cc/2018/Workshop/-/Paper305/Official_Review", "forum": "ByoT9Fkvz", "replyto": "ByoT9Fkvz", "signatures": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer1"], "content": {"title": "meta-learning without label (good idea to explore)", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposed a meta-learning method with unlabeled data. The paper argues that unsupervised learning has been suffering from not properly defined objective. This paper proposes to combine Mata-learning with unsupervised learning to make sure that the learned representation is meaningful.  \nI enjoy this idea. However, this work is still in its early stage. \n\nDetailed Comments: \n\n1. It is maybe a bit over claiming to say \"without labels\" since the method does need few data with labels for the linear regression.\n2. In the paper, the meta objective is only applied among two mini-batches of data. When it comes to many mini batches, how does it work? Will it encounter some side-effects?\n3. I am not sure whether the experiments are fair. Meta-learning uses more data than supervised learning, right? They just share the same amount of labeled data. \n4. Please explain more about the \"over-fit\" in the second part of 3.1. This is a rather challenging task. How does the method over-fit?\n5. Typos in the paper, such as \"rain\"-> \"train\"", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn Without Labels", "abstract": "A major goal of unsupervised learning is for algorithms to learn representations of data, useful for subsequent tasks, without access to supervised labels or other high-level attributes. Typically, these algorithms minimize a surrogate objective, such as reconstruction error or likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect (e.g. semi-supervised classification). In this work, we propose using meta-learning to learn an unsupervised learning rule, and meta-optimize the learning rule directly to produce good representations for a desired task. Here, our desired task (meta-objective) is the performance of the representation on semi-supervised classification, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations that perform well under this meta-objective. We examine the performance of the learned algorithm on several datasets and show that it learns useful features, generalizes across both network architectures and a wide array of datasets, and outperforms existing unsupervised learning techniques.\n", "paperhash": "metz|learning_to_learn_without_labels", "keywords": ["Meta-Learning", "unsupervised learning", "learning to learn"], "_bibtex": "@misc{\n  metz2018learning,\n  title={Learning to Learn Without Labels},\n  author={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\n  year={2018},\n  url={https://openreview.net/forum?id=ByoT9Fkvz}\n}", "authorids": ["lmetz@google.com", "nirum@google.com", "nirum@google.com", "bcheung@berkeley.edu"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We present a method to meta-learn an algorithm for unsupervised learning.", "pdf": "/pdf/d6eeda73a41ab273bcfdb5bcca40299568fcac65.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582725121, "id": "ICLR.cc/2018/Workshop/-/Paper305/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper305/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper305/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper305/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper305/AnonReviewer1"], "reply": {"forum": "ByoT9Fkvz", "replyto": "ByoT9Fkvz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper305/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper305/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582725121}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573553865, "tcdate": 1521573553865, "number": 47, "cdate": 1521573553528, "id": "rJ53C0Ctz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "ByoT9Fkvz", "replyto": "ByoT9Fkvz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Learn Without Labels", "abstract": "A major goal of unsupervised learning is for algorithms to learn representations of data, useful for subsequent tasks, without access to supervised labels or other high-level attributes. Typically, these algorithms minimize a surrogate objective, such as reconstruction error or likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect (e.g. semi-supervised classification). In this work, we propose using meta-learning to learn an unsupervised learning rule, and meta-optimize the learning rule directly to produce good representations for a desired task. Here, our desired task (meta-objective) is the performance of the representation on semi-supervised classification, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations that perform well under this meta-objective. We examine the performance of the learned algorithm on several datasets and show that it learns useful features, generalizes across both network architectures and a wide array of datasets, and outperforms existing unsupervised learning techniques.\n", "paperhash": "metz|learning_to_learn_without_labels", "keywords": ["Meta-Learning", "unsupervised learning", "learning to learn"], "_bibtex": "@misc{\n  metz2018learning,\n  title={Learning to Learn Without Labels},\n  author={Luke Metz and Niru Maheswaranathan and Brian Cheung and Jascha Sohl-Dickstein},\n  year={2018},\n  url={https://openreview.net/forum?id=ByoT9Fkvz}\n}", "authorids": ["lmetz@google.com", "nirum@google.com", "nirum@google.com", "bcheung@berkeley.edu"], "authors": ["Luke Metz", "Niru Maheswaranathan", "Brian Cheung", "Jascha Sohl-Dickstein"], "TL;DR": "We present a method to meta-learn an algorithm for unsupervised learning.", "pdf": "/pdf/d6eeda73a41ab273bcfdb5bcca40299568fcac65.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}