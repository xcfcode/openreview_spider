{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1362494700000, "tcdate": 1362494700000, "number": 3, "id": "VC6Ay131A-y1w", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "KHMdKiX2lbguE", "replyto": "KHMdKiX2lbguE", "signatures": ["Kyunghyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear reviewer (d5d4),\r\n\r\nThank you for your thorough review and comments.\r\n \r\n - 'the paper fails to compare against robust Boltzmann machines (Tang et al.,\r\n   CVPR 2012)'\r\n   \r\n   Thanks for pointing it out, and I agree that the RoBM be tried as well. It           \r\n   will be possible to use the already trained GRBMs to initialize an RoBM to  \r\n   see how much improvement the RoBM can bring.        \r\n \r\n - 'More thorough analysis and better training might ... make the conclusion\r\n   more convincing.'\r\n   \r\n   One of the main claims in this paper was to show that a family of Boltzmann\r\n   machines is a potential alternative to denoising autoencoders which have\r\n   recently been proposed and shown to excel in image denoising. Also, another\r\n   was that it is possible to perform *blind* image denoising where no prior \r\n   information on noise types and levels was available at the training time. For\r\n   this, I have only conducted the limited set of experiments that barely\r\n   confirms these claims.\r\n   \r\n   I fully agree that follow-up research/experiments will reveal more insights  \r\n   into the effect of model structures, training procedures and the choice of\r\n   training sets on the performance of image denoising. \r\n \r\n - 'How did you tune the hyperparameters?'\r\n   \r\n   This was one question to which I was not able to find a clear answer. Since\r\n   the task I considered was completely *blind*, meaning not even types of test\r\n   images were not known, I had to resort to using the reconstruction error on\r\n   the validation image patches, which, I believe, is not a good indicator of\r\n   the generalization performance in this case. \r\n   \r\n   I agree that more investigation is definitely required in this matter of             \r\n   validation in image denoising.                                                       \r\n \r\n - 'whether the authors faithfully implemented Xie et al.\u2019s method'                     \r\n   \r\n   The training procedure used in this paper is slightly different from the one         \r\n   used by Xie et al. The procedure is also different from how Burger et al.            \r\n   trained denoising autoencoders. Comparison to their trained models (for\r\n   instance, Burger et al. made their learned models parameters available onlin)        \r\n   will be one of the potential next steps in this research."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.", "pdf": "https://arxiv.org/abs/1301.3468", "paperhash": "cho|boltzmann_machines_and_denoising_autoencoders_for_image_denoising", "authors": ["KyungHyun Cho"], "authorids": ["kyunghyun.cho@aalto.fi"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362486600000, "tcdate": 1362486600000, "number": 4, "id": "CIGoQSPKoZIKs", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "KHMdKiX2lbguE", "replyto": "KHMdKiX2lbguE", "signatures": ["anonymous reviewer d5d4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Boltzmann Machines and Denoising Autoencoders for Image Denoising", "review": "A brief summary of the paper's contributions, in the context of prior work.\r\nThe paper proposed to use Gaussian deep Boltzmann machines (GDBM) for image denoising tasks, and it empirically compared the denoising performance to another state-of-the-art method based on stacked denoising autoencoders (Xie et al.). From empirical evaluations, the author confirms that deep learning models (DBM or DAE) achieve good performance in image denoising. Although DAE performs better than GDBM in many cases, GDBM can be still useful for image denoising since it doesn\u2019t require prior knowledge on the types or levels of noise.\r\n\r\n\r\nAn assessment of novelty and quality.\r\nThe main contribution of the paper is the use of Gaussian DBM for denoising. It also provides comparison against existing models (stacked denoising autoencoders). Although, technical novelty is limited, it is still interesting that GRBM without the knowledge of specific noise (in target tasks) can perform well for image denoising. \r\n\r\nOne major problem is that the paper fails to compare against a closely related work on robust Boltzmann machines (Tang et al., CVPR 2012), which is specifically designed for denoising tasks. \r\n\r\nConclusions drawn from empirical evaluation seem fairly reasonable, but not very surprising. Also, the results look somewhat random. More thorough analysis and better training might clean up the results and make the conclusion more convincing.\r\n\r\n\r\nOther comments:\r\nHow did you tune the hyperparameters (l2 regularization, learning rate, number of hidden nodes, etc.) of the model? The trained model is sensitive to these hyperparameters, so it should have been tuned to some validation task.\r\n\r\n\r\nA list of pros and cons (reasons to accept/reject).\r\npros:\r\n- Empirical evaluation of two deep models on image denoising tasks seems to confirm the usefulness of deep learning methods for image denoising.\r\n- It\u2019s very interesting that models trained from natural images (CIFAR-10) work well for unrelated images. \r\n\r\ncons:\r\n- The main contribution of the paper is the use of GRBM/DBM for denoising. However, it\u2019s not clear whether GRBM/DBMs are better than DAE(4).\r\n- There is no comparison against robust Boltzmann machines (Tang et al., CVPR 2012). \r\n- It would have been nice to make the results comparable to other published work (e.g., Xie et al.). The results in the paper raise questions about whether the authors faithfully implemented Xie et al.\u2019s method."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.", "pdf": "https://arxiv.org/abs/1301.3468", "paperhash": "cho|boltzmann_machines_and_denoising_autoencoders_for_image_denoising", "authors": ["KyungHyun Cho"], "authorids": ["kyunghyun.cho@aalto.fi"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362411780000, "tcdate": 1362411780000, "number": 1, "id": "ppSEYjkaMGYj5", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "KHMdKiX2lbguE", "replyto": "KHMdKiX2lbguE", "signatures": ["Kyunghyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear reviewers (bf00) and (9120),\r\n\r\nFirst of all, thank you for your thorough reviews. \r\n\r\nPlease, find my response to your comments below. A revision\r\nof the paper that includes the fixes made accordingly will\r\nbe available at the arXiv.org tomorrow (Tue, 5 Mar 2013\r\n01:00:00 GMT).\r\n\r\nTo both reviewers (bf00) and (9120):\r\n\r\nThank you for pointing out the mistakes in some of the\r\nequations. As both of you noticed, there was a problem in\r\nEq. (4). There should be as many binary matrices $D_n$ as\r\nthere are image patches from each test image. This mistake\r\nhappened as I was trying to put the procedure into a more\r\ncompact mathematical equation. There was no mistake in the\r\nimplementation. I have fixed the equation and its\r\naccompanying text description accordingly.\r\n\r\nAlso, in Eq. (5), the term inside the last expectation\r\nshoudl be p(v | h) instead of p(\tilde{v} | h). Thank you\r\nagain for pointing that out. \r\n\r\n\r\nTo reviewer (bf00):\r\n\r\n - 'The experiments could be not easily reproduced'\r\n   I have added the detailed configurations used for\r\n   training each model as an appendix. \r\n\r\n - 'models were not compared with any state-of-the-art\r\n   denoising method'\r\n   The aim of the paper was to propose an alternative deep\r\n   neural network model that might be used in place of\r\n   denoising autoencoders which were recently proposed to\r\n   excel in the task of image denoising. However, I agree\r\n   that the comparison with other approaches would make the\r\n   paper more interesting.\r\n\r\n - 'should describe how to construct training set in detail'\r\n   I have added how the training set was constructed.\r\n\r\n - 'high-resolution natural image data sets could be more\r\n   proper'\r\n   I fully agreed with you and thank you for the suggestion.\r\n   I have run the same set of experiment using the training\r\n   set constructed from the Berkeley Segmentation Benchmark\r\n   (BSD-500). The results closely resemble those presented\r\n   already in the paper, and the overall trend did not\r\n   change. I have appended the new figure (same format as\r\n   Fig.  2) obtained using the new training set in the\r\n   appendix.\r\n\r\n\r\nTo reviewer (9120):\r\n\r\n - 'Proper layer-sizes cross validation should be performed'\r\n   I fully agree with you. One most important thing that\r\n   be checked, in my opinion, is the performance of\r\n   single-layer models having the same number of hidden\r\n   units as multi-layer models (e.g., GRBM with 640 and 1280\r\n   hidden units trained on 8x8 patches). I will run the\r\n   experiment, and if time permits, will add the results in\r\n   the paper.\r\n\r\n\r\n - 'In eq of hat{v}_i'\r\n   Thank you for pointing it out. I have mistakenly put\r\n   p(h|\tilde{v}), implicitly assuming a case of RBM with\r\n   binary hidden units, where p(h|\tilde{v}) coincides with\r\n   E[h|\tilde{v}]. However, for a general BM, you are\r\n   correct and I have fixed it accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.", "pdf": "https://arxiv.org/abs/1301.3468", "paperhash": "cho|boltzmann_machines_and_denoising_autoencoders_for_image_denoising", "authors": ["KyungHyun Cho"], "authorids": ["kyunghyun.cho@aalto.fi"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362361020000, "tcdate": 1362361020000, "number": 2, "id": "tO_8tX3y-7SXz", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "KHMdKiX2lbguE", "replyto": "KHMdKiX2lbguE", "signatures": ["anonymous reviewer 9120"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Boltzmann Machines and Denoising Autoencoders for Image Denoising", "review": "The paper conducts an empirical performance comparison, on the task of image denoising, where the denoising of large images is based on combining densoing of small patches. In this context, the study compares usign, as small patch denoisers, deep denoising autoencoders (DAE) versus deep Boltzmann machines with a Gaussian visible layer (GDBM, which correspond to GRBM for a single hidden layer). Compared to recent work on deep DAE for image denoising shown to be competitive with state-of-the-art methods (Burger et al. CVPR'2012; Xie et al. NIPS'2012) this work rather considers *blind* denoising tasks (test noise kind and level not the same as that used during training). For the DBM part, the work builds on the author's authors' GDBM (presented at NIPS 2011 workshop on deep learning), and performs denoising as the expectation of visibles given inferred expected first layer hidden obtained through varitional approximation. \r\n\r\nThe paper essentially draws the following observations \r\na) GRBM / GDBM can be equally successful at image denoising as deep DAEs, \r\nb) increased depth seems to help denoising, particularly at higher noise levels. \r\nc) interestingly a GRBM (single layer) appears often competitive compared to a GDBN with more layers (while deeper DAEs more systematically improve over single layer DAE).\r\n\r\nPros:\r\n+ I find it is a worthy empirical comparison study to make.\r\n+ it reasonably supports observation a), which is not too surprising (also there's no clear winner).\r\n+ the observation I find most interesting, and worthy of further *digging*  is c) as it could be, as suggested by the authors, a concrete effect of the limitations of the variational approximation in the GDBN.\r\n\r\nCons:\r\n- empirical performance comparison of similar models, but does not yield much insight regarding wherefrom differences may arise (no other sensitivity analysis except final denoising perofrmance)\r\n- while I would a priori be inclined to believe in b), I find the methodology lacking here. It seems a single fixed hiddden layer size has been considered, the same for all layers, so that deeper networks had necessarily more parameters. Proper layer-sizes cross validation should be performed before we can hope to draw a scientific conclusion with respect to the benefit of depth. \r\n- mathematical notation is often a little sloppy or buggy:\r\nEq 4: if D is n x d as claimed Dx will be n x 1, so it cannot correspond to n 'patches' as claimed (unless your patches are but 1 pixel). \r\nEq 5: I belive last p(\tilde{v}|h) should be p(v|h)\r\nNext eq: p(v | h=mu) is an abuse since h are binary.\r\nIn eq of hat{v}_i : p(h|\tilde{v}) is problematic, since there's no bound value for h. Shouldn't it rather be E(h|\tilde{v}) ?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.", "pdf": "https://arxiv.org/abs/1301.3468", "paperhash": "cho|boltzmann_machines_and_denoising_autoencoders_for_image_denoising", "authors": ["KyungHyun Cho"], "authorids": ["kyunghyun.cho@aalto.fi"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362189720000, "tcdate": 1362189720000, "number": 5, "id": "PLgu8d4J3rRz9", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "KHMdKiX2lbguE", "replyto": "KHMdKiX2lbguE", "signatures": ["anonymous reviewer bf00"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Boltzmann Machines and Denoising Autoencoders for Image Denoising", "review": "This paper is an empirical comparison of the different models (Boltzmann Machines and Denoising Autoencoders) on the task of image denoising. Based on the experiments the authors claimed the increasing model depth improves the denoising performances when the level of noise is high.\r\n\r\nPROS\r\n+ Exploring DBMs for images denosing is indeed interesting and important. \r\nCONS\r\n- There is little novelty in this paper. \r\n- The experiments could be not easily reproduced since some important details of the experimental setting are not provided (see below).\r\n- The proposed models were not compared with any state-of-the-art denoising method.\r\n\r\nDetailed comments\r\nPage 4: The authors should explicitly specify how they constructed the matrix D.\r\nPage 4: There could be some  mistakes in equation 5. Please list the detailed derivation. \r\nPage 4: Equation 5 is not a standard routine. You should firstly make an assumption about noise, for example, $\tilde{v}=v+n,nsim mathcal{N}(mu,,sigma^2)$. Then $P(\tilde{v}|v)=\frac{P(\tilde{v}|v)P(v)}{P(\tilde{v})}propto P(\tilde{v}|v)P(v)$. \r\nPage 6: Some high-resolution natural image data sets (ImageNet or Berkeley Segmentation Benchmark) could be more proper than CIFAR-10 for this denoising task.\r\nPage 6: The authors should describe how to construct training set in detail."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.", "pdf": "https://arxiv.org/abs/1301.3468", "paperhash": "cho|boltzmann_machines_and_denoising_autoencoders_for_image_denoising", "authors": ["KyungHyun Cho"], "authorids": ["kyunghyun.cho@aalto.fi"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358345700000, "tcdate": 1358345700000, "number": 66, "id": "KHMdKiX2lbguE", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "KHMdKiX2lbguE", "signatures": ["kyunghyun.cho@aalto.fi"], "readers": ["everyone"], "content": {"title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "decision": "conferencePoster-iclr2013-workshop", "abstract": "Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.", "pdf": "https://arxiv.org/abs/1301.3468", "paperhash": "cho|boltzmann_machines_and_denoising_autoencoders_for_image_denoising", "authors": ["KyungHyun Cho"], "authorids": ["kyunghyun.cho@aalto.fi"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 6}