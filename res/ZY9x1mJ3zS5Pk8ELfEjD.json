{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458171486215, "tcdate": 1458171486215, "id": "gZ9vAQVn8tAPowrRUAZq", "invitation": "ICLR.cc/2016/workshop/-/paper/191/comment", "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "replyto": "71BErRxxQFAE8VvKUQqW", "signatures": ["~Volodymyr_Kuleshov1"], "readers": ["everyone"], "writers": ["~Volodymyr_Kuleshov1"], "content": {"title": "Clarifying a misunderstanding about the convexity of our objective", "comment": "First of all, thank you for the detailed review!\n\nWe agree with most of the comments. The only issue we would like to clarify is your concern about the correctness of our argument that the log-likelihood bound is concave.\n\nOur argument goes as follows:\n1. A concave, positive function is also log-concave (because if g is concave and non-decreasing, and f is concave, then g(f(x)) is concave)\n2. A sum of exponential families is concave and positive, hence log-concave.\n\nIn particular, we don't claim that a sum of log-concave functions is log-concave. We were running out of space, and our so explanation was a bit terse. Apologies for the confusion. We will make things clearer in the final version.\n\nIf this was your main concern, then it would be great if you could update your score; if there are other issues, please let us know, and we'll be happy to clarify!\n\nThanks,\nVolodymyr"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neural Variational Random Field Learning", "abstract": "We propose variational bounds on the log-likelihood of an undirected probabilistic graphical model p that are parametrized by flexible approximating distributions q. These bounds are tight when q = p, are convex in the parameters of q for interesting classes of q, and may be further parametrized by an arbitrarily complex neural network. When optimized jointly over q and p, our bounds enable us to accurately track the partition function during learning.", "pdf": "/pdf/ZY9x1mJ3zS5Pk8ELfEjD.pdf", "paperhash": "kuleshov|neural_variational_random_field_learning", "conflicts": ["mcgill.ca"], "authorids": ["kuleshov@stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Volodymyr Kuleshov", "Stefano Ermon"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455832711000, "ddate": null, "super": null, "final": null, "tcdate": 1455832711000, "id": "ICLR.cc/2016/workshop/-/paper/191/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/191/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457637226622, "tcdate": 1457637226622, "id": "71BErRxxQFAE8VvKUQqW", "invitation": "ICLR.cc/2016/workshop/-/paper/191/review/11", "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "replyto": "ZY9x1mJ3zS5Pk8ELfEjD", "signatures": ["ICLR.cc/2016/workshop/paper/191/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/191/reviewer/11"], "content": {"title": "review of \"neural variational random field learning\"", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes to estimate the log-partition function of Markov random fields by optimizing a variational bound over a model of proposal distributions. For the proposal distributions the authors consider uniform mixtures of exponential families. \n\nThe paper is concise and presents interesting ideas. A central idea of the paper is to define a proposal model for which the variational bound is convex. \nUnfortunately the arguments seem to be flawed or to require modifications. \n\nCOMMENTS\nIn Section 3.2. ``One can easily check that a non-negative concave function is also ... are in the exponential family, it follows that $\\sum_k \\pi_k q_{\\phi_k}(x)$ is log-concave, and hence the above expression is convex.'' \n\nThis appears faulty. \nFor an exponential family $q_{\\phi_k}(x)$ is log-concave in the natural parameters. \nHowever, a sum of log-concave functions is not necessarily log-concave. \nIn fact, one finds examples of sums of exponential families that are not entry-wise log-concave in the natural parameters. \nKindly verify / explain / improve this. \n\nOTHER COMMENTS \nIt may be worthwhile to consider the convexity problem in relation to ``convex exponential families'' and ``mixtures of exponential families with disjoint supports''. For these kinds of models, the maximizers of the likelihood function can be expressed in closed form or in terms of those of the individual mixture components. \n\nMINOR COMMENTS \n* It would be good to mention whether x is discrete or continuous, scalar or vector. Also whether $\\theta$ is finite dimensional. \n* In Section 2 ``closed-form expression'' is confusing. Given that $I$ is intractable, it seems that $I^2$ is also intractable. \n* In Section 2 variance of the estimate $\\hat I$, a $1/n$ factor seems to be missing. \n* In Section 2 the variance vanishes when $p=q$ can be inferred from the fact that $w(x)=I$ is constant, without using Jensen's inequality. \n* In Section 3 ``natural algorithm for computing'' should be ``estimating''. \n* In Section 3 ``as minimizing a tight upper bound'' should not say ``tight''. \n* In Section 3 ``This approach is complicated by the fact that unlike earlier methods that parametrized conditional distributions $q(z|x)$ over hidden variables $z$, our setting does not admit a natural input/output to a neural network.'' Why do you need an input/output in order to use a neural network? Maybe I just don't understand this sentence. \n* In Section 3.2 $\\pi_k$ has not been introduced. I suspect this is just $1/K$? \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neural Variational Random Field Learning", "abstract": "We propose variational bounds on the log-likelihood of an undirected probabilistic graphical model p that are parametrized by flexible approximating distributions q. These bounds are tight when q = p, are convex in the parameters of q for interesting classes of q, and may be further parametrized by an arbitrarily complex neural network. When optimized jointly over q and p, our bounds enable us to accurately track the partition function during learning.", "pdf": "/pdf/ZY9x1mJ3zS5Pk8ELfEjD.pdf", "paperhash": "kuleshov|neural_variational_random_field_learning", "conflicts": ["mcgill.ca"], "authorids": ["kuleshov@stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Volodymyr Kuleshov", "Stefano Ermon"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580185606, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580185606, "id": "ICLR.cc/2016/workshop/-/paper/191/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "replyto": "ZY9x1mJ3zS5Pk8ELfEjD", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/191/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457621834825, "tcdate": 1457621834825, "id": "MwVkVqMN1Iqxwkg1t7Wx", "invitation": "ICLR.cc/2016/workshop/-/paper/191/review/10", "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "replyto": "ZY9x1mJ3zS5Pk8ELfEjD", "signatures": ["ICLR.cc/2016/workshop/paper/191/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/191/reviewer/10"], "content": {"title": "Review of \"Neural Variational Random Field Learning\"", "rating": "7: Good paper, accept", "review": "This paper presents a simple and intuitive approach to variational inference for undirected graphical models. The paper is concise, and the bound is derived quite simply based on the variance of an importance sampler that estimates the marginal likelihood. (This unbiased estimate brings to mind pseudomarginal samplers.) The linearity bound on the log expectation also makes sense in order to derive a lower bound which produces unbiased stochastic gradients during optimization.\n\nI am concerned however with the scalability of the approach to both larger data and higher dimensions (the preliminary experiment is very small in both respects). The approach seems to share the downfalls of importance sampling in general. It is unclear the extent to which recent adaptive importance sampling techniques applied to variational inference are practical (unlike Burda et al. (2016) for example, there is not a baseline to say that at the least, it does not produce a worse bound than the typical KL(p ||q)).\n\nNevertheless, this is an interesting direction worth exploring for undirected graphical models, and the proposed directions using expressive variational models and recent importance sampling techniques make sense.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neural Variational Random Field Learning", "abstract": "We propose variational bounds on the log-likelihood of an undirected probabilistic graphical model p that are parametrized by flexible approximating distributions q. These bounds are tight when q = p, are convex in the parameters of q for interesting classes of q, and may be further parametrized by an arbitrarily complex neural network. When optimized jointly over q and p, our bounds enable us to accurately track the partition function during learning.", "pdf": "/pdf/ZY9x1mJ3zS5Pk8ELfEjD.pdf", "paperhash": "kuleshov|neural_variational_random_field_learning", "conflicts": ["mcgill.ca"], "authorids": ["kuleshov@stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Volodymyr Kuleshov", "Stefano Ermon"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580185924, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580185924, "id": "ICLR.cc/2016/workshop/-/paper/191/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "replyto": "ZY9x1mJ3zS5Pk8ELfEjD", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/191/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457325764130, "tcdate": 1457325764130, "id": "WL9MDrjwMI5zMX2Kf2KA", "invitation": "ICLR.cc/2016/workshop/-/paper/191/review/12", "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "replyto": "ZY9x1mJ3zS5Pk8ELfEjD", "signatures": ["ICLR.cc/2016/workshop/paper/191/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/191/reviewer/12"], "content": {"title": "review of \"Neural variational random field learning\"", "rating": "7: Good paper, accept", "review": "This paper presents an intriguing idea for training MRFs (or CRFs) together with an inference network by training the inference net to minimize a variational upper bound on the partition function. Unlike standard variational inference, this bound is in the right direction for learning. Compared with methods like tree-reweighted BP, this method potentially allows for more accurate bounds, as the approximating distribution can be made arbitrarily close to the true one.\n\nI'm uncomfortable with referring to (1) as a \"variational upper bound\" on the partition function, since the stochastic estimate is an upper bound only in expectation, and may underestimate the true value with overwhelming probability. (E.g., consider the case where q is uniform and p is peaked.) Similarly, (2) is technically a lower bound on the likelihood, but when estimated with samples from q, it may overestimate the true value with overwhelming probability. I'm not sure what the proper wording would be, but I think the abstract over-promises as currently written.\n\nAs the authors point out, Monte Carlo estimates of (1) and (2) could have very high variance, just like importance sampling based estimates of Z. I suspect this problem would be very hard to overcome on full-size models. \n\nStill, the idea is quite neat, and could be the basis of future work on training MRFs/CRFs. I would certainly recommend acceptance to the workshop track.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Neural Variational Random Field Learning", "abstract": "We propose variational bounds on the log-likelihood of an undirected probabilistic graphical model p that are parametrized by flexible approximating distributions q. These bounds are tight when q = p, are convex in the parameters of q for interesting classes of q, and may be further parametrized by an arbitrarily complex neural network. When optimized jointly over q and p, our bounds enable us to accurately track the partition function during learning.", "pdf": "/pdf/ZY9x1mJ3zS5Pk8ELfEjD.pdf", "paperhash": "kuleshov|neural_variational_random_field_learning", "conflicts": ["mcgill.ca"], "authorids": ["kuleshov@stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Volodymyr Kuleshov", "Stefano Ermon"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580185476, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580185476, "id": "ICLR.cc/2016/workshop/-/paper/191/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "replyto": "ZY9x1mJ3zS5Pk8ELfEjD", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/191/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455832702907, "tcdate": 1455832702907, "id": "ZY9x1mJ3zS5Pk8ELfEjD", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "ZY9x1mJ3zS5Pk8ELfEjD", "signatures": ["~Volodymyr_Kuleshov1"], "readers": ["everyone"], "writers": ["~Volodymyr_Kuleshov1"], "content": {"CMT_id": "", "title": "Neural Variational Random Field Learning", "abstract": "We propose variational bounds on the log-likelihood of an undirected probabilistic graphical model p that are parametrized by flexible approximating distributions q. These bounds are tight when q = p, are convex in the parameters of q for interesting classes of q, and may be further parametrized by an arbitrarily complex neural network. When optimized jointly over q and p, our bounds enable us to accurately track the partition function during learning.", "pdf": "/pdf/ZY9x1mJ3zS5Pk8ELfEjD.pdf", "paperhash": "kuleshov|neural_variational_random_field_learning", "conflicts": ["mcgill.ca"], "authorids": ["kuleshov@stanford.edu", "ermon@cs.stanford.edu"], "authors": ["Volodymyr Kuleshov", "Stefano Ermon"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 5}