{"notes": [{"id": "ofhTYDzB7N", "original": null, "number": 1, "cdate": 1585333167155, "ddate": null, "tcdate": 1585333167155, "tmdate": 1585333167155, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "Bkx2OPjKir", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Public_Comment", "content": {"title": "Questions on the backcast mechanism", "comment": "I am a student currently reading this paper and I do have some questions on the backcast mechanism. I understand your idea of feeding the next block i+1 by the residual x_i - x_i^hat produced by block i. However, why it\u2019s necessary to use x_i - x_i^hat as the residual rather than y_i - y_i^hat. After all, the latter one is more common in previous studies in other sub fields like boosting or pure statistical time series. Besides, creating a x_i^hat purposely must introduce more parameters and complexity of computation. Could the gain of choosing x_i -x_i^hat compensate for that? Another concern is that in order to make sure your essential idea of backcast topology work, you have to guarantee that x_i^hat and y_i^hat have some kind of correspondence. How did you do that? I know the \u2018forecasting materials\u2019 theta_i^f and theta_i^b are based on the same h_i,4. This could be regard as a kind of correspondence between x_i^hat and y_i^hat. Whereas, more strict mathematically, you should make sure that the linear_i^f and linear_i^b used to map h_i,4 must be the \u2018inverse\u2019 to each other. \u2018Inverse\u2019 is not the strict one used in linear algebra. Just wanna say that your paper seems have no mechanism to make sure x_i^hat is fully correspondent to y_i^hat.  I read your reply to reviewer#2 and it seems you haven\u2019t done an experiment using y_i-y_i^hat as the residual rather than x_i-x_i^hat. The idea of creating a x_i^hat to figure out what has been digested by block i and then feed the leftovers to block i+1 looks fancy. Intuitively, I believe this setting works but I really need your further explanation to convince myself. Thanks!"}, "signatures": ["~MENGYUE_ZHA1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~MENGYUE_ZHA1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504213590, "tmdate": 1576860589361, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Public_Comment"}}}, {"id": "r1ecqn4YwB", "original": "r1xMK6TyPH", "number": 125, "cdate": 1569438866050, "ddate": null, "tcdate": 1569438866050, "tmdate": 1583912020361, "tddate": null, "forum": "r1ecqn4YwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "FNpR7ygHS", "original": null, "number": 1, "cdate": 1576798688124, "ddate": null, "tcdate": 1576798688124, "tmdate": 1576800946942, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "r1ecqn4YwB", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper received positive recommendation from all reviewers. Accept.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1ecqn4YwB", "replyto": "r1ecqn4YwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715445, "tmdate": 1576800265358, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper125/-/Decision"}}}, {"id": "HkxhvW1H5S", "original": null, "number": 3, "cdate": 1572299108394, "ddate": null, "tcdate": 1572299108394, "tmdate": 1575087215022, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "r1ecqn4YwB", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper proposes a DL architecture that achieves better performance on time series prediction. The proposed architecture is relatively straightforward and composes residual blocks. While the paper does achieve superior results, a lot of the text is devoted to comparing to prior work and arguing that DL approaches can do better than hand-crafted approaches, instead of focussing on the importance of specific technical contributions made in the paper. \n\nMy main concerns are: \n(a) The main technical idea in this paper is the use of back-casting and forecasting (i.e. doubly residual connections). However, no ablations are provided to show how important doubly residual connections are. What is the performance, if the DL architecture is kept exactly the same, except for:\n    (i) No residual connections in backcasting (simply feed in the overall input to every block) \n    (ii) In addition to (i), make no backcasting predictions\n\n(b) Given no architectural details of the previous ML methods are provided, its unclear if the current architecture is better because it has more parameters or it is indeed the doubly residual idea that is important. \n\n(c) Authors make a point about interpretability \u2014 but interpretability is only achieved using domain specific knowledge. I am not really sure, what is so novel about this. \n\nOverall, this seems to be a good applications paper which has been optimized for performance. As a research paper, the contributions are less clear. Answers to my concerns above will help clarify. Given my current concerns, I cannot recommend this paper to be accepted. \n\n--------------------------\nPost Rebuttal: \n\nAuthors have addressed several of my concerns by providing detailed ablations and the paper reads much better. I still have some concerns: \n(a) Interpretability in this work is obtained by using standard basis functions used by the TS community. Such basis functions can be used with any architecture, not just the doubly residual connection networks. Also, its common for people to make use of domain specific knowledge, for e.g. predicting spectrograms when analyzing audio data. In these cases, yes predictions are interpretable. Not sure, what is the contribution that authors are claiming. \n\n(b) The connection to meta-learning seems to be a bit mis-leading. The authors claim that all the weights of the network form the outer loop and the stage-wise predictions of \\theta corresponds to the inner loop. Can I simply not think of \\theta as features? In that case, this logic will apply to any deep neural network. Not sure what exactly authors want to say. Under this view all deep networks are doing meta-learning. I might have misunderstood something here -- please correct me, if so. \n\nI am changing my rating to weal accept, but I also hope the authors will address the concerns outlined above.  \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper125/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1ecqn4YwB", "replyto": "r1ecqn4YwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576458581156, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper125/Reviewers"], "noninvitees": [], "tcdate": 1570237756702, "tmdate": 1576458581168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Review"}}}, {"id": "SJghIrYk9r", "original": null, "number": 1, "cdate": 1571947859563, "ddate": null, "tcdate": 1571947859563, "tmdate": 1574536531434, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "r1ecqn4YwB", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This goal of this paper is to present a strong empirical result showing that a \"pure\" machine learning based method can outperform all known methods on some of the most challenging time series forecasting benchmarks (TOURISM, M3 and especially M4). Since I am not from the field of forecasting, I can not be sure of this, but from my understanding these benchmark datasets are indeed challenging and the cited references back up the claims of the paper related to these datasets being important in the field. \n\nOn the most challenging dataset (M4), the best known performing method combines RNNs with a traditional smoothing algorithm. The model proposed in this paper outperforms it without being combined with any classical approach, though it does utilize ensembling.\nThe experimental setup is sound in my opinion, and the result appears to be of high potential significance.\nHowever, despite trying to go through Section 3 multiple times, the exact model architecture is not clear to me. Due to this reason, my current decision is a weak rejection since the model is a central contribution of the paper. I will be happy to increase my score if the authors can make the model description crystal clear.\n\nEven though my expertise is deep neural network architectures, I find it hard to follow the descriptions in Sec. 3. I faced the most difficulty understanding section 3.1, which obviously made the rest of subsections even harder to follow. Here are my main points of confusion:\n\n- One big issue is that the paper uses an illustration (Fig. 1) to explain the architecture instead of equations, but then uses symbols in the main text that do not appear on Fig. 1 at all such as g_theta. Is the \"FC Stack (4 layers)\" g_theta? \n\n- Where are (uppercase) phi functions? I could infer that these are the \"FC\" blocks but they should be labeled.\n\n- The Figure has the symbols g^b_theta and g^f_theta that do not appear in the text description. What exactly do they do? And is the theta that parameterizes each of them the same theta that parameterizes g_theta? How is this possible if g_theta is the \"FC Stack (4 layers)\"?\n\n- The description in second and third paragraph of Section 3.1 is very confusing and unclear. It should be replaced or augmented with equations using clearly defined symbols that match Figure 1.\n\n- More confusion stems from the use of the term \"parameters\" in (I believe) a different context than is used in neural networks, where \"parameters\" refers to connection weights. But here parameters are outputs of some functions, so either they are not connection weights or this is a fast-weight style architecture where outputs are weights [1], in which this should be made clear.\n\n- Design of the doubly residual architecture in Section 3.2 makes sense to me at a high level, but I feel it is still very hard to clearly understand and implement it. Again, use of equations to clearly define the computation would be very helpful.\n\n\n[1] Schmidhuber, J\u00fcrgen. \"Learning to control fast-weight memories: An alternative to dynamic recurrent networks.\" Neural Computation 4.1 (1992): 131-139.\n\n\n--- Update after rebuttal ---\n\nI am happy to see the paper greatly improved by the authors in their updates. My concerns related to the presentation of the model have been addressed, and I find the architecture much easier to understand. I also appreciate the detailed supplementary material that is likely to help readers interested in the area. Related to the areas I work in, I noticed the following missing references:\n\nDensenets: Lang, K.J. and Witbrock, M.J., 1988, June. Learning to tell two spirals apart. In Proceedings of the 1988 connectionist models summer school (No. 1989, pp. 52-59).\n\nMetalearning: Schmidhuber, J., 1987. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook (Doctoral dissertation, Technische Universit\u00e4t M\u00fcnchen).\n\nWhile improving papers is generally the objective of the rebuttal phase, I suggest that the authors to not take this as an opportunity to submit unpolished papers in the first phase. That said, I have increased my rating to reflect my satisfaction with the current version of the paper.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper125/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1ecqn4YwB", "replyto": "r1ecqn4YwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576458581156, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper125/Reviewers"], "noninvitees": [], "tcdate": 1570237756702, "tmdate": 1576458581168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Review"}}}, {"id": "B1xVpwzUiB", "original": null, "number": 8, "cdate": 1573427131581, "ddate": null, "tcdate": 1573427131581, "tmdate": 1573743343936, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "Sye4p4tVir", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We would like to sincerely thank Reviewer 1 once again for providing additional feedback on the placement of equations as well as for the insightful comments posted in the review. We have included the additional equations directly in the revised section 3, as advised by the Reviewer. We believe that we fully addressed the concerns raised by Reviewer 1 and this significantly improved the clarity of the paper in general, and Section 3 specifically. At the same time, we would be more than happy to further adjust the paper should Reviewer 1 have any additional comments, questions or concerns. Our more detailed response is provided below.\n\n1. We made sure that the notation in Figure 1 is aligned with the rest of the paper. Any differences between the figure and the text are explicitly pointed out and explained in the text.\n\n2. The exact nature of basis functions g is explained via equations to make it clear.\n\n3. The entire architecture is explained in detail via mathematical equations, both at the block level and at the higher level of doubly residual topology.\n\n4. We made clear that \\theta are the expansion coefficients. As Reviewer 1 mentioned, \\theta are not connection weights, and calling them parameters may be confusing.  Therefore, instead of referring to \\theta as parameters, we refer to them as expansion coefficients throughout the paper now."}, "signatures": ["ICLR.cc/2020/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper125/Authors|ICLR.cc/2020/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176039, "tmdate": 1576860556212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment"}}}, {"id": "Hke5AhiwjB", "original": null, "number": 9, "cdate": 1573530834301, "ddate": null, "tcdate": 1573530834301, "tmdate": 1573743120856, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "BkxaWAkZ9S", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "First of all, we would like to thank Reviewer 3 for the thorough review of the manuscript and for insightful questions. We believe that we fully addressed the concerns raised by Reviewer 3 in the revised manuscript and in our detailed response provided below. At the same time, we will be more than happy to provide additional clarifications and to introduce extra modifications to the manuscript should Reviewer 3 have any additional comments, questions or concerns. \n\nIn our experiments we have not observed significant difference in performance between the interpretable and the generic model configurations across different datasets. Most of the time, the forecasts of the two models are close and the forecasting errors are strongly correlated. This is clear, for example, from Appendix D1, Tables 11 and 12 (revised manuscript) where we can see that ensembling the interpretable and generic model only results in a marginal overall gain. Thus statistically speaking the outputs of the interpretable and the generic models are equivalent. At the same time, there are plenty of examples in which FORECAST-I and FORECAST-G deviate noticeably. This is expected, because ultimately the two models are different and thus they cannot be expected to produce exactly the same outputs. For example, the interpretable model uses weight sharing across stacks and its basis is constrained to trend in the first stack and Fourier basis in the second one; the generic model does not employ weight sharing and it relies on learnable basis layers. On top of that, even ensembles of models of the same type, e.g. two generic ensembles that were trained on different random batch sequences and different random initializations will have noticeably different, but statistically equivalent outputs. The inductive bias based on trend and seasonality may, of course, reduce the range of time series types for which the interpretable model will be effective. For example, time series with no regular patterns or trends will likely be better handled by the generic model (or an interpretable model of a different kind). At the same time, our empirical results on 3 different datasets containing time series of very different nature imply that the proposed interpretable model is broadly applicable. The range of time series forecasting scenarios where interpretable model based on trend and seasonality is effective appears to be sufficiently broad and includes many cases of practical interest.\n\nIn order to address the second question of Reviewer 3, we added Appendix F in the revised manuscript in which we included tables (Tables 17\u2013 22 in the revised manuscript) with the numerical values of time series depicted in Figure 2.  We made sure that the names of signals in Fig. 2 legends and in the table columns match, such that they can easily be cross-referenced. It is clear from Tables 17\u2013 22 that (i) traces STACK1-I and STACK2-I sum up to trace FORECAST-I, (ii) traces STACK1-G andSTACK2-G sum up to trace FORECAST-G, (iii) traces FORECAST-I and FORECAST-G are overall very similar even though their components may significantly differ from each other. This is expected and there is nothing wrong with it. First, it is expected that there may be many pairs of very different curves whose sums look very similar. Second, we conjecture that in a few cases in Fig.2 the perceived difference between -I and -G components may arise due to plot scaling. The small absolute magnitude signals shown at full scale in the plots may create a false impression that they should have a considerable impact on the combined line, but in reality they only contribute level shift. For example, Tables 19 (Fig. 2, row 3, Monthly) and 22 (Fig.2, row 6, Hourly) may be good illustrations of this behaviour. We hope that the tables with numerical values help to clarify this matter.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper125/Authors|ICLR.cc/2020/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176039, "tmdate": 1576860556212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment"}}}, {"id": "Bkx2OPjKir", "original": null, "number": 11, "cdate": 1573660532289, "ddate": null, "tcdate": 1573660532289, "tmdate": 1573679912992, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "HkxhvW1H5S", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "First of all, we would like to sincerely thank Reviewer 2 again for providing a thorough insightful review. We believe that we fully addressed the concerns raised by the Reviewer in the revised manuscript and in our detailed responses below. At the same time, we will be happy to address any additional questions and concerns that Reviewer 2 may have. We briefly summarize our previous message for clarity (please refer to the message dated 08 Nov 2019 for more details). To address (a,b) we conducted an additional ablation study to confirm the effectiveness of the doubly residual topology with respect to several alternatives. We included the results of the study in Appendix C.3, Tables 7-10. These results demonstrate the effectiveness of the proposed doubly residual topology proving that the proposed architecture provides gain that cannot be achieved by simply increasing the number of parameters. \n\nLooking at this from another angle and to additionally address (b): the proposed architecture allows to more efficiently use a fixed number of parameters. The conclusion is supported by the ablation study presented in the original manuscript. In Table 6, rows 3 and 4, the interpretable architecture is compared in configurations with 1 and 3 blocks per stack, respectively. Since the interpretable architecture shares all the parameters in blocks within a stack, the number of parameters in configurations 1 block per stack and 3 blocks per stack is actually the same! Yet, stacking blocks with exactly the same parameters turns out to be beneficial. We believe this is an interesting finding in itself and this is what we alluded to in Appendix A discussing the connections of N-BEATS to meta-learning paradigm.\n\nTo address the concern raised by the Reviewer 2 in (c) we would like to point out that the proposed approach to interpretability is novel, because (i) we propose a novel generic architectural approach that is very flexible and it can operate both in interpretable and non-interpretable regimes, (ii) we devise a concrete novel recipe on exactly how the proposed architecture can be configured to make its outputs interpretable and (iii) we empirically demonstrate that both generic and interpretable configurations have comparable performance across 3 different datasets.\n\nAdditionally, Reviewer 2 characterized our paper as \"applications paper which has been optimized for performance\". We believe that our paper proposes a novel architecture and is focused on proving that it is generically applicable to solving TS forecasting, rather than on optimizing its performance.  Concretely, our results are based on 3 different well established datasets. The datasets contain time series (TS) of very different nature and the size of datasets is drastically different (100,000; 3003 and 1311 time series). We applied N-BEATS to the datasets in two different configurations (interpretable and generic) and we kept all the architectural hyperparameters (number of blocks, layer width, number of layers, etc.) fixed at the same values (please refer to Table 12) across datasets and datasets' different subsets, varying significantly in their properties. N-BEATS still achieved SOTA on all of the datasets under these constraints. Since the submission of the manuscript we have significantly extended the empirical results by applying N-BEATS to 2 more datasets in the same architectural configuration and again achieved SOTA. We have not added those results to the revision to avoid creating impression that we add irrelevant material instead of focusing on addressing the points raised by Reviewers. However, we will gladly add those to a new revision if we are explicitly instructed to do so by Reviewers or Area Chairs. \n\nFinally, we would like to address the point about spending a lot of space to compare to prior work. The mission of our paper is to help to build a bridge between ML and TS forecasting. One of the objectives of our study is to fix a long standing problem pointed out by Makridakis et al., 2018a: \"... we hope that those in the field of AI and ML will accept the empirical findings and work to improve the forecasting accuracy of their methods. A problem with the academic ML forecasting literature is that the majority of published studies provide forecasts and claim satisfactory accuracies without comparing them with simple statistical methods or even naive benchmarks.\" To address it, we proposed a novel deep architecture, we used the datasets and error metrics established by the TS community and we convincingly demonstrated that ML is a promising tool for solving TS forecasting problems. On the other hand, in addition to the novel architecture, we expose the ML community to an important yet underrated problem of TS forecasting whose business impact ranges in millions of dollars per percent of accuracy (Kahn, 2003), to new datasets and a more rigorous evaluation methodology based on comparison to all known prior art, including classical models. "}, "signatures": ["ICLR.cc/2020/Conference/Paper125/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper125/Authors|ICLR.cc/2020/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176039, "tmdate": 1576860556212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment"}}}, {"id": "Sye4p4tVir", "original": null, "number": 6, "cdate": 1573323963595, "ddate": null, "tcdate": 1573323963595, "tmdate": 1573323963595, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "H1ei-PvyjS", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment", "content": {"title": "Suggestion", "comment": "Dear authors,\n\nI think equations and corresponding diagrams to explain the model as clearly as possible should be presented together.  Of course, since you are working with limited space, you may need to shorten some other sections or the diagrams, and then add larger versions of those to the appendices.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper125/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper125/Authors|ICLR.cc/2020/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176039, "tmdate": 1576860556212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment"}}}, {"id": "BJx_NbbQir", "original": null, "number": 4, "cdate": 1573224752225, "ddate": null, "tcdate": 1573224752225, "tmdate": 1573224752225, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "HkxhvW1H5S", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment", "content": {"title": "Ablation studies addressing concerns (a) and (b)", "comment": "We addressed the concerns (a) and (b) raised by Reviewer 2 by conducting an additional ablation study to confirm the effectiveness of the proposed doubly residual topology. The results of the ablation experiment are summarized in Appendix C.3 DOUBLY RESIDUAL STACKING. \n\nWe considered the following alternatives to the original architecture (N-BEATS-DRESS) proposed in the original draft. \n\nPARALLEL is depicted in Fig. 4b. This is the alternative where the backward residual connection is\ndisabled and the overall model input is fed to every block. The blocks then forecast in parallel using\nthe same input and their individual outputs are summed to make the final forecast.\n\nNO-RESIDUAL is depicted in Fig. 4c. This is the alternative where the backward residual connection\nis disabled. Unlike PARALLEL, in this case the backcast forecast of the previous block is fed as input\nto the next block. Unlike the usual feed-forward network, in the NO-RESIDUAL architecture, each\nblock makes a partial forecast and their individual outputs are summed to make the final forecast.\n\nLAST-FORWARD is depicted in Fig. 4d. This is the alternative where the backward residual\nconnection is active, however the model level forecast is derived only from the last block. So, the\npartial forward forecasts are disabled. This is the architecture that is closest to the classical residual\nnetwork.\n\nNO-RESIDUAL-LAST-FORWARD is depicted in Fig. 4e. This is the alternative where both\nbackward residual and the partial forward connections are disabled. This is therefore a simple\nfeed-forward network, but very deep.\n\nTables 7-10 in Appendix C.3 present the quantitative results comparing these architectural alternatives to the proposed N-BEATS configuration (N-BEATS-DRESS).  The results demonstrate that the doubly residual stacking topology provides a clear overall advantage over the alternative architectures in which backcast residual links or the partial forward forecast links or both of those are disabled.\n\nWe highly value the time and effort Reviewer 2 has invested in reviewing the draft and we would sincerely appreciate additional feedback from Reviewer 2 regarding the revised version of the paper including additional ablation studies."}, "signatures": ["ICLR.cc/2020/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper125/Authors|ICLR.cc/2020/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176039, "tmdate": 1576860556212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment"}}}, {"id": "Hyxr-nwyiB", "original": null, "number": 3, "cdate": 1572989949478, "ddate": null, "tcdate": 1572989949478, "tmdate": 1572989976932, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "BkxaWAkZ9S", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment", "content": {"title": "More discussion of Figure 2 will be added", "comment": "We would like to sincerely thank Reviewer 3 for the thorough analysis of the manuscript and for insightful feedback. We will extend the discussion of Figure 2 to thoroughly address the concerns raised by the reviewer. \n\nIn the meantime, we would like to clarify if  we should interpret \"the line for FORECAST-I and FORECAST-G deviates much\" that the forecast provided by FORECAST-I is different from the forecast provided by FORECAST-G?"}, "signatures": ["ICLR.cc/2020/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper125/Authors|ICLR.cc/2020/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176039, "tmdate": 1576860556212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment"}}}, {"id": "H1ei-PvyjS", "original": null, "number": 2, "cdate": 1572988675477, "ddate": null, "tcdate": 1572988675477, "tmdate": 1572988675477, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "SJghIrYk9r", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment", "content": {"title": "Equations will be added", "comment": "We would like to sincerely thank Reviewer 1 for the thorough analysis of the manuscript and for insightful feedback. We will add equations to describe the operation of the architecture in more detail, as suggested by the Reviewer. In the meanwhile, we would like to ask Reviewer's advice on the placement of the equations. Would it be better to have a separate section with mathematical equations in the appendix, or embedding the equations directly in the text of Section 3 is a better choice?"}, "signatures": ["ICLR.cc/2020/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper125/Authors|ICLR.cc/2020/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176039, "tmdate": 1576860556212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment"}}}, {"id": "rkxJOVv1oH", "original": null, "number": 1, "cdate": 1572988006940, "ddate": null, "tcdate": 1572988006940, "tmdate": 1572988006940, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "HkxhvW1H5S", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment", "content": {"title": "Details of experimental setup", "comment": "We would like to sincerely thank Reviewer 2 for the thorough analysis of the manuscript and for insightful feedback. We are committed to fully address the comments raised by the reviewer, including conducting the additional experiments. Our detailed response will follow as soon as the experimental results are obtained. In the meantime, we would like to ask the reviewer to kindly clarify the desired experimental setup. We have the following interpretations of the experimental setup and we would like to confirm which of our interpretations is correct. We apologize for any misunderstanding and we hope that Reviewer 2 may provide necessary clarifications.\n\n**Interpretation 1**\nWe interpret the setup in point (a) (i) as follows: Residual connections are disabled, and all blocks are fed the same overall input. We assume that the overall input refers to the model input. The blocks then forecast in parallel and their individual outputs are summed to make the final forecast.\nIn this interpretation, the setup described in point (a) (ii) is unclear to us: (a) (i) already seems to imply that the backcast predictions are not made.\n\n**Interpretation 2**\nWe interpret the setup in point (a) (i) as follows: residual connections are disabled and the backcast connection of the previous block is fed as the input to the next block. The model input is only fed in the first block.\nIn this interpretation, (a) (i) implies that the backcast connections are the only links between successive blocks. So, we interpret (a) (ii) as follows: the backcast connections are not made. Instead, the overall model input is fed into each block. The blocks then forecast in parallel and their individual outputs are summed to make the final forecast."}, "signatures": ["ICLR.cc/2020/Conference/Paper125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1ecqn4YwB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper125/Authors|ICLR.cc/2020/Conference/Paper125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176039, "tmdate": 1576860556212, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper125/Authors", "ICLR.cc/2020/Conference/Paper125/Reviewers", "ICLR.cc/2020/Conference/Paper125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Comment"}}}, {"id": "BkxaWAkZ9S", "original": null, "number": 2, "cdate": 1572040196975, "ddate": null, "tcdate": 1572040196975, "tmdate": 1572972635612, "tddate": null, "forum": "r1ecqn4YwB", "replyto": "r1ecqn4YwB", "invitation": "ICLR.cc/2020/Conference/Paper125/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper investigates a pure deep learning architecture for Univariate time series analysis by simply ensembling feed-forward networks, along with the residual stacking mechanism for fluid learning. Each of the generic block consists of 4 FC layers followed by the use of forward and backward predictor to have forecast and backcast output of the original input. These blocks forms the stacks, where each stack provides the residuals and the forecast responses further to the next stacks, which ultimately provide the global forecast. To make the internal stack outputs interpretable, assumptions are imposed on the trend model, which follows a polynomial function of time vector, and seasonality model which follows periodic Fourier series. Further, the ensembling of models based on different metrics and input windows is used for better accuracy.    \n\nVery well written paper and easy to follow. It advocates a pure DL framework (instead of hybrid statistical models and DL). I found the idea simple and effective, yielding results better than the previous approaches. Also, the experimental setup is well described. I also found the link between this work and meta-learning approaches interesting.\n\nHowever, I have several questions about the interpretability results. It looks like the inductive bias based on some general assumptions can fail in some cases. For example, in Fig. 2(a), the line for FORECAST-I and FORECAST-G deviates much in case of hourly/weekly/daily data frequency. What is the reason behind this? \n\nAlso, in Fig. 2, while the StackT-I (fig.2(d)) and StackS-I (fig.2(e)) provide response lines different from the counterparts in  Stack1-G (fig.2(b)) and Stack2-G (fig.2(c)), the summations in the combined line (fig.2(a)) yield similar curves of pretty much the same shapes, without much perceived difference. Is it expected or something is wrong?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper125/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper125/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["boris@elementai.com", "dmitri.carpov@elementai.com", "chapados@elementai.com", "yoshua.bengio@mila.quebec"], "title": "N-BEATS: Neural basis expansion analysis for interpretable time series forecasting", "authors": ["Boris N. Oreshkin", "Dmitri Carpov", "Nicolas Chapados", "Yoshua Bengio"], "pdf": "/pdf/8e7978f4e9295d1f68d2bc32a2643507a0d9546a.pdf", "TL;DR": "A novel deep interpretable architecture that achieves state of the art on three large scale univariate time series forecasting datasets ", "abstract": "We focus on solving the univariate times series point forecasting problem using deep learning. We propose a deep neural architecture based on backward and forward residual links and a very deep stack of fully-connected layers. The architecture has a number of desirable properties, being interpretable, applicable without modification to a wide array of target domains, and fast to train. We test the proposed architecture on several well-known datasets, including M3, M4 and TOURISM competition datasets containing time series from diverse domains. We demonstrate state-of-the-art performance for two configurations of N-BEATS for all the datasets, improving forecast accuracy by 11% over a statistical benchmark and by 3% over last year's winner of the M4 competition, a domain-adjusted hand-crafted hybrid between neural network and statistical time series models. The first configuration of our model does not employ any time-series-specific components and its performance on heterogeneous datasets strongly suggests that, contrarily to received wisdom, deep learning primitives such as residual blocks are by themselves sufficient to solve a wide range of forecasting problems. Finally, we demonstrate how the proposed architecture can be augmented to provide outputs that are interpretable without considerable loss in accuracy.", "keywords": ["time series forecasting", "deep learning"], "paperhash": "oreshkin|nbeats_neural_basis_expansion_analysis_for_interpretable_time_series_forecasting", "_bibtex": "@inproceedings{\nOreshkin2020N-BEATS:,\ntitle={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},\nauthor={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1ecqn4YwB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/a872edceb8a91ee81e87029e955b07a98d1bff82.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1ecqn4YwB", "replyto": "r1ecqn4YwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576458581156, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper125/Reviewers"], "noninvitees": [], "tcdate": 1570237756702, "tmdate": 1576458581168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper125/-/Official_Review"}}}], "count": 14}