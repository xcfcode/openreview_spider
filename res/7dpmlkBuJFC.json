{"notes": [{"id": "7dpmlkBuJFC", "original": "-REuZkcQTn", "number": 1989, "cdate": 1601308219045, "ddate": null, "tcdate": 1601308219045, "tmdate": 1616049862565, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "sPWa_Pjm3I", "original": null, "number": 1, "cdate": 1610040377645, "ddate": null, "tcdate": 1610040377645, "tmdate": 1610473970136, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "invitation": "ICLR.cc/2021/Conference/Paper1989/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "Based on the observation that the stochastic gradients for deep nets often stay in a low dimensional subspace, this paper proposes projected differential private SGD (DP-SGD) that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace. Under certain assumptions, the authors provide a theoretical analysis and empirical evaluations to show that the proposed algorithm can substantially improve the accuracy of DP-SGD in the high privacy regime. There is unanimous support to accept this paper after the author\u2019s response. Thus, I recommend accept."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "tags": [], "invitation": {"reply": {"forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040377632, "tmdate": 1610473970118, "id": "ICLR.cc/2021/Conference/Paper1989/-/Decision"}}}, {"id": "Y5UNRRaAdfw", "original": null, "number": 2, "cdate": 1603842070832, "ddate": null, "tcdate": 1603842070832, "tmdate": 1606782609954, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "invitation": "ICLR.cc/2021/Conference/Paper1989/-/Official_Review", "content": {"title": "New private SGD method to improve the utility", "review": "The paper proposes a new private SGD method by projecting the noise gradient onto a subspace, which is estimated using some public datasets. The proposed method can improve the utility guarantee by reducing the dependence of the problem dimension p. The idea of the proposed method is interesting, but the assumptions in the current paper seem to be very strong, and the evaluations in the current paper are not convincing to show that the proposed method is beneficial. \n1. It is not always true in practice that we can have access to the public dataset.\n2. In Theorem 4, the assumption about the principal component of the gradient looks very strong. \n3. How will the size of the public dataset affect the performance of the proposed method? It is important to have empirical evaluations on this.\n4. Another issue of the proposed method seems to be the computational barrier. Why can the proposed method only deal with 10000 samples in MNIST dataset compared with DPSGD, which can deal with the whole training dataset?\n5. I think the most valuable thing about the proposed method is to reduce the dependence of the problem dimension p. Therefore, it is very important for the authors to consider larger models such as resnet. If the proposed method can achieve better performance on larger models, I think the proposed method is very convincing. \n\n-----\nAfter reading the author response and other reviews, most of my concerns have been addressed. I would like to increase my score to 6. It would also be interesting to compare the performance of DP-SGD and PDP-SGD when $\\epsilon$ is relatively large as suggested by Figure 7.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1989/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1989/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106375, "tmdate": 1606915766193, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1989/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1989/-/Official_Review"}}}, {"id": "BpwasAobviN", "original": null, "number": 1, "cdate": 1603715750093, "ddate": null, "tcdate": 1603715750093, "tmdate": 1606400012503, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "invitation": "ICLR.cc/2021/Conference/Paper1989/-/Official_Review", "content": {"title": "Good theorem but experiments limit", "review": "The paper considers the problem of solving differentially private empirical risk minimization. To reduce the dependence on dimensionality $p$, they propose Projected DP-SGD (PDP-SGD) that projects the noisy gradients to a low-dimensional subspace computed from a free public dataset at each iteration. They prove that PDP-SGD is differentially private and has only a logarithmic dependence on $p$.\n\nPros:\n1. The paper is well written and easy to follow. \n2. It is very appreciated that authors analyze both convex and non-convex cases. All proof seems correct except for a minor mistake mentioned in the next part. What\u2019s more, The method of uniformly bounding $\\|M_t-\\Sigma_t\\|_2$ by generic chaining techniques may be an independent interest. \n\nCons and some discussion: \n1. The establishment of (28) requires $u$ is larger than a constant (like 3), even though this would not affect the correctness of Theorem 2. The last inequality of (55) should be equality.\n\nThe main concern of mine is that the experiment part is not so good as the theoretical part. \n\n2. The authors propose to reduce dimensionality by projection. The projected space varies for each iteration and is computed from a public dataset. However, I think the author didn\u2019t explain well the necessity of using an extra dataset. If reducing dimensionality is the most important, why not just use a random projection. [1] shows that a Johnson-Lindenstrauss transform preserves differential privacy, so it is natural to use random projection as a baseline (at least in experiments). However, the baseline considered in experiments excludes this easiest method. Besides, it is better to add non-private SGD as a baseline, which would help illustrate how the proposed method degrades accuracy.\n3. The proposed method has a huge space complexity and computation complexity as it requires to formulate $M_t \\in \\mathbb{R}^{p \\times p}$ and compute its top-$k$ eigenvectors at each iteration. It will require $O(p^2)$ space and $O(p^3)$ times. As a remedy, one may hope to reduce the frequency of eigenspace computation. It seems to make sense, since when $w_t$ starts to converge, the difference between consecutive $w_t$ is so small that the difference between consecutive $V(w_t)$ is also small, implying we can reuse the eigenspace estimated in the last iteration. It strikes me that it is better to explore such a heuristic method in experiments to further illustrate the usefulness of the proposed method.\n4. I think more datasets should be considered. The author only considered two image datasets. Real datasets like CIFAR can be considered. \n\n[1] Blocki, Jeremiah, et al. \"The johnson-lindenstrauss transform itself preserves differential privacy.\"\u00a02012 IEEE 53rd Annual Symposium on Foundations of Computer Science. IEEE, 2012.\n\n\n\n------------------------------------------------------------------------------------------\nI have read the authors\u2019 rebuttal. The authors have addressed most of my concerns. The JL random projection\u00a0baseline has been added and the heuristic method of reusing the eigenspace for some iterations has been explored, which I appreciate a lot. From the current experiments, the proposed method seems effective, though it will be more convincing if the method can be tested on larger models or harder datasets. \n\nI think the topic of the paper is quite interesting and the idea of bounding $M_t - \\Sigma_t$ uniformly is also interesting. The theory indeed manifests the effectiveness of the proposed method. As a result, I increase my point to 7.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1989/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1989/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106375, "tmdate": 1606915766193, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1989/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1989/-/Official_Review"}}}, {"id": "FtuLBsa-hh", "original": null, "number": 5, "cdate": 1606276072090, "ddate": null, "tcdate": 1606276072090, "tmdate": 1606276072090, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "invitation": "ICLR.cc/2021/Conference/Paper1989/-/Official_Comment", "content": {"title": "Common Response-Summary of Updates", "comment": "We thank all reviewers for your constructive suggestions and comments, which indeed help us improve the quality of this paper. We have submitted an updated version based on these comments and included additional experiments. Here, we summarize the updates of the revision. \n\n1) As suggested by Reviewer3, we added Johnson-Linderstrauss (JL) random projection with DP-SGD as a baseline in Figures 3 and 4 by using a Gaussian random projector.\n\n2) We explored a heuristic method suggested by Reviewer3 that we can reuse the eigenspace for some iterations to save the computation. Thus, we implemented PDP-SGD that updates the projector every s = 1, 10, 20 iterations, and we added the results in Figure 6 in the main paper. More results are deferred to the Appendix due to the space limitation. Those results show that PDP-SGD with a reduced eigenspace computation also achieves good performance and improves the accuracy over DP-SGD for a certain privacy level.\n\n3) As suggested by Reviewer2, \nto study whether the number of examples has any influence on the results, we added results for MNIST with 20000 samples, 50000 samples, and Fashion MNIST with 50000 samples in the main paper and the Appendix. We observed that PDP-SGD performs better than DP-SGD in the high privacy regime, which corresponds to small privacy loss $\\epsilon$. This is consistent with the results we reported using 10000 training samples. \n\n4) As suggested by Reviewer4, we added experiments with different sizes of public datasets to empirically study the effect of public data size.\nThe results are included in Figure 5(b) and additional results are added in the Appendix due to space limitation. \n\n5) As suggested by  Reviewer2, we added a discussion on the sample splitting method for bypassing the dependency issue with sample reuse in the first paragraph in Section 3.\n\n6) We have corrected typos and added more discussion with prior work. \n\n "}, "signatures": ["ICLR.cc/2021/Conference/Paper1989/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7dpmlkBuJFC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1989/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1989/Authors|ICLR.cc/2021/Conference/Paper1989/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853493, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1989/-/Official_Comment"}}}, {"id": "cEmL8PzdPX8", "original": null, "number": 4, "cdate": 1606275973214, "ddate": null, "tcdate": 1606275973214, "tmdate": 1606275973214, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": "BpwasAobviN", "invitation": "ICLR.cc/2021/Conference/Paper1989/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the constructive feedback. We are encouraged that the reviewer appreciates our uniform bound by generic chaining techniques. We have revised our paper and did more experiments based on your suggestions. Here we explain in detail how we address your comments. \n\n1. \u2018\u2019The establishment of (28) requires ...''\n\n We have corrected those typos.\n\n\n\n\n2. ''The necessity of using an extra dataset. If reducing dimensionality is the most important, why not just use a random projection.''\n\nThe reviewer has raised a very interesting point. We have discussed in the introduction that one could potentially implement the projection through private subspace identification on the private dataset, i.e., private PCA. However, to our best knowledge, all existing methods on private PCA have reconstruction error scaling with $\\sqrt{p}$ (Dwork et al., 2014), which will be propagated to the optimization error. \n\nWe have also added Johnson-Linderstrauss (JL) random projection with DP-SGD as a baseline in Figures 3 and 4 by using a Gaussian random projector. We observed that this approach in general has a much lower accuracy than our proposed method. We think this is because random projection introduces a larger subspace reconstruction error due to the random projector than the injected noise error reduced by projection. Also, we acknowledge that JL random projection with non-private SGD does not provide a formal privacy guarantee. The private JL projection method in [1] requires further Gaussian noise added to the rectangular diagonal matrix under the singular value decomposition of the data matrix. \n\n\n3. ''The proposed method has a huge space complexity and computation complexity. ''\n\nThanks to the structure of our problem, our proposed method actually does not suffer huge space and computational complexity.  We use the power method Lanczos in our experiments. In Lanczos, we compute the matrix-vector product $A^T v$, where $A$ can be written as $\\sum g_ig_i^T$ with individual stochastic gradient $g_i$. Thus, we do not have to explicitly store the matrix $A$. Also, the matrix-vector product can be decomposed into vector inner products:\n$$\nA^T v = \\sum g_ig_i^Tv = \\sum g_i(g_i^T v),\n$$\nwhich is a much faster computation than generic matrix-vector multiplication.\n\n\n4. ''the difference between consecutive $w_t$ is so small that the difference between consecutive $V(w_t)$ is also small, implying we can reuse the eigenspace''. \n\nWe appreciate the reviewer's input and we agree that it is a great idea to reuse the eigenspace if the subspaces are close for consecutive iterations. We have implemented this heuristic method, i.e., PDP-SGD that updates the projector every s = 1, 10, 20 iterations in our experiment and added results in Figure 6. Those results show that PDP-SGD with a reduced eigenspace computation also achieves good performance and improves the accuracy over DP-SGD for a certain privacy level.\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1989/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7dpmlkBuJFC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1989/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1989/Authors|ICLR.cc/2021/Conference/Paper1989/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853493, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1989/-/Official_Comment"}}}, {"id": "9nNaBQciQOu", "original": null, "number": 3, "cdate": 1606275630194, "ddate": null, "tcdate": 1606275630194, "tmdate": 1606275630194, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": "Y5UNRRaAdfw", "invitation": "ICLR.cc/2021/Conference/Paper1989/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for the insightful feedback. We are encouraged that the reviewer found our idea to be interesting. Here we explain how we address your concerns. \n\n1. \u2018\u2019It is not always true in practice that we can have access to the public dataset.\u2018\u2019\n\nThere is a long line of work that studies private data analysis with access to an auxiliary public dataset, i.e., Bassily et al., 2019b; 2020; Feldman et al., 2018; Avent et al., 2017; Papernot et al., 2017. This setting interpolates between private learning (where all examples are private) and classical learning (where all examples are public). Bassily et al., 2019b provide several motivations to consider this setting: First, in practice, it is feasible to collect some amount of \u201cpublic\u201c data that poses no privacy concerns. For example, in the language of consumer privacy, there is a considerable amount of data collected from the so-called \u201copt-in\u201d users, who voluntarily offer or sell their data to companies or organizations. There are also a variety of other sources of public data that can be harnessed. For example, the data collected by the US Census prior to the 2020 differential privacy deployment are publicly available. Leveraging (small) auxiliary public data has been a promising approach to differential privacy, especially since it enables data analyses that are impossible with only private data (e.g., releasing statistical queries with unbounded Littlestone dimension (Bassily et al., 2020)).\n\n\n2. \u2018\u2019In Theorem 4, the assumption about the principal component of the gradient looks very strong. \u2018\u2019\n\nThe principal component of the gradient assumption is motivated and supported by the known observations in Li et al., 2020; Gur-Ari et al.,\n2018 that, in an over-parameterized regime, the set of sample gradients along the training trajectory is often contained in the top principal subspace. We also provided our empirical evaluation of this structure in terms of the eigenvalues of the gradient second moments matrix in Figure 1, which shows that the eigenvalues decay very fast from the largest to smallest and only a few eigenvalues corresponding to the top principal component dominate. Thus, we believe it is reasonable to assume the principal component dominates.\n\nBesides, our main result in Theorem 4, i.e., the error rate of the $\\ell_2$-norm of the principal component of the gradient holds without assuming the principal component dominates. We have explicitly stated in the paper that, with the additional assumption that the principal component of the gradient dominates, one can derive a further bound on the gradient norm. \n\n\n3. \u2018\u2019How will the size of the public dataset affect the performance of the proposed method? It is important to have empirical evaluations on this.''\n\nAs shown in the theoretical bound, i.e., with $m$ public samples, the subspace reconstruction error scales as $ \\frac{1}{\\sqrt{m}}$ ignoring other factors. This shows that the error will decrease as the size of the public dataset increases. We also added empirical evaluations in the paper (Figure 5 (b)) with different public dataset sizes. Due to the space limit, more results on this are added in the Appendix. Our results show that the train/test accuracy increases as public size increases from 50 to 150. We also added a discussion on this behavior that increasing $m$ helps to reduce the subspace reconstruction error as suggested by the theoretical bound.   \n\n\n4. \u2018\u2019Another issue of the proposed method seems to be the computational barrier. Why can the proposed method only deal with 10000 samples in MNIST dataset compared with DPSGD, which can deal with the whole training dataset?''\n\n\nWe have also added results for MNIST with 20000 samples, 50000 samples, and Fashion MNIST with 50000 samples in the revision. \nResults have been added to the main paper in Figure 7. Due to the space limit, additional results are added in the Appendix. Those results show that the same story (PDP-SGD performs better than DP-SGD in the high privacy regime, where the privacy loss $\\epsilon$ is small) observed in 10000 training samples also holds for other numbers of training samples. \n\nTo deal with the computation, we explored a heuristic method suggested by Reviewer3 that we reuse the eigenspace for several iterations if the difference between consecutive subspaces is small. \nWe implemented PDP-SGD that updates the projector every s = 1, 10, 20 iterations and we added the results in Figure 6 in the main paper. Those results show that PDP-SGD with a reduced eigenspace computation also improves the accuracy over DP-SGD for a certain privacy level."}, "signatures": ["ICLR.cc/2021/Conference/Paper1989/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7dpmlkBuJFC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1989/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1989/Authors|ICLR.cc/2021/Conference/Paper1989/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853493, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1989/-/Official_Comment"}}}, {"id": "xUqn7F1V9j4", "original": null, "number": 2, "cdate": 1606275314323, "ddate": null, "tcdate": 1606275314323, "tmdate": 1606275314323, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": "EKEjdTzr7bV", "invitation": "ICLR.cc/2021/Conference/Paper1989/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the constructive suggestions on improving the quality of this paper. Here we explain how we revised our paper based on your comments.  \n\n1. Compare the results with Kairouz et al. (2020) in more detail.\n\nWe have updated our statement to acknowledge that the major difference with Kairouz et al. (2020) on convex problems is that Kairouz et al. (2020) only operates on private data. Our result on convex problems is presented in Theorem 6 (Due to the space limit, Theorem 6 is deferred to the Appendix.), we have added a paragraph after Theorem 6 to explicitly discuss the comparison with Kairouz et al. (2020). \n\n2. Line 4 of Alg 1: better to say $\\tilde{z_i} \\in S_h$ than enumerating i from 1 to m.\n\nWe have revised this part based on your comment.  \n\n3. I wonder if it worth discussing what would happen if partition the public data (given that we have more than T samples) into T disjoint subsets for each iteration. Would the trade-off be worse?\n\nThe reviewer has raised a very good point. We think it is definitely meaningful to discuss this scenario.  We have added a discussion on this data splitting method for dealing with the dependency after we present the Ahlswede-Winter Inequality (the first paragraph in Section 3.1). If splitting m samples into T disjoint subsets, one will have m/T public samples at every iteration and the deviation error will scale with $(\\sqrt{T}/\\sqrt{m})$. This will lead to a worse trade-off between the subspace construction error and optimization error due to the dependence on T.\n\n4. More intuition and explanation of $\\gamma(M,d)$.\n\nAt a high level, $\\gamma_2(M,d)$ is related to $\\sqrt{\\log N(M,d,\\epsilon)}$ where $N(M,d,\\epsilon)$ is the covering number of the set  $M$ with $\\epsilon$ balls with metric $d$, but it is considerably sharper. Such sharpening has happened in two stages in the literature: first, based on chaining, which considers an integral over all $\\epsilon$ yielding the Dudley bound, and subsequently, based on generic chaining, which considers a hierarchical covering, developed by Talagrand and colleagues, and which yields the sharpest bounds of this type. The official perspective of generic chaining is to view $\\gamma_2(M,d)$ as an upper (and lower) bound on suprema of Gaussian processes indexed on $M$ and with metric $d$ [Theorem 2.4.1 in Talagrand, 2014]. \nConsidering $d$ to be the $\\ell_2$ norm distance, $\\gamma_2(M,d)$ will be the same order as the Gaussian width of $M$ [Vershynin, 2019], which is a scaled version of the mean width of $M$. Structured sets (of gradients) have small Gaussian widths, e.g., a $L_1$ unit ball in $\\mathbb{R}^p$ has a Gaussian width of $O(\\sqrt{\\log p})$, wheras a $L_2$ unit ball in $\\mathbb{R}^p$ has a Gaussian width of $O(\\sqrt{p})$. Based on the composition properties, one can construct examples of the gradient spaces that is composed of different subsets.  If $M = M_1 + M_2 = \\{ m_1 + m_2, m_1 \\in M_1, m_2 \\in M_2\\}$,\nthe Minkowski sum, then $\\gamma_2(M,d) \\leq c (\\gamma_2(M_1,d) + \\gamma_2(M_2,d))$ (Theorem 2.4.15 in [Talagrand, 2014]), where $c$ is an absolute constant. If $M$ is a union of several subset, i.e., $M = \\cup_{h=1}^D M_{h}$, \nthen by using an union bound, we have $\\gamma_2(M,d) \\leq \\sqrt{\\log D} \\max_{h} \\gamma_2(M_{h},d)$. Thus, if  $M$ is an union of $D = p^s$ ellipsoids, i.e., polynomial in $p$, then $\\gamma_2(M,\\| \\cdot \\|_2) \\leq O\\left( \\sqrt{s}\\log p\\right)$. \nWe have added a more detailed discussion in the Appendix. \n\n\n5. The experiment setup: why a subset of 10000, whether the number of examples has any influence on the results.\n\nTo study whether the number of examples has any influence on the results, we also added results for MNIST with 20000 samples, 50000 samples, and Fashion MNIST with 50000 samples in the revision (The 100 public samples are randomly sampled from the rest 10000 samples.). Those results show that the same story (PDP-SGD performs better than DP-SGD for small $\\epsilon$ regime) observed in 10000 training samples also holds for other numbers of training samples. Note that a smaller training sample size imposes more challenges on private training.\n\n6. In the experiments in Figure 5, as k=50 (or 70) outperforms DPSGD without projection, it might make more sense to further increase k to demonstrate the tradeoff between the two types of errors.\n\nWe agree that it could be an interesting idea to explore different values of $k$. Our choice of $k$ (between 10 to 50) is motivated by the empirical low-rank structure in the stochastic gradients (see Figure 1). Similar structures have been observed in Papyan (2019) as well.\n\n7. ``A minor point: Sec 4 said \"Papernot et al. (2020) shows the accuracy of DP-SGD is around 80 ...\n\nWe thank the reviewer for pointing out this. We have revised our discussion in Sec 4 and we acknowledge that the result in Papernot et al. (2020) shows training dynamics in terms of privacy loss $\\epsilon$ instead of epochs. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1989/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "7dpmlkBuJFC", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1989/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1989/Authors|ICLR.cc/2021/Conference/Paper1989/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853493, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1989/-/Official_Comment"}}}, {"id": "EKEjdTzr7bV", "original": null, "number": 3, "cdate": 1603933500682, "ddate": null, "tcdate": 1603933500682, "tmdate": 1605024312783, "tddate": null, "forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "invitation": "ICLR.cc/2021/Conference/Paper1989/-/Official_Review", "content": {"title": "Review", "review": "The author proposes an algorithm that improves DPSGD by using public data to identify a lower-dimensional space where the gradients lie in. They show that the algorithm can provide a better convergence guarantee, specifically, p reduced to log(p). They also conducted experiments to show the proposed algorithm outperforms the generic DPSGD especially at small epsilon with a small amount of public data.\n\nThe topic of the paper is interesting, and the presentation is clear. I hope the significance of the paper can be better justified. On the theory side, I think it needs more comparison with prior work and needs a better explanation of some of the key concepts. On the experiment side, to make the results more convincing, it might be better to conduct evaluations on harder datasets.\n\nComments:\n\n- In related work, it is said that \"Kairouz et al. (2020) .... In comparison, our work studies both convex and non-convex problems and our analysis applies for more general low-dimensional structures that can be characterized by small \u03b32 functions (Talagrand, 2014) (e.g., low-rank gradients and fast decay in the gradient coordinates)\". I believe Kairouz et al was operating on private data only and that might be a major difference. And it might still be interesting to compare the results in more detail, e.g. how do the bounds compare on convex problems, how do the two measurements of dimensionality compare.\n\n- Line 4 of Alg 1: better to say \\tilde{z_i} \\in S_h than enumerating i from 1 to m (m is undefined in the algorithm).\n\n- It is mentioned that if M is evaluated on fresh public samples at each iteration, then the deviation of M from \u03a3 can be analyzed easily. I wonder if it worth discussing what would happen if partition the public data (given that we have more than T samples) into T disjoint subsets for each iteration. Would the tradeoff be worse?\n\n- Since \u03b3(M, d) is the main component in the analysis and the bound, can you provide more intuition and explanation of it, like how large it can get, on what kind of dataset it might be small/large, would model architecture make a difference etc.?\n\n- A question regarding the experiment setup: I didn't quite understand why a subset of 10000, instead of (60000 - 100), of the original data is picked for training. 10000 is definitely a legitimate setting, but I just wonder if there is a reason for picking that number and whether the number of examples has any influence on the results.\n\n- In the experiments in Figure 5, as k=50 (or 70) outperforms DPSGD without projection, it might make more sense to further increase k to demonstrate the tradeoff between the two types of errors.\n\n- A minor point: Sec 4 said \"Papernot et al. (2020) shows the accuracy of DP-SGD is around 80% when \u03b5 \u2248 1\", which I'm not sure is accurate. I didn't find Papernot et at (in Table 4) reporting the accuracy for epsilon around 1. If the statement is referring to the leftmost point in Fig 3 of Papernot et al., I believe it is not a fair comparison. The figure basically showed a training process in terms of epsilon instead of steps, so the result is not optimized for epsilon = 1 (but is rather for epsilon = 2.93). On the other hand, the [tensorflow privacy tutorial](https://github.com/tensorflow/privacy/tree/master/tutorials) achieves 95% at epsilon = 1.19.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1989/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1989/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification", "authorids": ["~Yingxue_Zhou1", "~Steven_Wu1", "~Arindam_Banerjee1"], "authors": ["Yingxue Zhou", "Steven Wu", "Arindam Banerjee"], "keywords": [], "abstract": "Differentially private SGD (DP-SGD) is one of the most popular methods for solving differentially private empirical risk minimization (ERM). Due to its noisy perturbation on each gradient update, the error rate of DP-SGD scales with the ambient dimension $p$, the number of parameters in the model. Such dependence can be problematic for over-parameterized models where $p \\gg n$, the number of training samples. Existing lower bounds on private ERM show that such dependence on $p$ is inevitable in the worst case. In this paper, we circumvent the dependence on the ambient dimension by leveraging a low-dimensional structure of gradient space in deep networks---that is, the stochastic gradients for deep nets usually stay in a low dimensional subspace in the training process. We propose Projected DP-SGD that performs noise reduction by projecting the noisy gradients to a low-dimensional subspace, which is given by the top gradient eigenspace on a small public dataset. We provide a general sample complexity analysis on the public dataset for the gradient subspace identification problem and demonstrate that under certain low-dimensional assumptions the public sample complexity only grows logarithmically in $p$. Finally, we provide a theoretical analysis and empirical evaluations to show that our method can substantially improve the accuracy of DP-SGD in the high privacy regime (corresponding to low privacy loss $\\epsilon$).\n\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|bypassing_the_ambient_dimension_private_sgd_with_gradient_subspace_identification", "pdf": "/pdf/72b062d9241693b3927766cc1cea70d6cd216bd2.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhou2021bypassing,\ntitle={Bypassing the Ambient Dimension: Private {\\{}SGD{\\}} with Gradient Subspace Identification},\nauthor={Yingxue Zhou and Steven Wu and Arindam Banerjee},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=7dpmlkBuJFC}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "7dpmlkBuJFC", "replyto": "7dpmlkBuJFC", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1989/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538106375, "tmdate": 1606915766193, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1989/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1989/-/Official_Review"}}}], "count": 9}