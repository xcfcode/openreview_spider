{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396490139, "tcdate": 1486396490139, "number": 1, "id": "ryGU2fLOe", "invitation": "ICLR.cc/2017/conference/-/paper299/acceptance", "forum": "SkgewU5ll", "replyto": "SkgewU5ll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers all agreed that the paper was well written, that the proposed approach is very sensible and intuitive and that the experiments are convincing. However, they are concerned that the proposed work is of limited interest to the ICLR community. The technical contribution is not significant enough for any of the reviewers to strongly recommend an acceptance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396490682, "id": "ICLR.cc/2017/conference/-/paper299/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkgewU5ll", "replyto": "SkgewU5ll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396490682}}}, {"tddate": null, "tmdate": 1484951238172, "tcdate": 1481893023255, "number": 2, "id": "HywiEvWEx", "invitation": "ICLR.cc/2017/conference/-/paper299/official/review", "forum": "SkgewU5ll", "replyto": "SkgewU5ll", "signatures": ["ICLR.cc/2017/conference/paper299/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper299/AnonReviewer3"], "content": {"title": "Interesting approach for learning input representations in RNN", "rating": "6: Marginally above acceptance threshold", "review": "I read the authors' response and maintain my rating.\n\n---\n\nThis paper introduces an approach for integrating a direct acyclic graph structure of the data into word / code embeddings, in order to leverage domain knowledge and thus help train an RNN with scarce data. It is applied to codes of medical visits. Each code is part of an ontology, which can be represented by a DAG, where codes correspond to leaf nodes, and where different codes may share common ancestors (non-leaf nodes) in the DAG. Instead of embedding merely the leaf nodes, one can also embed the non-leaf nodes, and the embeddings of the code and its ancestors can be combined using a convex sum. That convex sum can be seen as an attention mechanism over the representation. The attention weights depend on the embeddings and the weights of an MLP, meaning that the model can separate learning the code embeddings and the interaction between the codes. Embedding codes are pretrained using GloVe, then fine-tuned.\n\nThe model is properly evaluated on two medical datasets, with several variations to isolate the contribution of the DAG (GRAM or GRAM+ vs. RNN or RandomDAG) and of pretraining the embeddings (RNN+ vs RNN, GRAM+ vs GRAM). Both are shown to help achieve the best performance and the evaluation methodology seems thorough.\n\nThe paper is also well written, and the case for MLP attention instead of a plain dot product of embeddings was made by the authors.\n\nMy only two comments would be:\n1) Why is there a softmax in equation 4, given that the loss is multivariate cross-entropy (in the predicted visit, several codes could be equal to 1), not a a single-class cross-entropy?\n2) What is the embedding dimension m?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512632162, "id": "ICLR.cc/2017/conference/-/paper299/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper299/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper299/AnonReviewer2", "ICLR.cc/2017/conference/paper299/AnonReviewer3", "ICLR.cc/2017/conference/paper299/AnonReviewer1"], "reply": {"forum": "SkgewU5ll", "replyto": "SkgewU5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512632162}}}, {"tddate": null, "tmdate": 1484258763124, "tcdate": 1484258763124, "number": 8, "id": "rkQRpOBIe", "invitation": "ICLR.cc/2017/conference/-/paper299/public/comment", "forum": "SkgewU5ll", "replyto": "H1jIniFVg", "signatures": ["~Edward_Choi1"], "readers": ["everyone"], "writers": ["~Edward_Choi1"], "content": {"title": "re: How that compares to your KDD 2016 paper", "comment": "Thank you for your interest in this paper. Our responses to your questions are listed below:\n\n1) We would like to clarify that GRAM is not a graph-regularized Med2Vec. Actually GRAM comes from very different design philosophy. Med2Vec is an unsupervised method to perform two-level embedding of medical codes and visits, facilitating interpretation when connected to a static model such as logistic regression. GRAM is a general framework for any neural network-based supervised predictive model that enables the utilization of DAG-based prior knowledge to cope with data insufficiency.\n\n2) We showed the contribution of the prior knowledge in the paper by comparing GRAM with RandomDAG, RNN and RNN+ where GRAM consistently showed better predictive performance. Furthermore, if we are to plug Med2Vec to the RNN, we should also use Med2Vec to initialize the basic embeddings of GRAM for a fair comparison.\nOut of curiosity, we conducted an additional experiment where we trained a Med2Vec model with Sutter HF cohort, connected the Med2Vec weights to the RNN, then fine-tuned the Med2Vec weights as we trained the RNN for HF prediction. The predictive performance was similar to RNN+. This was expected, because, from our experience, we found that RNN was already a powerful sequence prediction model, and initializing the embedding weights with some co-occurrence-based methods such as Skip-gram or Med2Vec would provide limited improvement as opposed to using domain knowledge such as GRAM."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632500, "id": "ICLR.cc/2017/conference/-/paper299/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkgewU5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper299/reviewers", "ICLR.cc/2017/conference/paper299/areachairs"], "cdate": 1485287632500}}}, {"tddate": null, "tmdate": 1484257498926, "tcdate": 1484257498926, "number": 7, "id": "BJ71Y_S8l", "invitation": "ICLR.cc/2017/conference/-/paper299/public/comment", "forum": "SkgewU5ll", "replyto": "BJhmMPbNl", "signatures": ["~Edward_Choi1"], "readers": ["everyone"], "writers": ["~Edward_Choi1"], "content": {"title": "Authors' response", "comment": "We would like to thank you for your questions and insightful comments. We address your comments in detail below.\n\n1. Using the knowledge base factorization approach or graph convolutional approach?\nAs discussed in the Related Work section, knowledge base factorization methods such as Neural Tensor Network [Socher et al. 2013], TransE [Bordes et al. 2013] or TransH [Wang et al. 2014] learn the representations of entities and their relations in the knowledge graph, in order to perform tasks such as link prediction, triple classification or knowledge completion. Our goal, on the other hand, is fundamentally different in that we do not aim to learn the structure of the ontology (i.e. learning the entities and various relationships between entities), but focus on the parent-child relationship between medical concepts and learn how to properly combine the ancestors in the knowledge DAG to obtain robust representations for medical concepts under data insufficiency.\nGraph convolutional approaches such as Planetoid [Yang et al. 2016] or GCN [Kipf and Welling 2016] focus on learning the node representations to mainly perform node classification. Similar to the works mentioned above, they use the graph itself as the main data and perform prediction tasks for that graph. Our work, on the other hand, uses longitudinal EHR as the main data and perform prediction tasks for that EHR, using the knowledge DAG as the supplementary information. Also, graph convolutional approaches are similar to DeepWalk [Perozzi et al. 2014] or node2vec [Glover and Leskovec 2016] mentioned in the Related Work section in that they focus on graphs with flat hierarchy where all nodes are equal whereas GRAM utilizes hierarchical relationship (i.e. parent-child) of nodes.\nWe added more works to discuss them in the Related Work section.\n\n2. Regarding the fine-tuning of the nodes\nWe apologize for the confusion caused by our previous response. The basic embeddings of both leaf nodes and non-leaf nodes are fine-tuned during the training process, since the error signal flows from the output \\widehat{y}_t to the final representations g_i's which are convex combinations of the basic embeddings e_i's. We apologize for the confusion. We clarified this in the manuscript.\n\n3. Figure 2. Please use the same image format with the same resolution.\nWe replaced Figure 2 with tables for better readability. Also Figure 6 in the Appendix was replaced with tables as well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632500, "id": "ICLR.cc/2017/conference/-/paper299/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkgewU5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper299/reviewers", "ICLR.cc/2017/conference/paper299/areachairs"], "cdate": 1485287632500}}}, {"tddate": null, "tmdate": 1484257391787, "tcdate": 1484257391787, "number": 6, "id": "HJd_d_rLx", "invitation": "ICLR.cc/2017/conference/-/paper299/public/comment", "forum": "SkgewU5ll", "replyto": "HywiEvWEx", "signatures": ["~Edward_Choi1"], "readers": ["everyone"], "writers": ["~Edward_Choi1"], "content": {"title": "Authors' response", "comment": "Thank you for your review and clarifying questions. We address your questions in detail below.\n\n1. Why is there a softmax in equation 4, given that the loss is multivariate cross-entropy (in the predicted visit, several codes could be equal to 1), not a single-class cross-entropy?\nThe loss function in section 2.3 is a binary cross-entropy for each code we are predicting.\nIt is true that multiple codes could be 1 in a single visit. Considering this, we also experimented with a set of sigmoid functions, one for each code. But using the Softmax function showed better predictive performance. We believe this is due to the strong regularizing effect caused by the normalizing denominator of Softmax. Also, although there could be multiple codes equal to 1 in a single visit, the average number of codes per visit is limited to 2 in Sutter dataset and 13 in MIMIC-III dataset, and Softmax seems sufficient to capture that code distribution.\n\n2. What is the embedding dimension m?\nThe embedding dimension m is the size of the vector we used to represent the nodes (i.e. medical concepts) in the knowledge DAG. The values of m we used in the experiments, along with other hyperparameters, are described in Table 4 in Appendix C."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632500, "id": "ICLR.cc/2017/conference/-/paper299/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkgewU5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper299/reviewers", "ICLR.cc/2017/conference/paper299/areachairs"], "cdate": 1485287632500}}}, {"tddate": null, "tmdate": 1484257241730, "tcdate": 1484257241730, "number": 5, "id": "SkMJ__rIg", "invitation": "ICLR.cc/2017/conference/-/paper299/public/comment", "forum": "SkgewU5ll", "replyto": "ByxbiBc4g", "signatures": ["~Edward_Choi1"], "readers": ["everyone"], "writers": ["~Edward_Choi1"], "content": {"title": "Authors' response", "comment": "Thank you for your review and your insightful questions. Our responses to your comments are as below.\n\n1. Why is the patient\u2019s visit taken as just the sum of medical codes found in the visit, and not say the average or a learned weighted average? Wouldn\u2019t this bias for/against the number of codes in the visit?\nThe sum of medical codes is a natural approach as a patient (or a visit) can be viewed as the aggregate of medical symptoms and diseases, as opposed to averaging, which does not capture the complexity of the patient status. Your suggestion on the learned weighted average is related to one of our future research directions. Applying another attention mechanism to weight-sum the code embeddings could be a better way of representing a visit. However, under a proper supervised setting, the embedding matrix for the codes would learn values that incorporates the attention weights as well. This could potentially be an interesting comparison and can be a focus for another paper.\n \n2. I don\u2019t see why basic embeddings are not fine tuned as well. Did you find that to hurt performance? Do you have an explanation for that?\nWe apologize for the confusion. As we explained in the response for the first review, the basic embeddings for both leaf nodes and non-leaf nodes are fine-tuned, since the error signal flows from the output \\widehat{y}_t to the final representations of the code g_i\u2019s, which are convex combinations of the basic embeddings e_i\u2019s. We again apologize for the confusion. We clarified this in the paper.\n \n3. Looking at Figure 2, the results seem very close and the figures are not very clear (figure (b) top is missing). Also, I am wondering how significant the differences are so it would be nice to comment on that.\nWe replaced Figure 2 with tables for better readability. We also replaced Figure 6 in the Appendix with tables. In sequential diagnoses prediction, especially for MIMIC-III where the training data is even smaller, GRAM+ shows significantly better performance compared to RNN and RNN+ (maximum 6% improved AUC for Sutter PAMF dataset, 10% improved AUC for MIMIC-III) for infrequent codes. SimpleRollUp is quite effective since it is also uses prior knowledge.\nFor HF prediction, GRAM and GRAM+ consistently outperform other baselines. I believe these improvements are quite significant and hard to achieve given that the strong baseline performance (AUC>.8). We added more descriptions to the paper to emphasize the performance differences.\n \n4. Finally, I think this is an interesting application paper applying well-established deep learning techniques. The paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources. However, I would like the authors to comment on what there paper offers as new insights to the ICLR community and why they think ICLR is a good avenue for their work.\nThank you for appreciating the value of this work. To our knowledge, this work is the first attempt to apply the attention mechanism to a graph (DAG to be precise) in order to augment prior knowledge when learning representations. We believe that we have shown this to be not only viable, but also quite effective. We believe our work will invite the exciting follow-up research in both deep learning methods as well as important applications to healthcare to 1) invent different, or even better methods to apply attention on graphs and 2) introduce various graph-based knowledge to learn robust representations in healthcare as well as other fields."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632500, "id": "ICLR.cc/2017/conference/-/paper299/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkgewU5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper299/reviewers", "ICLR.cc/2017/conference/paper299/areachairs"], "cdate": 1485287632500}}}, {"tddate": null, "tmdate": 1484256818152, "tcdate": 1484256818152, "number": 4, "id": "SJ54UurLe", "invitation": "ICLR.cc/2017/conference/-/paper299/public/comment", "forum": "SkgewU5ll", "replyto": "SkgewU5ll", "signatures": ["~Edward_Choi1"], "readers": ["everyone"], "writers": ["~Edward_Choi1"], "content": {"title": "Changes in the last revision", "comment": "Major changes in the revision\n-- Figures 2 are replaced with tables for better readability\n-- Related works have been expanded\n-- Incorporated the reviewers\u2019 comments in the experiment section.\n-- Experiments for GRAM+ and RNN+ in heart failure prediction were re-run\n\nMore details\nWe discovered that, for GRAM+ and RNN+ in HF prediction with varying amounts of training data, we initialized the basic embeddings e_i\u2019s and the embedding matrix W_emb with GloVe vectors that were trained always on the 100% training data instead of the downsampled training data. This could have exaggerated the AUC of both models. Therefore we conducted a correct experiment where the initialization was done with GloVe vectors trained on the downsampled training data. We found that GRAM+\u2019s performance did not show noticeable difference, but RNN+\u2019s performance dropped approximately 0.5-1% AUC.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632500, "id": "ICLR.cc/2017/conference/-/paper299/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkgewU5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper299/reviewers", "ICLR.cc/2017/conference/paper299/areachairs"], "cdate": 1485287632500}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484256741791, "tcdate": 1478285032315, "number": 299, "id": "SkgewU5ll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkgewU5ll", "signatures": ["~Edward_Choi1"], "readers": ["everyone"], "content": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482476280234, "tcdate": 1482476280234, "number": 3, "id": "ByxbiBc4g", "invitation": "ICLR.cc/2017/conference/-/paper299/official/review", "forum": "SkgewU5ll", "replyto": "SkgewU5ll", "signatures": ["ICLR.cc/2017/conference/paper299/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper299/AnonReviewer1"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper addresses the problem of data sparsity in the healthcare domain by leveraging hierarchies of medical concepts organized in ontologies. The paper focuses on sequential prediction given a patient\u2019s medical record (a sequence of medical codes, some of which might occur very rarely). Instead of simply assigning each medical code an independent embedding before feeding it to an RNN, the proposed approach assigns each node in the medical ontology a \u201cbasic\u201d embedding, and composes a \u201cfinal\u201d embedding for each medical code by taking a learned weighted average (via an attention mechanism) of the medical code\u2019s ancestors in the ontology. Notably, the paper is well written and the approach is quite intuitive.\nI have the following comments: \n- Why is the patient\u2019s visit taken as just the sum of medical codes found in the visit, and not say the average or a learned weighted average? Wouldn\u2019t this bias for/against the number of codes in the visit?\n- I don\u2019t see why basic embeddings are not fine tuned as well. Did you find that to hurt performance? Do you have an explanation for that?\n- Looking at Figure 2, the results seem very close and the figures are not very clear (figure (b) top is missing). Also, I am wondering how significant the differences are so it would be nice to comment on that.\nFinally, I think this is an interesting application paper applying well-established deep learning techniques. The paper deals with an important issue that arises when applying deep learning models in domains with scarce data resources. However, I would like the authors to comment on what there paper offers as new insights to the ICLR community and why they think ICLR is a good avenue for their work.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512632162, "id": "ICLR.cc/2017/conference/-/paper299/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper299/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper299/AnonReviewer2", "ICLR.cc/2017/conference/paper299/AnonReviewer3", "ICLR.cc/2017/conference/paper299/AnonReviewer1"], "reply": {"forum": "SkgewU5ll", "replyto": "SkgewU5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512632162}}}, {"tddate": null, "tmdate": 1482441251880, "tcdate": 1482435667487, "number": 3, "id": "H1jIniFVg", "invitation": "ICLR.cc/2017/conference/-/paper299/public/comment", "forum": "SkgewU5ll", "replyto": "SkgewU5ll", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "How that compares to your KDD 2016 paper ", "comment": "Nice paper, I have one comment however. \n\nI read your KDD 2016 paper about \"Multi-layer Representation Learning for Medical Concepts\" where you presented a Med2vec model to map patients' visits and codes into a different space. You showed that the new representation is effective and interpretable. \n\nYour current submission to ICLR is very similar to your KDD paper except that you have regularized the model using DAG of concepts as a prior knowledge. \n1) Am I right on that? \n2) why you did not compare your current model to Med2vec? You can use Med2vec as input to any other model such as RNN or CNN for classification. This case, we can have a clear picture about the contribution of the prior knowledge to the model accuracy. If there is no significant difference in the results then why we would care about the prior knowledge. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632500, "id": "ICLR.cc/2017/conference/-/paper299/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkgewU5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper299/reviewers", "ICLR.cc/2017/conference/paper299/areachairs"], "cdate": 1485287632500}}}, {"tddate": null, "tmdate": 1481892387892, "tcdate": 1481892387892, "number": 1, "id": "BJhmMPbNl", "invitation": "ICLR.cc/2017/conference/-/paper299/official/review", "forum": "SkgewU5ll", "replyto": "SkgewU5ll", "signatures": ["ICLR.cc/2017/conference/paper299/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper299/AnonReviewer2"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "SUMMARY.\nThis paper presents a method for enriching medical concepts with their parent nodes in an ontology.\nThe method employs an attention mechanism over the parent nodes of a medical concept to create a richer representation of the concept itself.\nThe rationale of this is that for  infrequent medical concepts the attention mechanism will rely more on general concepts, higher in the ontology hierarchy, while for frequent ones will focus on the specific concept.\nThe attention mechanism is trained together with a recurrent neural network and the model accuracy is tested on two tasks.\nThe first task aims at prediction the diagnosis categories at each time step, while the second task aims at predicting whether or not a heart failure is likely to happen after the T-th step.\n\nResults shows that the proposed model works well in condition of data insufficiency.\n\n----------\n\nOVERALL JUDGMENT\nThe proposed model is simple but interesting.\nThe ideas presented are worth to expand but there are also some points where the authors could have done better.\nThe learning of the representation of concepts in the ontology is a bit naive, for example the authors could have used some kind of knowledge base factorization approach to learn the concepts, or some graph convolutional approach.\nI do not see why the the very general factorization methods for knowledge bases do not apply in the case of ontology learning.\nI also found strange that the representation of leaves are fine tuned while the inner nodes are not, it is a specific reason to do so?\n\nRegarding the presentation, the paper is clear and the qualitative evaluation is insightful.\n\n\n----------\n\nDETAILED COMMENTS\n\nFigure 2. Please use the same image format with the same resolution.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512632162, "id": "ICLR.cc/2017/conference/-/paper299/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper299/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper299/AnonReviewer2", "ICLR.cc/2017/conference/paper299/AnonReviewer3", "ICLR.cc/2017/conference/paper299/AnonReviewer1"], "reply": {"forum": "SkgewU5ll", "replyto": "SkgewU5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512632162}}}, {"tddate": null, "tmdate": 1480845425667, "tcdate": 1480844871798, "number": 2, "id": "BJl8UvWXe", "invitation": "ICLR.cc/2017/conference/-/paper299/public/comment", "forum": "SkgewU5ll", "replyto": "HyeqdN1Xg", "signatures": ["~Edward_Choi1"], "readers": ["everyone"], "writers": ["~Edward_Choi1"], "content": {"title": "re: Compatibility function", "comment": "Thank your for your question. A feedforward neural network with a single hidden layer like our formulation is well known to be a sufficient approximator for an arbitrary function including a dot product. Moreover, our formulation resembles the compatibility used in the original attention model  Bahdanau et al. 2014, where they calculate the compatibility between the encoder\u2019s hidden state and the decoder\u2019s hidden state with \u201cv_a dot tanh(W_a s_i + U_a h_j)\u201d. However, after testing both formulations, we empirically found that our formulation performed better in our use cases. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632500, "id": "ICLR.cc/2017/conference/-/paper299/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkgewU5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper299/reviewers", "ICLR.cc/2017/conference/paper299/areachairs"], "cdate": 1485287632500}}}, {"tddate": null, "tmdate": 1480702087927, "tcdate": 1480702087922, "number": 2, "id": "HyeqdN1Xg", "invitation": "ICLR.cc/2017/conference/-/paper299/pre-review/question", "forum": "SkgewU5ll", "replyto": "SkgewU5ll", "signatures": ["ICLR.cc/2017/conference/paper299/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper299/AnonReviewer3"], "content": {"title": "Compatibility function", "question": "What determined the choice of the specific form of the function computing the compatibility between embeddings e_i and e_j (Eq. 3)? For instance, why would a simple dot product between e_i and e_j not work?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959353240, "id": "ICLR.cc/2017/conference/-/paper299/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper299/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper299/AnonReviewer2", "ICLR.cc/2017/conference/paper299/AnonReviewer3"], "reply": {"forum": "SkgewU5ll", "replyto": "SkgewU5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959353240}}}, {"tddate": null, "tmdate": 1480701642629, "tcdate": 1480701626421, "number": 1, "id": "ryz6I4JQe", "invitation": "ICLR.cc/2017/conference/-/paper299/public/comment", "forum": "SkgewU5ll", "replyto": "BJV5NLnGx", "signatures": ["~Edward_Choi1"], "readers": ["everyone"], "writers": ["~Edward_Choi1"], "content": {"title": "re: embeddings", "comment": "The sequential disease prediction (SDP) task and the heart failure (HF) prediction task have fundamentally different natures. SDP tries to predict the occurrence of  diagnosis codes in the next visit V_{t+1} based on the information of past co-occurring diagnosis codes V_1, ..., V_t, as opposed to predicting a binary label for the entire sequence in HF task. The co-occurrence information infused by the initialized embedding technique significantly improves the performance of SDP, because it is more about finding co-occurrence of the codes including the same codes repeating over time. The dataset we used for the HF prediction task is a subset of the large Sutter PAMF dataset, which has also been used for one of the two SDP tasks (The construction of the HF dataset  is described in Appendix B). The characteristic of the two datasets are quite similar. Therefore, we believe the benefit of the initialized embedding is more evident in the SDP task because of the differences in the learning tasks rather than the datasets.\n\nAs for your last question, yes, the initialized embeddings are fine-tuned during the training, but only for the leaf nodes. We do not fine-tune the non-leaf nodes during the training. \n\nWe will clarify both points in the manuscript. Thanks for your questions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287632500, "id": "ICLR.cc/2017/conference/-/paper299/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkgewU5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper299/reviewers", "ICLR.cc/2017/conference/paper299/areachairs"], "cdate": 1485287632500}}}, {"tddate": null, "tmdate": 1480512651878, "tcdate": 1480512651873, "number": 1, "id": "BJV5NLnGx", "invitation": "ICLR.cc/2017/conference/-/paper299/pre-review/question", "forum": "SkgewU5ll", "replyto": "SkgewU5ll", "signatures": ["ICLR.cc/2017/conference/paper299/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper299/AnonReviewer2"], "content": {"title": "embeddings", "question": "Hi,\nI do not understand well the argument at page 6, where you speculate on the role of initialized embeddings on different tasks.\nI cannot understand why the initialized embeddings are more useful on a multiclass task than on a binary one. Can it be that it depends more on the input data that on the nature of the task?\nAre the initialized embeddings fine-tuned duing training?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GRAM: Graph-based Attention Model for Healthcare Representation Learning", "abstract": "Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain:\n- Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. \n- Interpretation: The representations learned by deep learning models should align with medical knowledge.\nTo address these challenges, we propose a GRaph-based Attention Model, GRAM that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. \nBased on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. \nWe compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task.\nCompared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts.", "pdf": "/pdf/e986742070dbf1d3a51ef5901c4360422b21498b.pdf", "TL;DR": "We propose a novel attention mechanism on graphs to learn representations for medical concepts from both data and medical ontologies to cope with insufficient data volume.", "paperhash": "choi|gram_graphbased_attention_model_for_healthcare_representation_learning", "keywords": ["Deep learning", "Applications"], "conflicts": ["usc.edu", "sutterhealth.org", "cmu.edu", "ibm.com"], "authors": ["Edward Choi", "Mohammad Taha Bahadori", "Le Song", "Walter F. Stewart", "Jimeng Sun"], "authorids": ["mp2893@gatech.edu", "bahadori@gatech.edu", "lsong@cc.gatech.edu", "stewarwf@sutterhealth.org", "jsun@cc.gatech.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959353240, "id": "ICLR.cc/2017/conference/-/paper299/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper299/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper299/AnonReviewer2", "ICLR.cc/2017/conference/paper299/AnonReviewer3"], "reply": {"forum": "SkgewU5ll", "replyto": "SkgewU5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper299/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959353240}}}], "count": 15}