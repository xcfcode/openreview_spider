{"notes": [{"id": "S1gARiAcFm", "original": "B1eGDiactQ", "number": 945, "cdate": 1538087894387, "ddate": null, "tcdate": 1538087894387, "tmdate": 1545355381760, "tddate": null, "forum": "S1gARiAcFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Modeling Dynamics of Biological Systems with Deep Generative Neural Networks", "abstract": "Biological data often contains measurements of dynamic entities such as cells or organisms in various states of progression. However, biological systems are notoriously difficult to describe analytically due to their many interacting components, and in many cases, the technical challenge of taking longitudinal measurements. This leads to difficulties in studying the features of the dynamics, for examples the drivers of the transition. To address this problem, we present a deep neural network framework we call Dynamics Modeling Network or DyMoN. DyMoN is a neural network framework trained as a deep generative Markov model whose next state is a probability distribution based on the current state. DyMoN is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems. Thus, DyMoN can be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. We show the advantage of learning deep models over shallow models such as Kalman filters and hidden Markov models that do not learn representations of the data, both in terms of learning embeddings of the data and also in terms training efficiency, accuracy and ability to multitask. We perform three case studies of applying DyMoN to different types of biological systems and extracting features of the dynamics in each case by examining the learned model. ", "keywords": ["neural networks", "markovian dynamics", "single-cell biology", "calcium imaging", "stochastic dynamics", "generative models"], "authorids": ["scott.gigante@yale.edu", "david.vandijk@yale.edu", "kevin.moon@usu.edu", "alexander.strzalkowski@yale.edu", "katie.ferguson@yale.edu", "jess.cardin@yale.edu", "guy.wolf@yale.edu", "smita.krishnaswamy@yale.edu"], "authors": ["Scott Gigante", "David van Dijk", "Kevin R. Moon", "Alexander Strzalkowski", "Katie Ferguson", "Guy Wolf", "Smita Krishnaswamy"], "TL;DR": "Dynamics Modeling Networks (DyMoN) offer advantages in representation, generation, visualization and feature extraction over shallow learning techniques for modeling stochastic dynamical systems in biology.", "pdf": "/pdf/991e2b1de4ebb49b53f18bc824b362fe744afffd.pdf", "paperhash": "gigante|modeling_dynamics_of_biological_systems_with_deep_generative_neural_networks", "_bibtex": "@misc{\ngigante2019modeling,\ntitle={Modeling Dynamics of Biological Systems with Deep Generative Neural Networks},\nauthor={Scott Gigante and David van Dijk and Kevin R. Moon and Alexander Strzalkowski and Katie Ferguson and Guy Wolf and Smita Krishnaswamy},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gARiAcFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ryl0czu-lE", "original": null, "number": 1, "cdate": 1544811157897, "ddate": null, "tcdate": 1544811157897, "tmdate": 1545354527833, "tddate": null, "forum": "S1gARiAcFm", "replyto": "S1gARiAcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper945/Meta_Review", "content": {"metareview": "The paper tackles an interesting problem, which is effectively modeling biological time-series data. The advantages of deep neural networks over structured models like HMMs are their ability to learn features from the data, whereas probabilistic graphical models suffer from \"model mismatch\", where the available data must be carefully processed in order to fit the assumptions of the PGM. Any work advancing this topic would be extremely welcome in the world of machine learning in biology.\n\nHowever, the reviewers each raised individual concerns about the paper regarding its clarity and quality, and the authors did not respond. Thus, the reviewers scores remain unchanged, and the rough consensus is a rejection.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Rejection, concerns not addressed by authors"}, "signatures": ["ICLR.cc/2019/Conference/Paper945/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper945/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modeling Dynamics of Biological Systems with Deep Generative Neural Networks", "abstract": "Biological data often contains measurements of dynamic entities such as cells or organisms in various states of progression. However, biological systems are notoriously difficult to describe analytically due to their many interacting components, and in many cases, the technical challenge of taking longitudinal measurements. This leads to difficulties in studying the features of the dynamics, for examples the drivers of the transition. To address this problem, we present a deep neural network framework we call Dynamics Modeling Network or DyMoN. DyMoN is a neural network framework trained as a deep generative Markov model whose next state is a probability distribution based on the current state. DyMoN is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems. Thus, DyMoN can be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. We show the advantage of learning deep models over shallow models such as Kalman filters and hidden Markov models that do not learn representations of the data, both in terms of learning embeddings of the data and also in terms training efficiency, accuracy and ability to multitask. We perform three case studies of applying DyMoN to different types of biological systems and extracting features of the dynamics in each case by examining the learned model. ", "keywords": ["neural networks", "markovian dynamics", "single-cell biology", "calcium imaging", "stochastic dynamics", "generative models"], "authorids": ["scott.gigante@yale.edu", "david.vandijk@yale.edu", "kevin.moon@usu.edu", "alexander.strzalkowski@yale.edu", "katie.ferguson@yale.edu", "jess.cardin@yale.edu", "guy.wolf@yale.edu", "smita.krishnaswamy@yale.edu"], "authors": ["Scott Gigante", "David van Dijk", "Kevin R. Moon", "Alexander Strzalkowski", "Katie Ferguson", "Guy Wolf", "Smita Krishnaswamy"], "TL;DR": "Dynamics Modeling Networks (DyMoN) offer advantages in representation, generation, visualization and feature extraction over shallow learning techniques for modeling stochastic dynamical systems in biology.", "pdf": "/pdf/991e2b1de4ebb49b53f18bc824b362fe744afffd.pdf", "paperhash": "gigante|modeling_dynamics_of_biological_systems_with_deep_generative_neural_networks", "_bibtex": "@misc{\ngigante2019modeling,\ntitle={Modeling Dynamics of Biological Systems with Deep Generative Neural Networks},\nauthor={Scott Gigante and David van Dijk and Kevin R. Moon and Alexander Strzalkowski and Katie Ferguson and Guy Wolf and Smita Krishnaswamy},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gARiAcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper945/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353024012, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1gARiAcFm", "replyto": "S1gARiAcFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper945/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper945/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper945/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353024012}}}, {"id": "HJgeVhWa3Q", "original": null, "number": 3, "cdate": 1541377063578, "ddate": null, "tcdate": 1541377063578, "tmdate": 1541533555718, "tddate": null, "forum": "S1gARiAcFm", "replyto": "S1gARiAcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper945/Official_Review", "content": {"title": "Modelling dynamics of biological data with deep Neural Networks (NN)", "review": "This paper tackles the important challenge of making sense of temporal measurements made in biological systems. Among other, those have the peculiarity that they are not independent but with a dependency structure, which can be encoded as a graph or a network. The authors claim that their approach, DyMoN is adapted to the many challenges of biological high-throughput data sets: noise, sparsity and lack of temporal resolution. The paper presents three very different use of the method in complex biological systems in Section 3: (i) Calcium imaging of visual cortex neurons, (ii) T--cell development in the thymus, and (iii) Human embryonic stem cell differentiation. Section 4 assesses the performance of the method on simulated data sets as well as on a face-recognition data set. Moreover, the authors demonstrate how the features of the NN can be interrogated to shed new insight about the process under scrutiny. They also show the gain in running time a comapred to other approaches.\n\nRemarks:\n - I know very little about the literautr in the subject, could you clarify how your work relates to/can be distinguished from: Testolin and Zorzi, Front Comput Neurosci. 201, Kiegeskorte's Ann. Rev. Vis. Sc. 2015 (https://doi.org/10.1146/annurev-vision-082114-035447), Kai Fan's PhD work (@ Duke University), Betzel and Bassel, Interface 2017, Wang et al. bioRxiv 2018 (https://doi.org/10.1101/247577), etc.\n - l-4p2: 'repetitions': what are those? Line below: 'sufficient for estimating $ P_{x}(y) $, means large sample size, no? No contradictory? And one line below: what is the precise meaning of 'similar' (twice)?\n - top p3: line continued from bottom of p2 -> is it to rubber out noise?\nwhat is $ n $ in $ \\mathbb{R}^{n} $?\n - Make Fig 1 (B) and (C) clearer, since the transition vectors are learnt, why are they in (B) (input states)?\n - below \"...distribution approximates the distribution $ \\mathcal{P}_{x} $...\" -> but $ P_{x} $ also depends on $ \\theta $, not an issue?\n - remark at the end of Section 2: extending DyMoN to higher-order: OK, but this might be computationally VERY expensive, don't you think?\n - link how your empirical validation data have features that, even remotely resemble those of the kind of biological data sets (on which no ground truth exist, I sympathise) you focus on.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper945/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modeling Dynamics of Biological Systems with Deep Generative Neural Networks", "abstract": "Biological data often contains measurements of dynamic entities such as cells or organisms in various states of progression. However, biological systems are notoriously difficult to describe analytically due to their many interacting components, and in many cases, the technical challenge of taking longitudinal measurements. This leads to difficulties in studying the features of the dynamics, for examples the drivers of the transition. To address this problem, we present a deep neural network framework we call Dynamics Modeling Network or DyMoN. DyMoN is a neural network framework trained as a deep generative Markov model whose next state is a probability distribution based on the current state. DyMoN is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems. Thus, DyMoN can be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. We show the advantage of learning deep models over shallow models such as Kalman filters and hidden Markov models that do not learn representations of the data, both in terms of learning embeddings of the data and also in terms training efficiency, accuracy and ability to multitask. We perform three case studies of applying DyMoN to different types of biological systems and extracting features of the dynamics in each case by examining the learned model. ", "keywords": ["neural networks", "markovian dynamics", "single-cell biology", "calcium imaging", "stochastic dynamics", "generative models"], "authorids": ["scott.gigante@yale.edu", "david.vandijk@yale.edu", "kevin.moon@usu.edu", "alexander.strzalkowski@yale.edu", "katie.ferguson@yale.edu", "jess.cardin@yale.edu", "guy.wolf@yale.edu", "smita.krishnaswamy@yale.edu"], "authors": ["Scott Gigante", "David van Dijk", "Kevin R. Moon", "Alexander Strzalkowski", "Katie Ferguson", "Guy Wolf", "Smita Krishnaswamy"], "TL;DR": "Dynamics Modeling Networks (DyMoN) offer advantages in representation, generation, visualization and feature extraction over shallow learning techniques for modeling stochastic dynamical systems in biology.", "pdf": "/pdf/991e2b1de4ebb49b53f18bc824b362fe744afffd.pdf", "paperhash": "gigante|modeling_dynamics_of_biological_systems_with_deep_generative_neural_networks", "_bibtex": "@misc{\ngigante2019modeling,\ntitle={Modeling Dynamics of Biological Systems with Deep Generative Neural Networks},\nauthor={Scott Gigante and David van Dijk and Kevin R. Moon and Alexander Strzalkowski and Katie Ferguson and Guy Wolf and Smita Krishnaswamy},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gARiAcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper945/Official_Review", "cdate": 1542234341052, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1gARiAcFm", "replyto": "S1gARiAcFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper945/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335838748, "tmdate": 1552335838748, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper945/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyehoaKh3m", "original": null, "number": 2, "cdate": 1541344675831, "ddate": null, "tcdate": 1541344675831, "tmdate": 1541533555501, "tddate": null, "forum": "S1gARiAcFm", "replyto": "S1gARiAcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper945/Official_Review", "content": {"title": "Novelty and performance evaluation are unclear", "review": "This paper describes a NN method called DyMoN for predicting transition vectors between states with a Markov process, and learning dynamics of stochastic systems.\n\nThree biological case studies are presented, it is unclear if any new biology was learned from these cases that we could not have learned using other methods, and how accurate they are. The empirical validations are all on nonbiological data and disconnected from the first part of the paper, making the main application/advantage of this method confusing.\n\nI agree with the computational advantages mentioned in the paper, however, interpretation of the representational aspect is challenging especially in the context of biological systems. Regarding denoising, what are the guarantees that this approach does now remove real biological heterogeneity? Also, a denoising method (MAGIC) was still used to preprocess the data prior to DyMon, there is no discussion about any contradictory assumptions.\n\nOverall, the main shortcoming of the paper is lack of performance evaluation, comparison to other methods and clarifying advantages or novel results over other methods. The description of the method could also be improved and clarified with presenting an algorithm.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper945/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modeling Dynamics of Biological Systems with Deep Generative Neural Networks", "abstract": "Biological data often contains measurements of dynamic entities such as cells or organisms in various states of progression. However, biological systems are notoriously difficult to describe analytically due to their many interacting components, and in many cases, the technical challenge of taking longitudinal measurements. This leads to difficulties in studying the features of the dynamics, for examples the drivers of the transition. To address this problem, we present a deep neural network framework we call Dynamics Modeling Network or DyMoN. DyMoN is a neural network framework trained as a deep generative Markov model whose next state is a probability distribution based on the current state. DyMoN is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems. Thus, DyMoN can be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. We show the advantage of learning deep models over shallow models such as Kalman filters and hidden Markov models that do not learn representations of the data, both in terms of learning embeddings of the data and also in terms training efficiency, accuracy and ability to multitask. We perform three case studies of applying DyMoN to different types of biological systems and extracting features of the dynamics in each case by examining the learned model. ", "keywords": ["neural networks", "markovian dynamics", "single-cell biology", "calcium imaging", "stochastic dynamics", "generative models"], "authorids": ["scott.gigante@yale.edu", "david.vandijk@yale.edu", "kevin.moon@usu.edu", "alexander.strzalkowski@yale.edu", "katie.ferguson@yale.edu", "jess.cardin@yale.edu", "guy.wolf@yale.edu", "smita.krishnaswamy@yale.edu"], "authors": ["Scott Gigante", "David van Dijk", "Kevin R. Moon", "Alexander Strzalkowski", "Katie Ferguson", "Guy Wolf", "Smita Krishnaswamy"], "TL;DR": "Dynamics Modeling Networks (DyMoN) offer advantages in representation, generation, visualization and feature extraction over shallow learning techniques for modeling stochastic dynamical systems in biology.", "pdf": "/pdf/991e2b1de4ebb49b53f18bc824b362fe744afffd.pdf", "paperhash": "gigante|modeling_dynamics_of_biological_systems_with_deep_generative_neural_networks", "_bibtex": "@misc{\ngigante2019modeling,\ntitle={Modeling Dynamics of Biological Systems with Deep Generative Neural Networks},\nauthor={Scott Gigante and David van Dijk and Kevin R. Moon and Alexander Strzalkowski and Katie Ferguson and Guy Wolf and Smita Krishnaswamy},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gARiAcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper945/Official_Review", "cdate": 1542234341052, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1gARiAcFm", "replyto": "S1gARiAcFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper945/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335838748, "tmdate": 1552335838748, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper945/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJl9Nkl5h7", "original": null, "number": 1, "cdate": 1541173042269, "ddate": null, "tcdate": 1541173042269, "tmdate": 1541533555286, "tddate": null, "forum": "S1gARiAcFm", "replyto": "S1gARiAcFm", "invitation": "ICLR.cc/2019/Conference/-/Paper945/Official_Review", "content": {"title": " The paper details about a feed-forward NN, called DyMoN, used to model dynamic features of objects that are at multiple phases of transition. The authors need to relate how the Markovian processes are realized within the hidden units and why at all a deep architecture is required.  ", "review": "\n> Even though the paper details the underlying Markovian setup in Section 2, it is unclear to the reader how this knits with the FFNN architecture, for example what are the Markovian functions at hidden layer and output layer. Are they all conditional probabilities? How do you prove that this is what occurs within each node?\n\n> Why is the functional form of f_\\theta in Eq 1? \n\n> How many hidden layers are in place?\n\n> What is the Stochastic dynamical process in Figure A and how is this tethered to DyMon? \n\n> The authors mention an nth-order Markovian process implemention but is this not the case with any fully connected neural network implementation? What the reader fails to see is why DyMoN is different to these already-existing architectures.\n\n> In the teapot example, the authors mention a DyMoN architecture. (Page 8). Is this what is used throughout for all the experiments? If yes, why is it generalizable and if not, what is DyMoN\u2019s architecture? You could open the DyMoN box in Figure 10 (1) and explain what DyMoN consists.\n \n\nSection 2 is the crux of the paper and needs more work - explain the math in conjunction to the \u2018deep\u2019 architecture, what is the 'deep' architecture and why it is needed at all. Then go on to show/prove that the Markovian processes are indeed being realized. \n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper945/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Modeling Dynamics of Biological Systems with Deep Generative Neural Networks", "abstract": "Biological data often contains measurements of dynamic entities such as cells or organisms in various states of progression. However, biological systems are notoriously difficult to describe analytically due to their many interacting components, and in many cases, the technical challenge of taking longitudinal measurements. This leads to difficulties in studying the features of the dynamics, for examples the drivers of the transition. To address this problem, we present a deep neural network framework we call Dynamics Modeling Network or DyMoN. DyMoN is a neural network framework trained as a deep generative Markov model whose next state is a probability distribution based on the current state. DyMoN is well-suited to the idiosyncrasies of biological data, including noise, sparsity, and the lack of longitudinal measurements in many types of systems. Thus, DyMoN can be trained using probability distributions derived from the data in any way, such as trajectories derived via dimensionality reduction methods, and does not require longitudinal measurements. We show the advantage of learning deep models over shallow models such as Kalman filters and hidden Markov models that do not learn representations of the data, both in terms of learning embeddings of the data and also in terms training efficiency, accuracy and ability to multitask. We perform three case studies of applying DyMoN to different types of biological systems and extracting features of the dynamics in each case by examining the learned model. ", "keywords": ["neural networks", "markovian dynamics", "single-cell biology", "calcium imaging", "stochastic dynamics", "generative models"], "authorids": ["scott.gigante@yale.edu", "david.vandijk@yale.edu", "kevin.moon@usu.edu", "alexander.strzalkowski@yale.edu", "katie.ferguson@yale.edu", "jess.cardin@yale.edu", "guy.wolf@yale.edu", "smita.krishnaswamy@yale.edu"], "authors": ["Scott Gigante", "David van Dijk", "Kevin R. Moon", "Alexander Strzalkowski", "Katie Ferguson", "Guy Wolf", "Smita Krishnaswamy"], "TL;DR": "Dynamics Modeling Networks (DyMoN) offer advantages in representation, generation, visualization and feature extraction over shallow learning techniques for modeling stochastic dynamical systems in biology.", "pdf": "/pdf/991e2b1de4ebb49b53f18bc824b362fe744afffd.pdf", "paperhash": "gigante|modeling_dynamics_of_biological_systems_with_deep_generative_neural_networks", "_bibtex": "@misc{\ngigante2019modeling,\ntitle={Modeling Dynamics of Biological Systems with Deep Generative Neural Networks},\nauthor={Scott Gigante and David van Dijk and Kevin R. Moon and Alexander Strzalkowski and Katie Ferguson and Guy Wolf and Smita Krishnaswamy},\nyear={2019},\nurl={https://openreview.net/forum?id=S1gARiAcFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper945/Official_Review", "cdate": 1542234341052, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1gARiAcFm", "replyto": "S1gARiAcFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper945/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335838748, "tmdate": 1552335838748, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper945/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}