{"notes": [{"id": "H1lfwAVFwr", "original": "Skxdy8PuwS", "number": 1168, "cdate": 1569439322512, "ddate": null, "tcdate": 1569439322512, "tmdate": 1577168233119, "tddate": null, "forum": "H1lfwAVFwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL", "authors": ["Tyler James Malloy", "Matthew Riemer", "Miao Liu", "Tim Klinger", "Gerald Tesauro", "Chris R. Sims"], "authorids": ["mallot@rpi.edu", "mdriemer@us.ibm.com", "miao.liu1@ibm.com", "tklinger@us.ibm.com", "gtesauro@us.ibm.com", "simsc3@rpi.edu"], "keywords": ["Reinforcement Learning", "Generalization", "Information Theory", "Rate-Distortion Theory"], "TL;DR": "Applying a limit to the amount of information used to represent policies affords some improvements in generalization in Reinforcement Learning", "abstract": "Biological and artificial agents must learn to act optimally in spite of a limited capacity for processing, storing, and attending to information. We formalize this type of bounded rationality in terms of an information-theoretic constraint on the complexity of policies that agents seek to learn. We present the Capacity-Limited Reinforcement Learning (CLRL) objective which defines an optimal policy subject to an information capacity constraint. This objective is optimized by drawing from methods used in rate distortion theory and information theory, and applied to the reinforcement learning setting. Using this objective we implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and discuss their similarities and differences. Our experiments show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while displaying high sample efficiency and minimal requirements for hyper-parameter tuning.", "pdf": "/pdf/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "paperhash": "malloy|capacitylimited_reinforcement_learning_applications_in_deep_actorcritic_methods_for_continuous_control", "original_pdf": "/attachment/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "_bibtex": "@misc{\nmalloy2020capacitylimited,\ntitle={{\\{}CAPACITY{\\}}-{\\{}LIMITED{\\}} {\\{}REINFORCEMENT{\\}} {\\{}LEARNING{\\}}: {\\{}APPLICATIONS{\\}} {\\{}IN{\\}} {\\{}DEEP{\\}} {\\{}ACTOR{\\}}-{\\{}CRITIC{\\}} {\\{}METHODS{\\}} {\\{}FOR{\\}} {\\{}CONTINUOUS{\\}} {\\{}CONTROL{\\}}},\nauthor={Tyler James Malloy and Matthew Riemer and Miao Liu and Tim Klinger and Gerald Tesauro and Chris R. Sims},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lfwAVFwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "9YeGsBVwgQ", "original": null, "number": 1, "cdate": 1576798716293, "ddate": null, "tcdate": 1576798716293, "tmdate": 1576800920210, "tddate": null, "forum": "H1lfwAVFwr", "replyto": "H1lfwAVFwr", "invitation": "ICLR.cc/2020/Conference/Paper1168/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents Capacity-Limited Reinforcement Learning (CLRL) which builds on methods in soft RL to enable learning in agents with limited capacity.\n\nThe reviewers raised issues that were largely around three areas: there is a lack of clear motivation for the work, and many of the insights given lack intuition; many connections to related literature are missing; and the experimental results remain unconvincing.\n\nAlthough the ideas presented in the paper are interesting, more work is required for this to be accepted. Therefore at this point, this is unfortunately a rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL", "authors": ["Tyler James Malloy", "Matthew Riemer", "Miao Liu", "Tim Klinger", "Gerald Tesauro", "Chris R. Sims"], "authorids": ["mallot@rpi.edu", "mdriemer@us.ibm.com", "miao.liu1@ibm.com", "tklinger@us.ibm.com", "gtesauro@us.ibm.com", "simsc3@rpi.edu"], "keywords": ["Reinforcement Learning", "Generalization", "Information Theory", "Rate-Distortion Theory"], "TL;DR": "Applying a limit to the amount of information used to represent policies affords some improvements in generalization in Reinforcement Learning", "abstract": "Biological and artificial agents must learn to act optimally in spite of a limited capacity for processing, storing, and attending to information. We formalize this type of bounded rationality in terms of an information-theoretic constraint on the complexity of policies that agents seek to learn. We present the Capacity-Limited Reinforcement Learning (CLRL) objective which defines an optimal policy subject to an information capacity constraint. This objective is optimized by drawing from methods used in rate distortion theory and information theory, and applied to the reinforcement learning setting. Using this objective we implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and discuss their similarities and differences. Our experiments show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while displaying high sample efficiency and minimal requirements for hyper-parameter tuning.", "pdf": "/pdf/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "paperhash": "malloy|capacitylimited_reinforcement_learning_applications_in_deep_actorcritic_methods_for_continuous_control", "original_pdf": "/attachment/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "_bibtex": "@misc{\nmalloy2020capacitylimited,\ntitle={{\\{}CAPACITY{\\}}-{\\{}LIMITED{\\}} {\\{}REINFORCEMENT{\\}} {\\{}LEARNING{\\}}: {\\{}APPLICATIONS{\\}} {\\{}IN{\\}} {\\{}DEEP{\\}} {\\{}ACTOR{\\}}-{\\{}CRITIC{\\}} {\\{}METHODS{\\}} {\\{}FOR{\\}} {\\{}CONTINUOUS{\\}} {\\{}CONTROL{\\}}},\nauthor={Tyler James Malloy and Matthew Riemer and Miao Liu and Tim Klinger and Gerald Tesauro and Chris R. Sims},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lfwAVFwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1lfwAVFwr", "replyto": "H1lfwAVFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727555, "tmdate": 1576800279814, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1168/-/Decision"}}}, {"id": "rylS12pUYS", "original": null, "number": 1, "cdate": 1571376093303, "ddate": null, "tcdate": 1571376093303, "tmdate": 1572972503850, "tddate": null, "forum": "H1lfwAVFwr", "replyto": "H1lfwAVFwr", "invitation": "ICLR.cc/2020/Conference/Paper1168/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper presents a reinforcement learning method that regularizes the objective using the mutual information term.\nThe idea is simple and the paper is easy to follow. \n\nHowever, the novelty is limited since the difference between the proposed method and Soft Actor Critic (SAC) is just adding the entropy term of \\pi(a) to the objective function if I understand the method correctly. In addition, the intuition of adding the entropy term of \\pi(a) to the objective is not clearly described.\n\nThe proposed method is evaluated on continuous control tasks.The results shown in the paper is mixed, and I cannot conclude that the proposed method outperforms SAC. Thus, the benefit of the proposed method is not clearly supported by the experimental results.\n\nFor the current form of the paper, I give \"weak reject\" due to the weak support of the experimental results and the unclear motivation of the method.\n\nOne of my concerns is that the way of estimating the \\pi(a) which is the marginal distribution of the action.\nFrom the current manuscript, I did not fully understand how it is estimated in the proposed method.\nI think that the accuracy of the estimation of \\pi(a) is crucial in the proposed method since it is the difference from SAC.\n\nA comment on the paper structure is that the connection to the \"capacity-limited\" objective should be described more explicitly in Section 2.1.\nAlthough the \"capacity-limited\" reminds me of the objective something like \\mathcal{L} + | C - I(X;Y)  | as in [Dupont, 2018],\nthe objective in the proposed method shown in page 2 is \\mathcal{L} + \\beta I(X;Y).\nI did not understand why the proposed method is \"capacity-limited\" until Section 5.\nI think authors should explicitly mention in Section 2.1 that \\beta is adjusted so as to limit the information capacity.\n\nTo improve the manuscript, I request authors the following things:\n\n- The proposed method can be interpreted as adding the penalizing the entropy of \\pi(a) to the entropy-regularized RL.\nI do not fully understand the intuition of penalizing the entropy of \\pi(a) in the context of RL. Please explain it.\n\n- I think some tasks should be performed with longer training. \nFor example, agent should be trained for 1-2 millions steps on Humanoid and Ant tasks.\nIn addition, evaluation with 10 trials are preferable.\n\n- Please cite papers that estimate the marginal distribution \\pi(a) in the same manner. \nIf there is no previous work, please explain the details of estimating \\pi(a) and how \\pi(a) is approximated from samples.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1168/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1168/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL", "authors": ["Tyler James Malloy", "Matthew Riemer", "Miao Liu", "Tim Klinger", "Gerald Tesauro", "Chris R. Sims"], "authorids": ["mallot@rpi.edu", "mdriemer@us.ibm.com", "miao.liu1@ibm.com", "tklinger@us.ibm.com", "gtesauro@us.ibm.com", "simsc3@rpi.edu"], "keywords": ["Reinforcement Learning", "Generalization", "Information Theory", "Rate-Distortion Theory"], "TL;DR": "Applying a limit to the amount of information used to represent policies affords some improvements in generalization in Reinforcement Learning", "abstract": "Biological and artificial agents must learn to act optimally in spite of a limited capacity for processing, storing, and attending to information. We formalize this type of bounded rationality in terms of an information-theoretic constraint on the complexity of policies that agents seek to learn. We present the Capacity-Limited Reinforcement Learning (CLRL) objective which defines an optimal policy subject to an information capacity constraint. This objective is optimized by drawing from methods used in rate distortion theory and information theory, and applied to the reinforcement learning setting. Using this objective we implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and discuss their similarities and differences. Our experiments show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while displaying high sample efficiency and minimal requirements for hyper-parameter tuning.", "pdf": "/pdf/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "paperhash": "malloy|capacitylimited_reinforcement_learning_applications_in_deep_actorcritic_methods_for_continuous_control", "original_pdf": "/attachment/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "_bibtex": "@misc{\nmalloy2020capacitylimited,\ntitle={{\\{}CAPACITY{\\}}-{\\{}LIMITED{\\}} {\\{}REINFORCEMENT{\\}} {\\{}LEARNING{\\}}: {\\{}APPLICATIONS{\\}} {\\{}IN{\\}} {\\{}DEEP{\\}} {\\{}ACTOR{\\}}-{\\{}CRITIC{\\}} {\\{}METHODS{\\}} {\\{}FOR{\\}} {\\{}CONTINUOUS{\\}} {\\{}CONTROL{\\}}},\nauthor={Tyler James Malloy and Matthew Riemer and Miao Liu and Tim Klinger and Gerald Tesauro and Chris R. Sims},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lfwAVFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lfwAVFwr", "replyto": "H1lfwAVFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575357257405, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1168/Reviewers"], "noninvitees": [], "tcdate": 1570237741352, "tmdate": 1575357257418, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1168/-/Official_Review"}}}, {"id": "HJxHhbyRFB", "original": null, "number": 2, "cdate": 1571840428553, "ddate": null, "tcdate": 1571840428553, "tmdate": 1572972503810, "tddate": null, "forum": "H1lfwAVFwr", "replyto": "H1lfwAVFwr", "invitation": "ICLR.cc/2020/Conference/Paper1168/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "\n# Summary\nThe paper replaces the entropy term in the objective of SAC with a KL(\\pi(a|s) || p(a)) where p(a) is the marginal distribution p(a) = \\int \\pi(a|s) \\mu(s) ds. It derives the gradient of this modified objective and then proceeds to evaluate the resulting \"SAC with mutual information\" to demonstrate (i) better sample efficiency and (ii) generalization to environment change.\n\n\n# Decision\nThe paper proposes an interesting approach but a more thorough evaluation is needed before it can be recommended for publication.\n\n\n# Comments\nThe theoretical contribution is quite small since the connection to RD theory is well established by now, e.g., see papers by Daniel Alexander Braun from Ulm and his students. Therefore, the main contribution is incorporation of the mutual information into SAC. Then the question is what properties/advantages/disadvantages such approach brings. And on this front, the paper is quite weak.\n\nThe chain environment is quite toy-ish and may only serve to indicate that the implementation is correct. The continuous control environments are more interesting, but the learning curves look very unreliable. Just a quick Google search for SAC results on Roboschool reveals much smoother and higher learning curves (e.g., https://medium.com/@kengz/soft-actor-critic-for-continuous-and-discrete-actions-eeff6f651954). It would be paramount to make sure that the baselines are fairly represented in the evaluations.\n\nThe generalization in continuous control is only evaluated on the pendulum by varying length and mass. It is insufficient to make a decisive judgement. Moreover, length and mass are coupled, such that different combinations of length and mass may yield similar dynamics.\n\n=> If better sample efficiency and better generalization are claimed, then a more thorough evaluation is required.\n\nIn general, writing can be improved, figure made nicer and smaller, introduction and connections section made shorter, and the main derivation moved from the Appendix into the main body and explained better.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1168/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1168/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL", "authors": ["Tyler James Malloy", "Matthew Riemer", "Miao Liu", "Tim Klinger", "Gerald Tesauro", "Chris R. Sims"], "authorids": ["mallot@rpi.edu", "mdriemer@us.ibm.com", "miao.liu1@ibm.com", "tklinger@us.ibm.com", "gtesauro@us.ibm.com", "simsc3@rpi.edu"], "keywords": ["Reinforcement Learning", "Generalization", "Information Theory", "Rate-Distortion Theory"], "TL;DR": "Applying a limit to the amount of information used to represent policies affords some improvements in generalization in Reinforcement Learning", "abstract": "Biological and artificial agents must learn to act optimally in spite of a limited capacity for processing, storing, and attending to information. We formalize this type of bounded rationality in terms of an information-theoretic constraint on the complexity of policies that agents seek to learn. We present the Capacity-Limited Reinforcement Learning (CLRL) objective which defines an optimal policy subject to an information capacity constraint. This objective is optimized by drawing from methods used in rate distortion theory and information theory, and applied to the reinforcement learning setting. Using this objective we implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and discuss their similarities and differences. Our experiments show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while displaying high sample efficiency and minimal requirements for hyper-parameter tuning.", "pdf": "/pdf/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "paperhash": "malloy|capacitylimited_reinforcement_learning_applications_in_deep_actorcritic_methods_for_continuous_control", "original_pdf": "/attachment/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "_bibtex": "@misc{\nmalloy2020capacitylimited,\ntitle={{\\{}CAPACITY{\\}}-{\\{}LIMITED{\\}} {\\{}REINFORCEMENT{\\}} {\\{}LEARNING{\\}}: {\\{}APPLICATIONS{\\}} {\\{}IN{\\}} {\\{}DEEP{\\}} {\\{}ACTOR{\\}}-{\\{}CRITIC{\\}} {\\{}METHODS{\\}} {\\{}FOR{\\}} {\\{}CONTINUOUS{\\}} {\\{}CONTROL{\\}}},\nauthor={Tyler James Malloy and Matthew Riemer and Miao Liu and Tim Klinger and Gerald Tesauro and Chris R. Sims},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lfwAVFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lfwAVFwr", "replyto": "H1lfwAVFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575357257405, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1168/Reviewers"], "noninvitees": [], "tcdate": 1570237741352, "tmdate": 1575357257418, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1168/-/Official_Review"}}}, {"id": "r1eDVHYJ9B", "original": null, "number": 3, "cdate": 1571947822916, "ddate": null, "tcdate": 1571947822916, "tmdate": 1572972503763, "tddate": null, "forum": "H1lfwAVFwr", "replyto": "H1lfwAVFwr", "invitation": "ICLR.cc/2020/Conference/Paper1168/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose capacity-limited reinforcement learning and apply an actor-critic method (CLAC) in some continuous control domains. The authors claim that CLAC gives improvements in generalization from training to modified test environments, and that it shows high sample efficiency and requires minimal hyper-parameter tuning.\n\nThe introduction started off making me think about this area in a new way, but as the paper continued I started to find some issues. To begin with, I think the motivation in the introduction could be improved. Why would I choose to limit capacity? This is not sufficiently motivated. I suspect that the author(s) want to argue that it *should* give better generalization, but this argument is not made very clearly in the introduction. Perhaps this is because it would be difficult to make this argument formally, and so it is merely suggested at?\n\nAre there connections between this and things like variational intrinsic control (VIC, Gregor et al. 2016) and diversity is all you need (DIAYN, Eysenbach et al., 2019)? These works aim to maximize the mutual information between latent variable policies and states/trajectories, whereas this work is really doing the opposite. I would be interested in understanding the author\u2019s take on how the two are related conceptually.\n\nMoving to the connections with past work, this paper seriously abuses notation in a way that actually hinders comprehension. Some of the parts that really bothered me, and should be fixed to be correct:\n\nMutual information is a function of two random variables, whereas it is repeatedly expressed as a function of the policy. Being explicit about the random variables / distribution here is pretty important.\n\nIn Equation 2 (and subsequent paragraph) the marginal distributions p_a(a) and p_s(s) are not well defined, marginalizing over what, what are these distributions? I might guess that p_s(s) is the steady state distribution under a policy pi, and that p_a(a) is marginalizing over the same distribution, essentially capturing the prior probability of each action under the policy. But these sort of things need to be said explicitly.\n\nIn KL-RL section there is a sentence with \u201cThis allows us to define KL-RL to be the case where p_0(a, s) = \\pi_0(a_t | s_t).\u201d What does this actually mean? One of these is a joint probability for state and action, and one is an action probability conditional on a state. \n\nWhat does \\pi_\\mu(a_t) \\sim \\mathcal{D} mean? \n\nIn the block just before Algorithm 1, many of these symbols are never defined. This needs a significant amount of care (by the authors) and right now relies on the reader to simply make a best guess at what the authors probably intend.\n\nOverall in the first three sections the message I would like the authors to understand is that, in striving for a concise explanation they have significantly overshot. These sections require some significant work to be considered publishable.\n\nThe experiment in section 4.1 is intended to give a clean intuitive understanding of the method, but falls a bit short here. It is clean, but I needed more explanation to really drive the intuition home. I see that CLAC finds a solution more sensitive to the beta distribution, but help me understand why this is the right solution in this particular case.\n\nI really disagree with the conclusions around the experiments in section 4.2. I do not think these results show that for the CLAC model increasing the mutual information coefficient increases performance on the perturbed environments. First, the obvious, how many seeds and where are the standard deviations? Second, the trend is extremely small and the gap between CLAC and SAC is just as minor. Finally, CLAC has better performance on the training distribution which means that it actually lost *more* performance than SAC when transferring to the testing and extreme testing distributions.\n\nThe results for section 4.3 are just not significant enough to draw any real conclusions. The massive temporal variability makes me very suspicious of those super tight error bands, but even without that question, the gap is just not very large.\n\nFinally, in section 4.4 we see the first somewhat convincing experimental results. These look reasonable, but even here I have a fairly pointed question: compared with the results in Packer et al (2018) the amount of regression from training to testing is extremely large (whereas they found vanilla algorithms transfer surprisingly well). Can you explain why there is such a big discrepancy between those results and these? But again, this section\u2019s results are in my opinion the most convincing that something interesting is happening here.\n\nLastly, in section 8.1 the range of hyper-parameters for the mutual information coefficient is very broad, which really makes it hard to buy the claim of requiring minimal hyper-parameter tuning.\n\nAll in all there is something truly interesting in this work, but in the present state I am unable to recommend acceptance, and the amount of work required along with questions raised lead me to be fairly confident in this assessment.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1168/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1168/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAPACITY-LIMITED REINFORCEMENT LEARNING: APPLICATIONS IN DEEP ACTOR-CRITIC METHODS FOR CONTINUOUS CONTROL", "authors": ["Tyler James Malloy", "Matthew Riemer", "Miao Liu", "Tim Klinger", "Gerald Tesauro", "Chris R. Sims"], "authorids": ["mallot@rpi.edu", "mdriemer@us.ibm.com", "miao.liu1@ibm.com", "tklinger@us.ibm.com", "gtesauro@us.ibm.com", "simsc3@rpi.edu"], "keywords": ["Reinforcement Learning", "Generalization", "Information Theory", "Rate-Distortion Theory"], "TL;DR": "Applying a limit to the amount of information used to represent policies affords some improvements in generalization in Reinforcement Learning", "abstract": "Biological and artificial agents must learn to act optimally in spite of a limited capacity for processing, storing, and attending to information. We formalize this type of bounded rationality in terms of an information-theoretic constraint on the complexity of policies that agents seek to learn. We present the Capacity-Limited Reinforcement Learning (CLRL) objective which defines an optimal policy subject to an information capacity constraint. This objective is optimized by drawing from methods used in rate distortion theory and information theory, and applied to the reinforcement learning setting. Using this objective we implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and discuss their similarities and differences. Our experiments show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while displaying high sample efficiency and minimal requirements for hyper-parameter tuning.", "pdf": "/pdf/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "paperhash": "malloy|capacitylimited_reinforcement_learning_applications_in_deep_actorcritic_methods_for_continuous_control", "original_pdf": "/attachment/f9faaeb99481bc0e85926249d884fcab6876ccc7.pdf", "_bibtex": "@misc{\nmalloy2020capacitylimited,\ntitle={{\\{}CAPACITY{\\}}-{\\{}LIMITED{\\}} {\\{}REINFORCEMENT{\\}} {\\{}LEARNING{\\}}: {\\{}APPLICATIONS{\\}} {\\{}IN{\\}} {\\{}DEEP{\\}} {\\{}ACTOR{\\}}-{\\{}CRITIC{\\}} {\\{}METHODS{\\}} {\\{}FOR{\\}} {\\{}CONTINUOUS{\\}} {\\{}CONTROL{\\}}},\nauthor={Tyler James Malloy and Matthew Riemer and Miao Liu and Tim Klinger and Gerald Tesauro and Chris R. Sims},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lfwAVFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lfwAVFwr", "replyto": "H1lfwAVFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1168/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575357257405, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1168/Reviewers"], "noninvitees": [], "tcdate": 1570237741352, "tmdate": 1575357257418, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1168/-/Official_Review"}}}], "count": 5}