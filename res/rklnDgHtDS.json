{"notes": [{"id": "rklnDgHtDS", "original": "HyeTkneKvB", "number": 2375, "cdate": 1569439843906, "ddate": null, "tcdate": 1569439843906, "tmdate": 1583912029071, "tddate": null, "forum": "rklnDgHtDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "2bws3MYb5P", "original": null, "number": 1, "cdate": 1576798747482, "ddate": null, "tcdate": 1576798747482, "tmdate": 1576800888596, "tddate": null, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "invitation": "ICLR.cc/2020/Conference/Paper2375/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The paper addresses the task of continual learning in NLP for seq2seq style tasks. The key idea of the proposed method is to enable the network to represent syntactic and semantic knowledge separately, which allows the neural network to leverage compositionality for knowledge transfer and also solves the problem of catastrophic forgetting. The paper has been improved substantially after the reviewers' comments and also obtains good results on benchmark tasks. The only concern is that the evaluation is on artificial datasets. In future, the authors should try to include more evaluation on real datasets (however, this is also limited by availability of such datasets). As of now, I'm recommending an Acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795703062, "tmdate": 1576800250335, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2375/-/Decision"}}}, {"id": "S1xahryRFr", "original": null, "number": 3, "cdate": 1571841460645, "ddate": null, "tcdate": 1571841460645, "tmdate": 1574367894112, "tddate": null, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "invitation": "ICLR.cc/2020/Conference/Paper2375/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper is about continual learning on NLP applications like natural language instruction learning or machine translation. The authors propose to exploit \"compositionality\" to separate semantics and syntax so as to facilitate the problem of interest.\n\nIn summary, the current manuscript is clearly not ready for publication. The writing is not good, as I cannot see clearly the backbone of the paper. Honestly, I got very confused by the presented contents. What\u2019s the problem/motivation? No preliminary background knowledge? What\u2019s the classic or na\u00efve method to solve the problem of interest? What\u2019s the main advantage/novelty of the presented method compared to that classic/na\u00efve method?\n\nPlease see the detailed comments below.\n\nIn the Abstract, you mentioned \"One of the key skills \u2026 ability to produce novel composition\u2019\u2019. Do you imply your method can continually learn new compositions? If so, how does that reflect in the technical parts and the experiments?\n\nThe paragraph before Section 3 might be redundant.\n\nMany typos exist. Such as the word \"iuput\u2019\u2019 in the 1st paragraph of Page 4.\n\nWhat\u2019s the problem of S2S-CL? Increasing input number n and output number m?\n\nWhat does the word \"COMPOSITIONALITY\u2019\u2019 mean in Section 3.2? Also, what\u2019s the relationship between the last two equations of Page 4?\n\nHow do you defend the simplifications adopted in the first Equation of Page 5?\n\nThe notation \"{0,1}^{Uxn}\u2019\u2019 usually represents a binary matrix of size Uxn. It is not suitable to use them to represent a matrix containing one-hot columns.\n\nAt the beginning of Page 6. Why entropy regularization can be introduced via L2 norm on the embedding matrixes p and f? Also why that L2 norm regularizations can `achieve disentanglement\u2019? Please provide the detailed proof or the reference. \n\nWhat are the detailed settings of the demonstrated experiments?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2375/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2375/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575751179916, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2375/Reviewers"], "noninvitees": [], "tcdate": 1570237723722, "tmdate": 1575751179934, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2375/-/Official_Review"}}}, {"id": "HkgYv3uhjB", "original": null, "number": 5, "cdate": 1573846112544, "ddate": null, "tcdate": 1573846112544, "tmdate": 1573848551557, "tddate": null, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "invitation": "ICLR.cc/2020/Conference/Paper2375/-/Official_Comment", "content": {"title": "Response to all reviews", "comment": "Thank you for reviews. We summarized some updates based on the suggestions.\n- Added flowchart in Figure 1.\n- Added standard deviations in Figure 3.\n- Revised the paper to improve writing and language.\n- Made the overall logic more clear.\n- Removed some redundant texts.\n- Corrected typos and confusing notations."}, "signatures": ["ICLR.cc/2020/Conference/Paper2375/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rklnDgHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference/Paper2375/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2375/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2375/Reviewers", "ICLR.cc/2020/Conference/Paper2375/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2375/Authors|ICLR.cc/2020/Conference/Paper2375/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142299, "tmdate": 1576860536033, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference/Paper2375/Reviewers", "ICLR.cc/2020/Conference/Paper2375/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2375/-/Official_Comment"}}}, {"id": "rklSQhOnjB", "original": null, "number": 4, "cdate": 1573846044577, "ddate": null, "tcdate": 1573846044577, "tmdate": 1573846044577, "tddate": null, "forum": "rklnDgHtDS", "replyto": "rke6W0h6YH", "invitation": "ICLR.cc/2020/Conference/Paper2375/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thanks very much for your helpful comments and support.\n\nQ1: Evaluation on the more realistic dataset.\nA1:  Yes, we agree. This is the initial research for compositional continual language learning, aiming at finding fundamental mechanism for such problem, so we started with these instruction learning and machine translation datasets. Also, artificial data helps to clearly show that the core idea is valid."}, "signatures": ["ICLR.cc/2020/Conference/Paper2375/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rklnDgHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference/Paper2375/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2375/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2375/Reviewers", "ICLR.cc/2020/Conference/Paper2375/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2375/Authors|ICLR.cc/2020/Conference/Paper2375/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142299, "tmdate": 1576860536033, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference/Paper2375/Reviewers", "ICLR.cc/2020/Conference/Paper2375/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2375/-/Official_Comment"}}}, {"id": "HkxxasO2jr", "original": null, "number": 3, "cdate": 1573845944122, "ddate": null, "tcdate": 1573845944122, "tmdate": 1573845944122, "tddate": null, "forum": "rklnDgHtDS", "replyto": "ryeyDTniFH", "invitation": "ICLR.cc/2020/Conference/Paper2375/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thanks very much for your constructive comments and support.\n\nQ1: The novelty is somewhat limited. However, this might be okay since the experiments are quite compelling and the method is applied to a different setting.\nA1: Thank you. We agree that this is okay. Our novelty is to propose continual language learning, and address it with compositionality to bridge LP-CL and S2S-CL.\n\nQ2: Notations and language in section 3.2-3.4.\nA2: Thank you very much. We revised the paper and fixed notations and language.\nFor the syntax embedding of a new word, the new word is still supposed to have syntactic information, but it is seen information, so that the model should still learn it to be close to some existing syntax embeddings. These syntax embeddings should correspond to empty syntactic information in instruction learning task. It might also be feasible to encode this to model design, but the current design is simpler.\n\nQ3: Freezing \\theta contradicts the claim that only \\phi is frozen.\nA3: In Section 3.2, we freeze \\phi to keep syntax processing ability in continual learning, and leave learning semantic parameters to LP-CL algorithm. In this case, the non-parametric LP-CL algorithm freezes a part of \\theta. We now made it more clear in Section 3.2.\n\nQ4: The scalability without co-adapting the other embedding. Perhaps it could be alleviated if f_predict has extremely high capacity at initialization?\nA4: This is the initial research for continual language learning, and we assume there is no new syntax information during continual learning. This assumption is reasonable to some extent, because syntax has less variations than semantics, and syntax does not change frequently. Since semantics and syntax are separated, increasing the capacity of f_predict at initialization may not address syntax continual learning, or at least it is not an efficient approach.\n\nQ5: Analysis on the decreasing performance in the instruction learning task.\nA5:  The performance may decrease during the end of the second phrase (discussion Section 5.2) of continual learning where the embeddings squeeze into the explored space, maybe because exploring becomes expensive with the dense population under regularization. We wrote the analysis more clearly in the discussion Section 5.2.\n\nQ6: Short-comings of the approach and how to overcome.\nA6: One short-coming is that this work focuses on continual learning for semantics, but not for syntax. To make it possible, we may need to address hierarchical compositionality, maybe with stacked attention models such as transformer.\n\nQ7: Overall flow chart.\nA7: We added an overall flow chart in Figure 1.\n\nQ8: Error bars to the plots.\nA8: We added standard deviations in Figure 3.\n\nQ9: What characteristics of the 2 tasks cause the baselines to behave so differently?\nA9: The 2 tasks are compositional continual language learning problems, with increasing numbers of vocabulary. The baselines do not handle such characteristics, while the proposed method does, so that the proposed method outperforms the baselines."}, "signatures": ["ICLR.cc/2020/Conference/Paper2375/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rklnDgHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference/Paper2375/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2375/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2375/Reviewers", "ICLR.cc/2020/Conference/Paper2375/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2375/Authors|ICLR.cc/2020/Conference/Paper2375/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142299, "tmdate": 1576860536033, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference/Paper2375/Reviewers", "ICLR.cc/2020/Conference/Paper2375/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2375/-/Official_Comment"}}}, {"id": "rygs4o_3jS", "original": null, "number": 2, "cdate": 1573845811016, "ddate": null, "tcdate": 1573845811016, "tmdate": 1573845811016, "tddate": null, "forum": "rklnDgHtDS", "replyto": "S1xahryRFr", "invitation": "ICLR.cc/2020/Conference/Paper2375/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for the review, question and suggestions.\n\nQ1: Writing and backbone of the paper.\nA1: We revised the paper and made the main points more clear. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality.\n\nQ2: Problem/motivation, preliminary background knowledge\nA2: The main problem is how to enable continual language learning with compositionality. The backgrounds are covered in introduction and related work sections (there are also pointers to references).\n\nQ3: classic or na\u00efve methods?\nA3: The classic or na\u00efve continual learning algorithms (Kirkpatrick et al., 2017a; Aljundi et al., 2018) mostly focus on label prediction tasks (with fixed input and output sizes), but we address sequence to sequence tasks (with unfixed input and output sizes), common in language learning. We use these methods as experiment baselines.\n\nQ4: Main advantage/novelty compared to classic/na\u00efve method?\nA4: Our approach bridges the gap between label prediction and sequence-to-sequence learning by using compositionality in language. To our knowledge, this is the first work for applying compositionality to continual learning of sequence-to-sequence tasks. Experiments show that the proposed method has significant improvement over multiple state-of-the-art baselines.\n\nQ5: Does \"One of the key skills \u2026 ability to produce novel composition\u2019\u2019 in abstract imply your method can continually learn new compositions? How does that reflect in the technical parts and the experiments?\nA5: Yes, this skill is the compositional generalization skill, which is reflected in Section 3.3 for technical parts, and the transfer experiments in Section 4. We made it more clear in the abstract.\n\nQ6: Redundant paragraph before Section 3.\nA6: We removed it.\n\nQ7: Typos.\nA7: Thank you for pointing out. We revised the paper carefully.\n\nQ8: What\u2019s the problem of S2S-CL? Increasing input number n and output number m?\nA8: S2S-CL is Sequence-to-Sequence Continual Learning problem. It is not about increasing input number n and output number m, but about increasing input and output vocabulary sizes. (Section 3.1)\n\nQ9: What does \"COMPOSITIONALITY\u2019\u2019 mean in Section 3.2? What\u2019s the relationship between the last two equations of Page 4?\nA9: In Section 3.2, compositionality means the language property that syntax and semantics can be separated, and the output syntactic information depends only the input syntactic information, and (given output syntactic information,) the semantic output information depends only on the input semantic information. Please refer to (Li, 2019) for more details. The last two equations (Eq. 2 and Eq. 3) on Page 4 are probabilistic interpretation of the compositionality property.\n\nQ10: Simplifications in the first Equation of Page 5?\nA10: This simplification is valid because we design the model in the way that for each output label, Y^f tells which x^p corresponds to the label, and X^p tells the value of x^p, so that this label can be inferred without knowing other labels. (Eq. 4)\n\nQ11: Notation \"{0,1}^{Uxn}\u2019\u2019.\nA11: We borrow the notation from (Li, 2019). We now do not use the notation, but explain it in the text.\n\nQ12: Entropy regularization with L2 norm.\nA12: Adding noise and using L2 regularization together reduce channel capacity (amount of information it can contain) in each representation and thus entropy for the representations. Please see Section 2 in (Li et al 2019) for more information.\n\nQ13: What are the detailed settings of the demonstrated experiments?\nA13: This is explained in the first paragraph of Section 4 and Appendix A provides more details. Please see Table 2 for simple examples. In summary, the experiments include two stages. The first stage is a standard process in which we train a model with combinations of multiple words in various sentence structures. In each continual stage, we add a new input word and corresponding new output symbol. The training dataset contains only one sample, whose input is a sentence with the new word, and output is a sequence with the new symbol. For each continual stage, we can only use the data in that stage, and have no access to data in previous or future stages.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2375/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rklnDgHtDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference/Paper2375/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2375/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2375/Reviewers", "ICLR.cc/2020/Conference/Paper2375/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2375/Authors|ICLR.cc/2020/Conference/Paper2375/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142299, "tmdate": 1576860536033, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2375/Authors", "ICLR.cc/2020/Conference/Paper2375/Reviewers", "ICLR.cc/2020/Conference/Paper2375/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2375/-/Official_Comment"}}}, {"id": "ryeyDTniFH", "original": null, "number": 1, "cdate": 1571700055134, "ddate": null, "tcdate": 1571700055134, "tmdate": 1572972346512, "tddate": null, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "invitation": "ICLR.cc/2020/Conference/Paper2375/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposes an approach for continual learning for applications in sequence-to-sequence continual learning (S2S-CL). More specifically, the paper addresses the problem of growing vocabulary. The overall approach is intuitive yet quite effective. The method assumes a split between the syntax (f) and semantics (p) of the sequence -- in other words, each token is associated with two labels. Furthermore, the syntax is assumed to be shared across all the training sequences and the sequences that are not encountered during the initial training. A sequence model (LSTM) is used for learning the syntax f over the initial training and is then frozen for all the downstream tasks (i.e. continual learning). The sequence model predicts the correspondence between the output token and input tokens. Another network (a 1-layer  MLP) is used to predict the semantic label of the selected token in the target domain. Barring some notational confusion, I believe the method is very reasonable and should work well. They perform experiments on 2 datasets (Instruction learning & machine translation) in 3 continuous learning paradigms with varying difficulties and demonstrate that the proposed method significantly outperforms the baselines by an impressive margin.\n\nI am leaning towards accepting this paper since I believe the proposed approach can be a promising direction for continual learning in S2S settings and the empirical results are convincing.\n\nHowever, I have some concerns that might require clarification or additional experiments:\n    1. The novelty of the proposed method is somewhat limited in my opinion. It seems it is a very simple adaptation of Li et al. 19 and the only addition to that work is the usage of a very simple label prediction scheme by fixing everything else. However, this might be okay since the experiments are quite compelling and the method is applied to a different setting.\n\n    2. Notations and language in section 3.2-3.4 are hard to follow:\n        - At the bottom of page 4, I believe some of the equations are not correct. For example, the first term of line 2 should be P(Y^f | X^f, X^p) rather than Y^p?\n        - In the second equation of page 6, the second term should be v_j = p\u2019 \\cdot a_j\n        - In Figure 1, k_r, E_r, and W are never defined (although their meaning can be inferred).\n        - In section 3.4, it reads that new word embedding is appended to both semantic and syntactic embedding matrices but this doesn\u2019t make sense because the syntactic network is already fixed so it shouldn\u2019t be able to handle new symbols; therefore, I believe only new row is added to the semantic embedding.\n\n    3. I believe that f_predict here is parameterized by \\theta and \\theta is also frozen during the continual learning phase which contradicts the claim at the end of section 3.2 that only \\phi is frozen. Otherwise, it\u2019s hard to see why f_predict does not suffer from catastrophic forgetting. From the provided source code, it seems that it is indeed the case that only the new embedding is being updated. In other words, the only thing happening at this stage is that the newly added embedding are optimized to adapt to the frozen f_predict.\n\n    4. Due to point 3, I have some doubts about how scalable this approach is if the other embedding is not allowed to co-adapt. However, perhaps this problem could be alleviated if f_predict has extremely high capacity at initialization? More on this in point 6.\n\n    5. Assuming all my assessment above is correct, it seems that the performance of the proposed method should *not* decrease at all so I would like to see more analysis on the decreasing performance in the instruction learning task (Figure 2, left column). Is this be due to the fact that f_predict or the other embedding is not allowed to update and the newly added embedding is mapped close to existing embedding? In that case, can increasing the model capacity solve this problem? I understand the paper makes argument about regularization but I believe this warrants thorough study for gauging the significance of this approach in more realistic settings.\n\n    6. I would like to see a discussion of the short-comings of the approach and possible ways to overcome them. For example, freezing the syntactic network seems limiting in machine translation settings if a new language, say Italian, is added. Intuitively, knowing how to translate English-French should help translating English to Italian but fixing the syntax prevents this. Another example is that prior knowledge about the syntax needs to be known about the language for labeling the f and p and this can be expensive and cannot handle words that have more than 1 usages (e.g. run can be used as a noun but also as a verb).\n\nI am willing to increase my score to accept if the revised manuscript can address the majority of, if not all of the concerns listed above.\n\n========================================================================================================\nMinor comments that did not affect my decision:\n    - These sections could greatly benefit from an overall flow chart of how everything fits together\n    - It says the experiments are repeated with 5 different random seeds so why not add error bars to the plots?\n    - What characteristics of the 2 tasks cause the baselines to behave so differently?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2375/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2375/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575751179916, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2375/Reviewers"], "noninvitees": [], "tcdate": 1570237723722, "tmdate": 1575751179934, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2375/-/Official_Review"}}}, {"id": "rke6W0h6YH", "original": null, "number": 2, "cdate": 1571831301417, "ddate": null, "tcdate": 1571831301417, "tmdate": 1572972346475, "tddate": null, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "invitation": "ICLR.cc/2020/Conference/Paper2375/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "*Summary \n\nThe paper proposes a continual learning algorithm for label prediction to deal with sequence-to-sequence continual learning problems. The proposed method is designed to leverage compositionality.  The key idea of the proposed method is to enable the network to represent syntactic and semantic knowledge separately. This allows the neural network to leverage compositionality for knowledge transfer while alleviating catastrophic forgetting.\nThe experiments showed that their method performed significantly better results than baseline methods.  The method was tested on two different datasets, e.g., instruction Learning and machine translation.\n\n\n*Decision and supporting arguments\n\nI think this paper has enough quality to be be accepted as a conference paper.\nThe main reasons of my decision are two-folds.\nFirst, the proposal is quite insightful. The separation of semantics and syntax of an input sentence for using compositionality is an excellent idea.\nSecond, the proposed method improved the performance on two dataset significantly. This supports the usefulness of the idea.\n\n\n*Additional feedback\n\nMy concern is about evaluation. Table 1 shows the significant difference between the proposed method and the baseline methods. It looks to nice. But, this suggests that the datasets might be too artificial for this evaluation. To my understanding, both of the datasets are artificial to some extent. Hopefully, the method should be evaluated on the more realistic dataset.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2375/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2375/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yuanpeng16@gmail.com", "lzhao4ever@gmail.com", "kenneth.ward.church@gmail.com", "mohamed.elhoseiny@gmail.com"], "title": "Compositional Language Continual Learning", "authors": ["Yuanpeng Li", "Liang Zhao", "Kenneth Church", "Mohamed Elhoseiny"], "pdf": "/pdf/4e2192bb77fa7bcaebfbef4e750c6f30c5ae38f0.pdf", "abstract": "Motivated by the human's ability to continually learn and gain knowledge over time, several research efforts have been pushing the limits of machines to constantly learn while alleviating catastrophic forgetting. Most of the existing methods have been focusing on continual learning of label prediction tasks, which have fixed input and output sizes. In this paper, we propose a new scenario of continual learning which handles sequence-to-sequence tasks common in language learning. We further propose an approach to use label prediction continual learning algorithm for sequence-to-sequence continual learning by leveraging compositionality. Experimental results show that the proposed method has significant improvement over state-of-the-art methods. It enables knowledge transfer and prevents catastrophic forgetting, resulting in more than 85% accuracy up to 100 stages, compared with less than 50% accuracy for baselines in instruction learning task. It also shows significant improvement in machine translation task. This is the first work to combine continual learning and compositionality for language learning, and we hope this work will make machines more helpful in various tasks.", "code": "https://github.com/yli1/CLCL", "keywords": ["Compositionality", "Continual Learning", "Lifelong Learning", "Sequence to Sequence Modeling"], "paperhash": "li|compositional_language_continual_learning", "_bibtex": "@inproceedings{\nLi2020Compositional,\ntitle={Compositional Language Continual Learning},\nauthor={Yuanpeng Li and Liang Zhao and Kenneth Church and Mohamed Elhoseiny},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rklnDgHtDS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9d0445817a704781fe49bcbb0371b963a7a82ab3.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rklnDgHtDS", "replyto": "rklnDgHtDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2375/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575751179916, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2375/Reviewers"], "noninvitees": [], "tcdate": 1570237723722, "tmdate": 1575751179934, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2375/-/Official_Review"}}}], "count": 9}