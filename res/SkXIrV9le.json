{"notes": [{"tddate": null, "tmdate": 1486867776943, "tcdate": 1486867776943, "number": 6, "id": "HkKB6SpOl", "invitation": "ICLR.cc/2017/conference/-/paper213/public/comment", "forum": "SkXIrV9le", "replyto": "S16zhMLux", "signatures": ["~Eder_Santana1"], "readers": ["everyone"], "writers": ["~Eder_Santana1"], "content": {"title": "Thanks for the reviews.", "comment": "We appreciate the reviews and suggestions. We will submit the paper to the Workshop Track."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682618, "id": "ICLR.cc/2017/conference/-/paper213/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkXIrV9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper213/reviewers", "ICLR.cc/2017/conference/paper213/areachairs"], "cdate": 1485287682618}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396436613, "tcdate": 1486396436613, "number": 1, "id": "S16zhMLux", "invitation": "ICLR.cc/2017/conference/-/paper213/acceptance", "forum": "SkXIrV9le", "replyto": "SkXIrV9le", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The strengths and weaknesses pointed out by the reviews were:\n \n Strengths\n Interesting methodology to achieve differentiable translation of an image (R1, R2)\n Graphics-motivation is unique, inspiring (R1)\n \n Weaknesses\n Experiments not clear (R2)\n Novelty is not enough to justify synthetic-only experiments (R1,R2)\n Missing important citations (R3)\n Lacking details, concern with reproducibility (R3)\n \n The authors acknowledged the reviews with a short sentence but did not provide any feedback or revisions.\n \n The AC and PC agree with the reviewers that the paper presents interesting preliminary work which is more suitable for a workshop in its current form.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396437152, "id": "ICLR.cc/2017/conference/-/paper213/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkXIrV9le", "replyto": "SkXIrV9le", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396437152}}}, {"tddate": null, "tmdate": 1483563283019, "tcdate": 1483563283019, "number": 5, "id": "rkjGbyorx", "invitation": "ICLR.cc/2017/conference/-/paper213/public/comment", "forum": "SkXIrV9le", "replyto": "HJUQPFZNl", "signatures": ["~Eder_Santana1"], "readers": ["everyone"], "writers": ["~Eder_Santana1"], "content": {"title": "reply", "comment": "Thanks for your review. We will use your suggestions to improve our work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682618, "id": "ICLR.cc/2017/conference/-/paper213/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkXIrV9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper213/reviewers", "ICLR.cc/2017/conference/paper213/areachairs"], "cdate": 1485287682618}}}, {"tddate": null, "tmdate": 1483563268206, "tcdate": 1483563268206, "number": 4, "id": "HJ2W-1irg", "invitation": "ICLR.cc/2017/conference/-/paper213/public/comment", "forum": "SkXIrV9le", "replyto": "rJyMMFbNx", "signatures": ["~Eder_Santana1"], "readers": ["everyone"], "writers": ["~Eder_Santana1"], "content": {"title": "reply", "comment": "Thanks for your review. We will use your suggestions to improve our work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682618, "id": "ICLR.cc/2017/conference/-/paper213/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkXIrV9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper213/reviewers", "ICLR.cc/2017/conference/paper213/areachairs"], "cdate": 1485287682618}}}, {"tddate": null, "tmdate": 1483563253173, "tcdate": 1483563253173, "number": 3, "id": "S1Tg-yjrg", "invitation": "ICLR.cc/2017/conference/-/paper213/public/comment", "forum": "SkXIrV9le", "replyto": "ByfxSqbNx", "signatures": ["~Eder_Santana1"], "readers": ["everyone"], "writers": ["~Eder_Santana1"], "content": {"title": "reply", "comment": "Thanks for your review. We will use your suggestions to improve our work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682618, "id": "ICLR.cc/2017/conference/-/paper213/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkXIrV9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper213/reviewers", "ICLR.cc/2017/conference/paper213/areachairs"], "cdate": 1485287682618}}}, {"tddate": null, "tmdate": 1483563238104, "tcdate": 1483563238104, "number": 2, "id": "ryRkW1jHg", "invitation": "ICLR.cc/2017/conference/-/paper213/public/comment", "forum": "SkXIrV9le", "replyto": "SydvmezVe", "signatures": ["~Eder_Santana1"], "readers": ["everyone"], "writers": ["~Eder_Santana1"], "content": {"title": "reply", "comment": "Thanks for your review. We will use your suggestions to improve our work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682618, "id": "ICLR.cc/2017/conference/-/paper213/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkXIrV9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper213/reviewers", "ICLR.cc/2017/conference/paper213/areachairs"], "cdate": 1485287682618}}}, {"tddate": null, "tmdate": 1481929569406, "tcdate": 1481929569406, "number": 3, "id": "SydvmezVe", "invitation": "ICLR.cc/2017/conference/-/paper213/official/review", "forum": "SkXIrV9le", "replyto": "SkXIrV9le", "signatures": ["ICLR.cc/2017/conference/paper213/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper213/AnonReviewer2"], "content": {"title": ".", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a generative model of videos composed of a background and a set of 2D objects (sprites). Optimization is performed under a VAE framework.\n\nThe authors' proposal of an outer product of softmaxed vectors (resulting in a 2D map that is delta-like), composed with a convolution, is a very interesting way to achieve translation of an image with differentiable parameters. It seems to be an attractive alternative to more complicated differentiable resamplers (such as those used by STNs) when only translation is needed.\n\nBelow I have made some comments regarding parts of the text, especially the experiments, that are not clear. The experimental section in particular seems rushed, with some results only alluded to but not given, not even in the appendix.\n\nFor an extremely novel and exotic proposal, showing only synthetic experiments could be excused. However, though there is some novelty in the method, it is disappointing that there isn't even an attempt at trying to tackle a problem with real data.\n\nI suggest as an example aerial videos (such as those taken from drone platforms), since the planar assumption that the authors make would most probably hold in that case.\n\nI also suggest that the authors do another pass at proof-reading the paper. There are missing references (\"Fig. ??\"), unfinished sentences (caption of Fig. 5), and the aforementioned issues with the experimental exposition.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512662363, "id": "ICLR.cc/2017/conference/-/paper213/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper213/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper213/AnonReviewer3", "ICLR.cc/2017/conference/paper213/AnonReviewer1", "ICLR.cc/2017/conference/paper213/AnonReviewer2"], "reply": {"forum": "SkXIrV9le", "replyto": "SkXIrV9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512662363}}}, {"tddate": null, "tmdate": 1481905409703, "tcdate": 1481905386392, "number": 2, "id": "ByfxSqbNx", "invitation": "ICLR.cc/2017/conference/-/paper213/official/review", "forum": "SkXIrV9le", "replyto": "SkXIrV9le", "signatures": ["ICLR.cc/2017/conference/paper213/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper213/AnonReviewer1"], "content": {"title": "Experimental results are too preliminary", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents an approach to modeling videos based on a decomposition into a background + 2d sprites with a latent hidden state. The exposition is OK, and I think the approach is sensible, but the main issue with this paper is that it is lacking experiments on non-synthetic datasets. As such, while I find the graphics inspired questions the paper is investigating interesting, I don't think it is clear that this work introduces useful machinery for modeling more general videos.\n\nI think this paper is more appropriate as a workshop contribution in its current form.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512662363, "id": "ICLR.cc/2017/conference/-/paper213/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper213/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper213/AnonReviewer3", "ICLR.cc/2017/conference/paper213/AnonReviewer1", "ICLR.cc/2017/conference/paper213/AnonReviewer2"], "reply": {"forum": "SkXIrV9le", "replyto": "SkXIrV9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512662363}}}, {"tddate": null, "tmdate": 1481901854382, "tcdate": 1481901854382, "number": 1, "id": "HJUQPFZNl", "invitation": "ICLR.cc/2017/conference/-/paper213/official/review", "forum": "SkXIrV9le", "replyto": "SkXIrV9le", "signatures": ["ICLR.cc/2017/conference/paper213/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper213/AnonReviewer3"], "content": {"title": "Mostly incremental generative model of video data with preliminary experimental results", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a generative model of video sequence data where the frames are assumed to be generated by a static background with a 2d sprite composited onto it at each timestep.  The sprite itself is allowed to dynamically change its appearance and location within the image from frame to frame.  This paper follows the VAE (Variational Autoencoder) approach, where a recognition/inference network allows them to recover the latent state at each timestep.\n\nSome results are presented on simple synthetic data (such as a moving rectangle on a black background or the \u201cMoving MNIST\u201d data.  However, the results are preliminary and I suspect that the assumptions used in the paper are far too strong too be useful in real videos.  On the Moving MNIST data, the numerical results are not competitive to state of the art numbers.\n\nThe model itself is also not particularly novel and the work currently misses some relevant citations.  The form of the forward model, for example, could be viewed as a variation on the DRAW paper by Gregor et al (ICML 2014).  Efficient Inference in Occlusion-Aware Generative Models of Images by Huang & Murphy (ICLR) is another relevant work, which used a variational auto-encoder with a spatial transformer and an RNN-like sequence model to model the appearance of multiple sprites on a background.\n\nFinally, the exposition in this paper is short on many details and I don\u2019t believe that the paper is reproducible from the text alone.  For example, it is not clear what the form of the recognition model is\u2026  Low-level details (which are very important) are also not presented, such as initialization strategy.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512662363, "id": "ICLR.cc/2017/conference/-/paper213/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper213/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper213/AnonReviewer3", "ICLR.cc/2017/conference/paper213/AnonReviewer1", "ICLR.cc/2017/conference/paper213/AnonReviewer2"], "reply": {"forum": "SkXIrV9le", "replyto": "SkXIrV9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512662363}}}, {"tddate": null, "tmdate": 1481900550834, "tcdate": 1481900550834, "number": 2, "id": "rJyMMFbNx", "invitation": "ICLR.cc/2017/conference/-/paper213/pre-review/question", "forum": "SkXIrV9le", "replyto": "SkXIrV9le", "signatures": ["ICLR.cc/2017/conference/paper213/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper213/AnonReviewer2"], "content": {"title": "Questions", "question": "1. Why does Algorithm 1 include both the delta-convolution and an STN? It seems that one would choose one or the other (Section 2) to implement a translation operator, not use both.\n\n2. In the experiments, it seems that the Convolutional-PUN uses an LSTM, while the STN-PUN uses an RNN. If so, this gives the Convolutional-PUN an unfair advantage. Why wasn't the same base RNN architecture used for both? If both use LSTMs, which is reasonable, the text should be clear that this is the case.\n\n3. Drawing conclusions from results that are not in the main text or even appendix (!) is very sloppy. In one instance (paragraph right before section 4.2) it is claimed that they can be obtained from the source code. In another (third paragraph of section 4.2) it is claimed that the proposed method generates high likelihood frames for longer than the baseline, but this is not shown or quantified anywhere. Can the authors comment on why this was not included even as appendix material?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481900551307, "id": "ICLR.cc/2017/conference/-/paper213/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper213/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper213/AnonReviewer1", "ICLR.cc/2017/conference/paper213/AnonReviewer2"], "reply": {"forum": "SkXIrV9le", "replyto": "SkXIrV9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481900551307}}}, {"tddate": null, "tmdate": 1481208821477, "tcdate": 1481208725417, "number": 1, "id": "S1T97lP7l", "invitation": "ICLR.cc/2017/conference/-/paper213/public/comment", "forum": "SkXIrV9le", "replyto": "HJNRU4Mme", "signatures": ["~Eder_Santana1"], "readers": ["everyone"], "writers": ["~Eder_Santana1"], "content": {"title": "RE: divergence in figure 4", "comment": "Dear reviewer, thanks for your comments. Please, see the answers below:\n\n1) Its not diverging, this is the test set. STN based model overfit much faster and didn't generalize as well as the other models we tested. The only model we could get to generate videos for several time steps without getting blurred was the conv PUN.\n\n2) Note that the conv PUN models uses a single convolution per step, a convolutional LSTM would require many more, making the computational and memory requirements very hard to compare. That being said, we already investigating a scalable version of PUN that uses conv LSTMs, conv LSTM + PUN compositions, etc. The experiments in the present paper focused on modular tests and understanding of the pieces. Extensive hyperparameter search should follow in future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287682618, "id": "ICLR.cc/2017/conference/-/paper213/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkXIrV9le", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper213/reviewers", "ICLR.cc/2017/conference/paper213/areachairs"], "cdate": 1485287682618}}}, {"tddate": null, "tmdate": 1480898300210, "tcdate": 1480898251624, "number": 1, "id": "HJNRU4Mme", "invitation": "ICLR.cc/2017/conference/-/paper213/pre-review/question", "forum": "SkXIrV9le", "replyto": "SkXIrV9le", "signatures": ["ICLR.cc/2017/conference/paper213/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper213/AnonReviewer1"], "content": {"title": "divergence in figure 4", "question": "Is there a reason why your stn based model seems to diverge? (fig. 4) What do the training set curves look like? Also, have you considered a LSTM baseline that uses spatial structure like your convolutional pun model? It would help disentangle whether your pun model is helping more or just using spatial information via a convolution is good enough. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481900551307, "id": "ICLR.cc/2017/conference/-/paper213/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper213/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper213/AnonReviewer1", "ICLR.cc/2017/conference/paper213/AnonReviewer2"], "reply": {"forum": "SkXIrV9le", "replyto": "SkXIrV9le", "writers": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper213/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481900551307}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478283592514, "tcdate": 1478276427454, "number": 213, "id": "SkXIrV9le", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkXIrV9le", "signatures": ["~Eder_Santana1"], "readers": ["everyone"], "content": {"title": "Perception Updating Networks: On architectural constraints for interpretable video generative models", "abstract": "We investigate a neural network architecture and statistical framework that models frames in videos using principles inspired by computer graphics pipelines. The proposed model explicitly represents \"sprites\" or its percepts inferred from maximum likelihood of the scene and infers its movement independently of its content. We impose architectural constraints that forces resulting architecture to behave as a recurrent what-where prediction network.", "pdf": "/pdf/54605a95663e1d427f978e6cd31a5b9f3e1bcb9b.pdf", "TL;DR": "Decoupled \"what\" and \"where\" variational statistical framework and equivalent multi-stream network ", "paperhash": "santana|perception_updating_networks_on_architectural_constraints_for_interpretable_video_generative_models", "keywords": ["Structured prediction", "Unsupervised Learning"], "conflicts": ["ufl.edu"], "authors": ["Eder Santana", "Jose C Principe"], "authorids": ["edercsjr@gmail.com", "principe@cnel.ufl.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": ["H1JBMVpdx"], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 13}