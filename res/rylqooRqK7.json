{"notes": [{"id": "LLINTR8To4L", "original": null, "number": 17, "cdate": 1585701999217, "ddate": null, "tcdate": 1585701999217, "tmdate": 1585701999217, "tddate": null, "forum": "rylqooRqK7", "replyto": "HklcIwZyJV", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Code has been released. ", "comment": "Thanks for your interest, we have released our implementation at https://github.com/SNAS-Series/SNAS-Series."}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "rylqooRqK7", "original": "r1x_4XTwtX", "number": 648, "cdate": 1538087842221, "ddate": null, "tcdate": 1538087842221, "tmdate": 1547310028965, "tddate": null, "forum": "rylqooRqK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 26, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1evyDwxlE", "original": null, "number": 1, "cdate": 1544742623001, "ddate": null, "tcdate": 1544742623001, "tmdate": 1545354514808, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Meta_Review", "content": {"metareview": "This paper provides an alternative way to enable differentiable optimization to the neural architecture search problem.  Different from DARTS, SNAS reformulates the problem and employs Gumbel random variables to directly optimize the NAS objective. In addition, the resource-constrained regularization is interesting. The major cons of the paper is that the empirical results are not quite impressive, especially when compared to DARTS, in terms of both accuracy and convergence. I think this is a borderline paper but maybe good enough for acceptance.\n\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Alternative way to differentiable NAS"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper648/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353140564, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": "rylqooRqK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353140564}}}, {"id": "HklcIwZyJV", "original": null, "number": 7, "cdate": 1543604049576, "ddate": null, "tcdate": 1543604049576, "tmdate": 1543604049576, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "content": {"comment": "Will you be releasing code for SNAS?  It would help fill in some of the details for how the architecture distribution parameters are trained.  ", "title": "Code Release?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311786229, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylqooRqK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311786229}}}, {"id": "SJxvc_VdhX", "original": null, "number": 1, "cdate": 1541060751509, "ddate": null, "tcdate": 1541060751509, "tmdate": 1543370165956, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Review", "content": {"title": "An incremental work on NAS with good experiment results. ", "review": "This work refines the NAS method for efficient neural architecture search. The paper brings new methods for gradient/reward updates and credit assignment. \n\npros: \n1. An improvement on gradient calculation and reward back-propagation mechanism\n2. Good experiment results and fair comparisons\n\ncons:\n1. Missing details on how to use the gradient information to generate child network structures. In eq.2, multiplying each one-hot random variable Zij to each edge (i, j) in the DAG can obtain a child graph whose intermediate nodes are xj. However, it is still unclear how to generate the child graph. More details on generating child network based on gradient information is expected. \n2. In SNAS, P(z) is assumed fully factorizable. Factors are parameterized with alpha and learnt along with operation parameters theta. The factorization of p(Z) is based on the observation that NAS is a task with fully delayed rewards in a deterministic environment. That is, the feedback signal is only ready after the whole episode is done and all state transitions distributions are delta functions. In eq. 3, the authors use the training/testing loss directly as reward, while the previous method uses a constant reward from validation accuracy. It is unclear why using the training/testing loss can improve the results? \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Review", "cdate": 1542234411695, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rylqooRqK7", "replyto": "rylqooRqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335771780, "tmdate": 1552335771780, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJgkGl3FRQ", "original": null, "number": 14, "cdate": 1543254022986, "ddate": null, "tcdate": 1543254022986, "tmdate": 1543286256023, "tddate": null, "forum": "rylqooRqK7", "replyto": "r1ewiLa-A7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Response to review (3) - Clarification on 1st-order optimization ", "comment": "We are sorry that in our last response we mistook 1st-order DARTS as single-level DARTS since the latter one was not reported by authors. It is reported in the newest version of DARTS, which is also added to our updated version. The \"1st-order SNAS\" in our last response actually meant single-level SNAS, because the neural operation parameters and architecture distribution parameters are updated simultaneously. In DARTS's newest version, it is stated that single-level DARTS performs much worse than bi-level, either 1st-order or 2nd-order, which is thus much worse than SNAS. This supports our claim that SNAS is less biased. \n\nAnd we can also provide an interpretation for 2nd-order DARTS's comparable performance with SNAS. From our understanding, DARTS is using meta-learning to look for a resolution for the bias proved by us in a data-driven way. Though authors cited [1], it could not justify that is optimizing the exact objective, for basically two reasons. Firstly, the connection between sufficient condition provided in [1] and DARTS is not discussed. Secondly, even if an explicit connection could be provided, 2nd-order DARTS is still biased due to the ignorance of the separate derivation scheme in the meta-learning loss (i.e. bi-level loss), which is proved by our experiments and single-level DARTS's unsatisfying performance. \n\nWe admit that the possibility of improvement with bi-level optimization exists even in less biased methods like SNAS. And the rationale is that some operations like skip connection affects the loss in next iteration more than the one in current iteration. When first proposed in [2], the skip connection is expected to help gradients' back-propagation. That is, skip connection plays a role of hyper-parameter for the gradient update process, the optimization of which prefers meta-learning. Unfortunately, we don't have enough time to run experiments to validate this rationale. It will be our next future work.\n\n[1] Franceschi et al., \"Bilevel programming for hyperparameter optimization and meta-learning\". ICML 2018.\n[2] He et al., \"Deep Residual Learning for Image Recognition\", CVPR 2016. "}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "SJlIOlhYAX", "original": null, "number": 15, "cdate": 1543254126269, "ddate": null, "tcdate": 1543254126269, "tmdate": 1543254126269, "tddate": null, "forum": "rylqooRqK7", "replyto": "HJegdD5FAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Manuscript updated", "comment": "We have updated Table 2 to include results of DARTS with single-level optimization as reported by DARTS\u2019s authors for fair comparison with SNAS. SNAS is single-level optimization because it simultaneously optimizes neural operation parameters and architecture distribution parameters over the same dataset. Analysis is included in Section 3.3 Results. For further details, please refer to the response to AnonReviewer1 [1].\n\n[1] https://openreview.net/forum?id=rylqooRqK7&noteId=HJgkGl3FRQ"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "HJegdD5FAQ", "original": null, "number": 13, "cdate": 1543247720198, "ddate": null, "tcdate": 1543247720198, "tmdate": 1543247720198, "tddate": null, "forum": "rylqooRqK7", "replyto": "BJeaD3s-CX", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Manuscript updated ", "comment": "We have updated Table 2 to include results from three runs of SNAS as requested by one reviewer. "}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "BJle6ULzA7", "original": null, "number": 12, "cdate": 1542772408090, "ddate": null, "tcdate": 1542772408090, "tmdate": 1542772488179, "tddate": null, "forum": "rylqooRqK7", "replyto": "SJxvc_VdhX", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Response to review", "comment": "Thank you for your review. \n\n1) How to use gradient information to generate child network\nSNAS does not directly use gradient information to generate child network. Search gradient is naturally applied to update architecture parameters, which are parameters for concrete distribution. Then in the derivation step, operations with largest probability in the concrete distribution are selected. \n\n2) Why using training/testing loss as reward can improve the results? \nThis is introduced with details in Section 2.3, as well as Appendix D and E, which we believe is one of our contribution. As stated in your comment, we first prove that NAS is a task in deterministic environment with fully delayed reward. Then a proof from [1] is introduced that TD-learning suffers from delayed bias when delayed reward exists. It is proposed and proved in [1] that Taylor decomposition of reward could resolve delayed bias because no temporal difference setting exists anymore. We then prove that the delayed reward in NAS could be decomposed and assigned to all structural decisions with gradient back-propagation, when differentiable training/testing loss is used as reward. Therefore, leveraging the proof from [1], we prove that SNAS should converge faster than ENAS, which is verified by our experiment as stated in Section 3.1. Intuitively speaking, we spot the unnecessary temporal difference setting in NAS and solve it by using training/testing loss as reward. \n\n[1] Arjona-Medina et al., \"Rudder: Return decomposition for delayed rewards\", arXiv 2018"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "SJg_LRib0X", "original": null, "number": 7, "cdate": 1542729296089, "ddate": null, "tcdate": 1542729296089, "tmdate": 1542736521613, "tddate": null, "forum": "rylqooRqK7", "replyto": "BkxZlzashX", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Response to review (1)", "comment": "Thank you very much for your review and questions! \n\n1) Clear win over DARTS:\nAs a NAS task, it is believed that the performance of a framework is evaluated with i) the efficiency and automation in searching process and ii) the accuracy and complexity of searching result, i.e. child networks. In this metric, SNAS's advantage over DARTS is three-fold, due to less-biased searching objective and the resource constraint: \n\nA. Less computing resources for the whole searching pipeline\nChild network derived from SNAS without any fine-tuning could maintain the accuracy, thus a) during searching, the accuracy in SNAS could reflect the performance of child network. DARTS, on the contrast, has to retrain the network for 100 epochs as stated in the caption of figure 3 to track the actual searching progress; b) after searching, DARTS has to retrain the child network even if there is no extension on cell number of channel number. SNAS, on the contrast, does not have this requirement. A retraining is only needed when the child network is extended, which in our work is basically for fair comparison. All these retraining will take much longer time when NAS is directly applied to a large dataset. \n\nB. Automated sparse network generation\nThough DARTS takes ZERO op, which represents deleting the edge, into account in the searching process, it is omitted in child network operation selection as discovered by one of our reviewers in this comment [1]. (This discovery is very interesting, as this reviewer discovered that ZERO tends to be the op with largest weight in DARTS. That is, in DARTS the \"soft-2nd-max\" is chosen.) The approach to delete edge is manually designed as \"to choose the top-k incoming edges for each node\". In SNAS, to keep or delete an edge is automatically learnt. That is to say, the ZERO op is acting its supposed job to engender sparsity. In our updated version, experiment showed that with an aggressive resource constraint, SNAS discovers architecture whose reduction cell has only two edges and two nodes but comparable accuracy with 1st-order DARTS in CIFAR-10, posing a question for the validity or optimality of manually designed scheme in DARTS. \n\nC. Comparable accuracy with less resource in child networks\nIn our updated version, we show that with a moderate resource constraint which plays the role of a regularizer, SNAS discovers architecture with slightly better accuracy and fewer parameters comparing to 1st-order DARTS, which is also comparable to 2nd-order DARTS. Note that in this paper we only show result of 1st-order SNAS due to limited time and extensive experiment required, though the 2nd-order extension is straight-forward [2]. As shown in DARTS, as well as [3], 2nd-order empirically brings better optimality, a fair comparison would be with 1st-order DARTS. Actually in 1st-order SNAS, an accuracy comparable with 1st-order DARTS could be achieved with 1/3 fewer parameters, when an aggressive constraint is applied. \n\nAs for transferring to ImageNet, there is no theoretical justification in the literature to the best of our knowledge, we provide it mainly for a fair comparison with DARTS. Our next step is to try a direct search on ImageNet leveraging that SNAS does not need retraining on the searching result. \n\n\n2) The effect of fine-tuning in evaluating child networks directly derived from DARTS\nIn our empirical study, a fine-tuning of the derived child networks can improve its performance, but could not remedy the gap completely after 100 epochs. (100 epoch is the plateau of fine-tuning, and also a fair comparison with SNAS.) And there seems always to be a small gap (-1.0+/-0.7)% between the accuracy after fine-tuning and at the end of searching.\n\nMore importantly, the 'gap' we want to discuss here is between the performance of a derived child network and the optimization objective in searching. As shown in Figure 3 in our paper, the optimization objective in searching is already converged to some optimum before this derivation, for both architecture parameters and operation parameters. Theoretically speaking, to use this parent network would be a justified result for the optimization problem. But with the absence of a guarantee that softmax weights will become discrete in the end, it would become an attention learning task, rather than NAS task. Though there exist methods like designing prior or extra learning objective to autonomously encourage one-hot-ness, a scheme is manually designed to delete a large portion of operations even though their weights are not 0. A natural question to ask is, why in this case the architecture parameter is still the optimal, even though the performance of child networks could be boosted with some fine-tuning. \n\n\n\n[1] https://openreview.net/forum?id=rylqooRqK7&noteId=rkeruyjYhX\n[2] Finn et al., \"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\", ICML 2017.\n[3] https://openreview.net/forum?id=rylqooRqK7&noteId=BJxaZ7Kojm\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "BJeaD3s-CX", "original": null, "number": 6, "cdate": 1542728804747, "ddate": null, "tcdate": 1542728804747, "tmdate": 1542735585472, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Manuscript updated", "comment": "We thank all reviewers for your recommendation, comments and expressing your concerns. We have updated the manuscript in Section 1, Section 3.1 and Section 3.2, taking your feedbacks into account. Here we provide a summary of these updates: \n\n1) We have tried extensive sweeps on the constraint hyperparameter \\eta. Previously we only tried \\eta that lies at the margin of appearance of ZERO op in the child network. The new discovery is that with a larger \\eta, the regularizing effect of resource constraint becomes obvious. A pair of new cells was discovered on CIFAR-10, which achieves better accuracy than 1st-order DARTS, as well as ENAS. Its accuracy is also on par with 2nd-order DARTS, with fewer parameters. In the updated version, we report its accuracy and parameter size, with the architecture attached in Appendix H. \n\n2) When a more aggressive constraint is applied, more edges are dropped in SNAS. A new figure is added to exhibit SNAS's capability of discovering sparse structures that ENAS and DARTS are not able to discover. With 1/3 fewer parameters, it achieves on-par accuracy with 1st-order DARTS. \n\n3) We updated figure 4 as ZERO was not excluded in the previous version. As discovered by some reviewer, it is omitted during operation selection for child graph in the code released by DARTS's authors. Besides, since to use variance to measure how much architecture weights differentiate from each other might be confusing, we updated it with entropy of softmax at every edge, which we hope to be more self-explanatory. Basically, the conclusion remains the same, in SNAS the learnt architecture distribution is more certain about the structural decisions. "}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "r1ewiLa-A7", "original": null, "number": 11, "cdate": 1542735519430, "ddate": null, "tcdate": 1542735519430, "tmdate": 1542735519430, "tddate": null, "forum": "rylqooRqK7", "replyto": "SJg_LRib0X", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Response to review (2)", "comment": "1.2) Clear win over ENAS\nFollow the metric defined above, SNAS's advantage over ENAS is three-fold, due to a better credit assignment mechanism and a resource constraint: \nA. Less epochs to converge to higher accuracy in searching;\nB. Automated sparse network generation;\nC. Slightly better accuracy with 1/3 fewer parameters in child networks."}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "SJgtwZ2WC7", "original": null, "number": 10, "cdate": 1542730081074, "ddate": null, "tcdate": 1542730081074, "tmdate": 1542730081074, "tddate": null, "forum": "rylqooRqK7", "replyto": "rkxfcO6yCm", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Response to questions", "comment": "Thank you for your questions. \n\n1) Reported DARTS accuracy lower than original paper\nThe purpose of providing this reproduced result is to evaluate the searching result from last subsection (Section 3.1). Sorry for the confusion it caused. We have removed it in the updated version. In our search we only achieve (3.02+/-0.14)% evaluation accuracy, which is still a bit lower than accuracy reported in original paper. We are sure that we ran experiments with correct hyper-parameters. But as mentioned in your Q5, in stochastic searching task, sometimes ones just need a bit of luck due to the stochastic nature of the objective. We are willing to reveal random seeds at request. \n\n2) Empirical study on rnn\nWe didn't have enough time to run extensive experiment on rnn. But we believe the theory proposed in our work could be sufficiently verified with our extensive experiments on cnn, whose extension to rnn would be straightforward. \n\n3) An explanation on Figure 4.\nIn the updated version, Figure 4 is updated to show stats of entropy of softmax weights at all edges in the searching result. With a lower entropy in the learnt parent graphs, SNAS is more certain on the structural decision. \n\n4.1) Training setting\nExperiments for ENAS were run with the default setting. And we noticed that the parent network in DARTS is a little bit different from ENAS, though parameter size is quite close. The reported result for SNAS were run with setting of DARTS for fair comparison, given that for some reason DARTS could not be fit into one GPU card with ENAS's setting. But we have also run SNAS experiment with ENAS's setting, whose searching curve has only negligible difference from the reported one. All experiments were run on the same training and testing set. \n\n4.2) Correlation of training accuracy and child network final accuracy\nFor SNAS, the correlation coefficient between training accuracy and child network final accuracy is 0.79. We didn't run and evaluate DARTS for statistically sufficient number of times to reach any claim of the correlation. Could you please provide some justification for that? As if this claim was true, it would help validate our claim that the manually designed child network derivation scheme is biased... \n\n5) Report results from more runs\nWe believe the reported result can support our claims. Nonetheless, we are evaluating more child networks to provide this variance. Thank you for your suggestion! "}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "S1ls0l2bCQ", "original": null, "number": 9, "cdate": 1542729938987, "ddate": null, "tcdate": 1542729938987, "tmdate": 1542729938987, "tddate": null, "forum": "rylqooRqK7", "replyto": "r1gLByK8TQ", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Further on parameter size", "comment": "Thank you for your explanation.\n\nThe experiments of SNAS are conducted only on convolutional cells due to limited time. Sorry we might not be able to answer questions regarding the size of the recurrent cells."}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "B1xdhCsZR7", "original": null, "number": 8, "cdate": 1542729391909, "ddate": null, "tcdate": 1542729391909, "tmdate": 1542729391909, "tddate": null, "forum": "rylqooRqK7", "replyto": "B1g-LCSsn7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "Response to review", "comment": "Thank you very much for your positive comments and detailed summary! \n\nWe have included experiments to show how the effect of ZERO op differentiates SNAS from DARTS. Please kindly have a check. "}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "rkxfcO6yCm", "original": null, "number": 6, "cdate": 1542604938469, "ddate": null, "tcdate": 1542604938469, "tmdate": 1542604964173, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "content": {"comment": "Hi authors.\nThis is an interesting work, but I feel that some experimental comparisons are a little bit unfair as follows:\n\n1. In Table 2, the authors report DARTS* has a test error of 3.15%, which is claimed as reproduced by the released code. I also run their codes and can obtain a similar performance with 2.86%. Reporting 3.15% might be misleading to others readers, and make other researchers wrongly refer the results of DARTS in the following papers.\n\n2. DARTS has conducted results on both CNN and RNN. Why SNAS does not report the RNN results?\n\n3. Would you mind to give more explanation about Figure 4?\n\n4. In Figure 3, how did you run ENAS? In addition, do these three methods use the same training and validation set? Based on the GitHub issue of DARTS, the validation accuracy of DARTS does not have an explicit connection with the performance of the discovered model. How about the connection between the validation accuracy of SNAS and the final discovered model of SNAS?\n\n5. For NAS approaches, there is usually a variance of the performance of the final discovered model. Would you mind to report the results of three runs of SNAS?", "title": "Some question about the experiments."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311786229, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylqooRqK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311786229}}}, {"id": "r1gLByK8TQ", "original": null, "number": 5, "cdate": 1541996350361, "ddate": null, "tcdate": 1541996350361, "tmdate": 1541996350361, "tddate": null, "forum": "rylqooRqK7", "replyto": "SkxDoOiBpQ", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "content": {"comment": "Thanks for your reply!! As I run the code in github of DARTS paper, the parameter size was about 7M when a recurrent cell with 7 intermediate nodes is evaluated under the setting of 20 layers and 36 initial channels. Thus, I thought I should have reduced the number of nodes or the number of layers whatever...", "title": "20 Layers + 36 initial channels -> Too much parameters"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311786229, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylqooRqK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311786229}}}, {"id": "SkxDoOiBpQ", "original": null, "number": 4, "cdate": 1541941406658, "ddate": null, "tcdate": 1541941406658, "tmdate": 1541941406658, "tddate": null, "forum": "rylqooRqK7", "replyto": "Skl7rA7zaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "On parameter size ", "comment": "Thank you for your question.\n\nWe have implemented a counting function to reproduce the result of DARTS and used it to count the parameter size of our network.\n\nDo you mind showing how you reached the claim that the size should be much larger?"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "Skl7rA7zaQ", "original": null, "number": 4, "cdate": 1541713466601, "ddate": null, "tcdate": 1541713466601, "tmdate": 1541713466601, "tddate": null, "forum": "rylqooRqK7", "replyto": "S1lt6f7zpm", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "content": {"comment": "Thank you for your quick and kind reply!\nTo my knowledge, for CIFAR10 evaluation, the number of layers was set to 20(reported in the paper), the number of nodes within a cell was set to 7(adopted from search) and the number of initial channels was set to 36(in the comment above). \nThen, the number of parameters should be much larger than 3.3M I think... Thus, think if the number of initial channels was set to 36, the number of layers should have been set to 8 as search mode to match the number of parameters reported in SOTA comparison table... Is it wrong?\n\n\n", "title": "More clarification"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311786229, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylqooRqK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311786229}}}, {"id": "S1lt6f7zpm", "original": null, "number": 3, "cdate": 1541710529214, "ddate": null, "tcdate": 1541710529214, "tmdate": 1541710529214, "tddate": null, "forum": "rylqooRqK7", "replyto": "Hyg3FnObpX", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "On initial channel number", "comment": "Thank you for the question.\n\nFor fair comparison with DARTS, we employed the same set of hyperparameters as specified in the code publicly released by its authors. I bet the number of initial channels in your question refers to the channel number used for the first cell in the network according to the code. During CIFAR10 evaluation, the number of initial channels is set to 36 rather than 16. This will lead to an increase in the number of parameters.\n\nWe will add more hyperparameter details in the revised version. Hope this can answer your question and thanks again for the comment.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "Hyg3FnObpX", "original": null, "number": 3, "cdate": 1541667972498, "ddate": null, "tcdate": 1541667972498, "tmdate": 1541667972498, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "content": {"comment": "Hi authors!\nI am so impressed by your paper because it feels like this is a connection between NAS and DARTS.\nHowever, I am curious about some details on CIFAR10 evaluation.\nWithin a cell, there are 7 nodes right as DARTS right?\nThen, how is the number of initial channels set? The same number of initial channels 16 applies as search mode?\nIf so, it seems to me that the number of parameters reported seems so large.\nCould you specify the number of initial channels plus another hyperparmeters to clarify the evaluation setting?\n\nAnyway, thanks for the nice paper!", "title": "Derivation of child network for SOTA comparison"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311786229, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylqooRqK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311786229}}}, {"id": "BkxZlzashX", "original": null, "number": 3, "cdate": 1541292520770, "ddate": null, "tcdate": 1541292520770, "tmdate": 1541533809374, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Review", "content": {"title": "Novel approach that addresses some shortcomings of the previous NAS techniques.", "review": "This paper improves upon ENAS and DARTS by taking a differentiable approach to NAS and optimizing the objective across the distribution of child graphs. This technique allows for end-to-end architecture search while constraining resource usage and allowing parameter sharing by generating effective reusable child graphs.\n\nSNAS employs Gumbel random variables which gives it better gradients and makes learning more robust compared to ENAS. The use of Gumbel variables also allow SNAS to directly optimize the NAS objective which is an advantage over DARTS.\n\nThe resource constraint regularization is interesting. Regularizing on the parameters that describe the architecture can help constrain resource usage during the forward pass. \n\nThe proposed method is novel but the main concern here is that there is no clear win over existing techniques in terms of performance. I can't see anywhere in the tables where you demonstrate a clear improvement over DARTS or ENAS.\n\nFurthermore, in your child network evaluation with CIFAR-10, you mention that the comparison is without fine-tuning. Do you think this might be contributing to the performance gap in DARTS?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Review", "cdate": 1542234411695, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rylqooRqK7", "replyto": "rylqooRqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335771780, "tmdate": 1552335771780, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1g-LCSsn7", "original": null, "number": 2, "cdate": 1541262920758, "ddate": null, "tcdate": 1541262920758, "tmdate": 1541533809172, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Review", "content": {"title": "Official Review", "review": "Summary:\nThis paper proposes Stochastic Neural Architecture Search (SNAS), a method to automatically and efficiently search for neural architectures. It is built upon 2 existing works on these topics, namely ENAS (Pham et al 2018) and DARTS (Liu et al 2018).\n\nSNAS provides nice theory and explanation of gradient computations, unites the strengths and avoid the weaknesses of ENAS and DARTS. There are many details in the paper, including the Appendix. The idea is as follows:\n+------------+---------------------+-------------------------+\n| Method | Differentiable | Directly Optimize |\n|                |                           |    NAS reward       |\n+------------+---------------------+-------------------------+\n| ENAS     |      No                |        Yes                   |\n| DARTS   |      Yes               |        No                    |\n| SNAS     |      Yes               |        Yes                   |\n+------------+---------------------+-------------------------+\nSNAS inherits the idea of ENAS and DARTS by superpositioning all possible architectures into a Directed Acyclic Graph (DAG), effectively sharing the weights among all architectures. However, SNAS improves over ENAS and DARTS as follows (Section 2.2):\n\n1. SNAS improves over ENAS in that it allows independent sampling at edges in the shared DAG, leading to a more tractable gradient at the edges of the DAG, which in turn allows more tractable Monte Carlo estimation of the gradients with respect to the architectural parameters.\n\n2. While DARTS also has the property (1), DARTS implements this by computing the expected value at each node in the DAG, with respect to the joint distribution of the input edges and the operations. This makes DARTS not optimize the direct NAS objective. SNAS, due to their smart manipulation of architectural gradients using Gumbel variables, still optimizes the same objective with NAS and ENAS, but has a smoother gradients.\n\nExperimental results in the paper show that SNAS finds architectures on CIFAR-10 that are comparable to those found by ENAS and DARTS, using a reasonable amount of computing resource. These architectures can also be transferred to learn competent models on ImageNet, like those of DARTS. Furthermore, experimental observations (Figure 3) are consistent with the theory above, that is:\n\n1. The search process of SNAS is more stable than that of ENAS (as SNAS samples with a smaller variance).\n2. Architectures found by SNAS perform better than those of DARTS, as SNAS searches directly for the NAS reward of the sampled models. \n\nStrengths:\n1. SNAS unites the strengths and avoids the weaknesses of ENAS and DARTS\n\n2. SNAS provides a nice theory, which is verified through their experimental results.\n\nWeaknesses:\nI don\u2019t really have any complaints about this paper. Some presentations of the paper might have been improved, e.g. the discussion on the ZERO operation in other comments should have been included.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper648/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Review", "cdate": 1542234411695, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rylqooRqK7", "replyto": "rylqooRqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335771780, "tmdate": 1552335771780, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkxvf8zq27", "original": null, "number": 2, "cdate": 1541182990520, "ddate": null, "tcdate": 1541182990520, "tmdate": 1541204042964, "tddate": null, "forum": "rylqooRqK7", "replyto": "rkeruyjYhX", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "ZERO operation is just as other operations if no complexity constraint is added", "comment": "Thank you for your interesting discovery and the questions. \n\n0) derivation code for SNAS is the same as DARTS? \nNo. We have implemented our derivation method to replace the one provided by DARTS as SNAS uses fundamentally different one. But in our replication of DARTS, we ran the implementation publicly released without checking the code for derivation. We are trying to replicate DARTS's result again, taking your claim into consideration. \n\n1) is the logit of ZERO the largest in most edges of the normal cell? \nAs introduced in Section 2.4 in our paper, SNAS employs a complexity loss to encourage sparsity in child network. This is different from ENAS and DARTS which directly select two input edges for each node. With a relatively large hyperparameter for this complexity loss, the logit of ZERO dominates some of the edges, though the child network still have other non-ZERO edges to keep it connected. If no complexity loss is added, the child network tends to remain the complete topology, which is actually one of our motivations to introduce complexity loss. \n\n2) reason for this discrepancy \nAssuming that your result is valid, it is explained by DARTS\u2018s authors to be the underdetermined contribution and rescaling effect of ZERO in the mixed op. We have the following two hypotheses for the discrepancy between SNAS and DARTS, which would be added to revised version if we can prove them with mathematical deduction: \na. different from the softmax attention in DARTS, SNAS employs Gumbel-Softmax, whose mechanism involves Gumbel random variables. With equivalent logit and temperature, the random variable vector from Gumbel-Softmax is possibly more one-hot than the deterministic softmax attention. The more discrete network could amplify the incapability of ZERO for a smaller loss, thus the logit of it would not be boosted. \nb. the gradients back-propagated through deterministic softmax and Gumbel-Softmax are different. As provided in Section 2.3 and Appendix C in our paper, there is stochasticity (random variables Z) involved in the search gradients in SNAS. Given the special trait of ZERO that O(x)=0, it is possible to be a local optimum or a gradient blackhole for deterministic softmax, which is probably escaped by Gumbel-Softmax with the stochasticity. "}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "rkeruyjYhX", "original": null, "number": 2, "cdate": 1541152621350, "ddate": null, "tcdate": 1541152621350, "tmdate": 1541152621350, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "content": {"comment": "I have tried to run the released code of DARTS and found that in the code, the operation ZERO is omitted during the derivation process. I also checked the logit of ZERO operation learned by DARTS, and found that in the normal cell, it is the largest in most edges.\n\nIs the same derivation code used by SNAS and thus omitting ZERO operation in the experiments? If not, could you please give an explanation to:\n1) whether the logit of ZERO is the largest in most edges of the normal cell learned by SNAS?\n2) If the result is different from DARTS, why is this difference?", "title": "On ZERO operation"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311786229, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylqooRqK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311786229}}}, {"id": "S1ZWK63j7", "original": null, "number": 1, "cdate": 1540311288517, "ddate": null, "tcdate": 1540311288517, "tmdate": 1540311288517, "tddate": null, "forum": "rylqooRqK7", "replyto": "BJxaZ7Kojm", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "content": {"title": "On child network derivation", "comment": "Thank you for the question. \n\nThe derivation method for DARTS is stated in Section 2.4 in their paper, details could also be found in the implementation publicly released by authors. In our paper we paraphrase it to give an explanation to the drop in accuracy after this derivation. As stated in your comments, \"these removed operations and edges make up a very large percentage of the softmax score\". Through this comparison we want to emphasize SNAS's consistency in child network derivation, because of explicitly taking it, i.e. sampling, into account in the searching loss. \n\nIn this comparison, child networks are directly tested after the derivation for both SNAS and our replication of DARTS. No re-scaling or any other extra transformation is involved. But we do find that by using the unrolled option, DARTS's result (54.66%) falls less than the non-unrolled one (34.37%). SNAS, in contrast, shows slightly better result (90.67%) after derivation than at the end of searching (88.54%), probably as the latter one is a Monte Carlo estimate of expectation. \n\nWe will add more details into the revised version, thanks again for this comment. "}, "signatures": ["ICLR.cc/2019/Conference/Paper648/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610166, "tddate": null, "super": null, "final": null, "reply": {"forum": "rylqooRqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper648/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper648/Authors|ICLR.cc/2019/Conference/Paper648/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610166}}}, {"id": "BJxaZ7Kojm", "original": null, "number": 1, "cdate": 1540227845018, "ddate": null, "tcdate": 1540227845018, "tmdate": 1540281140367, "tddate": null, "forum": "rylqooRqK7", "replyto": "rylqooRqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "content": {"comment": "It is unclear how the authors obtain the child network for DARTS. As is mentioned by the paper, the architecture derivation step in DARTS consists of two steps. (1) Remove operations with relatively weak attention and the zero operation. (2) Remove relatively ambiguous edges. As we know, these removed operations and edges make up a very large percentage of the softmax score. After the removal, the scale of the value of the nodes (output feature maps) drops significantly. Do we need to re-scale the value of output feature maps for compensation?  I cannot reproduce the result of 54.66% in Table 1.", "title": "Missing detail"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper648/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SNAS: stochastic neural architecture search", "abstract": "We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.", "keywords": ["Neural Architecture Search"], "authorids": ["xiesirui@sensetime.com", "zhenghehui@sensetime.com", "liuchunxiao@sensetime.com", "linliang@ieee.org"], "authors": ["Sirui Xie", "Hehui Zheng", "Chunxiao Liu", "Liang Lin"], "pdf": "/pdf/d049c259ed478bb595acb974f6e19f8d7a275529.pdf", "paperhash": "xie|snas_stochastic_neural_architecture_search", "_bibtex": "@inproceedings{\nxie2018snas,\ntitle={{SNAS}: stochastic neural architecture search},\nauthor={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rylqooRqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper648/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311786229, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rylqooRqK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper648/Authors", "ICLR.cc/2019/Conference/Paper648/Reviewers", "ICLR.cc/2019/Conference/Paper648/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311786229}}}], "count": 27}