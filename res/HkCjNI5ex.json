{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396487286, "tcdate": 1486396487286, "number": 1, "id": "H1JUhfIOg", "invitation": "ICLR.cc/2017/conference/-/paper295/acceptance", "forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The reviewers agreed that the idea proposed in this paper is sensible and possibly very useful, and that the experiments are thorough with good results. However, they share strong doubts regarding the novelty of the proposed approach. Hopefully the discussion will help the authors refine this work. With a more thorough discussion of related ideas, such as entropy maximization, within the machine learning literature and a careful placement of this idea within that context, this could be a strong submission to a future conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396487861, "id": "ICLR.cc/2017/conference/-/paper295/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396487861}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484966408203, "tcdate": 1478284454349, "number": 295, "id": "HkCjNI5ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HkCjNI5ex", "signatures": ["~Gabriel_Pereyra1"], "readers": ["everyone"], "content": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 17, "writable": false, "overwriting": ["HyhbYrGYe"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484875608636, "tcdate": 1484370635356, "number": 7, "id": "SkQCGEPUx", "invitation": "ICLR.cc/2017/conference/-/paper295/public/comment", "forum": "HkCjNI5ex", "replyto": "rJpk35xNg", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "Author response", "comment": "Thank you for the encouraging review and for highlighting the empirical rigor of the manuscript.\n\nThank you for pointing out this confusing use of terminology. The \u201cunigram smoothing\u201d applies to the datasets where the input is a sequence of tokens (e.g., language modeling), where n-gram statistics would be applicable.\n\nWe are running experiments to quantify the gradient norm during training and will add the results to the paper. We have now added these to the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634611, "id": "ICLR.cc/2017/conference/-/paper295/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634611}}}, {"tddate": null, "tmdate": 1484875560065, "tcdate": 1484370508081, "number": 6, "id": "HkVUGEvLe", "invitation": "ICLR.cc/2017/conference/-/paper295/public/comment", "forum": "HkCjNI5ex", "replyto": "Hk_oKaNNe", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "Author response", "comment": "We thank the reviewer for their helpful comments.\n\nWe now list state-of-the-art results for the speech recognition tasks.\n\nYes, the gradient has an intuitive form. We now describe this in the text.\n\nYes, we agree that it could be a problem. We partially address this question with the speech experiments which use a beam search during decoding (but do not use a separately trained LM) and these models do benefit from the increased uncertainty. Without the penalty, the model may overly penalize deviations from the most likely path, making the beam search less effective."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634611, "id": "ICLR.cc/2017/conference/-/paper295/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634611}}}, {"tddate": null, "tmdate": 1484370585602, "tcdate": 1483559386986, "number": 5, "id": "rJ7JGAcre", "invitation": "ICLR.cc/2017/conference/-/paper295/public/comment", "forum": "HkCjNI5ex", "replyto": "B1met8z4x", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "RE: Reviewer 4 Review", "comment": "Thank you for the encouraging review.\n\nThank you for the many citations to related literature; they have been very interesting to read. We will revise the related work section to appropriately situate our work.\n\nI agree with your statement that experimental results should always be accompanied by significance tests and error analysis, however, this is rarely done (to the detriment of the field) and thus standards are not well established. Do you have an example with a good presentation of this analysis? As you mention, I can see three sources of randomness that we might want to quantify: randomness due to the particular train set, randomness due to the particular test set, and randomness inherent in the model training procedure. One approach would be to use the bootstrap estimate, is that what you would suggest? We will make a best effort to address this concern, however, for the larger models on the large datasets running repeated trials is particularly onerous.\n\n=====================================\n\nWe have extended the related work section and better contextualized our contributions.\n\nWe have added error bars for some tasks. Although we have not strictly quantified the uncertainty due to the finite test set size, we feel that the results taken as a whole over multiple independent tasks support our conclusions.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634611, "id": "ICLR.cc/2017/conference/-/paper295/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634611}}}, {"tddate": null, "tmdate": 1482115488285, "tcdate": 1482115488285, "number": 3, "id": "Hk_oKaNNe", "invitation": "ICLR.cc/2017/conference/-/paper295/official/review", "forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer3"], "content": {"title": "Experimental investigations on a slightly modified version of label smoothing.", "rating": "6: Marginally above acceptance threshold", "review": "The paper experimentally investigates a slightly modified version of label smoothing technique for neural network training, and reports results on various tasks. Such smoothing idea is not new, but was not investigated previously in wide range of machine learning tasks.\n\nComments:\nThe paper should report the state-of-the-art results for speech recognition tasks (TIMIT, WSJ), even if models are not directly comparable.\nThe error back-propagation of label smoothing through softmax is straightforward and efficient. Is there an efficient solution for BP of the entropy smoothing through softmax?\nAlthough the classification accuracy could remain the same, the model will not estimate the true posterior distribution with this kind of smoothing.\nThis might be an issue in complex machine learning problems where the decision is made on higher level and based on the posterior estimations, e.g. language models in speech recognition.\nMore motivation is necessary for the proposed smoothing.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512633529, "id": "ICLR.cc/2017/conference/-/paper295/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper295/AnonReviewer5", "ICLR.cc/2017/conference/paper295/AnonReviewer4", "ICLR.cc/2017/conference/paper295/AnonReviewer3"], "reply": {"forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512633529}}}, {"tddate": null, "tmdate": 1481955563116, "tcdate": 1481955563116, "number": 2, "id": "B1met8z4x", "invitation": "ICLR.cc/2017/conference/-/paper295/official/review", "forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "content": {"title": "perfectly sensible idea, with apparently good results, but lacks scholarship ", "rating": "5: Marginally below acceptance threshold", "review": "Specifically, this paper suggests regularizing the estimator of a probability distribution to prefer high-entropy distributions.  This avoids overfitting.\n\nI generally like this idea.  Regularizing the behavior of the model often makes more sense than regularizing its parameters.  After all, the behavior is interpretable, whereas the parameters are uninterpretable and work together in mysterious ways to produce the behavior.  So one might be able to choose a more sensible prior over the behavior.  In other words, prefer parameters not because they are individually close to 0 but because they jointly lead to a distribution that is plausible or low-risk a priori.\n\nPro: I believe that the idea is natural and sound (that is, I do not share the doubts of AnonReviewer5).\n\nPro: It's possible that this hasn't been well-explored yet in neural networks (not sure).\n\nPro: The experimental results look good.  So maybe everyone should use this kind of regularizer. \n\nCon: It is a kind of pollution of the scientific literature to introduce this idea to the community as if it were unconnected to (almost) anything else in machine learning.  There are many, many papers that include a scaled entropy term in the optimization objective!  It's not just for reinforcement learning.  Please see the long list of connections in my pre-review questions / comments.  \n\nCon: Experimental results should always be accompanied by significance tests and error analysis.  Is your trained model actually doing better on the distribution of test data, or was your test set too small to tell?  Are the improvements robust across many different training sets?  What errors does your model fix, and what errors does it introduce?  \n\nSummary recommendation: Revise and resubmit.  ICLR has lots of submissions.  I would prefer to reward authors who not only tried something, but who properly contextualized it and carefully evaluated it.  Otherwise, there's a race to the bottom where everyone wants to be the first to try something, so that readers are confronted with a confusing sea of slapdash papers with unclear relationships.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512633529, "id": "ICLR.cc/2017/conference/-/paper295/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper295/AnonReviewer5", "ICLR.cc/2017/conference/paper295/AnonReviewer4", "ICLR.cc/2017/conference/paper295/AnonReviewer3"], "reply": {"forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512633529}}}, {"tddate": null, "tmdate": 1481954945281, "tcdate": 1481954945281, "number": 4, "id": "BJKKUUGNg", "invitation": "ICLR.cc/2017/conference/-/paper295/official/comment", "forum": "HkCjNI5ex", "replyto": "HyqQd_AGe", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "content": {"title": "connection to maximum entropy", "comment": "I was going to put this explanation in the review, but it's probably better here:\n\n> Surely you've heard of the maximum entropy principle (Jaynes 1957), which breaks ties among distributions that satisfy the data constraints by choosing the one with maximum entropy? \n\nA maximum-likelihood log-linear distribution (e.g., exponential family distribution) falls out of the goal to maximize the entropy while exactly predicting some expectations in the data.  The log-linear coefficients emerge as Lagrange multipliers on the predictive constraints.  One good tutorial is https://aclweb.org/anthology/J/J96/J96-1002.pdf , which I cited above.\n\nIf some slack is allowed in the predictions, then one gets a Lagrangian objective rather like the one you use, which balances high entropy with low slack penalty (high predictive accuracy).  That leads to the log-linear distribution that maximizes a regularized likelihood.  A reasonable review with refs is http://www.aclweb.org/anthology/W03-1018.\n\nDifferences from your work: These maximum entropy papers did not make any assumptions about the form of the distribution.  The parametric form simply emerged from the constraint that it should approximately predict a vector of feature expectations.  In contrast, you are restricting the distribution a priori to a parametric form, e.g., a neural network.  And instead of constraining the distribution to approximately predict a vector of feature expectations, you are constraining it to achieve good performance on some task.  \n\nBut the various later papers I cited are closer to your setup, e.g., deterministic annealing, entropy regularization, ..."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634362, "id": "ICLR.cc/2017/conference/-/paper295/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634362}}}, {"tddate": null, "tmdate": 1481952956320, "tcdate": 1481952956320, "number": 3, "id": "HkNa0HzNe", "invitation": "ICLR.cc/2017/conference/-/paper295/official/comment", "forum": "HkCjNI5ex", "replyto": "HyqQd_AGe", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "content": {"title": "more related work - connection to posterior regularization", "comment": "A bit more related work to discuss. My review says:\n\n\"I generally like this idea.  Regularizing the behavior of the model often makes more sense than regularizing its parameters.  The behavior is interpretable, whereas the parameters are uninterpretable and work together in mysterious ways.  So one might be able to choose a more sensible prior over the behavior.  In other words, prefer parameters if they lead to a plausible distribution.\"\n\nThe following group of papers all independently had the idea, at the same time, of regularizing toward a plausible distribution:\n\n* posterior regularization (http://jmlr.csail.mit.edu/papers/volume11/ganchev10a/ganchev10a.pdf)\n* generalized expectation criteria (http://www.jmlr.org/papers/volume11/mann10a/mann10a.pdf)\n* learning from measurements (https://cs.stanford.edu/~pliang/papers/measurements-icml2009.pdf)\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634362, "id": "ICLR.cc/2017/conference/-/paper295/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634362}}}, {"tddate": null, "tmdate": 1481951038374, "tcdate": 1481951038374, "number": 2, "id": "ryUSDrGNx", "invitation": "ICLR.cc/2017/conference/-/paper295/official/comment", "forum": "HkCjNI5ex", "replyto": "r1LRQQRMx", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "content": {"title": "Re: pushing the predictive distribution toward the base rate", "comment": "Regarding AnonReviewer5's calibration concern and your reply:\n\nIn my pre-review questions, I mentioned the line of work on deterministic annealing.  This typically pushes the distribution toward uniform (at least in early stages of optimization).  But the idea of pushing it instead toward some other known reasonable distribution does appear in the literature, under the name \"skewed deterministic annealing.\"  \n\nHowever, the motivation in that case was not to calibrate the final results, but rather to improve search by starting at the known distribution (rather than at uniform) and annealing away from it.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634362, "id": "ICLR.cc/2017/conference/-/paper295/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634362}}}, {"tddate": null, "tmdate": 1481843511682, "tcdate": 1481771126661, "number": 2, "id": "SkJtOFJ4e", "invitation": "ICLR.cc/2017/conference/-/paper295/public/comment", "forum": "HkCjNI5ex", "replyto": "Hku8ZhJQe", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "RE: Questions", "comment": "We did not experiment with virtual adversarial training, however will add it to the related work section. The primary reasons for not using it are:\n\n* Two hyper-parameters\n* The approximated gradient of the LDS can be computed with no more than three pairs of forward and back propagations,\nwhich is significantly more computation in grid-searching and training than simply penalizing low entropy.\n\nThe second smoothing technique is denoted label smoothing in our manuscript.\n\nWe will add error bars. The difference between label smoothing and confidence penalty on MNIST is likely not statistically significant, but the later experiments back up the performance of the method.\n\nWe will extend the range of hyperparameters for the MNIST and Cifar10 experiments.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634611, "id": "ICLR.cc/2017/conference/-/paper295/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634611}}}, {"tddate": null, "tmdate": 1481841636890, "tcdate": 1481841636886, "number": 1, "id": "rJpk35xNg", "invitation": "ICLR.cc/2017/conference/-/paper295/official/review", "forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer5"], "content": {"title": "Simple idea, analysis lacking, comprehensive experiments", "rating": "5: Marginally below acceptance threshold", "review": "The authors propose a simple idea. They penalize confident predictions by using the entropy of the predictive distribution as a regularizer. The authors consider two variations on this idea. In one, they penalize the divergence from the uniform distribution. In the other variation, they penalize distance from the base rates. They term this variation \"unigram\" but I find the name odd as I've never seen multi-class labels described as unigrams before. What would a bigram be? \n\nThe idea is simple,  and while it's been used in the context of reinforcement learning, it hasn't been popularized as a regularizer for improving generalization in supervised learning. \n\nThe justifications for the idea still lacks analysis. And the author responses comparing it to L2 regularization have some holes. A simple number line example with polynomial regression makes clear how L2 regularization could prevent a model from badly overfitting to accommodate every data point. In contrast, it seems trivial to fit every data point and satisfy arbitrarily high entropy. Of course, the un-regularized optimization is to maximize log likelihood, not simply to maximize accuracy.  And perhaps something interesting may be happening at the interplay between the log likelihood objective and the regularization objective. But the paper doesn't indicate precisely what.\n\nI could imagine the following scenario: when the network outputs probabilities near 0, it can get high loss (if the label is 1). The entropy regularization could be stabilizing the gradient, preventing sharp loss on outlier examples. The regularization then might owe mainly to faster convergence. Could the authors analyze the effect empirically, on the distribution of the gradient norms? \n\nThe strength of this paper is its empirical rigor. The authors take their idea and put it through its paces on a host of popular and classic benchmarks spanning CNNs and RNNs. It appears that on some datasets, especially language modeling, the confidence penalty outperforms label smoothing. \n\nAt present, I rate this paper as a borderline contribution but I'm open to revising my review pending further modifications. \n\nTypo:\nIn related work: \"Penalizing entropy\" - you mean penalizing low entropy\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512633529, "id": "ICLR.cc/2017/conference/-/paper295/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper295/AnonReviewer5", "ICLR.cc/2017/conference/paper295/AnonReviewer4", "ICLR.cc/2017/conference/paper295/AnonReviewer3"], "reply": {"forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512633529}}}, {"tddate": null, "tmdate": 1481824073400, "tcdate": 1481771694549, "number": 4, "id": "SJ8hctyEx", "invitation": "ICLR.cc/2017/conference/-/paper295/public/comment", "forum": "HkCjNI5ex", "replyto": "SJP-AeCzg", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "RE: Questions 2", "comment": "Although accuracy does not change when smoothing the distribution, the log likelihood does change, which is what we optimize during training. This is similar to weight decay, where again scaling all of the weights trivially reduces the L2 norm without changing accuracy, but it does change the log likelihood. In particular, what changes is the ratio between the different classes and as described in the distillation paper, we think this plays a role in the regularization effect we see. \n\nAnother observation is that once an example is correctly predicted, the only way to increase the log likelihood is to make the prediction sharper. Hence the outputs become uncalibrated and unless capacity is controlled in some way, the network will put very sharp distributions on its predictions. These regularizers prevent this behavior. Moreover, the Inception paper notes that the gradient vanishes on confident correctly predicted samples. This does not happen with label smoothing and the confidence penalty, which means that we do get some training signal for the lower layers even on correct predictions. This may improve data efficiency when doing multiple epochs (normally during the later passes only the few samples that the net doesn\u2019t get confidently have any influence). This is similar to the trick from Yann LeCun\u2019s \u201cEfficient Backprop\u201d in which the hyperbolic tangent was used on the output of the net, but it was scaled to have a range of [-1.1, 1.1], while the targets were +-1. Thus the net was never driven into saturation.\n\nWe can interpret our approach as a regularizer encouraging the predictive distribution to be close to uniform. So, when the model has little evidence it is regularized to the uniform, however, when sufficient evidence is accumulated, the predictive distribution matches the data."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634611, "id": "ICLR.cc/2017/conference/-/paper295/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634611}}}, {"tddate": null, "tmdate": 1481771254265, "tcdate": 1481771254259, "number": 3, "id": "BkClKYy4l", "invitation": "ICLR.cc/2017/conference/-/paper295/public/comment", "forum": "HkCjNI5ex", "replyto": "HyqQd_AGe", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "RE: Questions", "comment": "Thank you for pointing out these references. We will expand the relevant works situation to address these omissions and better situate our contributions, which are primarily experimental.\n\nFor our tables, above the line are results from the literature and below the line is our own implementation. We changed the tables to only bold the best result.\n\nWe compare with and without dropout depending on how severely performance degrades without dropout. For example, if dropout is removed from the recurrent neural network used for Penn Treebank, it degrades to 120 perplexity. In these cases, we only compare with dropout. The goal was to show that our entropy penalty was something that could be added to models without retuning their hyper-parameters and achieve improved performance.\n\nWe will add supplementary plots showing performance as the hyperparameters are varied. We will also expand the range of hyperparameters considered for the experiments in sections 4.1.1 and 4.1.2.\n\nWe have completed the results in Table 6.\n\nYes, we agree, see our response to AnonReviewer5\u2019s question 2 for more detail.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634611, "id": "ICLR.cc/2017/conference/-/paper295/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634611}}}, {"tddate": null, "tmdate": 1480733008379, "tcdate": 1480733008375, "number": 3, "id": "Hku8ZhJQe", "invitation": "ICLR.cc/2017/conference/-/paper295/pre-review/question", "forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer3"], "content": {"title": "questions", "question": "Have the authors compared their results with other smoothing techniques? E.g.:\n  - the virtual adversarial training\n  - Other smoothing technique is to force the network to estimate uniform output, which is then interpolated with the true CE objective:\n    L = -log(p_c) - lambda * Sum_i(t_i * log(p_i))  where t_i = 1/N, N: num of classes, p_c is the estimated posterior of the correct output.\n\nMNIST result: Comparison with similar model size or compared to label smoothing: is the 0.08% or 0.06% improvement really significant?\n  confidence penalty: 1.0 is maybe not optimal. Have the authors tried larger value?\n\nCIFAR: confidence penalty, 0.1 might be not optimal, have you tried \\beta<0.1?  \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959355669, "id": "ICLR.cc/2017/conference/-/paper295/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper295/AnonReviewer5", "ICLR.cc/2017/conference/paper295/AnonReviewer4", "ICLR.cc/2017/conference/paper295/AnonReviewer3"], "reply": {"forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959355669}}}, {"tddate": null, "tmdate": 1480658478254, "tcdate": 1480652834037, "number": 2, "id": "HyqQd_AGe", "invitation": "ICLR.cc/2017/conference/-/paper295/pre-review/question", "forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer4"], "content": {"title": "isn't this a well-known idea with a long history?", "question": "Your current related work section claims: \"Penalizing the [low] entropy of a network's output distribution has not been explored in supervised learning.\"  Could you please clarify the sense in which this is true?\n\nSurely you've heard of the maximum entropy principle (Jaynes 1957), which breaks ties among distributions that satisfy the data constraints by choosing the one with maximum entropy?  Or the whole line of work in the 1990's on deterministic annealing and information bottleneck methods?  These explicitly add an entropy regularizer similar to yours.  And it's not as if these papers were limited to unsupervised methods (Rao et al. 1990; Tishby et al., 1999) or to single-layer probabilistic classifiers (Berger et al. 1996).  For example, consider Miller et al. (1995) https://scl.ece.ucsb.edu/sites/scl.ece.ucsb.edu/files/publications/b95_4_0.pdf : \"... probability distributions chosen to maximize entropy subject to a constraint on the expected classification error.  ... Our method is applicable to a variety of classifier structures, including ... multilayer perceptron-based classifiers.\"\n\nConversely, in some settings it makes sense to take beta < 0 and penalize HIGH entropy, on the grounds that the true classification is unambiguous and thus the true classifier will be confident.  This yields the entropy regularization technique of Grandvalet and Bengio (2005), which, in their semi-supervised setting, can be seen as a probabilistic analogue of transductive SVMs. \n\nThese ideas are well known and have been applied in many settings and with many variants.  Thus, I hope you can spend a day identifying a wider swath of relevant previous work (including work before 2014, and any work that may have conducted experiments similar to yours) and situating your own contribution more precisely with respect to it.  \n\n---------------------\n\nI'm not sure I understand your result tables.  Usually they have two sections.  Is the idea that the numbers above the line are copied from the literature, while the numbers below the line are from your own implementations?  You seem to boldface the lowest test number in each section, except in Table 4 - why is Table 4 different?  And are the differences among systems statistically significant?  \n\nIn some sections you compare label smoothing or confidence penalty to dropout, while in other sections you combine them with dropout and compare to dropout alone.  Why the change?\n\nHow sensitive were your results to the choice of hyperparameter?  (You say something about this in section 4.4.2, although you don't seem to report your actual hyperparameters in section 4.4.2.)  Also, are you sure you did enough of a grid search in sections 4.1.1 and 4.1.2, given that you often picked one of the extreme values?\n\nThe results are missing in the last line of Table 6.  Do you have these results now?\n\n---------------------\n\nI'm interested in AnonReviewer5's question 2, but AnonReviewer5 seems to be assuming that the main objective only cares about the ranking of the classes y, e.g., that it scores argmax_y p(y | x).  Is your answer that it's ok because your main objective term is log p(y|x)?  (\\sum_y p(y|x) loss(y,y*) would also work, as in the many papers on deterministic annealing in supervised settings.)\n\n---------------------\n\nNOTE: I may edit this post to add additional questions here.  Thought I'd be able to post them separately, but apparently not."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959355669, "id": "ICLR.cc/2017/conference/-/paper295/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper295/AnonReviewer5", "ICLR.cc/2017/conference/paper295/AnonReviewer4", "ICLR.cc/2017/conference/paper295/AnonReviewer3"], "reply": {"forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959355669}}}, {"tddate": null, "tmdate": 1480641502858, "tcdate": 1480621567545, "number": 1, "id": "SJP-AeCzg", "invitation": "ICLR.cc/2017/conference/-/paper295/pre-review/question", "forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "signatures": ["ICLR.cc/2017/conference/paper295/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper295/AnonReviewer5"], "content": {"title": "The effect of entropy regularization on calibration", "question": "In this draft, the models are only evaluated (as is standard) by accuracy. But often model probabilities are used as input to decision theory.\n\nIt seems one consequence of the entropy regularization is that it could un-calibrate probabilities. \n\nConsider a problem with two classes. The first class has base rate 20% and the second has base rate 80%. \n\nIn the extreme case, if you crank up the entropy regularization you might get a model which always gives both classes 50% probability. But this overestimates the frequency of the first label and underestimates the frequency of the second.\n\nIn short, this problem arises in part because this form of the confidence penalty pushes label distributions towards uniform. Another formulation might push label distributions towards the base rate distribution instead. Have you considered such an alternative formulation?\n\n*UPDATE* Thanks for the answer. This makes sense.\n\nQuestion 2:\nIt might seem that achieving arbitrarily high entropy while preserving prediction accuracy should be trivial to accomplish. Any softmax softening maintains the order of the predictions exactly while increasing the entropy.\n\nSo while the experimental results appear strong and are clearly the interesting component of this paper, I suggest there's a big question mark over why precisely this method works. This regularizer doesn't limit the model complexity.\n\nOne explanation given in the paper suggests that the confident predictions are a symptom of overfitting, therefore by addressing overconfident predictions we might mitigate overfitting.\n\nThis argument appears doubly dubious.\n\nFirst, there's a logical error: curing a symptom doesn't cure a disease (although it might be a valid source of hypothesis generation).\nSecond It's not clear that having more confident predictions is necessarily bad, or precisely at what point it becomes bad.\n\nAn alternative hypothesis is that you might get sharp errors (from cross entropy) when probabilities are very confident. Entropy regularization might effectively be providing some gradient clipping. \n\nThis could lead to faster convergence and a growing body of work (see Moritz Hardt's \"Train faster, generalize better\" suggests that faster convergence leads to better generalization).\n\nI'm interested to hear your thoughts and a deeper explanation / discussion of what precisely is working - especially in light of the triviality of satisfying the entropy maximization goal while generating predictions with identical accuracy. \n\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959355669, "id": "ICLR.cc/2017/conference/-/paper295/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper295/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper295/AnonReviewer5", "ICLR.cc/2017/conference/paper295/AnonReviewer4", "ICLR.cc/2017/conference/paper295/AnonReviewer3"], "reply": {"forum": "HkCjNI5ex", "replyto": "HkCjNI5ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper295/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959355669}}}, {"tddate": null, "tmdate": 1480631246189, "tcdate": 1480631246184, "number": 1, "id": "r1LRQQRMx", "invitation": "ICLR.cc/2017/conference/-/paper295/public/comment", "forum": "HkCjNI5ex", "replyto": "SJP-AeCzg", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "Re: The effect of entropy regularization on calibration", "comment": "Thank you for highlighting this point. Indeed, entropy regularization can mess up calibration.  We do report using the unigram frequency to push the predictive distribution towards the base rate and it works well on a language task where the class frequencies are very unequal.\n\nMoreover, we also noticed that the confidence penalty is KL with the uniform flipped (compared to label smoothing), which suggests a way to do \"unigram\" confidence penalty (i.e., KL with the unigram flipped from unigram label smoothing) which we leave to future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287634611, "id": "ICLR.cc/2017/conference/-/paper295/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HkCjNI5ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper295/reviewers", "ICLR.cc/2017/conference/paper295/areachairs"], "cdate": 1485287634611}}}], "count": 18}