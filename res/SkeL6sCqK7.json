{"notes": [{"id": "SkeL6sCqK7", "original": "HkxZi6s9F7", "number": 803, "cdate": 1538087869682, "ddate": null, "tcdate": 1538087869682, "tmdate": 1545355424318, "tddate": null, "forum": "SkeL6sCqK7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS", "abstract": "Understanding the groundbreaking performance of Deep Neural Networks is one\nof the greatest challenges to the scientific community today. In this work, we\nintroduce an information theoretic viewpoint on the behavior of deep networks\noptimization processes and their generalization abilities. By studying the Information\nPlane, the plane of the mutual information between the input variable and\nthe desired label, for each hidden layer. Specifically, we show that the training of\nthe network is characterized by a rapid increase in the mutual information (MI)\nbetween the layers and the target label, followed by a longer decrease in the MI\nbetween the layers and the input variable. Further, we explicitly show that these\ntwo fundamental information-theoretic quantities correspond to the generalization\nerror of the network, as a result of introducing a new generalization bound that is\nexponential in the representation compression. The analysis focuses on typical\npatterns of large-scale problems. For this purpose, we introduce a novel analytic\nbound on the mutual information between consecutive layers in the network.\nAn important consequence of our analysis is a super-linear boost in training time\nwith the number of non-degenerate hidden layers, demonstrating the computational\nbenefit of the hidden layers.", "keywords": ["Deep neural network", "information theory", "training dynamics"], "authorids": ["ravid.ziv@mail.huji.ac.il", "amichai.painsky@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "authors": ["Ravid Shwartz-Ziv", "Amichai Painsky", "Naftali Tishby"], "TL;DR": "Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities", "pdf": "/pdf/1a3fa6cd8f5bb55993afe27558e70e44caf4270a.pdf", "paperhash": "shwartzziv|representation_compression_and_generalization_in_deep_neural_networks", "_bibtex": "@misc{\nshwartz-ziv2019representation,\ntitle={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeL6sCqK7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1g0JdLXx4", "original": null, "number": 1, "cdate": 1544935397943, "ddate": null, "tcdate": 1544935397943, "tmdate": 1545354491317, "tddate": null, "forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper803/Meta_Review", "content": {"metareview": "The authors admit the paper \"was not written carefully enough and requires major rewriting.\"  This seems to be a frustratingly common phenomenon with work on the information bottleneck. \n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Needs a rewrite"}, "signatures": ["ICLR.cc/2019/Conference/Paper803/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper803/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS", "abstract": "Understanding the groundbreaking performance of Deep Neural Networks is one\nof the greatest challenges to the scientific community today. In this work, we\nintroduce an information theoretic viewpoint on the behavior of deep networks\noptimization processes and their generalization abilities. By studying the Information\nPlane, the plane of the mutual information between the input variable and\nthe desired label, for each hidden layer. Specifically, we show that the training of\nthe network is characterized by a rapid increase in the mutual information (MI)\nbetween the layers and the target label, followed by a longer decrease in the MI\nbetween the layers and the input variable. Further, we explicitly show that these\ntwo fundamental information-theoretic quantities correspond to the generalization\nerror of the network, as a result of introducing a new generalization bound that is\nexponential in the representation compression. The analysis focuses on typical\npatterns of large-scale problems. For this purpose, we introduce a novel analytic\nbound on the mutual information between consecutive layers in the network.\nAn important consequence of our analysis is a super-linear boost in training time\nwith the number of non-degenerate hidden layers, demonstrating the computational\nbenefit of the hidden layers.", "keywords": ["Deep neural network", "information theory", "training dynamics"], "authorids": ["ravid.ziv@mail.huji.ac.il", "amichai.painsky@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "authors": ["Ravid Shwartz-Ziv", "Amichai Painsky", "Naftali Tishby"], "TL;DR": "Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities", "pdf": "/pdf/1a3fa6cd8f5bb55993afe27558e70e44caf4270a.pdf", "paperhash": "shwartzziv|representation_compression_and_generalization_in_deep_neural_networks", "_bibtex": "@misc{\nshwartz-ziv2019representation,\ntitle={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeL6sCqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper803/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353081755, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper803/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper803/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper803/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353081755}}}, {"id": "S1xlhulPCX", "original": null, "number": 3, "cdate": 1543076007954, "ddate": null, "tcdate": 1543076007954, "tmdate": 1543076007954, "tddate": null, "forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper803/Public_Comment", "content": {"comment": "The relation between compression (information reduction), flat minima (SGD), and generalization is also described in Achile https://arxiv.org/abs/1706.01350 which proves that flatness bounds information in the weights, and information in the weights bounds information in the activations, which is the form of compression discussed in this paper. That work should be referenced.\n", "title": "related work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS", "abstract": "Understanding the groundbreaking performance of Deep Neural Networks is one\nof the greatest challenges to the scientific community today. In this work, we\nintroduce an information theoretic viewpoint on the behavior of deep networks\noptimization processes and their generalization abilities. By studying the Information\nPlane, the plane of the mutual information between the input variable and\nthe desired label, for each hidden layer. Specifically, we show that the training of\nthe network is characterized by a rapid increase in the mutual information (MI)\nbetween the layers and the target label, followed by a longer decrease in the MI\nbetween the layers and the input variable. Further, we explicitly show that these\ntwo fundamental information-theoretic quantities correspond to the generalization\nerror of the network, as a result of introducing a new generalization bound that is\nexponential in the representation compression. The analysis focuses on typical\npatterns of large-scale problems. For this purpose, we introduce a novel analytic\nbound on the mutual information between consecutive layers in the network.\nAn important consequence of our analysis is a super-linear boost in training time\nwith the number of non-degenerate hidden layers, demonstrating the computational\nbenefit of the hidden layers.", "keywords": ["Deep neural network", "information theory", "training dynamics"], "authorids": ["ravid.ziv@mail.huji.ac.il", "amichai.painsky@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "authors": ["Ravid Shwartz-Ziv", "Amichai Painsky", "Naftali Tishby"], "TL;DR": "Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities", "pdf": "/pdf/1a3fa6cd8f5bb55993afe27558e70e44caf4270a.pdf", "paperhash": "shwartzziv|representation_compression_and_generalization_in_deep_neural_networks", "_bibtex": "@misc{\nshwartz-ziv2019representation,\ntitle={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeL6sCqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper803/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311749176, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SkeL6sCqK7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper803/Authors", "ICLR.cc/2019/Conference/Paper803/Reviewers", "ICLR.cc/2019/Conference/Paper803/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper803/Authors", "ICLR.cc/2019/Conference/Paper803/Reviewers", "ICLR.cc/2019/Conference/Paper803/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311749176}}}, {"id": "Skg-RwX2aX", "original": null, "number": 2, "cdate": 1542367176752, "ddate": null, "tcdate": 1542367176752, "tmdate": 1542367176752, "tddate": null, "forum": "SkeL6sCqK7", "replyto": "rJx3jDQham", "invitation": "ICLR.cc/2019/Conference/-/Paper803/Official_Comment", "content": {"title": "Authors' response to the reviewers' comments - part 2 ", "comment": "4. This new bound directly leads to the empirical representation compression in the information plane, as reported by Shwartz-Ziv and Tishby for both saturated and ReLU nonlinearities, without any assumption on binning or discretization of the units!\n\n5. This result refutes the main claims of Saxe et al: (a) that the observed compression depends on the binning (b) that it results from the saturation of the units and (c) has nothing to do with the stochastic gradients or generalization.\n\n6. It also gives the first proof, to our knowledge, that convergence to flat minima improves generalization, as conjectured by many others without any mathematical explanation.\n\n7. We finally briefly scratched (due to lack of space) our most striking corollary: due to this diffusion compression, the convergence to good generalization is faster with more hidden layers and the convergence time scales as a negative power of the number of effective layers.  We agree that this striking new result is hard to understand from this paper alone and requires a separate publication.\n\nReferences - \n[1]Saxe, A. M., Bansal, Y., Dapello, J., Advani, M., Kolchinsky, A., Tracey, B. D., & Cox, D. D. On the information bottleneck theory of deep learning, ICLR, 2018\n[2] Tishby, Naftali, and Noga Zaslavsky. \"Deep learning and the information bottleneck principle.\" Information Theory Workshop (ITW), 2015 IEEE. IEEE, 2015.\n[3] Shwartz-Ziv, Ravid, and Naftali Tishby. \"Opening the black box of deep neural networks via information.\" arXiv preprint arXiv:1703.00810 (2017).\n[4] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic gradient algorithms. arXiv:1511.06251, 2015.\n[5] Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate Bayesian inference. The Journal of Machine Learning Research, 18(1):4873\u20134907, 2017.\n[6] Chris Junchi Li, Lei Li, Junyang Qian, and Jian-Guo Liu. Batch size matters: A diffusion approximation framework on nonconvex stochastic gradient descent. arXiv:1705.07562v1, 2017\n[7] Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient descent. arXiv:1710.06451, 2018.\n[5] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. arXiv:1710.11029, 2017.\n[8] Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in SGD. arXiv:1711.04623, 2017.\n[9] Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma. The anisotropic noise in stochastic gradient descent: Its behavior of escaping from minima and regularization effects. arXiv:1803.00195, 2018.\n[10] Jing An, Jianfeng Lu, and Lexing Ying. Stochastic modified equations for the asynchronous stochastic gradient descent. arXiv:1805.08244, 2018."}, "signatures": ["ICLR.cc/2019/Conference/Paper803/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper803/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper803/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS", "abstract": "Understanding the groundbreaking performance of Deep Neural Networks is one\nof the greatest challenges to the scientific community today. In this work, we\nintroduce an information theoretic viewpoint on the behavior of deep networks\noptimization processes and their generalization abilities. By studying the Information\nPlane, the plane of the mutual information between the input variable and\nthe desired label, for each hidden layer. Specifically, we show that the training of\nthe network is characterized by a rapid increase in the mutual information (MI)\nbetween the layers and the target label, followed by a longer decrease in the MI\nbetween the layers and the input variable. Further, we explicitly show that these\ntwo fundamental information-theoretic quantities correspond to the generalization\nerror of the network, as a result of introducing a new generalization bound that is\nexponential in the representation compression. The analysis focuses on typical\npatterns of large-scale problems. For this purpose, we introduce a novel analytic\nbound on the mutual information between consecutive layers in the network.\nAn important consequence of our analysis is a super-linear boost in training time\nwith the number of non-degenerate hidden layers, demonstrating the computational\nbenefit of the hidden layers.", "keywords": ["Deep neural network", "information theory", "training dynamics"], "authorids": ["ravid.ziv@mail.huji.ac.il", "amichai.painsky@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "authors": ["Ravid Shwartz-Ziv", "Amichai Painsky", "Naftali Tishby"], "TL;DR": "Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities", "pdf": "/pdf/1a3fa6cd8f5bb55993afe27558e70e44caf4270a.pdf", "paperhash": "shwartzziv|representation_compression_and_generalization_in_deep_neural_networks", "_bibtex": "@misc{\nshwartz-ziv2019representation,\ntitle={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeL6sCqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper803/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619070, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeL6sCqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper803/Authors", "ICLR.cc/2019/Conference/Paper803/Reviewers", "ICLR.cc/2019/Conference/Paper803/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper803/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper803/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper803/Authors|ICLR.cc/2019/Conference/Paper803/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper803/Reviewers", "ICLR.cc/2019/Conference/Paper803/Authors", "ICLR.cc/2019/Conference/Paper803/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619070}}}, {"id": "rJx3jDQham", "original": null, "number": 1, "cdate": 1542367140092, "ddate": null, "tcdate": 1542367140092, "tmdate": 1542367140092, "tddate": null, "forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper803/Official_Comment", "content": {"title": "Authors' response to the reviewers' comments - part 1 ", "comment": "We thank the reviewers for their comments.\n\nWe agree with the reviewers that the submitted paper was not written carefully enough and requires major rewriting.\n\nYet, the reviewers, in particular reviewer 1, missed or dismissed our main and new results, which rigorously refutes - one by one - the misleading claims of Saxe et.al. [1].\n\nThe Information Bottleneck theory of Deep Learning [2-3] has received significant attention in the past year, as can be seen from the number of related, or inspired by, submissions to this conference alone. This is despite the fact that the theory was not properly and correctly described anywhere (certainly not by Saxe et al 2017 despite their title). Most of this impact is due to Tishby\u2019s presentations and online talks. This is the reason we found it necessary to first review some of its basic claims. This review was obviously too long for this paper as the really new results were squeezed into the last pages. \n\nOur main novel results are summarized below:\n\n1. We provide a rigorous proof ( Thm. 2) that the mutual information between successive layers decreases during the diffusion phase of the SGD training - for any nonlinearity of the units, saturated, linear, or piecewise linear as ReLU. \n\n2. The only important assumption in our proof is that there is a distinct diffusion phase in the SGD training, as reported and well established by many others [5-10]. This phenomenon is related to the convergence to \u201ca flat minimum\u201d of the training error. We also assume that the mini-batches are statistically independent and that the layers are sufficiently wide to justify our usage of the central limit theorem for the diffusion weights. All other assumptions are standard technical conditions which are met with probability 1 in standard deep learning. Our results do not rely in any way on continuous time SGD, nor on the assumption that the gradient fluctuations are Gaussian  - these requirements are clearly confusing and irrelevant. The continuous time approximation to SGD is in fact justified in [4], but is not essential to our analysis in this paper.\n\n3. To demonstrate this result, numerical simulations in this paper have been done with ResNets with RelU nonlinearities, as explicitly stated in the paper - in contrast to the claim of reviewer 2."}, "signatures": ["ICLR.cc/2019/Conference/Paper803/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper803/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper803/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS", "abstract": "Understanding the groundbreaking performance of Deep Neural Networks is one\nof the greatest challenges to the scientific community today. In this work, we\nintroduce an information theoretic viewpoint on the behavior of deep networks\noptimization processes and their generalization abilities. By studying the Information\nPlane, the plane of the mutual information between the input variable and\nthe desired label, for each hidden layer. Specifically, we show that the training of\nthe network is characterized by a rapid increase in the mutual information (MI)\nbetween the layers and the target label, followed by a longer decrease in the MI\nbetween the layers and the input variable. Further, we explicitly show that these\ntwo fundamental information-theoretic quantities correspond to the generalization\nerror of the network, as a result of introducing a new generalization bound that is\nexponential in the representation compression. The analysis focuses on typical\npatterns of large-scale problems. For this purpose, we introduce a novel analytic\nbound on the mutual information between consecutive layers in the network.\nAn important consequence of our analysis is a super-linear boost in training time\nwith the number of non-degenerate hidden layers, demonstrating the computational\nbenefit of the hidden layers.", "keywords": ["Deep neural network", "information theory", "training dynamics"], "authorids": ["ravid.ziv@mail.huji.ac.il", "amichai.painsky@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "authors": ["Ravid Shwartz-Ziv", "Amichai Painsky", "Naftali Tishby"], "TL;DR": "Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities", "pdf": "/pdf/1a3fa6cd8f5bb55993afe27558e70e44caf4270a.pdf", "paperhash": "shwartzziv|representation_compression_and_generalization_in_deep_neural_networks", "_bibtex": "@misc{\nshwartz-ziv2019representation,\ntitle={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeL6sCqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper803/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619070, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeL6sCqK7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper803/Authors", "ICLR.cc/2019/Conference/Paper803/Reviewers", "ICLR.cc/2019/Conference/Paper803/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper803/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper803/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper803/Authors|ICLR.cc/2019/Conference/Paper803/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper803/Reviewers", "ICLR.cc/2019/Conference/Paper803/Authors", "ICLR.cc/2019/Conference/Paper803/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619070}}}, {"id": "BkxBcybAh7", "original": null, "number": 2, "cdate": 1541439373364, "ddate": null, "tcdate": 1541439373364, "tmdate": 1542143754965, "tddate": null, "forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper803/Official_Review", "content": {"title": "Similar to previous, fails to mention criticisms of the research program", "review": "This paper interprets the optimization of deep neural networks in terms of a two phase process: first a drift phase where gradients self average, and second a diffusion phase where the variance is larger than the square of the mean. As argued by first by Tishby and Zaslavsky and then by Shwartz-Ziv and Tishby (arxiv:1703.00810), the first phase corresponds to the hidden layers becoming more informative about the labels, and the second phase corresponds to a compression of the hidden representation keeping the informative content relatively fixed as in the information bottleneck of Tishby, Pereira, and Bialek. \n\nA lot of this paper rehashes discussion from the prior work and does not seem sufficiently original. The main contribution seems to be a bound that is supposed to demonstrate representation compression in the diffusion phase. The authors further argue that this shows that adding hidden layers lead to a boosting of convergence time.\n\nFurthermore, the analytic bound relies on a number of assumptions that make it difficult to evaluate. One example is using the continuum limit for SGD (1), which is very popular but not necessarily appropriate. (See, e.g., the discussion in section 2.3.3 in arxiv:1810.00004.)\n\nAdditionally, there has been extensive discussion in the literature regarding whether the results of Shwartz-Ziv and Tishby (arxiv:1703.00810) hold in general, centering in particular on whether there is a dependence on the choice of the hyperbolic tangent activation function. I find it highly problematic that the authors continue to do all their experiments using the hyperbolic tangent, even though they claim their analytic bounds are supposed to hold for any choice of activation. If the bound is general, why not include experimental results showing that claim? The lack of discussion of this point and the omission of such experiments is highly suspicious.\n\nPerhaps more importantly, the authors do not even mention or address this contention or even cite this Saxe et al. paper (https://openreview.net/forum?id=ry_WPG-A-) that brings up this point. They also cite Gabrie et al. (arxiv:1805:09785) as promising work about computing mutual information for deep networks, while my interpretation of that work was pointing out that such methods are highly dependent on choices of binning or regulating continuous variables when computing mutual informations. In fact, I don't see any discussion at all this discretization problem, when it seems absolutely central to understanding whether there is a sensible interpretation of these results or not.\n\nFor all these reasons, I don't see how this paper can be published in its present form.", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper803/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS", "abstract": "Understanding the groundbreaking performance of Deep Neural Networks is one\nof the greatest challenges to the scientific community today. In this work, we\nintroduce an information theoretic viewpoint on the behavior of deep networks\noptimization processes and their generalization abilities. By studying the Information\nPlane, the plane of the mutual information between the input variable and\nthe desired label, for each hidden layer. Specifically, we show that the training of\nthe network is characterized by a rapid increase in the mutual information (MI)\nbetween the layers and the target label, followed by a longer decrease in the MI\nbetween the layers and the input variable. Further, we explicitly show that these\ntwo fundamental information-theoretic quantities correspond to the generalization\nerror of the network, as a result of introducing a new generalization bound that is\nexponential in the representation compression. The analysis focuses on typical\npatterns of large-scale problems. For this purpose, we introduce a novel analytic\nbound on the mutual information between consecutive layers in the network.\nAn important consequence of our analysis is a super-linear boost in training time\nwith the number of non-degenerate hidden layers, demonstrating the computational\nbenefit of the hidden layers.", "keywords": ["Deep neural network", "information theory", "training dynamics"], "authorids": ["ravid.ziv@mail.huji.ac.il", "amichai.painsky@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "authors": ["Ravid Shwartz-Ziv", "Amichai Painsky", "Naftali Tishby"], "TL;DR": "Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities", "pdf": "/pdf/1a3fa6cd8f5bb55993afe27558e70e44caf4270a.pdf", "paperhash": "shwartzziv|representation_compression_and_generalization_in_deep_neural_networks", "_bibtex": "@misc{\nshwartz-ziv2019representation,\ntitle={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeL6sCqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper803/Official_Review", "cdate": 1542234373572, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper803/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335805954, "tmdate": 1552335805954, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper803/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJx9I0BgaQ", "original": null, "number": 3, "cdate": 1541590610266, "ddate": null, "tcdate": 1541590610266, "tmdate": 1541590610266, "tddate": null, "forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper803/Official_Review", "content": {"title": "Interesting, but hard to interpret the technical results.", "review": "This paper presents some results about the information bottleneck view of generalization in deep learning studied in recent work by Tishby et al.\nSpecifically this line of work seeks to understand the dynamics of stochastic gradient descent using information theory. In particular, it quantifies the mutual information between successive layers of a neural network. Minimizing mutual information subject to empirical accuracy intuitively corresponds to compression of the input and removal of superfluous information.\nThis paper further formalizes some of these intuitive ideas. In particular, it gives a variance/generalization bound in terms of mutual information and it proves an asymptotic upper bound on mutual information for the dynamics of SGD.\n\nI think this is an intriguing line of work and this paper makes an meaningful contribution to it. The paper is generally well-written (modulo some typos), but it jumps into the technical details (stochastic calculus!) without giving much intuition to help digest the results or discussion of how they relate to the broader picture. (Although I appreciate the difficulty of working with a page limit.) \n\nTypos, etc.:\np1. \"ereas\" should be \"whereas\"\np2. double comma preceeding \"the weights are fixed realizations\"\np5. extra of in \"needed to represent of the data\"\nThm 1. L(T_m) has not been formally defined when T_m contains a set of representations rather than data points.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper803/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS", "abstract": "Understanding the groundbreaking performance of Deep Neural Networks is one\nof the greatest challenges to the scientific community today. In this work, we\nintroduce an information theoretic viewpoint on the behavior of deep networks\noptimization processes and their generalization abilities. By studying the Information\nPlane, the plane of the mutual information between the input variable and\nthe desired label, for each hidden layer. Specifically, we show that the training of\nthe network is characterized by a rapid increase in the mutual information (MI)\nbetween the layers and the target label, followed by a longer decrease in the MI\nbetween the layers and the input variable. Further, we explicitly show that these\ntwo fundamental information-theoretic quantities correspond to the generalization\nerror of the network, as a result of introducing a new generalization bound that is\nexponential in the representation compression. The analysis focuses on typical\npatterns of large-scale problems. For this purpose, we introduce a novel analytic\nbound on the mutual information between consecutive layers in the network.\nAn important consequence of our analysis is a super-linear boost in training time\nwith the number of non-degenerate hidden layers, demonstrating the computational\nbenefit of the hidden layers.", "keywords": ["Deep neural network", "information theory", "training dynamics"], "authorids": ["ravid.ziv@mail.huji.ac.il", "amichai.painsky@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "authors": ["Ravid Shwartz-Ziv", "Amichai Painsky", "Naftali Tishby"], "TL;DR": "Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities", "pdf": "/pdf/1a3fa6cd8f5bb55993afe27558e70e44caf4270a.pdf", "paperhash": "shwartzziv|representation_compression_and_generalization_in_deep_neural_networks", "_bibtex": "@misc{\nshwartz-ziv2019representation,\ntitle={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeL6sCqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper803/Official_Review", "cdate": 1542234373572, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper803/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335805954, "tmdate": 1552335805954, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper803/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1gMg00tn7", "original": null, "number": 1, "cdate": 1541168618084, "ddate": null, "tcdate": 1541168618084, "tmdate": 1541533677326, "tddate": null, "forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "invitation": "ICLR.cc/2019/Conference/-/Paper803/Official_Review", "content": {"title": "It is a paper written in a rush that its clarity is a main problem.", "review": "The authors are providing an information theoretic viewpoint on the behavior of DNN based on the information bottleneck.  The clarity of the paper is my main concern.  It contains quite a number of typos and errors.  For example, in section 6, the results of MNIST in the first experiment was presented after introducing the second experiment.  Also, the results shown in Fig 1b seems to have nothing to do with Fig. 1a.  It makes use of some existing results from other literature but it is not clearly explained how and why the results are being used.   It might be a very good paper if the writing could be improved.   The paper also contains some experimental results.  But they are too brief and I do not consider the experiments as sufficient to justify the correctness of the bounds proved in the paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper803/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "REPRESENTATION COMPRESSION AND GENERALIZATION IN DEEP NEURAL NETWORKS", "abstract": "Understanding the groundbreaking performance of Deep Neural Networks is one\nof the greatest challenges to the scientific community today. In this work, we\nintroduce an information theoretic viewpoint on the behavior of deep networks\noptimization processes and their generalization abilities. By studying the Information\nPlane, the plane of the mutual information between the input variable and\nthe desired label, for each hidden layer. Specifically, we show that the training of\nthe network is characterized by a rapid increase in the mutual information (MI)\nbetween the layers and the target label, followed by a longer decrease in the MI\nbetween the layers and the input variable. Further, we explicitly show that these\ntwo fundamental information-theoretic quantities correspond to the generalization\nerror of the network, as a result of introducing a new generalization bound that is\nexponential in the representation compression. The analysis focuses on typical\npatterns of large-scale problems. For this purpose, we introduce a novel analytic\nbound on the mutual information between consecutive layers in the network.\nAn important consequence of our analysis is a super-linear boost in training time\nwith the number of non-degenerate hidden layers, demonstrating the computational\nbenefit of the hidden layers.", "keywords": ["Deep neural network", "information theory", "training dynamics"], "authorids": ["ravid.ziv@mail.huji.ac.il", "amichai.painsky@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "authors": ["Ravid Shwartz-Ziv", "Amichai Painsky", "Naftali Tishby"], "TL;DR": "Introduce an information theoretic viewpoint on the behavior of deep networks optimization processes and their generalization abilities", "pdf": "/pdf/1a3fa6cd8f5bb55993afe27558e70e44caf4270a.pdf", "paperhash": "shwartzziv|representation_compression_and_generalization_in_deep_neural_networks", "_bibtex": "@misc{\nshwartz-ziv2019representation,\ntitle={{REPRESENTATION} {COMPRESSION} {AND} {GENERALIZATION} {IN} {DEEP} {NEURAL} {NETWORKS}},\nauthor={Ravid Shwartz-Ziv and Amichai Painsky and Naftali Tishby},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeL6sCqK7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper803/Official_Review", "cdate": 1542234373572, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeL6sCqK7", "replyto": "SkeL6sCqK7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper803/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335805954, "tmdate": 1552335805954, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper803/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}