{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124450893, "tcdate": 1518469755289, "number": 270, "cdate": 1518469755289, "id": "S1XFzYyPM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "S1XFzYyPM", "signatures": ["~Satrajit_Chatterjee1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Learning and Memorization", "abstract": "In the machine learning research community, it is generally believed that\nthere is a tension between memorization and generalization. In this work we\nexamine to what extent this tension exists, by exploring if it is\npossible to generalize through memorization alone. Although direct memorization\nwith a lookup table obviously does not generalize, we find that introducing\ndepth in the form of a network of support-limited lookup tables leads to\ngeneralization that is significantly above chance and closer to those\nobtained by standard learning algorithms on several tasks derived from MNIST \nand CIFAR-10. Furthermore, we demonstrate through a series of\nempirical results that our approach allows for a smooth tradeoff between\nmemorization and generalization and exhibits some of the most salient\ncharacteristics of neural networks: depth improves performance; random data\ncan be memorized and yet there is generalization on real data; and \nmemorizing random data is harder in a certain sense than memorizing real\ndata. The extreme simplicity of the algorithm and potential connections\nwith stability provide important insights into the impact of depth on\nlearning algorithms, and point to several interesting directions for future\nresearch.", "paperhash": "chatterjee|learning_and_memorization", "keywords": ["deep learning", "network architecture", "memorization", "generalization error"], "_bibtex": "@misc{\n  chatterjee2018learning,\n  title={Learning and Memorization},\n  author={Satrajit Chatterjee},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XFzYyPM}\n}", "authorids": ["satrajit@gmail.com"], "authors": ["Satrajit Chatterjee"], "TL;DR": "It is possible to generalize a fair bit by memorizing alone; and this thought experiment leads to an interesting toy model.", "pdf": "/pdf/f4b3131310baed4d4cc41624cd7e1525ef842596.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582844852, "tcdate": 1520588801834, "number": 1, "cdate": 1520588801834, "id": "r19bOAyFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper270/Official_Review", "forum": "S1XFzYyPM", "replyto": "S1XFzYyPM", "signatures": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer3"], "content": {"title": "Exploring the generalization and memorization in deep nets but lack contribution and clarity ", "rating": "4: Ok but not good enough - rejection", "review": "This paper investigates the memorization and generalization problems and examines the possibility of generalization through memorization alone. \nThis work touches very interesting and important problem. However, the model/experiments proposed in the paper is very ambiguous and unclear to me. In addition, it is not clear to me what is the contribution here. \nAnother issue is that the main claim of the paper, generalization is possible with pure memorization, in the paper is not conclusive due to that fact that the experiments are done on Binary-MNIST and Binary-CIFAR-10 and in my view, these experiments are not thorough enough to reach this conclusion. \n\nGiven the focus of the workshop track which is late-breaking development, very novel ideas, or position paper,  I am not convinced this paper is a good fit for the workshop track. \n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Memorization", "abstract": "In the machine learning research community, it is generally believed that\nthere is a tension between memorization and generalization. In this work we\nexamine to what extent this tension exists, by exploring if it is\npossible to generalize through memorization alone. Although direct memorization\nwith a lookup table obviously does not generalize, we find that introducing\ndepth in the form of a network of support-limited lookup tables leads to\ngeneralization that is significantly above chance and closer to those\nobtained by standard learning algorithms on several tasks derived from MNIST \nand CIFAR-10. Furthermore, we demonstrate through a series of\nempirical results that our approach allows for a smooth tradeoff between\nmemorization and generalization and exhibits some of the most salient\ncharacteristics of neural networks: depth improves performance; random data\ncan be memorized and yet there is generalization on real data; and \nmemorizing random data is harder in a certain sense than memorizing real\ndata. The extreme simplicity of the algorithm and potential connections\nwith stability provide important insights into the impact of depth on\nlearning algorithms, and point to several interesting directions for future\nresearch.", "paperhash": "chatterjee|learning_and_memorization", "keywords": ["deep learning", "network architecture", "memorization", "generalization error"], "_bibtex": "@misc{\n  chatterjee2018learning,\n  title={Learning and Memorization},\n  author={Satrajit Chatterjee},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XFzYyPM}\n}", "authorids": ["satrajit@gmail.com"], "authors": ["Satrajit Chatterjee"], "TL;DR": "It is possible to generalize a fair bit by memorizing alone; and this thought experiment leads to an interesting toy model.", "pdf": "/pdf/f4b3131310baed4d4cc41624cd7e1525ef842596.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582844668, "id": "ICLR.cc/2018/Workshop/-/Paper270/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper270/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper270/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper270/AnonReviewer2"], "reply": {"forum": "S1XFzYyPM", "replyto": "S1XFzYyPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper270/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper270/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582844668}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582774858, "tcdate": 1520635717032, "number": 2, "cdate": 1520635717032, "id": "r1pSyclYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper270/Official_Review", "forum": "S1XFzYyPM", "replyto": "S1XFzYyPM", "signatures": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer1"], "content": {"title": "Great experiment", "rating": "9: Top 15% of accepted papers, strong accept", "review": "In this paper the author proposes a simple experiment: they build a network of support-limited lookup tables, structured like deep dense neural networks, and show that such a simple memorization mechanism is still able to generalize, in the sense that it performs much better than chance on a test set.\n\nNot only is this paper well-written, it performs a simple experiment that should, once again, make us reconsider many of our preconceived notions regarding neural networks. \nSince it is fairly self-contained, I don't have many remarks:\n- What is the distribution of the random data that you use? (I imagine bits with p=1/2)\n- Does randomly breaking ties/missing data make a difference compared to outputting a constant? (e.g. 0)\n- You compare to a convnet for completeness, but I don't think that's fair given the geometrical prior. How hard would it be to implement a \"lookup-table-convnet\"?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Memorization", "abstract": "In the machine learning research community, it is generally believed that\nthere is a tension between memorization and generalization. In this work we\nexamine to what extent this tension exists, by exploring if it is\npossible to generalize through memorization alone. Although direct memorization\nwith a lookup table obviously does not generalize, we find that introducing\ndepth in the form of a network of support-limited lookup tables leads to\ngeneralization that is significantly above chance and closer to those\nobtained by standard learning algorithms on several tasks derived from MNIST \nand CIFAR-10. Furthermore, we demonstrate through a series of\nempirical results that our approach allows for a smooth tradeoff between\nmemorization and generalization and exhibits some of the most salient\ncharacteristics of neural networks: depth improves performance; random data\ncan be memorized and yet there is generalization on real data; and \nmemorizing random data is harder in a certain sense than memorizing real\ndata. The extreme simplicity of the algorithm and potential connections\nwith stability provide important insights into the impact of depth on\nlearning algorithms, and point to several interesting directions for future\nresearch.", "paperhash": "chatterjee|learning_and_memorization", "keywords": ["deep learning", "network architecture", "memorization", "generalization error"], "_bibtex": "@misc{\n  chatterjee2018learning,\n  title={Learning and Memorization},\n  author={Satrajit Chatterjee},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XFzYyPM}\n}", "authorids": ["satrajit@gmail.com"], "authors": ["Satrajit Chatterjee"], "TL;DR": "It is possible to generalize a fair bit by memorizing alone; and this thought experiment leads to an interesting toy model.", "pdf": "/pdf/f4b3131310baed4d4cc41624cd7e1525ef842596.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582844668, "id": "ICLR.cc/2018/Workshop/-/Paper270/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper270/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper270/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper270/AnonReviewer2"], "reply": {"forum": "S1XFzYyPM", "replyto": "S1XFzYyPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper270/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper270/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582844668}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582662720, "tcdate": 1520757177728, "number": 3, "cdate": 1520757177728, "id": "rJGaKvMFG", "invitation": "ICLR.cc/2018/Workshop/-/Paper270/Official_Review", "forum": "S1XFzYyPM", "replyto": "S1XFzYyPM", "signatures": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer2"], "content": {"title": "Interesting model, merits discussion", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper proposes an interesting way of building a hierarchy of features using memorization alone. Each feature is a lookup table that maps k-bit binary strings to {0, 1} corresponding to the two classes in a binary classification problem. The entry for a particular k-bit string is 0 or 1 depending on the majority class among data points that lead to that k-bit string as the input. The model consists of layers of such features where each feature looks at a random subset of k features from the preceding layer.\n\nThis model is reminiscent of a cascade of random forests. It has the interesting property of being deep and non-linear and at the same time very easy to construct by memorization in a layer-by-layer manner. While the results are not great, given the simplicity of the model, they are promising enough to merit discussion and more investigation.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Memorization", "abstract": "In the machine learning research community, it is generally believed that\nthere is a tension between memorization and generalization. In this work we\nexamine to what extent this tension exists, by exploring if it is\npossible to generalize through memorization alone. Although direct memorization\nwith a lookup table obviously does not generalize, we find that introducing\ndepth in the form of a network of support-limited lookup tables leads to\ngeneralization that is significantly above chance and closer to those\nobtained by standard learning algorithms on several tasks derived from MNIST \nand CIFAR-10. Furthermore, we demonstrate through a series of\nempirical results that our approach allows for a smooth tradeoff between\nmemorization and generalization and exhibits some of the most salient\ncharacteristics of neural networks: depth improves performance; random data\ncan be memorized and yet there is generalization on real data; and \nmemorizing random data is harder in a certain sense than memorizing real\ndata. The extreme simplicity of the algorithm and potential connections\nwith stability provide important insights into the impact of depth on\nlearning algorithms, and point to several interesting directions for future\nresearch.", "paperhash": "chatterjee|learning_and_memorization", "keywords": ["deep learning", "network architecture", "memorization", "generalization error"], "_bibtex": "@misc{\n  chatterjee2018learning,\n  title={Learning and Memorization},\n  author={Satrajit Chatterjee},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XFzYyPM}\n}", "authorids": ["satrajit@gmail.com"], "authors": ["Satrajit Chatterjee"], "TL;DR": "It is possible to generalize a fair bit by memorizing alone; and this thought experiment leads to an interesting toy model.", "pdf": "/pdf/f4b3131310baed4d4cc41624cd7e1525ef842596.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582844668, "id": "ICLR.cc/2018/Workshop/-/Paper270/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper270/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper270/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper270/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper270/AnonReviewer2"], "reply": {"forum": "S1XFzYyPM", "replyto": "S1XFzYyPM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper270/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper270/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582844668}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573548192, "tcdate": 1521573548192, "number": 22, "cdate": 1521573547772, "id": "H1VhCRRKM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "S1XFzYyPM", "replyto": "S1XFzYyPM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning and Memorization", "abstract": "In the machine learning research community, it is generally believed that\nthere is a tension between memorization and generalization. In this work we\nexamine to what extent this tension exists, by exploring if it is\npossible to generalize through memorization alone. Although direct memorization\nwith a lookup table obviously does not generalize, we find that introducing\ndepth in the form of a network of support-limited lookup tables leads to\ngeneralization that is significantly above chance and closer to those\nobtained by standard learning algorithms on several tasks derived from MNIST \nand CIFAR-10. Furthermore, we demonstrate through a series of\nempirical results that our approach allows for a smooth tradeoff between\nmemorization and generalization and exhibits some of the most salient\ncharacteristics of neural networks: depth improves performance; random data\ncan be memorized and yet there is generalization on real data; and \nmemorizing random data is harder in a certain sense than memorizing real\ndata. The extreme simplicity of the algorithm and potential connections\nwith stability provide important insights into the impact of depth on\nlearning algorithms, and point to several interesting directions for future\nresearch.", "paperhash": "chatterjee|learning_and_memorization", "keywords": ["deep learning", "network architecture", "memorization", "generalization error"], "_bibtex": "@misc{\n  chatterjee2018learning,\n  title={Learning and Memorization},\n  author={Satrajit Chatterjee},\n  year={2018},\n  url={https://openreview.net/forum?id=S1XFzYyPM}\n}", "authorids": ["satrajit@gmail.com"], "authors": ["Satrajit Chatterjee"], "TL;DR": "It is possible to generalize a fair bit by memorizing alone; and this thought experiment leads to an interesting toy model.", "pdf": "/pdf/f4b3131310baed4d4cc41624cd7e1525ef842596.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}