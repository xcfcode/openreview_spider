{"notes": [{"id": "5m3SEczOV8L", "original": "O2aJ9EyrZHV", "number": 562, "cdate": 1601308068777, "ddate": null, "tcdate": 1601308068777, "tmdate": 1616026293246, "tddate": null, "forum": "5m3SEczOV8L", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Q8pW8y6kgN", "original": null, "number": 1, "cdate": 1610040499786, "ddate": null, "tcdate": 1610040499786, "tmdate": 1610474106395, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "This work presents a method to combine EBMs and VAEs in two stages. First, the VAE model is learned; second, an EBM-based correction term is learned via MLE. The methodology is novel and of interest to the ICLR community."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040499773, "tmdate": 1610474106378, "id": "ICLR.cc/2021/Conference/Paper562/-/Decision"}}}, {"id": "mbeMO6foEWF", "original": null, "number": 3, "cdate": 1603936362055, "ddate": null, "tcdate": 1603936362055, "tmdate": 1606783606285, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Review", "content": {"title": "Review", "review": "Strengths:\nThe paper provides a thorough overview of recent work towards training EBMs.  \nThe approach generates high quality image samples by combining EBMs and VAE based models. \nThe paper is well written and is easy to follow \nI find it quite interesting that a combination of both models leads to significant overall improved generative performance \nI also enjoyed the proposed change in the paper -- and it seems to elegantly solve several problem in EBM training. \n\nWeaknesses:\nMy most major concern is that since we are utilizing a maximum likelihood objective to train models, it would be good to evaluate  the overall likelihood of the trained model, even if only in the  2D domain. \nThe histogram of likelihoods of data points is a bit disappointing -- it falls a similar trend of other EBM models, but it would nicer if it followed a Gaussian distribution \nWhat happens when more Langevin sampling steps are applied to the model? (greater than the few used in training) \nI'm also curious on what sampling only the trained energy model looks like (without using the trained VAE parameterization) at evaluation time \nI would also be curious to see how the trained EBM, with the VAE generator  can compose together with other models. See for example [1].\n\n[1] Yilun Du, Shuang Li, Igor Mordatch. Compositional Visual Generation and Inference with Energy Based Models. NeurIPS 2020\n\n#### Post Rebuttal-Update\n\nI thank the authors for responding to my concerns. I enjoyed reading the paper and maintain my rating.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140360, "tmdate": 1606915800055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Review"}}}, {"id": "ARQ0Ecwt8ci", "original": null, "number": 1, "cdate": 1603817661089, "ddate": null, "tcdate": 1603817661089, "tmdate": 1605982822886, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Review", "content": {"title": "A promising symbiosis between VAEs and EBMs", "review": "The authors propose a generative model that is a combination (product) of a VAE and an EBM, where the goal of the EBM is to reduce the probability of out-of-manifold samples, which are typically generated by VAEs. The authors propose efficient training and sampling procedures, in which the VAE is trained first and during the EBM negative-phase, samples are drawn from the joint (x, z) VAE space using reparameterization. The method is shown to achieve high quality samples on several modern image datasets, good FID scores and mode coverage. Ablation studies show the contribution of the different elements.\n\nThis is, in my opinion, a very good work, which combines a novel and well-motivated idea with clear writing and extensive experimental evidence.\n\nSome comments and questions:\n- Does the separate twos-stage training enable the model to reach the optimal point that can be reached in joint training, or is it an approximation? If its an approximation, I think it should be discussed or perhaps bounded.\n- Does the combined model allow computing the likelihood? Can it be evaluated and compared to other models in terms of bits/dimension (e.g. as in VAE or NVAE)?\n- It might be interesting (not something that I think is mandatory) to measure the NVAE log-likelihood of samples generated by the combined model compared to samples generated just by the NVAE.\n\nTo summarize:\npros:\n- novelty\n- significance\n- experimental evidence\n- quality of writing\n\ncons:\n- combining two separately trained models - perhaps sub-optimal\n\n**Update: I thanks the authors for their answers and revised version and keep my positive rating.**\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140360, "tmdate": 1606915800055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Review"}}}, {"id": "F0441ZBzKeB", "original": null, "number": 6, "cdate": 1605565178316, "ddate": null, "tcdate": 1605565178316, "tmdate": 1605565326929, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment", "content": {"title": "Manuscript updated and responses submitted", "comment": "Dear reviewers, \n\nWe want to thank all reviewers for your useful comments. We have submitted responses to all official reviews and uploaded an updated draft. The new draft mainly adds two sections:\n\n1. Section 5.4 which includes an experiment on exact likelihood estimation on 2-d toy dataset. Results show that VAEBM greatly improves the test data likelihood over the VAE.\n\n2. Appendix B.1 that studies sampling in $(\\mathbf{x}, \\mathbf{z})$-space versus sampling in $(\\mathbf{\\epsilon}_x, \\mathbf{\\epsilon}_z)$-space. We show that sampling in the $(\\mathbf{x}, \\mathbf{z})$-space is mathematically equivalent to sampling in the $(\\mathbf{\\epsilon}_x, \\mathbf{\\epsilon}_z)$-space, if we adjust the LD step size by the variance of each component , and directly sampling from $(\\mathbf{x}, \\mathbf{z})$-space without adjusting the steps size for each variable separately leads to poor sample quality. \n\nWe hope our responses addressed your concerns, and we will be very happy to answer any of your concerns or questions in the remaining rebuttal period. Thank you!\n\nAuthors of submission 562"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5m3SEczOV8L", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper562/Authors|ICLR.cc/2021/Conference/Paper562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment"}}}, {"id": "b5LfQsNZSgo", "original": null, "number": 2, "cdate": 1605563399890, "ddate": null, "tcdate": 1605563399890, "tmdate": 1605564599020, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "PeAZ5Vopb9P", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for providing positive comments on our manuscript. Below we reply to each comment in detail.\n\n* Latent EBM where the energy function is defined on $(\\mathbf{x}, \\mathbf{z})$\n\nThank you for the suggestion. We agree that modeling the energy in the joint $(\\mathbf{x}, \\mathbf{z})$-space is an excellent future extension of our current model. At this point, we choose to let the energy function only take x as input mainly for the motivation that we want to correct the distribution on data space learned by the VAE. We also borrow the motivation from GANs, where the discriminator network operates only on x. \n\n* Long-run MCMC sampling chain\n\nWhen we examine long-run MCMC chains on real datasets, we observe that most chains still stay around the local mode. Other works such as [1] and [2] also report that models trained with short-run MCMC have non-mixing long-run chains. For example, figure 2 in [4] shows that the long-run chains do not exhibit mode traversal regardless of whether the LD in training is convergent or not. We believe that in order to further improve mixing, we need better sampling techniques such as Hamiltonian Monte Carlo (HMC) for both training and test sampling. This would be an interesting direction for future work.\n\n* Compare with sampling in the $(\\mathbf{x}, \\mathbf{z})$-space\n\nThanks for suggesting this experiment. This indeed can show the effectiveness of sampling in the reparameterized space. In the **newly added Appendix B.1**, we first show that sampling in the $(\\mathbf{x}, \\mathbf{z})$-space is mathematically equivalent to sampling in the $(\\epsilon_x, \\epsilon_z)$-space, if we adjust the LD step size by the variance of each component of $\\mathbf{x}$ and $\\mathbf{z}$. However, this is not easy to implement because in VAE\u2019s reparametrization, each latent variable and pixel component has different variance. More importantly, our latent variable $\\mathbf{z}$ in the prior follows block-wise auto-regressive Gaussian distributions, so the variance of each component $\\mathbf{z_i}$ depends on the value of $\\mathbf{z}_{<i}$. We foresee that because of this dependency, using a fixed step size per component of $\\mathbf{z}$ will not be effective, even when it is set differently for each component. Working in $(\\epsilon_x, \\epsilon_z)$-space avoids such dependency. In this appendix, we empirically show that directly sampling from $(\\mathbf{x}, \\mathbf{z})$ without adjusting the steps size for each variable separately leads to poor sample quality. We hope this additional study demonstrates the benefits of sampling from the reparametrized distribution.\n\n* Possible mode dropping issues\n\nWe thank the reviewer for pointing this out. It is an interesting observation that we did not notice before. We agree that the diversity among dog samples seems to be limited. Interestingly, we find such a phenomenon is also observed in other recent works such as [3][4] which obtains impressive FID scores. In [3]\u2019s Figure 18 and [4]\u2019s Figure 11, we observe that most of the generated dog images contain white front faces. One reason might be that the dataset is slightly biased towards this pattern. While our model tends to cover all the modes of the data distribution, as indicated by the quantitative results on StackedMNIST in table 5, overlapped train/test likelihood histogram in Figure 6 and the diversity of generated images on CelebA and LSUN, we admit that there might be imbalance on the weights assigned to the different modes. This imbalance may be more pronounced for challenging tasks such as modeling CIFAR10. On such a challenging dataset, unconditional generative models may not have enough capacity to model each mode exactly as the true distribution, and it is an interesting extension to use a conditional model so that modes within each class can be modeled more faithfully. Another possible explanation is that we use short-run MCMC to obtain samples, so theoretically speaking what we obtained are not exact samples from our model. The effect of imbalanced modes may be exacerbated by the inaccurate sampling (the sampling scheme may tend to always ignore some modes with lower density). We believe that this issue is an interesting direction for future research.\n\nThank you again and we will be very happy to answer any of your concerns or questions in the remaining rebuttal period.\n\n[1] On learning non-convergent short-run mcmc toward energy-based model. Nijkamp et al. https://arxiv.org/abs/1904.09770\n\n[2] On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based Models Nijkamp et al. https://arxiv.org/abs/1903.12370\n\n[3] Denoising Diffusion Probabilistic Models. Ho et al. https://arxiv.org/abs/2006.11239\n\n[4] Learning Energy-Based Models by Diffusion Recovery Likelihood https://openreview.net/forum?id=v_1Soh8QUNc\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5m3SEczOV8L", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper562/Authors|ICLR.cc/2021/Conference/Paper562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment"}}}, {"id": "PwbbIh-CNOj", "original": null, "number": 3, "cdate": 1605563878673, "ddate": null, "tcdate": 1605563878673, "tmdate": 1605564585632, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "mbeMO6foEWF", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for providing positive comments on our manuscript. Below, we address the concerns in detail.\n\n* Evaluating the overall likelihood of the trained model\n\nWe thank the reviewer for suggesting likelihood estimation on a 2-d toy distribution. We think it is a very good idea, as we can estimate the partition function by numerical integration. In contrast, estimating normalized likelihood for EBMs is challenging when the data dimension is high. Techniques like annealed importance sampling for estimating the partition function are computationally expensive and inaccurate. Therefore, we run a simple likelihood estimation experiment on the widely used 25 Gaussian Mixtures data set, and we present this experiment in **section 5.4 in the revised draft**. Results show that our model obtains better log likelihood (-1.5 nats) than the VAE (-2.9 nats), and it is close to the likelihood under the true distribution (-1.1 nats). Please refer to section 5.4 of our revised draft for more details.\n\n* The histogram of likelihoods of data points\n\nWe plot the histogram of likelihoods to show that the likelihoods of train and test data are similar, suggesting that our model generalizes well and does not overfit the training data.\n\n* More Langevin sampling steps are applied to the model\n\nWhen we examine long-run MCMC chains on real datasets, we observe that most chains still stay around the local mode. Other work such as [1] and [2] also reports that models trained with short-run MCMC have non-mixing long-run chains. For example, figure 2 in [2] shows that the long-run chains do not exhibit mode traversal regardless of whether the LD in training is convergent or not. We believe that in order to improve mixing further, we need better sampling techniques such as Hamiltonian Monte Carlo (HMC) for both training and test sampling. This would be an interesting direction for future work.\n\n* Sampling from only the trained energy model without VAE\n\nWe think directly sampling from the energy model will not produce good samples.  In our model, the energy-based component is used to correct and modify the density of the distribution learned by the VAE. It can be thought of as learning the residual between the VAE\u2019s distribution and true data distribution. Therefore, the energy function itself does not represent a meaningful distribution on x and we cannot sample from it without the VAE.\n\n* Composing our models with other models\n\nWe thank the reviewer for the suggestion. Indeed, one of the main advantages of EBMs is their flexibility to compose with other models. We believe it is an interesting future direction to explore the composition of our model with other models for different tasks.\n\nThank you again and we will be very happy to answer any of your concerns or questions in the remaining rebuttal period.\n\n[1] On learning non-convergent short-run mcmc toward energy-based model. Nijkamp et al.  https://arxiv.org/abs/1904.09770\n\n[2] On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based Models Nijkamp et al. https://arxiv.org/abs/1903.12370\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5m3SEczOV8L", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper562/Authors|ICLR.cc/2021/Conference/Paper562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment"}}}, {"id": "aC9zknzPJ3G", "original": null, "number": 4, "cdate": 1605564177867, "ddate": null, "tcdate": 1605564177867, "tmdate": 1605564558423, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "SqmBjHN2yWi", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We would like to thank the reviewer for providing positive feedback. We address the concerns regarding the discrepancy between our energy networks and the networks used in [1].\n\nOn CIFAR10 where we compare our results with [1], we use the energy network with exactly the same backbone structure (i.e, number of residual blocks, number of channels in each block) as [1]. In particular, [1] also uses resblocks and therefore we do not replace CNN with resblocks. This suggests that our networks have the same number of parameters as those in [1].\n\n[1] reported results using LeakyReLU activations. However,  in their Appendix A.12, they said \u201cwe found that using either ReLU, LeakyReLU, or Swish activation in EBMs lead to good performance. The Swish activation in particular adds a noticeable boost to training stability.\u201d Our observation is similar: in our model, LeakyReLU and Swish both work well, and Swish is slightly better in terms of performance and stability. Therefore we use Swish throughout the experiments.\n\nWe indeed find that replacing spectral normalization (SN) with weight normalization (WN) leads to a performance boost. However, this is not applicable to the training of EBMs on data space as in [1]. We tried to train a EBM on data space without SN and with WN, but the training quickly diverged. We conjecture that training VAEBM is more stable than training EBM on data space, and therefore we can drop some strong regularizations such as SN or the 2-norm regularization for the energy that significantly constrain the expressivity of the model. This can be another potential benefit of our method.\n\nIn addition, as shown in table 4, we train an EBM on data space. The FID score is similar to the one reported in [1], which is significantly worse than our model. We want to emphasize that we have tried several different settings to improve this baseline, and therefore we are confident that the large improvement of VAEBM over EBM on data space is not due to network architecture design.\n\nThank you again and we will be very happy to answer any of your concerns or questions in the remaining rebuttal period.\n\n[1] Implicit Generation and Modeling with Energy-Based Models. Yilun Du and Igor Mordatch. https://arxiv.org/abs/1903.08689\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5m3SEczOV8L", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper562/Authors|ICLR.cc/2021/Conference/Paper562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment"}}}, {"id": "4obB5VKQaEp", "original": null, "number": 5, "cdate": 1605564539163, "ddate": null, "tcdate": 1605564539163, "tmdate": 1605564539163, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "ARQ0Ecwt8ci", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for providing positive feedback. We address the reviewer\u2019s comments and concerns below.\n\n* Regarding the two-stage training\n\nThe two-stage training shares some similarities with coordinate descent algorithms, where the parameters are split into two parts and we update one part of parameters while keeping the other part fixed. We are not optimizing an approximate objective; instead, we are just using an alternative optimization approach. The two-stage optimization may lead to suboptimal solutions (actually we cannot easily guarantee optimal solutions with any optimization algorithm); however, we enjoy huge benefits brought by two-stage training. Per-step updates for the energy network are very expensive due to the MCMC, and pre-training the VAE provides a good initial approximation to the data density, so the energy network that corrects and modifies the density can be trained with a small number of updates (only a few epochs, as stated in Appendix E). \n\n* Exact likelihood estimation\n\nEstimating the exact likelihood of EBMs is a challenging task. Typically, the estimation involves annealed importance sampling (AIS), as done in [1]. This is computationally intense, as [1] reported that it took over 2 days on CIFAR-10. Our model has a large VAE component, so the time needed is even longer. More importantly, AIS will provide a stochastic lower bound on $\\log Z$, and since the log likelihood of EBM involves negative $\\log Z$, it will overestimate the log likelihood. In contrast, estimating the likelihood of the VAE (for example, by computing the IWAE bound) returns a lower bound on log likelihood. This makes the log likelihood estimates for EBM and VAE not comparable (we cannot compare a lower bound with an upper bound). \n\nAlternatively, as suggested by reviewer 2, we conduct a likelihood estimation experiment on a 2-d toy dataset, where we can accurately estimate $\\log Z$ by numerical integration. We use the commonly used 25 Gaussian Mixtures data, and we present this experiment in **section 5.4 in the revised draft**. Results show that our model obtains better log likelihood (-1.5 nats) than the VAE (-2.9 nats), and it is close to the likelihood under the true distribution (-1.1 nats). Please refer to section 5.4 for more details.\n\nThank you again and we will be very happy to answer any of your concerns or questions in the remaining rebuttal period.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5m3SEczOV8L", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper562/Authors|ICLR.cc/2021/Conference/Paper562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Comment"}}}, {"id": "SqmBjHN2yWi", "original": null, "number": 2, "cdate": 1603848381757, "ddate": null, "tcdate": 1603848381757, "tmdate": 1605024659587, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Review", "content": {"title": "Review", "review": "Pros:\nthis method proposed to use VAE+EBM for generative modelling. Unlike other VAE+GAN/EBM-liked model, it added a EBM after VAE.\nOverall method is easy to understand and follow.\nTo accelerate the training, the authors also applied a buffer to store the previous examples for easy sampling.\n\nCons:\nIn the experiment, the authors compared other models with VAEBM, it is reasonable to compare the results with reported scores in other works, however, since the architecture is a fairly important factor (such that swish instead relu, resblock instead of cnn, weight norm instead of spectral norm), etc, is it also reasonable that the improvement is partially contributed by such design of architecture.\nSo I will suggest that the authors should use the same architecture design (choose other one or two models for all tasks), and test whether the proposed method can actually gain that much of improvement.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140360, "tmdate": 1606915800055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Review"}}}, {"id": "PeAZ5Vopb9P", "original": null, "number": 4, "cdate": 1604016856373, "ddate": null, "tcdate": 1604016856373, "tmdate": 1605024659450, "tddate": null, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "invitation": "ICLR.cc/2021/Conference/Paper562/-/Official_Review", "content": {"title": "Interesting paper that learns EBM as a correction of VAE.", "review": "This paper proposes a model that corrects VAE by an energy-based model defined on image space. The model is learned in two phase. The first phase learns the VAE model, while the second phase learns the EBM correction term by MLE. Experimental results show that the proposed method outperforms pure EBM defined on image space and also pure VAE models by large margins. \n\n- pros: the paper is clear written and easy to follow. The ablation study shows clearly the advantage over baseline methods. Sampling from EBM on image space is hard. With VAE as a backbone, the sampling can be transferred to the latent space and the residual \\epsilon in the image space, which is much more friendly to MCMC sampling. \n\n- cons:\n1. The energy term is used to correct only on image space. Would be interesting to see if VAE can be corrected by a latent EBM where the energy function is defined on (x, z). \n2. After learning, would long-run MCMC sampling chain remain stable and mix well? It would be interesting to diagnose the long run chain behavior, and compare the difference of sampling in the space (\\epsilon_x, \\epsilon_z) and (x, z). \n3. For the synthesized results of CIFAR-10, it seems that some patterns appear repeatedly (e.g., the white dog face). Is the model suffered from mode collapsing problem? \n\nOverall, it is a good submission that proposes a principled method to combine VAE and EBM and demonstrates strong empirical results. I tend to accept this paper.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper562/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper562/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models", "authorids": ["~Zhisheng_Xiao1", "~Karsten_Kreis1", "~Jan_Kautz1", "~Arash_Vahdat3"], "authors": ["Zhisheng Xiao", "Karsten Kreis", "Jan Kautz", "Arash Vahdat"], "keywords": ["Energy-based Models", "Variational Auto-encoder", "MCMC"], "abstract": "Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256$\\times$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "xiao|vaebm_a_symbiosis_between_variational_autoencoders_and_energybased_models", "one-sentence_summary": "We introduce an energy-based generative model where the data distribution is defined jointly by a VAE and an energy network.", "pdf": "/pdf/e45436941ac36b0258992469d1932909c6cbed5e.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nxiao2021vaebm,\ntitle={{\\{}VAEBM{\\}}: A Symbiosis between Variational Autoencoders and Energy-based Models},\nauthor={Zhisheng Xiao and Karsten Kreis and Jan Kautz and Arash Vahdat},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5m3SEczOV8L}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5m3SEczOV8L", "replyto": "5m3SEczOV8L", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538140360, "tmdate": 1606915800055, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper562/-/Official_Review"}}}], "count": 11}