{"notes": [{"id": "HkxCcJHtPr", "original": "H1xlk2COvr", "number": 1897, "cdate": 1569439637936, "ddate": null, "tcdate": 1569439637936, "tmdate": 1577168257214, "tddate": null, "forum": "HkxCcJHtPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6CYya7dyCj", "original": null, "number": 1, "cdate": 1576798735331, "ddate": null, "tcdate": 1576798735331, "tmdate": 1576800901045, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "HkxCcJHtPr", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Decision", "content": {"decision": "Reject", "comment": "This work propose a compression-aware training (CAT) method to allows efficient compression of  feature maps during inference. I read the paper myself. The proposed method is quite straightforward and looks incremental compared with existing approaches based on entropy regularization. \n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HkxCcJHtPr", "replyto": "HkxCcJHtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724269, "tmdate": 1576800275885, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Decision"}}}, {"id": "rklhGYNC5B", "original": null, "number": 2, "cdate": 1572911380146, "ddate": null, "tcdate": 1572911380146, "tmdate": 1573452884085, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "HkxCcJHtPr", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "The format of the paper does not meet the requirement of ICLR. Due to this, I will give a 3. I suggest the authors to change it as soon as possible.\n\nBesides that, the main idea of the paper is to regularize the training of a neural network to reduce the entropy of its activations. There are extensive experiments in the paper.\n\nThe paper introduce two kinds of method to regularize the entropy. The first method is a soft version of the original entropy, and the second is the compressibility loss. After adding the regularization, the performance drop of the compressed network is reduced. The experiment performance is promising.\n\nI think the method is straightforward and reasonable with only a few questions:\n1. Why do you quantize the weight? Seems it's not necessary because the paper only address activation quantization.\n2. What will happen if the weights are quantized to lower bits? For example, 4bit?\n2. How about adding the regularization to weights?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1897/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1897/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxCcJHtPr", "replyto": "HkxCcJHtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575692573770, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1897/Reviewers"], "noninvitees": [], "tcdate": 1570237730716, "tmdate": 1575692573786, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Official_Review"}}}, {"id": "BJgsBhO8oB", "original": null, "number": 5, "cdate": 1573452866815, "ddate": null, "tcdate": 1573452866815, "tmdate": 1573452866815, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "rJxtzQieiB", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment", "content": {"title": "Thank You for Your Response", "comment": "I appreciate the quick response, and I find them reasonable enough. I will change the score."}, "signatures": ["ICLR.cc/2020/Conference/Paper1897/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1897/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxCcJHtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1897/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1897/Authors|ICLR.cc/2020/Conference/Paper1897/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149294, "tmdate": 1576860544451, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment"}}}, {"id": "B1x25mjeiS", "original": null, "number": 4, "cdate": 1573069716237, "ddate": null, "tcdate": 1573069716237, "tmdate": 1573069716237, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "rJghghXPtr", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment", "content": {"title": "Answer to reviewer #1", "comment": "Thank you for your review and your comments. We would like to provide an answer to the issues raised in your review.\n\nQ: The authors could have applied CAT to other tasks such as Image Detection while proving inference times on CPUs. Indeed, it is unclear to me what would be the influence of the entropic decoder which is claimed to be fast for \"efficient implementations\" by the authors.\nA: We have applied the CAT to object detection task (SSD512, Table 3). As we have written in the answer to Reviewer #3, even for naive implementation of Huffman coding, the overhead for GPU-running code is approx. 3-4%. When considering dedicated hardware implementations, the overhead can be made negligible.\n \nQ: The idea of regularizing by the entropy is not novel (see for instance \"Entropic Regularization\", Grandvalet et al.), as well as the idea of further encoding the weights using entropic coders (as in \"Deep Compression\", Han et al.).\nA: The idea of using entropy regularization, in general, is indeed not novel and we mention relevant works in the Related Work section. However, using a differentiable entropy approximation as a loss term to improve the compressibility of the activations is novel."}, "signatures": ["ICLR.cc/2020/Conference/Paper1897/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxCcJHtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1897/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1897/Authors|ICLR.cc/2020/Conference/Paper1897/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149294, "tmdate": 1576860544451, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment"}}}, {"id": "rJxtzQieiB", "original": null, "number": 3, "cdate": 1573069584773, "ddate": null, "tcdate": 1573069584773, "tmdate": 1573069584773, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "rklhGYNC5B", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment", "content": {"title": "Answer to reviewer #4", "comment": "Once again, thank you for your review and your comments. We will answer the questions in the review. \n\nQ: Why do you quantize the weight? Seems it's not necessary because the paper only addresses activation quantization. \nA: While our paper focuses on activation quantization, one of the goals of our research was an actual hardware implementation of the neural network, which is significantly more efficient when the weights are quantized. We, therefore, decided to demonstrate the combined effect of weight and activation quantization; however, in our experiments, we also show limit cases of 32-bit weights which do not impact the general picture. \n\n\nQ: What will happen if the weights are quantized to lower bits? For example, 4bit? \nA: We believe that our method can be used along with methods of weight quantization only minor impact of the results. However, adding a complicated weight quantization method to CAT would require more resources without contributing much to the analysis of CAT. Thus, we have chosen a simple quantization method that does not achieve baseline accuracy at lower bits. \n\nQ: How about adding the regularization to weights?\nA: Our method could also be applied to the weights. In this paper, we focus on activations, since the benefit of the lossless compression activation is more significant due to the fact they are usually responsible for most of the memory accesses.\nIn addition, the paper by Aytekin et al. proposed compressibility loss and has successfully applied it to the weights.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1897/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxCcJHtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1897/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1897/Authors|ICLR.cc/2020/Conference/Paper1897/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149294, "tmdate": 1576860544451, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment"}}}, {"id": "HkgO4zslsS", "original": null, "number": 2, "cdate": 1573069360021, "ddate": null, "tcdate": 1573069360021, "tmdate": 1573069360021, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "BklsH3PCcr", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment", "content": {"title": "Answer to reviewer #3", "comment": "Thank you for your review and your comments. We fixed the wrong reference format and switched the equations as proposed. We also would like to provide an answer to the issues raised in your review.\n\nQ: I am interested in learning the time taken by the proposed method during the inference procedure vs other related methods.\nA: In inference time, our method requires only applying an entropy encoder, for ResNet-18 we measured a 3-4% overhead for building a Huffman tree with our naive implementation. More efficient implementations would probably make the difference totally negligible. We can further reduce the overhead by fixing the tree which will make it even faster. Mapping the values is in fact as fast as fetching them from the main memory. We believe, lower level implementation should yield latency improvements due to faster I/O with main memory.\n\nQ:  Whether the accuracy will be affected by other differentiable entropy approximations?\nA: Entropy approximation does not affect accuracy directly. The accuracy decrease is a result of an additional loss term. By changing the relative weight of the entropy term, we can reach different accuracy/compression ratios, as shown in Fig. 3. However, for the same model accuracy, the better is the entropy approximation, the better compression will be attained.\n\nQ:  what is the impact on accuracy if only part of the batch is considered.\nA: Since there is only minor difference between the two methods (soft entropy based only on part of the tensor and compressibility loss which is calculated on the whole tensor), we believe there is no significant impact of using only part of the batch. Since the tensors are large, the amount of values used for approximation is still relatively big. To check this assumption, we ran a soft entropy evaluation on a single tensor and compared it to real values, and added the results to Appendix B.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1897/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxCcJHtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1897/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1897/Authors|ICLR.cc/2020/Conference/Paper1897/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149294, "tmdate": 1576860544451, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment"}}}, {"id": "rJgdsSVysS", "original": null, "number": 1, "cdate": 1572976031806, "ddate": null, "tcdate": 1572976031806, "tmdate": 1572976031806, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "rklhGYNC5B", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment", "content": {"title": "Format fix", "comment": "Thank you for your review!\n\nAs proposed, we have updated the pdf with the right format. For some reason, one of TeX packages interfered with it. We are going to address other points of your review later."}, "signatures": ["ICLR.cc/2020/Conference/Paper1897/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HkxCcJHtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1897/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1897/Authors|ICLR.cc/2020/Conference/Paper1897/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149294, "tmdate": 1576860544451, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1897/Authors", "ICLR.cc/2020/Conference/Paper1897/Reviewers", "ICLR.cc/2020/Conference/Paper1897/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Official_Comment"}}}, {"id": "rJghghXPtr", "original": null, "number": 1, "cdate": 1571400691803, "ddate": null, "tcdate": 1571400691803, "tmdate": 1572972409439, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "HkxCcJHtPr", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThe authors propose a method for training easy-to-quantize models that are quantized after training (post-training quantization). They do so by regularizing by the entropy, thereby forcing the weight distribution to be more compressible. They further compress the weights using entropy coding.\n\nStrengths of the paper:\n- The paper presents strong experimental results on ResNet, SqueezeNet VGG and MobileNet architectures and provides the code, which looks sensible. \n\nWeaknesses of the paper:\n- The authors could have applied CAT to other tasks such as Image Detection, while proving inference times on CPUs. Indeed, it is unclear to me what would be the influence of the entropic decoder which is claimed to be fast for \"efficient implementations\" by the authors.\n- The idea of regularizing by the entropy is not novel (see for instance \"Entropic Regularization\", Grandvalet et a.l),  as well as the idea of further encoding the weights using entropic coders (as in \"Deep Compression\", Han et al.).\n\nJustification of rating:\nThe authors present an intuitive method (yet not novel) for quantizing the weights of a neural network. My main concern would be about the inference time but I consider that the experimental results suggest strong evidence that CAT performs well on a wide variety of architectures. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1897/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1897/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxCcJHtPr", "replyto": "HkxCcJHtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575692573770, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1897/Reviewers"], "noninvitees": [], "tcdate": 1570237730716, "tmdate": 1575692573786, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Official_Review"}}}, {"id": "BklsH3PCcr", "original": null, "number": 3, "cdate": 1572924483439, "ddate": null, "tcdate": 1572924483439, "tmdate": 1572972409357, "tddate": null, "forum": "HkxCcJHtPr", "replyto": "HkxCcJHtPr", "invitation": "ICLR.cc/2020/Conference/Paper1897/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this article, the authors propose a compression-aware method to achieve efficient compression of feature maps during the inference procedure. When in the training step, the authors introduce a regularization term to the target loss that optimizes the entropy of the activation values. At inference time, an entropy encoding method is applied to compress the activations before writing them into memory. The experimental results indicate that the proposed method achieves better compression than other methods while holding accuracy.\nThere are still some issues as follows:\n1.\tThe authors should carefully check the format of the references in the whole article. For example, in section 2, line 5 from the top and line 8 from the bottom, \u201cXiao et al. (2017), Xing et al. (2019)\u201d and \u201c(Chmiel et al., 2019)\u201d are in the wrong format.\n2.\tIt is suggested that the authors swap the order of formulation (8) and (9) in section 3.2 so that it will be a good correlation with the formulation (3) and (4).\n3.\tI am interested in learning the time taken by the proposed method during the inference procedure vs other related methods.\n4.\tThe authors studied two differentiable entropy approximation in the paper, and they stated that they calculate soft entropy only on the part of the batch for the reduction of both memory requirements and time complexity in training. I hope the authors will clarify 1) Whether the accuracy will be affected by other differentiable entropy approximations; 2) what is the impact on accuracy if only part of the batch is considered.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1897/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1897/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "CAT: Compression-Aware Training for bandwidth reduction", "authors": ["Chaim Baskin", "Brian Chmiel", "Evgenii Zheltonozhskii", "Ron Banner", "Alex M. Bronstein", "Avi Mendelson"], "authorids": ["chaimbaskin@cs.technion.ac.il", "brian.chmiel@intel.com", "evgeniizh@campus.technion.ac.il", "ron.banner@intel.com", "bron@cs.technion.ac.il", "avi.mendelson@cs.technion.ac.il"], "keywords": ["compression", "quantization", "efficient inference", "memory bandwidth", "entropy", "compression-aware training", "Huffman", "variable length coding"], "TL;DR": "Adding an entropy-reducing regularization to the loss to improve activation compression in inference time reducing memory bandwidth.", "abstract": "Convolutional neural networks (CNNs) have become the dominant neural network architecture for solving visual processing tasks. One of the major obstacles hindering the ubiquitous use of CNNs for inference is their relatively high memory bandwidth requirements, which can be a main energy consumer and throughput bottleneck in hardware accelerators. Accordingly, an efficient feature map compression method can result in substantial performance gains. Inspired by quantization-aware training approaches, we propose a compression-aware training (CAT) method that involves training the model in a way that allows better compression of feature maps during inference. Our method trains the model to achieve low-entropy feature maps, which enables efficient compression at inference time using classical transform coding methods. CAT significantly improves the state-of-the-art results reported for quantization. For example, on ResNet-34 we achieve 73.1% accuracy (0.2% degradation from the baseline)  with an average representation of only 1.79 bits per value. Reference implementation accompanies the paper.", "pdf": "/pdf/f15449000c32b6967c2ac36f53ba21b0f376527c.pdf", "code": "https://github.com/CAT-teams/CAT", "paperhash": "baskin|cat_compressionaware_training_for_bandwidth_reduction", "original_pdf": "/attachment/62be024fad0b838095b6a214ddab0e1fad77a887.pdf", "_bibtex": "@misc{\nbaskin2020cat,\ntitle={{\\{}CAT{\\}}: Compression-Aware Training for bandwidth reduction},\nauthor={Chaim Baskin and Brian Chmiel and Evgenii Zheltonozhskii and Ron Banner and Alex M. Bronstein and Avi Mendelson},\nyear={2020},\nurl={https://openreview.net/forum?id=HkxCcJHtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HkxCcJHtPr", "replyto": "HkxCcJHtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1897/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575692573770, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1897/Reviewers"], "noninvitees": [], "tcdate": 1570237730716, "tmdate": 1575692573786, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1897/-/Official_Review"}}}], "count": 10}