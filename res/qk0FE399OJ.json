{"notes": [{"id": "qk0FE399OJ", "original": "YZVXSNL8BqC", "number": 1099, "cdate": 1601308123613, "ddate": null, "tcdate": 1601308123613, "tmdate": 1614985661819, "tddate": null, "forum": "qk0FE399OJ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "x6bcFYVvHh", "original": null, "number": 1, "cdate": 1610040500084, "ddate": null, "tcdate": 1610040500084, "tmdate": 1610474106704, "tddate": null, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper analyzes the behavior of random search-based NAS and provided new insights (e.g., a low ranking correlation among top-20% candidate architectures in the search phase). An extensive set of experiments were also conducted. However, most reviewers found the incremental nature and similarity with previous works to be a concern. I would encourage the authors to better position their work and better explain the novel methodological aspects.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040500069, "tmdate": 1610474106688, "id": "ICLR.cc/2021/Conference/Paper1099/-/Decision"}}}, {"id": "pGpwF8Q7FAe", "original": null, "number": 3, "cdate": 1605559002128, "ddate": null, "tcdate": 1605559002128, "tmdate": 1605660996577, "tddate": null, "forum": "qk0FE399OJ", "replyto": "WTd3nt-wddt", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Official_Comment", "content": {"title": "Reply to Reviewer#1", "comment": "We thank the reviewer for the very detailed and useful comments. Below we provide responses to each concern.\nQ1: May missing study with recent NAS methods?\nWe sincerely thank you again for your concern. Also, we\u2019d like to humbly point out that, in the work, we did explore NASBench-201, DARTS search space (CIFAR-10 and CIFAR-100),  4 DARTS sub search space (CIFAR-10, CIFAR-100, and SVHN), and DARTS NLP search space with EPS. We choose RandomNAS as the baseline and compare our algorithm with several recent NAS on the above 16 cases. EPS achieves state-of-the-art results on most of them. The results are shown in Table 2-6.\n\nQ2: The details of the algorithm of EPS are unclear. For example, it's not clear what exactly is being done in the RandomInitArch and Mutate operations. Also, what is the reason for using sample_set instead of population queue?\n1. RandomInitArch is to randomly, uniformly sample architecture from the global search space.\nThank you for pointing out the unclear definition, we made a definition of the item in Section 3 in the revised version.\n2. Mutate operations: \nFor NASBench-201, the rule for mutation is: The current operation on the edge has a fixed probability to be mutated to a new operation(including itself) on the same edge. \nFor the DARTS search space family, the rules for mutation are 1. The current operation on the edge has a fixed probability to be mutated to a new operation(including itself) on the same edge. 2. If a node has unconnected predecessors, one of its edges has a fixed probability to switch to an edge linked to the unconnected predecessors, and the operation on the new edge will be chosen randomly.\nWe have the definition of the mutate operation in Appendix B. \nThank you again for pointing out the vague definition, we put the reference in Section 3 in the revised version and our open-sourced code can also help better understand and reduplicate the work.\n3. The notion of the sample set comes from the tournament selection. Using a small sample set can improve validation efficiency. Also, we run different settings with sample set size in {32,64} and population size in {64,128,256} on NASBench-201 for the detailed study. The results are shown in Table 14.\n\nQ3: The definitions of population and the proxy search space.\nproxy search space(PS) is a subset with a certain number of architectures sampled from the global search space(GS). We have the definition of PS in the introduction and we re-define it cleanly in Section 2 in the revised version for better understanding. Thank you again for pointing it out.\nThe population is initialized with P architectures uniformly sampled from the global search space. So it\u2019s a proxy search space.\nQ4: In section 2, the authors give an analysis of random search-based NAS, but does this hold for other conditions such as for the search space used in DARTS and Robust DARTS?\nSince DARTS search space contains 10^18 architectures which is hard for the full training for each architecture from the scratch, we provide several pieces of evidence support that the assumption holds for the DARTS and Robust DARTS:\n1.  In the paper, we did an experiment that randomly sampled 27 architectures from the last validation interval (after 80,000 training iterations) for EPS in the DARTS search space. Then each architecture is trained three times from scratch and the mean of the three accuracies are considered as the ground truth. We show the Spearman\u2019s \u03c1 between the ranking of ground truth and EPS\u2019 prediction, the \u03c1 between the ranking of ground truth and RandomNAS\u2019s prediction in Figure 4. We observed that the architectures sampled from the final proxy search space surpass the random sampling baseline by a large margin (the average accuracy of samples 95.54% vs. 95.15%).  Moreover, EPS delivers 0.68 Spearman\u2019s \u03c1 while RandomNAS performs worse (0.41) at distinguishing the difference between them. The observation supports the assumption that the final proxy search space consists of good architectures and EPS can deliver a higher correlation compared with the RandomNAS in GS.\n2. The extensive experimental results show that EPS can find better architectures than RandomNAS on both DARTS and DARTS sub search space (please see Table 3-6).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1099/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper1099/Reviewers", "ICLR.cc/2021/Conference/Paper1099/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qk0FE399OJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1099/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1099/Authors|ICLR.cc/2021/Conference/Paper1099/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863702, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1099/-/Official_Comment"}}}, {"id": "wGRMMggRThJ", "original": null, "number": 4, "cdate": 1605559107767, "ddate": null, "tcdate": 1605559107767, "tmdate": 1605660986360, "tddate": null, "forum": "qk0FE399OJ", "replyto": "GXoIMZKdE4-", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Official_Comment", "content": {"title": "Reply to Reviwer#3", "comment": "Thank you very much for your very detailed and very useful review. We summarize your questions and hope our answers can address your concerns.\n\nQ1. The incremental novelty.\nThe difference between CARS and EPS is as you mentioned we have a very solid motivation from the observations of RandomNAS. Also, we pointed out that the aging mechanism is critical for the EPS in Table 1, while CARS doesn\u2019t use the aging mechanism. On the other hand, our proposed size regularization is a simple yet effective way for the improvement of RandomNAS ranking correlation. Besides the proposed EPS, our observation of the RandomNAS drawbacks equally contribute to the paper\u2019s novelty. To the best of our knowledge, we are the first to do the detailed study for RandomNAS and both the observations and the new methods can help the AutoML community for further exploration.\n\nQ2. The intuition of the solutions\nWe\u2019d like to appreciate your concerns. Here we use simple words to re-describe the two observations we present in Section 2:\n1. The original RandomNAS trained on the global search space is hard to get a correct ranking for a set of \u201cgood\u201d architectures. \n2. The original RandomNAS tends to take architectures with smaller sizes as the better ones.\nWe hope the above descriptions can help you better understand the overall Section 2. And we welcome the comments if you have new advice.\n\nQ3. Q_pop\nQ_pop is the population of the EPS. Utilizing the aging mechanism, Q_pop is considered as a queue with architectures first in first out.\n\nQ4.  The proxy search space\nproxy search space(PS) is a subset with a certain number of architectures sampled from the global search space(GS)\n\n\nQ5. How to choose the 4000 architectures from RandomNAS?\nThe 4000 architectures are randomly sampled from the global search space. The overall 25.6% portion is representative for the global search space.\n\nQ6. Batch validation vs. full validation\nFirst, all the experiments in Section 2 use full validation. To address the concern of the gap between batch validation and full validation, we calculate the best ranking correlation achieved using the batch validation using RandomNAS. The full validation ranking correlation is 0.78 and the batch validation correlation is 0.74. Since the full validation on 102400 architectures (80000(iteration)/50(interval)*64(validated architectures)) requires 80+ GPU hrs. For the EPS search algorithm, we use the batch validation for a trade-off between the correlation and the speed. \n\n\nQ8. Results on PTB\n1. In another work (Zhang et al. 2020) they reduplicated the RandomNAS and only achieves 59.7 valid perplexity. \n2. Also, we followed the RandomNAS work to do the 4 runs comparison of 300 epochs training and summarize the results as follow:\n\n|Method|run1|run2|run3|run4|Avg.|\n|----------|------|------|------|------|-----------|\n|DARTS|67.3| 66.3| 63.4| 63.4|65.1|\n|RSPS|66.3| 64.6 | 64.1 | 63.8 |  64.7|\n|EPS|65.10|63.74*|64.72|64.77| 64.58|\n\nIt shows that the average perplexity of architectures found by EPS surpass both DARTS and RSPS. We added it to the appendix in the revised version.\nQ9. Some typos\nThank you for pointing out, we fixed it in the revised version."}, "signatures": ["ICLR.cc/2021/Conference/Paper1099/Authors"], "readers": ["ICLR.cc/2021/Conference/Paper1099/Reviewers", "ICLR.cc/2021/Conference/Paper1099/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qk0FE399OJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1099/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1099/Authors|ICLR.cc/2021/Conference/Paper1099/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863702, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1099/-/Official_Comment"}}}, {"id": "lH0Mb9yTpkA", "original": null, "number": 5, "cdate": 1605559195082, "ddate": null, "tcdate": 1605559195082, "tmdate": 1605660979316, "tddate": null, "forum": "qk0FE399OJ", "replyto": "br1_AZschfU", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Official_Comment", "content": {"title": "Reply to Reviewer#2", "comment": "Thank you very much for your very detailed and very useful review. We summarize your questions and reply to you:\nQ1. Experiment on large dataset\nWe admit that we have not run our models on a larger dataset like ImageNet yet. The concentration of our work is to study the drawbacks of RandomNAS and use the EPS to overcome them. Hence, we choose a direction to study several computationally lite cases that are friendly for repetition and reproduction. Multiple repeats can be done for each experiment, and we can see the theoretic performance gain by different NAS algorithms rather than the engineering gains on ImageNet. In our work, we explored NASBench-201, DARTS search space (CIFAR-10 and CIFAR-100),  4 DARTS sub search space (CIFAR-10, CIFAR-100, and SVHN), and DARTS NLP search space with EPS. The total 16 cases help us further study the RandomNAS and support the EPS can find good architectures in various computer vision tasks and even the NLP task. Since our work is reproducible and easy to follow, we open-sourced our code for the community for future study.\nQ2. We summarize the statistic of NASBench-201 (CIFAR-10) in the table below:\n\n|                    |Validation  acc.\t                  |Test acc.                                 |\n\n|-----------------|---------|---------|---------|---------|---------|---------|---------|---------|\n\n|                    |Lowest|Highest|Median|Mean||Lowest|Highest|Median|Mean|\n\n|-----------------|---------|---------|---------|---------|---------|---------|---------|---------|\n\n|Top 100%    |9.71     |91.61  |87.27   |83.65  |10.00  |94.37  |90.71  |87.04  |\n\n|Top 60%      |86.31   |91.61  |88.71   |88.58  |89.52   |94.37  |92.02  |91.85  |\n\n|Top 20%      |89.13   |91.61  |89.64   |89.80  |92.36   |94.37  |92.93  |92.98  |\n\nAlso, we plotted the histogram in the link below for better understanding:\nhttps://drive.google.com/file/d/16m9USGB58-gS4TA2FjGGHPUEdjw3beTG/view?usp=sharing\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1099/Authors"], "readers": ["ICLR.cc/2021/Conference/Paper1099/Reviewers", "ICLR.cc/2021/Conference/Paper1099/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qk0FE399OJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1099/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1099/Authors|ICLR.cc/2021/Conference/Paper1099/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863702, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1099/-/Official_Comment"}}}, {"id": "tSSzPSP5ZR", "original": null, "number": 2, "cdate": 1605558841213, "ddate": null, "tcdate": 1605558841213, "tmdate": 1605660958765, "tddate": null, "forum": "qk0FE399OJ", "replyto": "0AZt0pjH9uJ", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Official_Comment", "content": {"title": "Reply to Reviewer#4", "comment": "Thank you very much for your very detailed and useful review. We summarize your questions and reply to you:\nQ1: The similarity between the EPS and the aging evolution in Real et al.\nThe motivation for using the aging mechanism is different from the Real et al. Here's the reason: 1) in a weight-sharing supernet, a young architecture may have a higher loss and be removed before well trained.  2) an old architecture survives in the population when it performs well in the early stage and produces many mutations, which may dominate the population and mislead the search direction. We compared EPS with the one w/o aging in Table 1, which indicates that the aging mechanism is necessary for the EPS. Another difference is Real et al. (2019) trained every visited architecture from scratch while EPS utilizes the weight-sharing supernet and leads to a search time similar to one architecture's training time. Hence, Real et al. take more than 9,000X consumption than ours.\nBesides the proposed EPS, we are the first to look into the details of the RandomNAS and our observation of the RandomNAS drawbacks equally contribute to the paper\u2019s novelty. Also, our proposed size regularization is a simple yet effective way for the improvement of RandomNAS ranking correlation.\nQ2: Can other NAS algorithms utilize the proxy search space?\nTo address the question that if other NAS algorithms (e.g., NAS in a progressive way) can help build a better proxy search space than the EPS, we choose two alternative algorithms in the NASBench-201 for a detailed comparison:\n1. Progressive search: Similar to the work Progressive DARTS[1], we start with a supernet with N = 1. (N is defined in Figure 1 in the NAS-BENCH-201 paper, the number of searchable cells are 3N.) And we gradually increase N by 1 every 15000 iterations as the interval until N = 5 (default setting in the benchmark). Other settings are adopted from EPS.\n2. MLP for the exploration: Instead of using the criterion of mutating the k architectures with the lowest loss, we introduce an MLP to learn the loss of architectures in the population and predict other architectures' performance. \nThe table below shows the five-run experiment results in NASBench-201. Original EPS outperforms the other two methods on CIFAR-10. Since Progressive EPS speeds up ~1.5x, we considered it as a good trade-off between the performance and the speed.\n\n|                   |Val acc.|Test acc.|\n|----------------|----------|------------|\n|EPS            |91.50   |94.34     |\n|Progressive|91.33   |94.00     |\n|MLP            |89.17  |92.57     |\n\nWe updated the details of the above 2 algorithms\u2019  implementation in the Appendix and uploaded the source code to the Github repo.\n[1] Chen, Xin, et al. \"Progressive differentiable architecture search: Bridging the depth gap between search and evaluation.\" Proceedings of the IEEE International Conference on Computer Vision. 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1099/Authors"], "readers": ["ICLR.cc/2021/Conference/Paper1099/Reviewers", "ICLR.cc/2021/Conference/Paper1099/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "qk0FE399OJ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1099/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1099/Authors|ICLR.cc/2021/Conference/Paper1099/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863702, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1099/-/Official_Comment"}}}, {"id": "br1_AZschfU", "original": null, "number": 1, "cdate": 1603641549276, "ddate": null, "tcdate": 1603641549276, "tmdate": 1605024531427, "tddate": null, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "Summary:\\\nThis paper finds an assumption in neural architecture search (NAS) where \"performance estimated in a weight-sharing setting translates to actualy achievable performance\" only holds for uniformly sampled architecture but breaks down when sampling from top-performing architectures. As a result a new sampling method based on tournament selection for NAS is proposed.\n-----\n+Strengths\\\n+Novel insights are revealed for NAS work which may be useful for future research.\n+Clear experiments are devised to demonstrate the insights.\n+Strong experiment and ablation results compared to prior NAS approaches.\n-----\n-Concerns\\\n-My biggest question here would be how the best architecture from EPS compare to other NAS-based models that are successfully adopted in a wide range of vision application, such as EFficientNet. It would be interesting to show performance on larger dataset.\n-I am also interested to know the range of performance between best and worst for top 100/60/20% of models to better understand how poorly correlated top architectures in GS actually translate to loss in performance beyond performance of the final architecture.\n-----\nRecommendation\\\nTo my best knowledge, this paper uncovers some interesting insight and show performance improvements. My main concern is the viability of the proposed structure in more practical vision problems such as recognition on larger dataset. My recommendation is leaning towards weak accept.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1099/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1099/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127015, "tmdate": 1606915800129, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1099/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1099/-/Official_Review"}}}, {"id": "GXoIMZKdE4-", "original": null, "number": 2, "cdate": 1603892868301, "ddate": null, "tcdate": 1603892868301, "tmdate": 1605024531356, "tddate": null, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Official_Review", "content": {"title": "Please explain the novelty and motivation clearly", "review": "Motivated by exploring the ranking correlations of the existing RandomNAS in NASBench-201, this paper proposes EPS to improve the search efficiency and keep good ranking correlations by evolving the proxy search space (PS) in RandomNAS. Specially, EPS contains three stages: 1) training the supernet in PS, 2) validating the architectures among the PS and 3) evolving the PS by tournament selection with the aging mechanism. Furthermore, a model-size-based regularization is introduced in the selection stage. Experiments on some popular benchmarks demonstrate the effectiveness of the method.\n\nStrengths\n1) The paper is well written and easy to follow. The algorithm procedure is clearly provided and the code is released.\n2) Some empirical evidences are provided to explain the limitations of the existing RandomNAS.\n\nWeaknesses\n1) Lack of novelty. One the one hand, EPS seems a combination of [1] with a weight-sharing supernet, and the differences 2,3,4 with CARS do not convince me well. Except for the motivation and the fitness in EA, EPS and CARS are highly similar. One the other hand, the intuition of the solutions for the limitations which the authors presented in Sec. 2 is not clearly provided. From my view, Q_pop in Algorithm 1 is same with the population in EA methods. The proxy search space is not clearly explained.\n2) How to choose the 4000 architectures from RandomNAS in Fig. 2(a)?\n3) EPS validates each architecture only on a single batch. Dose one batch validation bring biases to the performance and the ranking evaluation of the architectures? Does the phenomenons in Sec. 2 are caused by one batch validation due to the biases? Comparison with Full batches validation (the whole validation set) should be considered.\n4) Results on PTB are not promising. RandomNAS (RSPS) achieves better perplexity with less search cost.\n5) \u201c32 x 5 runs\u201d and \u201c32 settings\u201d should be \u201c36 x 5 runs\u201d and \u201c36 settings\u201d, respectively.\n\nThe novelty and the similarity with previous works are my main concerns. I am currently leaning towards a negative score but would like to see the authors' responses and other reviewer's comments.\n\n[1] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In AAAI, 2019.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1099/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1099/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127015, "tmdate": 1606915800129, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1099/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1099/-/Official_Review"}}}, {"id": "WTd3nt-wddt", "original": null, "number": 3, "cdate": 1603902703596, "ddate": null, "tcdate": 1603902703596, "tmdate": 1605024531294, "tddate": null, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Official_Review", "content": {"title": "Review of Improving Random-Sampling Neural Architecture Search", "review": "This paper claims that random search-based NAS methods show a low ranking correlation among top-20% candidate architectures in the search phase. To address this issue, this paper proposes to introduce a proxy search space consisting of good architectures and evolve it using evolutionary algorithms. This paper also proposes a simple size regularization to help the NAS algorithm escape from the small architecture traps. The experimental results show that the proposed approach achieves competitive performance with baseline methods.\n\nPros\n- This paper analyzes the behavior in the random search-based NAS in detail and proposes a new strategy based on the observation to tackle the issue.\n\nCons\n- The proposed method should be compared with recent NAS methods to clarify the contribution of this paper. Many related studies are missing.\n- The details of the algorithm of EPS are unclear. For example, it's not clear what exactly is being done in the RandomInitArch and Mutate operations. Also, what is the reason for using sample_set instead of population queue?\n- What do you mean by the population is a proxy search space on page 4? Does the proxy search space mean a specific architecture without over parameterization or an over parameterized architecture? Please elaborate on the definition of the proxy search space and each population.\n- In section 2, the authors give an analysis on random search-based NAS, but does this hold for other conditions such as for the search space used in DARTS and Robust DARTS?\n\nOverall, the analysis in this paper is interesting, but there are some unclear points to be clarified for publication as mentioned above. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1099/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1099/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127015, "tmdate": 1606915800129, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1099/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1099/-/Official_Review"}}}, {"id": "0AZt0pjH9uJ", "original": null, "number": 4, "cdate": 1603913046537, "ddate": null, "tcdate": 1603913046537, "tmdate": 1605024531230, "tddate": null, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "invitation": "ICLR.cc/2021/Conference/Paper1099/-/Official_Review", "content": {"title": "The paper aims to bring the help of search strategies to construct the approximate search space.", "review": "This paper proposes Evolving the Proxy Search Space (EPS) as a new RandomNAS-based approach. The goal is to find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. EPS runs in three stages iteratively: Training the supernet by randomly sampling from a PS; Validating the architectures among the PS on a subset of the validation dataset in the training interval; Evolving the PS by a tournament selection evolutionary algorithm with the aging mechanism.\n\n\nThe paper is well written and easy to follow. The idea of efficiently sampling from the search space sounds interesting and the paper aims to bring the help of search strategies to construct the search space by itself. However, it is kind of incremental work since the EPS (Algorithm 1) is exactly similar to the aging evolution in Real et al. (2018, 2019) and this paper is using this search strategy to gradually build the search space on the fly. \n\nI think any of the existing search strategies 1) random search 2) Evolutionary algorithms (Real et al. (2018)) 4) progresive decision process (PNAS; Liu, et al 2018), etc can be used to find a proxy search space. While a random sample from GS is simply random search, proposed EPS is exactly the evolutionary strategy to build search space. One may even use progressive NAS algorithm as a proxy search space!  Comparing how these different proxy search spaces improve efficiency will improve the novelty and make the paper more strong.\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1099/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1099/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space", "authorids": ["~Yuhong_Li2", "congh@illinois.edu", "xiaofan3@illinois.edu", "~Jinjun_Xiong1", "~Wen-mei_Hwu1", "~Deming_Chen1"], "authors": ["Yuhong Li", "Cong Hao", "Xiaofan Zhang", "Jinjun Xiong", "Wen-mei Hwu", "Deming Chen"], "keywords": ["Neural Architecture Search", "AutoML", "Computer Vision"], "abstract": "Random-sampling Neural Architecture Search (RandomNAS) has recently become a prevailing NAS approach because of its search efficiency and simplicity. There are two main steps in RandomNAS: the training step that randomly samples the weight-sharing architectures from a supernet and iteratively updates their weights, and the search step that ranks architectures by their respective validation performance. Key to both steps is the assumption of a high correlation between estimated performance(i.e., accuracy) for weight-sharing architectures and their respective achievable accuracy (i.e., ground truth) when trained from scratch. We examine such a phenomenon via NASBench-201, whose ground truth is known for its entire NAS search space. We observe that existing RandomNAS can rank a set of architectures uniformly sampled from the entire global search space(GS), that correlates well with its ground-truth ranking. However, if we only focus on the top-performing architectures (such as top 20\\% according to the ground truth) in the GS, such a correlation drops dramatically.  This raises the question of whether we can find an effective proxy search space (PS) that is only a small subset of GS to dramatically improve RandomNAS\u2019s search efficiency while at the same time keeping a good correlation for the top-performing architectures. This paper proposes a new RandomNAS-based approach called EPS (Evolving the Proxy Search Space) to address this problem. We show that, when applied to NASBench-201, EPS can achieve near-optimal NAS performance and beat all existing state-of-the-art. When applied to different-variants of DARTS-like search spaces for tasks such as image classification and natural language processing, EPS is able to robustly achieve superior performance with shorter or similar search time compared to some leading NAS works. The code is available at https://github.com/IcLr2020SuBmIsSiOn/EPS", "one-sentence_summary": "A state-of-the-art Random sampling Neural Architecture Search method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|improving_randomsampling_neural_architecture_search_by_evolving_the_proxy_search_space", "pdf": "/pdf/01e6876e5b7a289d90a4759708453db75862d158.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cVi0k2vb7J", "_bibtex": "@misc{\nli2021improving,\ntitle={Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space},\nauthor={Yuhong Li and Cong Hao and Xiaofan Zhang and Jinjun Xiong and Wen-mei Hwu and Deming Chen},\nyear={2021},\nurl={https://openreview.net/forum?id=qk0FE399OJ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "qk0FE399OJ", "replyto": "qk0FE399OJ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1099/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538127015, "tmdate": 1606915800129, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1099/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1099/-/Official_Review"}}}], "count": 10}