{"notes": [{"id": "rygwLgrYPB", "original": "HJg-4ceKPB", "number": 2326, "cdate": 1569439822980, "ddate": null, "tcdate": 1569439822980, "tmdate": 1587954109631, "tddate": null, "forum": "rygwLgrYPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "oV9WXWF198", "original": null, "number": 1, "cdate": 1576798746235, "ddate": null, "tcdate": 1576798746235, "tmdate": 1576800889887, "tddate": null, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "invitation": "ICLR.cc/2020/Conference/Paper2326/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper presents an interesting and novel idea that is likely to be of interest to the community. The most negative reviewer did not acknowledge the author response. The AC recommends acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721542, "tmdate": 1576800272645, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2326/-/Decision"}}}, {"id": "ryl86MI0tS", "original": null, "number": 2, "cdate": 1571869374402, "ddate": null, "tcdate": 1571869374402, "tmdate": 1574421080846, "tddate": null, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "invitation": "ICLR.cc/2020/Conference/Paper2326/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This submission belongs to the general field of neural networks and sub-field of activation regularisation. In particular, this submission proposes a novel approach for activation regularisation whereby a distribution of activations within minibatch are regularised to have standard normal distribution. The approach, projected error function regularisation (PER), accomplishes that by minimising an upper-bound on 1-Wasserstein distance between empirical and standard normal distributions. \n\nI think the idea described in this submission is interesting. Unfortunately, I have issues with 1) presentation, 2) experimental results, 3) English. \n\nThe PER is presented as an objective function that minimises an upperbound on 1-Wasserstein. I believe I have seen no evidence to the origin of PER other than it is the upper-bound on 1-Wasserstein. Therefore, I find it strange to see a presentation where first an objective function is introduced, then 1-Wasserstein is described, and after applying standard inequality you obtain an expression that is PER. The current presentation seems to indicate that before this derivation has been done no one new the connection between PER and the upper bound on 1-Wasserstein. I disagree and say that you obtained the upper bound on 1-Wasserstein and called it PER. For unknown reasons you decided to present first PER, then upper bound and finally claim connection. This is a mistake as it is not a connection but merely a consequence. \n\nSimply looking up CIFAR-10 best numbers on any search engine I can find significantly better numbers. It is therefore unclear why did you decide to use sub-optimal configuration without commenting on that. The same applies to PTB and possibly to WikiText2. \n\nThere are numerous places where English is not adequate. For instance, \"new perspective of concerning the target distribution\". \n\nFollowing the rebuttal stage where the authors have made significant changes to the manuscript I have decided to increase my assessment score. \n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2326/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2326/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575694685163, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2326/Reviewers"], "noninvitees": [], "tcdate": 1570237724424, "tmdate": 1575694685185, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2326/-/Official_Review"}}}, {"id": "rke4QlgvsS", "original": null, "number": 3, "cdate": 1573482523525, "ddate": null, "tcdate": 1573482523525, "tmdate": 1573483231956, "tddate": null, "forum": "rygwLgrYPB", "replyto": "Hkl5pWjptr", "invitation": "ICLR.cc/2020/Conference/Paper2326/-/Official_Comment", "content": {"title": "Author response to Reviewer 3", "comment": "We appreciate Reviewer 3 for the carefully reading our work and providing valuable comments. We address your cons as follows:\n\n\n- Difference between PER and Huang et. al. 2018\nYes, it is true that DBN (Huang et. al., 2018) also captures the interaction between hidden units though whitening. However, there are many cases PER and DBN have different behavior in making activations to follow the standard normal distribution since PER aims to match the distributions and DBN aims to whiten the activations. For instance, DBN cannot make change activations from a skewed distribution or a multimodal distribution having zero mean and the identity covariance matrix, unlike PER. This limitation of DBN can be found in Bilen & Vedaldi (2017) and Deecke et al. (2019) pointing out the inadequacy of normalizing multi-modal distributions by single mean and variance. To clarify this difference, we added a new figure (Fig. 1) and a new paragraph in P. 2-3 (last paragraph of the section 2) in the revised manuscript. \n\n-------\nReference\n\nLei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorrelated batch normalization. In IEEE Conference on Computer Vision and Pattern Recognition, 2018.\nHakan Bilen and Andrea Vedaldi. Universal representations: The missing link between faces, text, planktons, and cat breeds. arXiv preprint arXiv:1701.07275, 2017.\nLucas Deecke, Iain Murray, and Hakan Bilen. Mode normalization. In International Conference on Learning Representations, 2019.\n\n\n- Computational cost of PER\nThanks for this great suggestion. As per Reviewer 3 pointed out, PER has non-negligible computational costs in backward pass having O(n d_l s) time complexity while BN has the time complexity of O(n d_l) where b is the size of mini-batch, s is the number of projection, and d_l is the number of hidden units in layer l. In terms of the wall clock running time, a vanilla network, BN, VCL, and PER take 0.071, 0.083, 0.087, and 0.093 seconds for a single forward/backward iteration in 11-layer CNN on a single NVIDIA TITAN X, respectively. The clarification and comparison of computational costs are added in section 4.3.1 of the revised manuscript.\n\n\n- Experiments on larger datasets/models\nWe appreciate the suggestion from Reviewer 3. As Reviewer 3 suggested, we performed additional experiments on tiny ImageNet (a subset of ImageNet). It has 2x more training samples, 2x more categories, and 2x bigger image size. Besides, following the experiment given in VCL, we used 2x more filters in the experiment, i.e., 2x larger model. As other experiments performed in the original manuscript, we obtained better results than BN, VCL, and a vanilla network, and added the experiment in the revised manuscript.\n\n\n- Typos\nThanks for carefully reviewing our manuscript. We modified the typos."}, "signatures": ["ICLR.cc/2020/Conference/Paper2326/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygwLgrYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference/Paper2326/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2326/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2326/Reviewers", "ICLR.cc/2020/Conference/Paper2326/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2326/Authors|ICLR.cc/2020/Conference/Paper2326/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143024, "tmdate": 1576860541047, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference/Paper2326/Reviewers", "ICLR.cc/2020/Conference/Paper2326/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2326/-/Official_Comment"}}}, {"id": "HyeFyggvoH", "original": null, "number": 1, "cdate": 1573482464596, "ddate": null, "tcdate": 1573482464596, "tmdate": 1573483211092, "tddate": null, "forum": "rygwLgrYPB", "replyto": "ryl86MI0tS", "invitation": "ICLR.cc/2020/Conference/Paper2326/-/Official_Comment", "content": {"title": "Author response to Reviewer 1", "comment": "We appreciate Reviewer 1 for giving constructive feedback. Following your comments, we thoroughly revised the manuscript and believe the comments significantly improve the clarity of the manuscript. We address your three concerns as follows:\n\n\n- Presentation\nWe sincerely thank Reviewer 1 for this insightful comment. As per Reviewer 1 pointed out, it is true that we obtained PER by applying the Minkowski inequality to 1-Wasserstein. In the original manuscript, we presented PER then derived PER from the 1-Wasserstein for emphasizing the difference between BN and PER, and now we admit that was a mistake. In the revised manuscript, we thoroughly revised the presentation as deriving the PER from the 1-Wasserstein and then explaining its difference with BN. We believe this change significantly improves the presentation of the manuscript and emphasizes the difference with existing methods even better.\n\n- Experimental result\nWe thank for pointing out missing details in the experimental configuration. As Reviewer 1 indicated, the experimental configurations in the manuscript may be sub-optimal. However, to carefully compare PER with existing methods (BN, VCL, and L1 and L2 activation regularizations), we use the default hyperparameters of baseline models given in their papers. To clarify this point, we added explicit comments about this in each experiment and provided the benchmark results of literature in the result tables.\n\n\n- There are numerous places where English is not adequate.\nIn response to Reviewer 1, we have very carefully proofread the manuscript again and corrected grammatical errors, typos, and inadequate expressions. We hope that Reviewer will notify us if there are still places where English is not adequate such as \"new perspective of concerning the target distribution.\""}, "signatures": ["ICLR.cc/2020/Conference/Paper2326/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygwLgrYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference/Paper2326/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2326/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2326/Reviewers", "ICLR.cc/2020/Conference/Paper2326/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2326/Authors|ICLR.cc/2020/Conference/Paper2326/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143024, "tmdate": 1576860541047, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference/Paper2326/Reviewers", "ICLR.cc/2020/Conference/Paper2326/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2326/-/Official_Comment"}}}, {"id": "HygKZgeDjH", "original": null, "number": 2, "cdate": 1573482497332, "ddate": null, "tcdate": 1573482497332, "tmdate": 1573483197100, "tddate": null, "forum": "rygwLgrYPB", "replyto": "BJl1kZ14cr", "invitation": "ICLR.cc/2020/Conference/Paper2326/-/Official_Comment", "content": {"title": "Author response to Reviewer 2", "comment": "We thank Reviewer 2 for your time and efforts to point out the missing details of the original manuscript in computational cost that is an important issue when proposing a new regularizer. We added the benchmarking result with computational cost analysis in section 4.3.1 of the revised manuscript and address your comments as follows:\n\n- Computational cost of PER and BN\nIn terms of time complexity, PER has the complexity of O(b d_l s) for projection operation where b is the size of mini-batch, s is the number of projection, and d_l is the number of hidden units in layer l. On the other hand, BN has O(b d_l) complexities for element-wise arithmetic operations and computations of mean and variance. \n\n- Training time for PER and BN in CIFAR\nIn our wall clock running time measure, each training iteration takes 0.071 seconds for a vanilla network, 0.083 seconds for BN, 0.087 for VCL, and 0.093 seconds for PER on a single NVIDIA TITAN X. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2326/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygwLgrYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference/Paper2326/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2326/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2326/Reviewers", "ICLR.cc/2020/Conference/Paper2326/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2326/Authors|ICLR.cc/2020/Conference/Paper2326/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143024, "tmdate": 1576860541047, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference/Paper2326/Reviewers", "ICLR.cc/2020/Conference/Paper2326/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2326/-/Official_Comment"}}}, {"id": "S1ejPeevsS", "original": null, "number": 4, "cdate": 1573482594534, "ddate": null, "tcdate": 1573482594534, "tmdate": 1573482594534, "tddate": null, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "invitation": "ICLR.cc/2020/Conference/Paper2326/-/Official_Comment", "content": {"title": "General statement", "comment": "We sincerely thank the reviewers for their insightful comments. We have uploaded a revised manuscript and summarize the major changes below. \n\n1. In section 2, we added the paragraph illustrating the difference between PER, BN, and decorrelated BN.\n\n2. In section 3, we changed the presentation of the proposed method as deriving the PER from the 1-Wasserstein and then explaining its difference with BN.\n\n3. In section 4.1, we conducted and added an experiment on the larger dataset with the larger model.\n\n4. In section 4.3.1, we added computational complexity analysis."}, "signatures": ["ICLR.cc/2020/Conference/Paper2326/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rygwLgrYPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference/Paper2326/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2326/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2326/Reviewers", "ICLR.cc/2020/Conference/Paper2326/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2326/Authors|ICLR.cc/2020/Conference/Paper2326/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504143024, "tmdate": 1576860541047, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2326/Authors", "ICLR.cc/2020/Conference/Paper2326/Reviewers", "ICLR.cc/2020/Conference/Paper2326/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2326/-/Official_Comment"}}}, {"id": "Hkl5pWjptr", "original": null, "number": 1, "cdate": 1571824065652, "ddate": null, "tcdate": 1571824065652, "tmdate": 1572972353249, "tddate": null, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "invitation": "ICLR.cc/2020/Conference/Paper2326/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new method to normalize activations in neural networks, based on an upper bound of the sliced Wasserstein distance between the empirical activation distribution and a standard Gaussian distribution. I think this feels like a \"borderline\" case to me. The paper clearly has merits, at the same time there're some issues to be addressed.\n\nPros:\n- The idea is clearly presented.\n- Better performance than BN is achieved in many experiments.\n- Empirical evidence in Section 4.3 looks good, suggesting the proposed method does do the job as expected. The means and variances stabilize as training progresses.\n\nCons:\n- While the method based on sliced Wasserstein distances sounds new, the novelty seems limited since the idea of whitening the activation distribution to unit Gaussian was introduced before as mentioned by the authors. The paper claims the random projection may capture \u201cinteraction between hidden units\u201d, but it seems the method proposed in e.g. Huang et. al. 2018 also has projection matrices that might be doing similar things?\n\n- I\u2019m concerned about the actual computation cost of the proposed method. Although the method does not introduce any additional parameter compared to BN or VCL, it seems to require multiple random projections for each layer (s=256 in the experiments)? This could be much slower than the BN. A clarification/comparison of the wall clock running time would be desirable.\n\n- In terms of the image experiments, I do expect to see results with larger datasets/models, though not absolutely necessary.\n\nTypos:\n- Page 5, Eq. 9, x_i should be h_i instead?\n- Page 9, beta^l_j = 0 and ??^_j = 1"}, "signatures": ["ICLR.cc/2020/Conference/Paper2326/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2326/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575694685163, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2326/Reviewers"], "noninvitees": [], "tcdate": 1570237724424, "tmdate": 1575694685185, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2326/-/Official_Review"}}}, {"id": "BJl1kZ14cr", "original": null, "number": 3, "cdate": 1572233430638, "ddate": null, "tcdate": 1572233430638, "tmdate": 1572972353154, "tddate": null, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "invitation": "ICLR.cc/2020/Conference/Paper2326/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces \"projected error function regularization loss\" or PER, an alternative to batch normalization. PER is based on the Wasserstein metric. The experimental results show that PER outperforms batch normalization on CIFAR-10/100 with most activation functions. The authors also test their method on language modeling tasks.\n\nCaveat: I'm not an expert in this domain. Hence, please take my rating with a large grain of salt.\n\nComments/questions:\n- What's the computational cost of using PER over batch norm?  \n- Related to my other question: For the CIFAR-10 & CIFAR-100 comparison. What was the training time for BN vs PER?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2326/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2326/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["tjoo@estsoft.com", "emppunity@gmail.com", "byungkim@hanyang.ac.kr"], "title": "Regularizing activations in neural networks via distribution matching with the Wasserstein metric", "authors": ["Taejong Joo", "Donggu Kang", "Byunghoon Kim"], "pdf": "/pdf/4783bd6584a791c3e1de7a2cbb2714228df2a22d.pdf", "abstract": "Regularization and normalization have become indispensable components in training deep neural networks, resulting in faster training and improved generalization performance. We propose the projected error function regularization loss (PER) that encourages activations to follow the standard normal distribution. PER randomly projects activations onto one-dimensional space and computes the regularization loss in the projected space. PER is similar to the Pseudo-Huber loss in the projected space, thus taking advantage of both $L^1$ and $L^2$ regularization losses. Besides, PER can capture the interaction between hidden units by projection vector drawn from a unit sphere. By doing so, PER minimizes the upper bound of the Wasserstein distance of order one between an empirical distribution of activations and the standard normal distribution. To the best of the authors' knowledge, this is the first work to regularize activations via distribution matching in the probability distribution space. We evaluate the proposed method on the image classification task and the word-level language modeling task.\n", "keywords": ["regularization", "Wasserstein metric", "deep learning"], "paperhash": "joo|regularizing_activations_in_neural_networks_via_distribution_matching_with_the_wasserstein_metric", "_bibtex": "@inproceedings{\nJoo2020Regularizing,\ntitle={Regularizing activations in neural networks via distribution matching with the Wasserstein metric},\nauthor={Taejong Joo and Donggu Kang and Byunghoon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rygwLgrYPB}\n}", "original_pdf": "/attachment/7283c00bd0d77bc8c52598c48396c9f2c3bb87bc.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rygwLgrYPB", "replyto": "rygwLgrYPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2326/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575694685163, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2326/Reviewers"], "noninvitees": [], "tcdate": 1570237724424, "tmdate": 1575694685185, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2326/-/Official_Review"}}}], "count": 9}