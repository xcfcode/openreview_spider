{"notes": [{"id": "SklbPkUkzE", "original": null, "number": 1, "cdate": 1546768217394, "ddate": null, "tcdate": 1546768217394, "tmdate": 1546937366496, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Public_Comment", "content": {"comment": "We reproduce the results for this 2019 ICLR submission under the ICLR reproducibility challenge 2019. We also review the claims and findings of the paper carefully. We study the probability distributions which the algorithm converges to during different runs, and try to explain the behaviour. We point out some limitations in their work, provide a discussion of their results. We also try to verify the extent of their claims, and actual effect of their algorithm. Precisely, we try to find how much tuning does it require and how exactly is it benefiting the test accuracy. This underscores some limitations in their work. Finally, we offer some suggestions to improve their research. Our full report can be found at https://openreview.net/forum?id=SJlGaRBJMN . Our reproducibility code can be found at https://github.com/AnayMehrotra/Reproducibility_Challenge_ICLR_2019 .", "title": "Summary of submission to ICLR reproducibility challenge 2019"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311926208, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "H1lGHsA9KX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311926208}}}, {"id": "H1lGHsA9KX", "original": "S1eNFL0-YX", "number": 68, "cdate": 1538087738029, "ddate": null, "tcdate": 1538087738029, "tmdate": 1545355415630, "tddate": null, "forum": "H1lGHsA9KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJxdIaIpkE", "original": null, "number": 1, "cdate": 1544543567641, "ddate": null, "tcdate": 1544543567641, "tmdate": 1545354499246, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Meta_Review", "content": {"metareview": "It is a simple but good idea to consider the choice of mini-batch size as a multi-armed bandit problem. Experiments also show a slight improvement compared to the best fixed batch size.\n\nThe main concerns from the reviewers are that (1) treating the choice of hyper-parameters as a bandit problem is known and has been exploited in different context, and this paper is limited to the choice of the mini-batch size, (2) the improvement in the test error is not significant. The authors' feedback did not solve the concerns raised by R2.\n\nThis paper conveys a nice idea but as the current form it falls slightly below the standard of the ICLR publications. One direction for improvement, as suggested by the reviewer, would be extending the idea for a wider hyper-parameter selection problems.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Good simple idea but with limited novelty and gain of performance."}, "signatures": ["ICLR.cc/2019/Conference/Paper68/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper68/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353348650, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper68/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper68/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper68/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353348650}}}, {"id": "H1eLL5ttCQ", "original": null, "number": 4, "cdate": 1543244365554, "ddate": null, "tcdate": 1543244365554, "tmdate": 1543244365554, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "Bke3qSqd37", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Official_Comment", "content": {"title": "As a result of additional experiments, there were no significant performance gap according to the number of grid values or bandit loss.", "comment": "\n1. About bandit-based hyperparameter optimization and wider context of parameter/model selection:\n  First, the authors were shortsighted to consider batch size problem as a hyperparameter optimization problem, and for this problem, the proposed algorithm does not provide the best batch size instead it provides a policy (of adaptively sampling for different batch size) to attain performance which is obtained by using a best fixed batch size. It is not clear whether the same strategy will be applicable for other hyperparameters. Rather than giving the best possible hyperparameter through certain strategy as in previously proposed bandit-based hyperparameter optimization methods, this algorithm adaptively samples the batch size with each epoch and evaluates its performance on the validation data in order to update the statistics of the batch size. The accumulated performance is compared to the best performing hyperparameter\u2019s accumulated performance.\n\n2. The number and choice of the grid values and the bandit loss:\n  The authors are most grateful for the questions regarding the number of the grid values and the bandit loss. The authors conducted additional experiments by varying the number of grid values (reduced subset, extended superset) and using graduated loss (hinge, ratio) on the three different datasets: MNIST, CIFAR10, and CIFAR100. However, additional grid search with either reduced or extended grid set was not performed.\n  As ratio loss, the signed percentage of change in validation loss was considered. The probability of initially sampled batch size increases as validation loss decreased. In general, the validation loss decreases in the beginning of training regardless of which batch size sampled. The probability of the initially sampled batch size continued to rise, and started to hinder exploration from functioning normally. For this reason, the authors considered only percentage of non-negative changes in validation loss.\n  Interestingly, there are a little gap in performance, but they are not significant and there is no clear trend. Detailed experimental settings and results are described in the revised paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper68/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper68/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610980, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1lGHsA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper68/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper68/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper68/Authors|ICLR.cc/2019/Conference/Paper68/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610980}}}, {"id": "r1x4pUtK0X", "original": null, "number": 3, "cdate": 1543243451749, "ddate": null, "tcdate": 1543243451749, "tmdate": 1543243451749, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "Hylaq3TunQ", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Official_Comment", "content": {"title": "The details about OMD and probability update rule were described in revised paper.", "comment": "The authors would like thank the reviewer for his/her detailed comments.\n\n1. Some explanation of the surprising results: \n  RMGD samples the batch size every epoch to minimize the cumulative cost relative to that of the best fixed batch size. The cost (the performance of the sampled batch size) is measured in terms of increased validation loss. Thus, the RMGD attempts to dynamically select batch sizes that minimize the number of increase validation loss compared to that of the best fixed batch size. Minimizing the number of increase in validation loss during training will lead to small overall validation loss. However, this point was not explicitly conveyed in submitted manuscript. Although RMGD outperformed the best fixed batch size in all the experiments, it is not the intention of this algorithm: the algorithm only tries to minimize the regret and perform as well as the best. The authors believe that the theoretical and in-depth analysis of why RMGD outperformed the best performance of grid search is a worthwhile topic in future study.\n\n2. Motivation of probability update rule:\n  \\pi(\u03c0) lies in the probability simplex and a strongly convex regularizer offers a tighter regret bound in the follow-the-regularized-leader (FTRL) framework, so the RMGD uses negative entropic regularization function. Online mirror descent (OMD) is applied instead of FTRL to update the probability distribution. The OMD relies on Bregman divergence and projection, and if regularizer is strongly convex, then the OMD and FTRL produce equivalent predictions for each epoch. When the regularizer is negative entropic function, the probability distribution is updated as randomized weighted majority algorithm. The details are described in 'Appendix A' of the revised paper.\n\n3. Explanation of online mirror descent:\n  The FTRL has a problem that it requires solving an optimization problem at each epoch. To solve this problem, OMD is applied. The OMD computes the current decision iteratively based on a gradient update rule and the previous decision, and lies in the update being carried out in a \u2018dual\u2019 space defined by regularizer. This follows from considering the gradient of the regularizer. In terms of implementation, the current probability is updated by the gradient update in the \u2018dual\u2019 space (regularizer\u2019s gradient space) and Bregman projection onto convex set (simplex S). The OMD achieves the same regret bound as FTRL. The details are described in 'Appendix A' of the revised paper.\n\n4. \\beta z >= -1?:\n  In this algorithm, \\beta z >= 0 as pointed out by the reviewer. By the Theorem 2.22 in Shalev-Shwartz et al. (2012), the equation (6) in 'Appendix A' is derived for \\beta z \u2265 -1, and this algorithm obviously satisfies this condition. The authors modified the manuscript to make this point clearer.\n\n5. Citation of the equation following \\beta z >= -1:\n  The equation (6) and (7) are derived by the Theorem 2.22 and 4.1 in Shalev-Shwartz et al. (2012). The reference is cited in the revised paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper68/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper68/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610980, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1lGHsA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper68/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper68/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper68/Authors|ICLR.cc/2019/Conference/Paper68/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610980}}}, {"id": "rkxzgGYFCX", "original": null, "number": 2, "cdate": 1543242217691, "ddate": null, "tcdate": 1543242217691, "tmdate": 1543242217691, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "HJx9ZYNchX", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Official_Comment", "content": {"title": "The comparator in this paper functions exactly as described by the reviewer.", "comment": "The authors would like thank the reviewer for his/her insightful comment; however, we believe the manuscript may have misguided the reviewer into thinking that the comparator is adaptive but as the reviewer so rightly has pointed out the comparator must be fixed. The proposed RMGD does it exactly as described by the reviewer. The comparator in this paper functions exactly as described by the reviewer. In other words, a fixed i that minimizes cumulative sum is used throughout the run such that each epoch uses same i which represents the i-th batch in the batch set. Thus, the comparator is equivalent to \\sum_t y(w(b^*, ..., b^*), b^*)). The authors have revised the manuscript to make this point more clear. We thank the reviewer for making this pointing."}, "signatures": ["ICLR.cc/2019/Conference/Paper68/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper68/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610980, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1lGHsA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper68/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper68/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper68/Authors|ICLR.cc/2019/Conference/Paper68/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610980}}}, {"id": "H1ex_gYFR7", "original": null, "number": 1, "cdate": 1543241831830, "ddate": null, "tcdate": 1543241831830, "tmdate": 1543241831830, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Official_Comment", "content": {"title": "Summary of paper revision", "comment": "The followings were modified from the original paper.\n\nMajor revision:\n  - Additional experimental settings and results for varying the number of grid values and using graduated bandit loss on three datasets (MNIST, CIFAR10, CIFAR100) were added in \u20185. Experiments and Appendix B\u2019.\n  - The description of online mirror descent (OMD) and the derivation of the probability update rule were described in \u2018Appendix A\u2019.\n\nMinor revision:\n  - We removed one reference [Bergstra & Bengio (2012)] not mentioned in this paper and added three references [Jamieson & Talwalkar (2016) and Li et al. (2017) in \u20182. Related works\u2019 for bandit-based hyperparameter optimization, and Hazan & Kale (2010) in \u2018Appendix A\u2019 for the OMD theory].\n  - We modified or added some sentences in \u2018Abstract, 4.2 Regret bound, conclusion, and Appendix A\u2019 to make more clear.\n  - We added some equation numbers.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper68/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper68/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610980, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1lGHsA9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper68/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper68/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper68/Authors|ICLR.cc/2019/Conference/Paper68/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper68/Reviewers", "ICLR.cc/2019/Conference/Paper68/Authors", "ICLR.cc/2019/Conference/Paper68/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610980}}}, {"id": "HJx9ZYNchX", "original": null, "number": 3, "cdate": 1541191937670, "ddate": null, "tcdate": 1541191937670, "tmdate": 1541534313428, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Official_Review", "content": {"title": "Well-motivated idea seemingly showing a small improvement in performance, but at little cost and with a potential increase in robustness", "review": "The authors consider the problem of determining the minibatch size for SGD by first fixing a set of candidate sizes, and then learning a distribution over those sizes using a MAB algorithm. A minibatch size is first sampled from the distribution, then one training epoch is performed. A validation error is then computed, and if it is lower than that of the last epoch, the cost of the minibatch is taken to be zero (otherwise one), and the distribution is updated. This is Algorithm 1.\n\nIn Section 4.2, they prove a regret bound, but I don\u2019t think that regret is really the correct notion, here (although it\u2019s very close). This is a subtle point, so I\u2019ll set up some notation. Let w(b_1, ..., b_t) be the result at the tth epoch, if the batch sizes b_1, \u2026, b_t were used at the 1st through tth epochs. Let y(w,b) be 0 if training one epoch starting at w with batch size b would improve the validation error, and 1 otherwise.\n\nThey show (unnumbered inequality on the middle of page 5) that \\sum_t y(w(b_1,...,b_{t-1}),b_t) is close to \\sum_t y(w(b_1,...,b_{t-1}),b^*), where b_t is the batch size that was chosen at time t, and b^* is the best fixed batch size. The key point here is that the comparator (the second sum) starts each epoch at the result that was found by their adaptive algorithm, *not* what would have been found if a batch size of b^* had been used from the beginning.\n\nIn other words, their result does *not* show that their algorithm is close to outperforming a fixed choice of batch size (for that to hold, the comparator would need to be \\sum_t y(w(b^*,...,b^*),b^*)). What they show is similar, but subtly different. They don\u2019t put too much weight on this theoretical result, and in fact don\u2019t even explicitly claim that the comparator in this result is that for a fixed choice of batch size, so really this is a minor issue, but I think that this is something that should be clarified, since it would be easy for a reader to draw an incorrect conclusion.\n\nWith that said, their approach is well-motivated, and their experiments seem to show consistent small improvements in performance. I don\u2019t think the performance improvements are totally conclusive, but one of the most appealing properties of their proposal is that it shouldn\u2019t be much more computationally expensive than using a fixed minibatch size. Furthermore, their approach is potentially more robust, since you can presumably be less careful about choosing the set of candidate minibatch sizes, than you would be for choosing only one. So while the experiments don\u2019t show a big improvement, their proposal has other benefits.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper68/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Official_Review", "cdate": 1542234545090, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper68/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335641973, "tmdate": 1552335641973, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper68/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hylaq3TunQ", "original": null, "number": 2, "cdate": 1541098645286, "ddate": null, "tcdate": 1541098645286, "tmdate": 1541534313217, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Official_Review", "content": {"title": "Interesting extension of minibatch GD using a straight-forward application of bandits", "review": "This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit for achieving best performance in grid search by selecting an appropriate batch size at each epoch with a probability defined as a function of its previous success/failure. Its results suggest that RMGD faster than MGD with grid search, and generalizes better.\n\nThe paper is well written. The idea itself is a simple and relatively straightforward application of bandits. The paper has some merits as it proposes an efficient and theoretically sound method to replace grid search in MGD.\n\nOne result that stands out is that RMGD achieves better results than the best performing batch size. The authors may want to discuss this in more depth. This may be due to the fact that the problem is inherently contextual: each epoch is different from other epochs, and may require a better-suited bach-size. Maybe contextual bandits would be a good candidate to try.\n\nComments:\n- offer some analysis or explanation of the surprising results\n- add equation numbers for ease of reference\n- in 4.1, why did you use this particular probability update? motivate/explain this choice.\n- appendix A: Specify that <> is dot product.\n                        introduce Beta\n                        briefly explain mirror descent\n                        why is beta z >= -1? My sense is that it is >= 0. can it be negative?\n                        explain, motivate or cite the equation following beta z >= -1\n\nI am pretty familiar wit bandit literature. Less so with GD literature. The paper's hybrid approach, although simple, exposes interesting questions. I tend towards accepting the paper.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper68/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Official_Review", "cdate": 1542234545090, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper68/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335641973, "tmdate": 1552335641973, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper68/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bke3qSqd37", "original": null, "number": 1, "cdate": 1541084563656, "ddate": null, "tcdate": 1541084563656, "tmdate": 1541534313011, "tddate": null, "forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper68/Official_Review", "content": {"title": "The contribution is incremental due to lack of originality and somewhat narrow scope of the application", "review": "The paper applies multi-armed bandits for choosing the size of the minibatch to be used in each training epoch of a standard CNN. The loss of the bandit is binary: zero if the validation loss decreases and 1 otherwise. In the experiments, the Exp3 bandit algorithm is run with Adam and Adagrad on MNIST, CIFAR-10, and CIFAR-100. The results show that the bandit approach allows to obtain a test error better (although not significantly better) than the test error corresponding to the best minibatch size among those considered by the bandit.\n\nThe idea of viewing the choice of hyperparameters in a learning algorithm as a bandit problem is known and has been explored in different contexts, although the specific application to minibatch size is new as far as I know.\n\nThe paper could have gained strength if bandits had been considered in wider context of parameter/model selection in deep learning.\n\nIt is not clear how results scale with the number and choice of the grid values.\n\nI would have liked to see a more thorough investigation of the impact of the bandit loss on the experiments. It is true that as far as the theory is concerned, any bounded loss is OK. But I practice I would expect that a graduated loss (e.g., signed percentage of change in validation loss), would be more informative.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper68/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit", "abstract": "Determining the appropriate batch size for mini-batch gradient descent is always time consuming as it often relies on grid search. This paper considers a resizable mini-batch gradient descent (RMGD) algorithm based on a multi-armed bandit that achieves performance equivalent to that of best fixed batch-size. At each epoch, the RMGD samples a batch size according to a certain probability distribution proportional to a batch being successful in reducing the loss function. Sampling from this probability provides a mechanism for exploring different batch size and exploiting batch sizes with history of success.  After obtaining the validation loss at each epoch with the sampled batch size, the probability distribution is updated to incorporate the effectiveness of the sampled batch size. Experimental results show that the RMGD achieves performance better than the best performing single batch size. It is surprising that the RMGD achieves better performance than grid search. Furthermore, it attains this performance in a shorter amount of time than grid search.", "paperhash": "cho|a_resizable_minibatch_gradient_descent_based_on_a_multiarmed_bandit", "TL;DR": "An optimization algorithm that explores various batch sizes based on probability and automatically exploits successful batch size which minimizes validation loss.", "authorids": ["ipcng00@kaist.ac.kr", "sunghun.kang@kaist.ac.kr", "cd_yoo@kaist.ac.kr"], "authors": ["Seong Jin Cho", "Sunghun Kang", "Chang D. Yoo"], "keywords": ["Batch size", "Optimization", "Mini-batch gradient descent", "Multi-armed bandit"], "pdf": "/pdf/9b31dd8a6abc5d5d37be4088ce2eef5d8cb66d13.pdf", "_bibtex": "@misc{\ncho2019a,\ntitle={A Resizable Mini-batch Gradient Descent based on a Multi-Armed Bandit},\nauthor={Seong Jin Cho and Sunghun Kang and Chang D. Yoo},\nyear={2019},\nurl={https://openreview.net/forum?id=H1lGHsA9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper68/Official_Review", "cdate": 1542234545090, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1lGHsA9KX", "replyto": "H1lGHsA9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper68/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335641973, "tmdate": 1552335641973, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper68/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}