{"notes": [{"tddate": null, "tmdate": 1487105688979, "tcdate": 1487105688979, "number": 23, "id": "BJ-s01bYg", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "Update", "comment": "We sincerely thank all reviewers for your feedback and comments! We have updated our paper, making minor adjustments such as fixing typos. We have also included some additional ablation studies, requested during the review process, in the appendix."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487105580902, "tcdate": 1478289313579, "number": 399, "id": "rJeKjwvclx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJeKjwvclx", "signatures": ["~Caiming_Xiong1"], "readers": ["everyone"], "content": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 32, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1487015187248, "tcdate": 1487015187248, "number": 22, "id": "Hkofat1Kx", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "BynFWpUvl", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: Typos", "comment": "Thanks Radu!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396562455, "tcdate": 1486396562455, "number": 1, "id": "B1c52fUOe", "invitation": "ICLR.cc/2017/conference/-/paper399/acceptance", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. All reviewers agree that this is a good piece of work that should be accepted to ICLR. Authors are encouraged to incorporate reviewer feedback to further strengthen the paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396562921, "id": "ICLR.cc/2017/conference/-/paper399/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396562921}}}, {"tddate": null, "tmdate": 1485390212108, "tcdate": 1485390212108, "number": 21, "id": "BynFWpUvl", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "HkhVcYrDg", "signatures": ["~Radu_Kopetz1"], "readers": ["everyone"], "writers": ["~Radu_Kopetz1"], "content": {"title": "Typos", "comment": "Thanks Victor, really helpful information !\n\nHere are some typos that I think were made in the paper:\n1. After equation 4, on 3rd page: \"We define U = [u1, . . . , um] \". U should be 2l*m instead of l*m\n2. W1 in page 5 should have dimension p*l*3l instead of p*l*6l\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1485310516488, "tcdate": 1485310516488, "number": 20, "id": "HkhVcYrDg", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "B1R5HQrwl", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: Implementation details", "comment": "Hi Radu,\n\nPlease find the answers to your questions below:\n\n1. What is the sentinel initialized with ? Is is just a 0 filled vector ? \n\nWe randomly initialize the sentinel. The embeddings for the sentinel is then optimized.\n\n2. What are the initial start / end values that you feed to LSTM state before the first iteration ?\n\nIt is 0.\n\n3. What initialization values you use for W's and bias ?\n\nrandom initialization.\n\n4. In the ablation study, you evaluated two layers of MLP. Is that applied on each word individually ? Or on the whole sequence ? In other words, the first layer matrix dimension is [2l, 2l] or [2l*m, 2l*m] ?\n\nIt is applied to each word individually.\n\n\n5. You mention using dropout. Is is used only on the question/document encodding ? And what amount ? Do you also use L2 regularization ?\n\nWe do not use L2 regularization. We apply 0.3 dropout on the question and document encodings."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1485284843626, "tcdate": 1485284758501, "number": 19, "id": "B1R5HQrwl", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["~Radu_Kopetz1"], "readers": ["everyone"], "writers": ["~Radu_Kopetz1"], "content": {"title": "Implementation details", "comment": "Thanks a lot for a great paper. Could you please provide some implementation details ?\n1. What is the sentinel initialized with ? Is is just a 0 filled vector ? \n2. What are the initial start / end values that you feed to LSTM state before the first iteration ?\n3. What initialization values you use for W's and bias ?\n4. In the ablation study, you evaluated two layers of MLP. Is that applied on each word individually ? Or on the whole sequence ? In other words, the first layer matrix dimension is [2l, 2l] or [2l*m, 2l*m] ?\n5. You mention using dropout. Is is used only on the question/document encodding ? And what amount ? Do you also use L2 regularization ?\n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1484936840853, "tcdate": 1484936840853, "number": 18, "id": "HkbcU0yPe", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "BJVRCsJvx", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: Response", "comment": "Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1484926726066, "tcdate": 1481991598905, "number": 2, "id": "B1P2rym4l", "invitation": "ICLR.cc/2017/conference/-/paper399/official/review", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["ICLR.cc/2017/conference/paper399/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper399/AnonReviewer1"], "content": {"title": "Interesting model but need some more analyses", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Summary: The paper proposes a novel deep neural network architecture for the task of question answering on the SQuAD dataset. The model consists of two main components -- coattention encoder and dynamic pointer decoder. The encoder produces attention over the question as well as over the document in parallel and thus learns co-dependent representations of the question and the document. The decoder predicts the starting and the end token of the answer iteratively, with the motivation that multiple iterations will help the model escape local maxima and thus will reduce the errors made by the model. The proposed model achieved state-of-art result on SQuAD dataset at the time of writing the paper. The paper reports some analyses of the results such as performance across question types, document, question, answer lengths, etc. The paper also performs some ablation studies such as performing only single round of iteration on decoder, etc.\n\nStrengths:\n\n1. The paper is well-motivated with two main motivations -- co-attending to the document and the question, and iteratively producing the answer.\n\n2. The proposed model architecture is novel and the design choices made seem reasonable.\n\n3. The experiments show that the proposed model outperforms the existing model (at the time of writing the paper) on the SQuAD dataset by significant margin.\n\n4. The analyses of the results and the ablation studies performed (as per someone's request) provide insights into the various modelling design choices made.\n\nWeaknesses/Questions/Suggestions:\n\n1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.\n\n2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?\n\n3. In order to estimate how much modelling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).\n\n4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n\n5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn\u2019t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang\u2019s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.\n\n6. In section 2.1, \u201cn\u201d and \u201cm\u201d are swapped when explaining the Document and Question encoding matrix. Please fix it.\n\nReview Summary: The paper presents a novel and interesting model for the task of question answering on SQuAD dataset and shows that the model outperforms existing models. However, to gain more insights into the functioning of the model, I would like see more analyses of the results and one more ablation study (see weaknesses section above).\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512597713, "id": "ICLR.cc/2017/conference/-/paper399/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper399/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper399/AnonReviewer3", "ICLR.cc/2017/conference/paper399/AnonReviewer1", "ICLR.cc/2017/conference/paper399/AnonReviewer2"], "reply": {"forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512597713}}}, {"tddate": null, "tmdate": 1484926668206, "tcdate": 1484926668206, "number": 3, "id": "BJVRCsJvx", "invitation": "ICLR.cc/2017/conference/-/paper399/official/comment", "forum": "rJeKjwvclx", "replyto": "rk6555C8g", "signatures": ["ICLR.cc/2017/conference/paper399/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper399/AnonReviewer1"], "content": {"title": "Response", "comment": "Thanks for the above results. It is informative to know that the model can produce reasonable responses for questions that require reasoning across multiple sentences. I am changing the review score to \"Top 50% of accepted papers, clear accept\"."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592216, "id": "ICLR.cc/2017/conference/-/paper399/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper399/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper399/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592216}}}, {"tddate": null, "tmdate": 1484855956684, "tcdate": 1484855956684, "number": 17, "id": "rk6555C8g", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "H1ZuntA8l", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "Model prediction on SQuAD paper Table 3", "comment": "Hi,\n\nWe have produced predictions using our model for the examples in Table 3 of the original SQuAD paper. Please find them below:\n\nDocument 1:\nThe Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine is used, the TS diagram begins to resemble the Carnot cycle.\n\nQuestion:\nWhat is the Rankine cycle sometimes called?\n\nGround truth:\npractical Carnot cycle\n\nModel response:\npractical Carnot cycle\n\n\n------------------------------------------\n\nDocument 2:\nWhile the Commision has a monopoly on initiating legislation, the European Parliament and the Council of the European Union have powers of amendment and veto during the legislative progress.\n\nQuestion:\nWhich two governing bodies have legislative veto power?\n\nGround truth:\nthe European Parliament and the Council of the European Union\n\nModel response:\nEuropean Parliament and the Council of the European Union\n\n\n------------------------------------------\n\nDocument 3:\nCurrent faculty include the anthropologist Marshall Sahlins, historian Dipesh Chakrabarty, ... Shakespeare scholar David Bevington, and renowned political scientists John Mearsheimer and Robert Pape.\n\nQuestion:\nWhat Shakespeare scholar is currently on the university\u2019s faculty?\n\nGround truth:\nDavid Bevington\n\nModel response:\nDavid Bevington\n\n\n------------------------------------------\n\nDocument 4:\nThe V&A Theatre & Performance galleries, formerly the Theatre Museum, opened in March 2009. The collections are stored by the V&A, and are available for research, exhibitions and other shows. They hold the UK's biggest national collection of material about live performance in the UK since Shakespeare's day, covering drama, dance, musical theatre, circus, music hall, rock and pop, and most other forms of live entertainment.\n\nQuestion:\nWhat collection does the V&A Theatre & Performance galleries hold?\n\nGround truth:\nMaterial about live performance\n\nModel response:\nUK's biggest national collection of material about live performance in the UK since Shakespeare's day\n\n\n------------------------------------------\n\nDocument 5:\nAlong with giving the offender his \"just deserts\", achieving crime control via incapacitation and deterrence is a major goal of crime punishment.\n\nQuestion:\nWhat is the main goal of criminal punishment of civil disobedients?\n\nGround truth:\nachieving crime control via incapacitation and deterrence\n\nModel response:\nachieving crime control via incapacitation and deterrence"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1484852328748, "tcdate": 1484852328748, "number": 16, "id": "H1ZuntA8l", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "rktwvI0Ul", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: Response", "comment": "Thanks for the suggestion! I will certainly run our model on these examples and add them to our paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1484838753190, "tcdate": 1484838753190, "number": 2, "id": "rktwvI0Ul", "invitation": "ICLR.cc/2017/conference/-/paper399/official/comment", "forum": "rJeKjwvclx", "replyto": "B14376qSl", "signatures": ["ICLR.cc/2017/conference/paper399/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper399/AnonReviewer1"], "content": {"title": "Response", "comment": "Thanks for carrying out the ablation studies. For the analysis of the model w.r.t different types of reasoning as shown in table 3 in SQuAD paper, it would be great if you could present the outputs of the model for the examples shown in table 3. You could also check-out Appendices B and C in the most-revised version of \"Machine Comprehension Using Match-LSTM and Answer Pointer\" paper (also an ICLR 2017 submission) to see how they have done the same analysis."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592216, "id": "ICLR.cc/2017/conference/-/paper399/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper399/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper399/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592216}}}, {"tddate": null, "tmdate": 1483555836797, "tcdate": 1483555836797, "number": 15, "id": "ByS-46cBx", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "BysmQxf4e", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "Response", "comment": "Thank you very much for your thoughtful review."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1483555755603, "tcdate": 1483555755603, "number": 14, "id": "B14376qSl", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "B1P2rym4l", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "Response", "comment": "Thank you very much for your thoughtful reviews.\n\n>>>>1. In order to gain insights into how much each additional iteration in the decoder help, I would like to see the following -- for every iteration report the mean F1 for questions that converged in that iteration along with the number of questions that converged in that iteration.\n\nWe do not have strong support (number of examples) for high iteration (e.g. 3-4 iterations) examples. We did find in our ablation study that allowing the model multiple iterations, as opposed to forcing it to a single iteration, improves performance.\n\nBelow, we show the 4-iteration model\u2019s performance across iterations needed to converge. \n----------  ------------------           ---------\niterations   mean f1                            frequency\n1                 0.7660381405761181     9935\n2                0.5698158862824868   527\n3                0.5464594744971136    50\n4                0.6213130048723088    58\n----------  ------------------           ---------\n\n>>>>2. Example of Question 3 in figure 5 is an interesting example where the model is unable to decide between multiple local maxima despite several iterations. Could authors please report how often this happens?\n\nThis occurs very rarely. For example, on the development, only 58 examples reach the maximum of 4 iterations.\n\n>>>>3. In order to estimate how much modeling of attention in the encoder helps, it would be good if authors could report the performance of the model when attention is not modeled at all in the encoder (neither over question, nor over document).\n\nWe attempted a model without attention, in which the encoder first ingests the question then the document. The decoder then points over the document encoding as usual. The best validation performance we obtained for this model is 41.9% F1 and 33.3% exact match.\n\n>>>>4. I would like to see the variation in the performance of the proposed model for questions that require different types of reasoning (table 3 in SQuAD paper). This would provide insights into what are the strengths and weaknesses of the proposed model w.r.t the type reasoning required.\n\nI emphasize with your position that we analyze the model with respect to the reasoning required to gain more insights into what it is doing. In particular, I would be very interested in seeing how each top model submitted to SQuAD differs in this regard. That said, we contacted Rajpurkar et al, the original authors of the SQuAD paper, who declined to share the 200 sampled used to estimate the statistics in Table 3 of the SQuAD paper. In particular, Pranav offered that releasing the 192 examples would have very little value of information of tuning and comparing models. That said, they are considering including in the next iteration of the dataset a more thorough analysis for the purposes of analyzing the strengths and weaknesses of each model. We will update our results when the dataset is updated.\n\n>>>>5. In Wang and Jiang (2016), the attention is predicted over question for each word in the document. But in table 2, when performing ablation study to make the proposed model similar to Wang and Jiang, C^D is set to C^Q. But isn\u2019t C^Q attention over document for each word in the question? So, how is this similar to Wang and Jiang\u2019s attention? I think QA^D will be similar to Wang and Jiang's attention since QA^D is attention over question for each word in the document. Please clarify.\n\nYou are correct, this is indeed a typo (it\u2019s suppose to be QA^D as opposed to C^Q).\n\n>>>>6. In section 2.1, \u201cn\u201d and \u201cm\u201d are swapped when explaining the Document and Question encoding matrix. Please fix it.\n\nThanks! We will fix it! \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1483555548300, "tcdate": 1483555548300, "number": 13, "id": "B14yQa9Sl", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "ByNVo754g", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "Response", "comment": "Thank you very much for your thoughtful review.\n\n>>>>I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? \n\nYes, the model consistently performs better with with more iterations, with diminishing returns as the number of iteration increases. With more iterations, the model performs better on examples in which there exists multiple potential answers (e.g. multiple people for a \"who question\").\n\n>>>>As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc?\n\nWe agree, we will analyze the advantages of different choices in the proposed model across different question types. Also as suggested by Reviewer 1, we think it would be interesting to analyze the model (e.g. across encoder types, decoder types, and iteration counts) with respect to the type of reasoning involved once the dataset is updated with such information.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1482468139868, "tcdate": 1482468139868, "number": 3, "id": "ByNVo754g", "invitation": "ICLR.cc/2017/conference/-/paper399/official/review", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["ICLR.cc/2017/conference/paper399/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper399/AnonReviewer2"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "\nPaper Summary: \nThe paper introduces a question answering model called Dynamic Coattention Network (DCN). It extracts co-dependent representations of the document and question, and then uses an iterative dynamic pointing decoder to predict an answer span. The proposed model achieves state-of-the-art performance, outperforming all published models.\n\nPaper Strengths: \n-- The proposed model introduces two new concepts to QA models -- 1) using attention in both directions, and 2) a dynamic decoder which iterates over multiple answer spans until convergence or maximum number of iterations.\n-- The paper also presents ablation study of the proposed model which shows the importance of their design choices.\n-- It is interesting to see the same idea of co-attention performing well in 2 different domains -- Visual Question Answering and machine reading comprehension.\n-- The performance breakdown over document and question lengths (Figure 6) strengthens the importance of attention for QA task.\n-- The proposed model achieves state-of-the-art result on SQuAD dataset.\n-- The model architecture has been clearly described.\n\nPaper Weaknesses / Future Thoughts: \n-- The paper provides model's performance when the maximum number of iterations is 1 and 4. I would like to see how the performance of the model changes with the number of iterations, i.e., the model performance when that number is 2 and 3. Is there a clear trend? What type of questions is the model able to get correct with more iterations?\n-- As with many deep learning approaches, the overall architecture seems quite complex, and the design choices seem to be driven by performance numbers. As future work, authors might try to analyze qualitative advantages of different choices in the proposed model. What type of questions are correctly answered because of co-attention mechanism instead of attention in a single direction, when using Maxout Highway Network instead of a simple MLP, etc?\n\nPreliminary Evaluation: \nNovel and state-of-the-art question answering approach. Model is clearly described in detail. In my thoughts, a clear accept.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512597713, "id": "ICLR.cc/2017/conference/-/paper399/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper399/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper399/AnonReviewer3", "ICLR.cc/2017/conference/paper399/AnonReviewer1", "ICLR.cc/2017/conference/paper399/AnonReviewer2"], "reply": {"forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512597713}}}, {"tddate": null, "tmdate": 1481929535270, "tcdate": 1481929506938, "number": 1, "id": "BysmQxf4e", "invitation": "ICLR.cc/2017/conference/-/paper399/official/review", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["ICLR.cc/2017/conference/paper399/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper399/AnonReviewer3"], "content": {"title": "Official Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposed a dynamic coattention network for the question answering task with long contextual documents. \nThe model is able to encode co-dependent representations of the question and the document, and a dynamic decoder iteratively pointing the potential answer spans to locate the final answer. \n\nOverall, this is a well-written paper. \nAlthough the model is a bit complicated (coattention encoder, iterative dynamic pointering decoder and highway maxout network), the intuitions behind and the details of the model are clearly presented. \nAlso the performance on the SQuAD dataset is good. \nI would recommend this paper to be accepted.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512597713, "id": "ICLR.cc/2017/conference/-/paper399/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper399/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper399/AnonReviewer3", "ICLR.cc/2017/conference/paper399/AnonReviewer1", "ICLR.cc/2017/conference/paper399/AnonReviewer2"], "reply": {"forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512597713}}}, {"tddate": null, "tmdate": 1481543701381, "tcdate": 1481543701374, "number": 12, "id": "BkaMxzhQx", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "rJ_xrk3Ql", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: RE:RE: escape the local maxima", "comment": "Hi Dirk,\n\nWhen we say that the model is forced to a single pass, we mean that it is forced to a single pass during both training and evaluation."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1481532655785, "tcdate": 1481532655777, "number": 11, "id": "rJ_xrk3Ql", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "B1Vf4pyXx", "signatures": ["~Dirk_Weissenborn1"], "readers": ["everyone"], "writers": ["~Dirk_Weissenborn1"], "content": {"title": "RE:RE: escape the local maxima", "comment": "I have a small question to your ablation study. You say, that you are forcing the model to a single pass in the ablation study, but is it still the model trained on 4 passes? If that is the case, it might be interesting to see what happens when you train on a single pass model, since the training would become different. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1480884345844, "tcdate": 1480738365896, "number": 9, "id": "ByIH86J7l", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "S1OVS31Xl", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: Objective Function", "comment": "Hi,\n\nThank you for your thoughtful comments!\n\nDuring each iteration i of the decoder, we compute the start scores (alpha in the paper) and the end scores (beta in the paper) for each position of the document. For each score, we compute the softmax cross entropy losses using the ground truth answer start position and ground truth answer end position. We sum up these losses over each iteration. If the model does not stop, then we collect losses for a maximum of T iterations. If the model stops after R iterations, then we only collect losses for the R iterations that occurred. Mathematically (please excuse my formatting here), the loss is then:\n\nsum_i^{min(T, R)} softmax_crossentropy(alpha_i, ground_truth_start) + softmax_crossentropy(beta_i, ground_truth_end)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1480884316877, "tcdate": 1480737804034, "number": 8, "id": "B1Vf4pyXx", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "HyNsShymg", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: escape the local maxima", "comment": "Hi,\n\nThank you for your thoughtful comments!\n\nIn our ablation study in Table 2 (page 8), we show the same model but with the decoder forced to a single iteration. As you suggested, most of the time, the coattention encoder learns good enough representations and the model is able to predict the correct answer in one pass. However, we found that allowing more passes in the decoder helped us achieve better performance. Namely, allowing the model a maximum of four passes as opposed to forcing it to a single pass increased our development F1 by 1.6%, thereby achieving the state of the art result.\n\nRegarding your point that given the model needs several iterations to localize, why must the iterations occur in the decoder? This is purely a design decision to reduce computation complexity. Indeed, we are interested in exploring an iterative encoder (e.g. the entire model is iterative) in future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1480884297266, "tcdate": 1480791744259, "number": 10, "id": "SJd6I9xml", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "rJQ3BT1Qg", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: Clarification questions", "comment": "Hi,\n\nThank you for your thoughtful comments!\n\n1. We chose to use the same encoder module for the question and the document to avoid overfitting on the limited amount of rather short training questions. However, since the grammatical structure of question sentence is different from the structure of sentences in the document, we would like to allow for some variations between the representation space of the hidden state of questions and the space of documents. Therefore, we add nonlinear transformation at the output of the hidden state of questions. Empirically we found that it yields 0.5%~1% F1 improvement on the development set. \u2028\n\n2. As shown in our ablation study, allowing for iterations in the dynamic encoder increases our development F1 by 1.6%, thereby achieving the state of the art result.\u2028 In our ablation study in Table 2 (page 8), we show the same model but with the decoder forced to a single iteration.\n\n\n3. Different from statistical QA model, our model is a end-to-end system without any hand-crafted features, which is a key part in statistical QA models. \u2028\u2028Similar to neural QA models, we use an attention mechanism in the encoder module and feed the encoder representation to decoder for answer output. The whole framework is an attention-based encoder-decoder neural framework. But different from prior work, we propose a new co-attention encoder which compute the attention maps both from document to question and from question to document, and use the two directional attention maps to learn a co-dependent representation of the question and documen (e.g. the coattention context). We also proposed a novel dynamic decoder which allow for several iterations to locate the final answer, because given a question, there may exist several intuitive answer spans within the document, each corresponding to a local maxima. This iterative procedure provides opportunity for the model to recover from initial local maxima corresponding to incorrect answer spans. There are some qualitative results shown in Figure 5. Our ablation study and table 2 also show the improvement with our proposed co-attention model and dynamic decoder respectively. Finally, Table 1 shows that our model achieves state-of-the-art performance ."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1480738219475, "tcdate": 1480738219468, "number": 3, "id": "rJQ3BT1Qg", "invitation": "ICLR.cc/2017/conference/-/paper399/pre-review/question", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["ICLR.cc/2017/conference/paper399/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper399/AnonReviewer2"], "content": {"title": "Clarification questions", "question": "1) What is the reason behind allowing some variation between the question encoding space and the document encoding space? What happens if the non-linear projection layer on top of the question embedding is removed?\n\n2) How does the performance (EM and F1) change with respect to iterations in dynamic decoder?\n\n3) Related work talks about previous Statistical QA and Neural QA works. How does the proposed approach relate to / different from all the mentioned approaches?  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959302967, "id": "ICLR.cc/2017/conference/-/paper399/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper399/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper399/AnonReviewer1", "ICLR.cc/2017/conference/paper399/AnonReviewer3", "ICLR.cc/2017/conference/paper399/AnonReviewer2"], "reply": {"forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959302967}}}, {"tddate": null, "tmdate": 1480734309703, "tcdate": 1480734108437, "number": 2, "id": "HyNsShymg", "invitation": "ICLR.cc/2017/conference/-/paper399/pre-review/question", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["ICLR.cc/2017/conference/paper399/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper399/AnonReviewer3"], "content": {"title": "escape the local maxima", "question": "The paper mentioned the dynamic decoder is able to escape the initial local maxima several times. I wonder how serious the issue is? If the encoder learns good enough representations, it is able to predict the answer span in one time. If it really needs several iterations to localize, why it must be done in decoder? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959302967, "id": "ICLR.cc/2017/conference/-/paper399/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper399/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper399/AnonReviewer1", "ICLR.cc/2017/conference/paper399/AnonReviewer3", "ICLR.cc/2017/conference/paper399/AnonReviewer2"], "reply": {"forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959302967}}}, {"tddate": null, "tmdate": 1480733999730, "tcdate": 1480733999724, "number": 1, "id": "S1OVS31Xl", "invitation": "ICLR.cc/2017/conference/-/paper399/pre-review/question", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["ICLR.cc/2017/conference/paper399/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper399/AnonReviewer1"], "content": {"title": "Objective Function", "question": "Dear Authors,\n\nCould you please clarify how do you combine the cross entropy loss of the start and end points across all iterations? A mathematical formulation of the final objective would be helpful. \n\nThanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959302967, "id": "ICLR.cc/2017/conference/-/paper399/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper399/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper399/AnonReviewer1", "ICLR.cc/2017/conference/paper399/AnonReviewer3", "ICLR.cc/2017/conference/paper399/AnonReviewer2"], "reply": {"forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper399/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959302967}}}, {"tddate": null, "tmdate": 1480360195460, "tcdate": 1480360195455, "number": 7, "id": "rJoWW-5Gl", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "HygmEhvMg", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: A few questions about vocabulary size", "comment": "Hi Sahil,\n\nIn practice there are much less than 2.2 million words in SQuAD. We use GloVe embeddings for all words that exists in both SQuAD and in Common Crawl. For words that are in SQuAD and are not in Common Crawl, we use zero embeddings. You can also interpret this as replacing it with an <UNKNOWN> token that has zero embedding."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1480209431724, "tcdate": 1480209431718, "number": 6, "id": "HygmEhvMg", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["~Sahil_Sharma1"], "readers": ["everyone"], "writers": ["~Sahil_Sharma1"], "content": {"title": "A few questions about vocabulary size", "comment": "Hi,\n\nA sentence from Section 4 reads: \"We limit the vocabulary to words that are present in the Common Crawl corpus and set embeddings for out-of-vocabulary words to zero.\" Suppose GloVe vectors' vocabulary size is k (k ~ 2.2 million). Does this mean that the vocabulary size used in this paper is exactly 1 more than GloVe's vocabulary size (k+1)? Does this mean that effectively every out of vocabulary word is replaced with an <UNKNOWN> token which has an all-zeros embedding?\n\nThanks! "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1479457403971, "tcdate": 1479457403967, "number": 5, "id": "By4Y9Vnbe", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "H1hx3Ysbl", "signatures": ["~Dirk_Weissenborn1"], "readers": ["everyone"], "writers": ["~Dirk_Weissenborn1"], "content": {"title": "RE: Ablation study needed", "comment": "Perfect. Seems like co-attention gives the largest boost after all. Good to know."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1479412724147, "tcdate": 1479412724144, "number": 4, "id": "H1hx3Ysbl", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "Skmr1m7Wg", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: Ablation study needed", "comment": "Hey Dirk, we've updated the paper with numbers from our ablation study :)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1478860603558, "tcdate": 1478860603553, "number": 3, "id": "Skmr1m7Wg", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "H1IQ4Sf-x", "signatures": ["~Dirk_Weissenborn1"], "readers": ["everyone"], "writers": ["~Dirk_Weissenborn1"], "content": {"title": "re", "comment": "Sounds good :)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1478804510107, "tcdate": 1478804510042, "number": 2, "id": "H1IQ4Sf-x", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "rklX27Mbx", "signatures": ["~Victor_Zhong1"], "readers": ["everyone"], "writers": ["~Victor_Zhong1"], "content": {"title": "RE: Ablation study needed", "comment": "Hi Dirk, we plan on performing more ablation study and adding to the paper. Thanks for your feedback!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}, {"tddate": null, "tmdate": 1478798359827, "tcdate": 1478798359819, "number": 1, "id": "rklX27Mbx", "invitation": "ICLR.cc/2017/conference/-/paper399/public/comment", "forum": "rJeKjwvclx", "replyto": "rJeKjwvclx", "signatures": ["~Dirk_Weissenborn1"], "readers": ["everyone"], "writers": ["~Dirk_Weissenborn1"], "content": {"title": "Ablation study needed", "comment": "The paper introduces several interesting mechanisms for answer extraction in a QA setup. However, I find it really difficult to see where the actual benefits come from compared to related architectures like the work of Wang et al. 2016, which was also submitted to ICLR. I believe much of the improvements stem from the dynamic pointer decoder which itself contains various sub-architectures. For the sake of comparability, I would be interested to see how performances would change when:\n\n- not using co-attention but only attention on the question for each context token, similar to Wang et al.\n- exchanging the rather large Highway Maxout Network by something simpler, e.g. omitting m1 and m2 completely to get directly to the output, lowering pool-sizes because they seem rather large as well or using a simple MLP instead of the HMN. The intuition behind using Maxout sounds appealing but it has not been verified in the paper.\n- allowing only one iteration for the pointer decoder (i.e., direct prediction)\n\nThanks for considering these suggestions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dynamic Coattention Networks For Question Answering", "abstract": "Several deep learning models have been proposed for question answering. How- ever, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointer decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.", "pdf": "/pdf/8db6d8f1072c3fb894057a8b06f6ba30c781540a.pdf", "TL;DR": "An end-to-end dynamic neural network model for question answering that achieves the state of the art and best leaderboard performance on the Stanford QA dataset.", "paperhash": "xiong|dynamic_coattention_networks_for_question_answering", "keywords": ["Natural language processing", "Deep learning", "Applications"], "conflicts": ["salesforce.com", "metamind.io"], "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "vzhong@salesforce.com", "rsocher@salesforce.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287592344, "id": "ICLR.cc/2017/conference/-/paper399/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJeKjwvclx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper399/reviewers", "ICLR.cc/2017/conference/paper399/areachairs"], "cdate": 1485287592344}}}], "count": 33}