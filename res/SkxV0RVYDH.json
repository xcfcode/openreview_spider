{"notes": [{"id": "SkxV0RVYDH", "original": "Byx4hwc_PB", "number": 1427, "cdate": 1569439436146, "ddate": null, "tcdate": 1569439436146, "tmdate": 1577168215406, "tddate": null, "forum": "SkxV0RVYDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders", "authors": ["Walter Gerych", "Elke Rundensteiner", "Emmanuel Agu"], "authorids": ["wgerych@wpi.edu", "rundenst@wpi.edu", "emmanuel@wpi.edu"], "keywords": ["Anomaly detection", "outliers", "deep learning", "distribution mapping", "wasserstein autoencoders"], "TL;DR": "An extension of Wasserstein autoencoders such that anomalies in the feature-space remain anomalies in the latent space ", "abstract": " State-of-the-art deep learning methods for outlier detection make the assumption that anomalies will appear far away from inlier data in the latent space produced by distribution mapping deep networks. However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming,  we introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this we leverage the insight that outliers are likely to have a higher reconstruction error than inliers. We thus achieve outlier-preserving distribution mapping through weighting the reconstruction error of individual points by the value of a multivariate Gaussian probability density function evaluated at those points. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions. We show that if the  global minimum of our newly proposed loss function is achieved, \n then our OP-DMA  maps inliers to regions with a Mahalanobis distance less than delta, and outliers to regions past this delta, delta being the inverse Chi Squared CDF evaluated at (1-alpha) with alpha the percentage of outliers in the dataset. Our experiments confirm that OP-DMA  consistently outperforms  the state-of-art  methods on a rich variety of outlier detection benchmark datasets.", "pdf": "/pdf/5cc16b174cf3873400642efe211c20a5e07b31f9.pdf", "paperhash": "gerych|versatile_anomaly_detection_with_outlier_preserving_distribution_mapping_autoencoders", "original_pdf": "/attachment/90fe635406305959cbe10ec548896676e9e2d0c0.pdf", "_bibtex": "@misc{\ngerych2020versatile,\ntitle={Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders},\nauthor={Walter Gerych and Elke Rundensteiner and Emmanuel Agu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxV0RVYDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OGiTNr_XP", "original": null, "number": 1, "cdate": 1576798723007, "ddate": null, "tcdate": 1576798723007, "tmdate": 1576800913561, "tddate": null, "forum": "SkxV0RVYDH", "replyto": "SkxV0RVYDH", "invitation": "ICLR.cc/2020/Conference/Paper1427/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes an outlier detection method that maps outliers to low probability regions of the latent space. The novelty is in proposing a weighted reconstruction error penalizing the mapping of outliers into high probability regions. The reviewers find the idea promising.\nThey have also raised several questions. It seems the questions are at least partially addressed in the rebuttal, and as a result one of our expert reviewers (R5) has increased their score from WR to WA. But since we did not have a champion for this paper and its overall score is not high enough, I can only recommend a reject at this stage.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders", "authors": ["Walter Gerych", "Elke Rundensteiner", "Emmanuel Agu"], "authorids": ["wgerych@wpi.edu", "rundenst@wpi.edu", "emmanuel@wpi.edu"], "keywords": ["Anomaly detection", "outliers", "deep learning", "distribution mapping", "wasserstein autoencoders"], "TL;DR": "An extension of Wasserstein autoencoders such that anomalies in the feature-space remain anomalies in the latent space ", "abstract": " State-of-the-art deep learning methods for outlier detection make the assumption that anomalies will appear far away from inlier data in the latent space produced by distribution mapping deep networks. However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming,  we introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this we leverage the insight that outliers are likely to have a higher reconstruction error than inliers. We thus achieve outlier-preserving distribution mapping through weighting the reconstruction error of individual points by the value of a multivariate Gaussian probability density function evaluated at those points. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions. We show that if the  global minimum of our newly proposed loss function is achieved, \n then our OP-DMA  maps inliers to regions with a Mahalanobis distance less than delta, and outliers to regions past this delta, delta being the inverse Chi Squared CDF evaluated at (1-alpha) with alpha the percentage of outliers in the dataset. Our experiments confirm that OP-DMA  consistently outperforms  the state-of-art  methods on a rich variety of outlier detection benchmark datasets.", "pdf": "/pdf/5cc16b174cf3873400642efe211c20a5e07b31f9.pdf", "paperhash": "gerych|versatile_anomaly_detection_with_outlier_preserving_distribution_mapping_autoencoders", "original_pdf": "/attachment/90fe635406305959cbe10ec548896676e9e2d0c0.pdf", "_bibtex": "@misc{\ngerych2020versatile,\ntitle={Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders},\nauthor={Walter Gerych and Elke Rundensteiner and Emmanuel Agu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxV0RVYDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkxV0RVYDH", "replyto": "SkxV0RVYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712169, "tmdate": 1576800261509, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1427/-/Decision"}}}, {"id": "HygKPD2hqr", "original": null, "number": 2, "cdate": 1572812641318, "ddate": null, "tcdate": 1572812641318, "tmdate": 1575643463052, "tddate": null, "forum": "SkxV0RVYDH", "replyto": "SkxV0RVYDH", "invitation": "ICLR.cc/2020/Conference/Paper1427/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #5", "review": "This work proposes an outlier detection method based on WAE framework. WAE is trained to ensure that 1) latent distribution follows a prior distribution 2) weighted reconstruction error is low where prior PDF is used to weight the reconstruction error.\n\nPositives\n------------\n1.I liked the intuition behind the proposed method and I felt its worth exploring. Paper points out that in previous works, there is no mechanism to prevent outliers from getting mapped to high probability areas in the model [21]. Authors claim that their method will over-come this issue. I believe this is the core contribution of the paper.\n\n2.I agree with authors point that WAE is a better choice than VAE for outlier detection because, former \"encourages the latent representations as a whole to match the prior \". \n\n3.I agree training with a distributional divergence loss along with a weighed reconstruction loss will be helpful for learning a robust representation (as outliers in the training dataset would be assigned a lower weight). \n\n4.Authors have compared the performance of their method on OOD dataset where they have  compared against three other baseline methods, where they obtain better performance in majority of cases.\n\n\nNegatives\n--------------\nA. Outlier detection and anomaly/novelty detection are two very different problems.  Outliers are 'bad eggs' coming from the same class as normal data. On the other hand, anomaly/novelty are unexpected data possibly coming from other classes. This is the taxonomy followed by majority of works. In my understanding this work is about 'outlier detection'. I hope authors will use the term 'outlier detection' consistency through the paper.\n\nB. Authors have not done a good survey on existing outlier detection methods.  Eg:\n    I. Chong You, Rene Vidal, Provable Self-Representation Based Outlier Detection in a Union of Subspaces, CVPR 17\n    II. Yan Xia, Xudong Cao, Fang Wen, Gang Hua, Jian Sun, Learning Discriminative Reconstructions for Unsupervised Outlier Removal, ICCV 15\n   III. Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, Ehsan Adeli, Adversarially Learned One-Class Classifier for Novelty Detection, CVPR 18. (they have experiments on outlier detection)\n\nC. Although existing methods have not explicitly stated the problem identified in (1) above, their proposals are indirectly solving this problem. Therefore, authors should have compared with papers listed in (B) to demonstrate the effectiveness of their method for a meaningful comparison.\n\nD. This is the first time I'm seeing the OOD dataset used in the paper. Have other works published their results on this dataset? Can they be included in your paper?. If not, consider reporting results on standard datasets used in papers (B). I believe reporting results on at least two datasets is necessary to demonstrate the generalizability of the method.\n\nOther comments\n------------------------\na. What is the dimensionality used in the latent space? I believe a larger latent space may be required in modeling more complex data such as images. Is the weighting mechanism effective when a very large latent space is used due to the curse of dimensionality. \n\nb. I don't think the synthetic dataset experiment is giving any interesting insights. This space is better used if an additional dataset is used instead.\n\nIn conclusion, I like the idea presented in this paper; however, I believe experimental results needs to be improved significantly to demonstrate the effectiveness of the proposed method. I cannot recommend to accept the paper in its present condition. \n\n\nPost Rebuttal:\nAuthors have partially addressed my concerns. In light of new experiments provided, I'm changing my decision to weak accept.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1427/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1427/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders", "authors": ["Walter Gerych", "Elke Rundensteiner", "Emmanuel Agu"], "authorids": ["wgerych@wpi.edu", "rundenst@wpi.edu", "emmanuel@wpi.edu"], "keywords": ["Anomaly detection", "outliers", "deep learning", "distribution mapping", "wasserstein autoencoders"], "TL;DR": "An extension of Wasserstein autoencoders such that anomalies in the feature-space remain anomalies in the latent space ", "abstract": " State-of-the-art deep learning methods for outlier detection make the assumption that anomalies will appear far away from inlier data in the latent space produced by distribution mapping deep networks. However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming,  we introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this we leverage the insight that outliers are likely to have a higher reconstruction error than inliers. We thus achieve outlier-preserving distribution mapping through weighting the reconstruction error of individual points by the value of a multivariate Gaussian probability density function evaluated at those points. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions. We show that if the  global minimum of our newly proposed loss function is achieved, \n then our OP-DMA  maps inliers to regions with a Mahalanobis distance less than delta, and outliers to regions past this delta, delta being the inverse Chi Squared CDF evaluated at (1-alpha) with alpha the percentage of outliers in the dataset. Our experiments confirm that OP-DMA  consistently outperforms  the state-of-art  methods on a rich variety of outlier detection benchmark datasets.", "pdf": "/pdf/5cc16b174cf3873400642efe211c20a5e07b31f9.pdf", "paperhash": "gerych|versatile_anomaly_detection_with_outlier_preserving_distribution_mapping_autoencoders", "original_pdf": "/attachment/90fe635406305959cbe10ec548896676e9e2d0c0.pdf", "_bibtex": "@misc{\ngerych2020versatile,\ntitle={Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders},\nauthor={Walter Gerych and Elke Rundensteiner and Emmanuel Agu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxV0RVYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxV0RVYDH", "replyto": "SkxV0RVYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877651084, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1427/Reviewers"], "noninvitees": [], "tcdate": 1570237737554, "tmdate": 1575877651098, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1427/-/Official_Review"}}}, {"id": "S1l4S4y9jS", "original": null, "number": 3, "cdate": 1573676091864, "ddate": null, "tcdate": 1573676091864, "tmdate": 1573751461292, "tddate": null, "forum": "SkxV0RVYDH", "replyto": "ryehE_265r", "invitation": "ICLR.cc/2020/Conference/Paper1427/-/Official_Comment", "content": {"title": "Thank you", "comment": "We thank you for your detailed feedback and time on our work. Your insights are very much appreciated. We have fixed the typos, incorrect references to figures and added in reference to AAEs in our experimental section. \n\nQUESTION  1.  \u201cthe novelty here is to enforce that on the latent space in the context of a variational auto-encoder. I am not sure if, from anomaly detection perspective, this is any better than simply using the reconstruction score. Why go the VAE route at all?\u201d \n\nOUR RESPONSE. To address your question, the reason the distribution mapping framework is important is that it allows us to calculate the likelihood of each point in the latent space, which can be leveraged to weight the reconstruction error of each point by its likelihood. The reason we do this weighting  rather than just directly using this reconstruction error is that in standard autoencoders the average reconstruction error for outliers and the average reconstruction error for inliers often tends to converge to the same value. This is in fact what we show experimentally in Figure 4 (a). \n\nAs can be seen, the autoencoder initially has a higher reconstruction error for outliers than inliers, but iit is quickly able to reproduce both inliers and outliers roughly equally well before converging. Our OP-DMA solution, on the other hand, succeeds  to maintain this difference in reconstruction error throughout the training process. We accomplish this by weighting the reconstruction loss for outliers by their likelihood in the latent space. By performing distribution mapping and making the distribution of the latent space match a prior distribution for which the PDF is known and tractible, we can calculate the likelihood of each point in the latent space.\n\nAlso, just to be clear,  we indeed use a WAE architecture  rather than a VAE solution in our work as you can see in Section 3. The reason for this is that, unlike a VAE, the WAE encourages the latent representations as a whole to match the prior, as also detailed in our Section 3.\n\nQUESTION 2:   \u201cIs there a possibility that assuming a single multi-variate Gaussian, as a prior, is too restrictive? Could it result in a high false alarm rate as well?\u201d\n\nOUR RESPONSE. A multivariate Gaussian prior for the latent space of VAEs and WAEs has been shown to be sufficient to represent data in well in a variety of domains including those characterized by very complicated data, including  images [1] and text [2]. \n\nQUESTION 3:   \u201cIn most score-based algorithms, the anomaly score is computed without assuming any prior knowledge about the contamination proportion. However, in the case of OP-DMA, the contamination parameter is used to train the auto-encoder that scores the data.\u201d\n\nOUR RESPONSE.  No, this observation is incorrect.\nThe autoencoder of OP-DMA does ***NOT***  use the contamination parameter. Only the EllipticEnvelope method requires the contamination parameter, in order to fit a robust covariance estimate to the encoded data. This means that one could switch out another algorithm for the anomaly detection step, such as OC-SVM, that does not require the contamination parameter and nothing about the OP-DMA network training procedure would change. \n\nStill, to address your concern, we have now added an additional sensitivity analysis of OP-DMA on the Satellite dataset to the Appendix of our paper. This analysis shows that the performance of EllipticEnvelope on the encoded data is robust as long as the contamination percentage is not grossly underestimated. \n\nQUESTION 4\u201cAdditional comparison with other non-distribution mapping state-of-the-art models such as LOF, oc-SVM, KNN would give a clearer idea of the performance.\u201d\n\nOUR RESPONSE.  As you have requested, we have now added additional experiments with the state-of-the-art models including the ones you have suggested, namely, OC-SVM and LOF, into our paper. The results are shown in Table 2.  It is apparent from the results that indeed our  OP-DMA solution consistently outperforms these state-of-the-art methods in all but 2 of the datasets. \n\nQUESTION 5: \u201cThe set H in theorem 3 has not been defined\u201d\n\nOUR RESPONSE: We are sorry for this mistake. The set H are the inlier points. We have added this to the statement of theorem 3. \n\nQUESTION 4: \u201cIn figure 3, in the training process, the authors have describe to add the divergence between the latent and prior distribution to the loss function, however, nothing like this is clearly shown in the figure.\u201d\n\nOUR RESPONSE: Thank you for alerting us to this. We have added an arrow from the divergence to the loss function in Figure 3. \n\n[1] Pu, Yunchen, et al. \"Variational autoencoder for deep learning of images, labels and captions.\" Advances in neural information processing systems. 2016.\n\n[2] Yang, Zichao, et al. \"Improved variational autoencoders for text modeling using dilated convolutions.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1427/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders", "authors": ["Walter Gerych", "Elke Rundensteiner", "Emmanuel Agu"], "authorids": ["wgerych@wpi.edu", "rundenst@wpi.edu", "emmanuel@wpi.edu"], "keywords": ["Anomaly detection", "outliers", "deep learning", "distribution mapping", "wasserstein autoencoders"], "TL;DR": "An extension of Wasserstein autoencoders such that anomalies in the feature-space remain anomalies in the latent space ", "abstract": " State-of-the-art deep learning methods for outlier detection make the assumption that anomalies will appear far away from inlier data in the latent space produced by distribution mapping deep networks. However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming,  we introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this we leverage the insight that outliers are likely to have a higher reconstruction error than inliers. We thus achieve outlier-preserving distribution mapping through weighting the reconstruction error of individual points by the value of a multivariate Gaussian probability density function evaluated at those points. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions. We show that if the  global minimum of our newly proposed loss function is achieved, \n then our OP-DMA  maps inliers to regions with a Mahalanobis distance less than delta, and outliers to regions past this delta, delta being the inverse Chi Squared CDF evaluated at (1-alpha) with alpha the percentage of outliers in the dataset. Our experiments confirm that OP-DMA  consistently outperforms  the state-of-art  methods on a rich variety of outlier detection benchmark datasets.", "pdf": "/pdf/5cc16b174cf3873400642efe211c20a5e07b31f9.pdf", "paperhash": "gerych|versatile_anomaly_detection_with_outlier_preserving_distribution_mapping_autoencoders", "original_pdf": "/attachment/90fe635406305959cbe10ec548896676e9e2d0c0.pdf", "_bibtex": "@misc{\ngerych2020versatile,\ntitle={Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders},\nauthor={Walter Gerych and Elke Rundensteiner and Emmanuel Agu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxV0RVYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxV0RVYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference/Paper1427/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1427/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1427/Reviewers", "ICLR.cc/2020/Conference/Paper1427/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1427/Authors|ICLR.cc/2020/Conference/Paper1427/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156176, "tmdate": 1576860560975, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference/Paper1427/Reviewers", "ICLR.cc/2020/Conference/Paper1427/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1427/-/Official_Comment"}}}, {"id": "HJgfgdccsS", "original": null, "number": 4, "cdate": 1573722090322, "ddate": null, "tcdate": 1573722090322, "tmdate": 1573722710128, "tddate": null, "forum": "SkxV0RVYDH", "replyto": "HygKPD2hqr", "invitation": "ICLR.cc/2020/Conference/Paper1427/-/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for your feedback and insight from your experience in this domain.  Unfortunately, we do not believe we can edit the name of the paper on this submission site in order to remove the reference to \"anomaly\" detection. However, all references to \"anomalies\" have been changed to \"outliers\" within the actual document. To address your other points:\n\nQUESTION 1:  \u201cAuthors have not done a good survey on existing outlier detection methods.  Eg:...\u201d\n\nOUR RESPONSE.   Thank you for alerting us about these particular methods. As you have suggested, we will add these three papers to our related work section. \n\nWe have added the method used \u201cAdversarially Learned One-Class Classifier for Novelty Detection\u201d (ALOCC) as one of the methods we compare our OP-DMA against. While the authors have released their implementation of ALOCC, this method was designed to work with image data. As we did not wish to drastically change the architecture of this method in order to work with non-image data, as added a single dense layer to the beginning of the network that takes in input data of any shape and outputs the data in a shape that can be accepted by the following convolutional layer. Our analysis shows that we outperform ALOC in all but 1 of the dataset we tested. \n\nAdditionally, we have now  added into our comparative study an additional recent state-of-the-art deep outlier detection method [1]. We have chosen this method not only because it is also a deep neural network method, but also in addition because this work had in fact  already compared their method against several datasets we also have evaluated OP-DMA against. \n\nAs you can see in our experimental result table in Table 2, OP-DMA outperforms this state-of-the-art method on all ODDs datasets besides 2 datasets. \n\n\nQUESTION 2:  \u201cThis is the first time I'm seeing the OOD dataset used in the paper. Have other works published their results on this dataset? Can they be included in your paper?. If not, consider reporting results on standard datasets used in papers (B). I believe reporting results on at least two datasets is necessary to demonstrate the generalizability of the method\u201d\n\nOUR RESPONSE. We believe that there may be some misunderstanding here. Namely, OOD is not a data set, but instead a benchmark repository that archives and makes available a rich variety of distinct  (labeled) data sets for outlier analysis research.  It is indeed a popular benchmark used by related work systems focussing on outlier research in their experimental studies.\nFurther, we note that we have tested on 11 different datasets all coming from this repository, not just 1. While all of the datasets came from the ODD repository, each of the 11 datasets we worked with are unique real-world data sets. Also, as stated above, we have added in a comparison to this select recent deep outlier detection method [1] which had already been evaluated on several of the datasets we used also from this ODD repository.\n\n[1] Generative Adversarial Active Learning for Unsupervised Outlier Detection\nYezheng Liu, Zhe Li, Chong Zhou, Yuanchun Jiang, Jianshan Sun, Meng Wang & Xiangnan He\nIEEE Transactions on Knowledge and Data Engineering (TKDE 2019)\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1427/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders", "authors": ["Walter Gerych", "Elke Rundensteiner", "Emmanuel Agu"], "authorids": ["wgerych@wpi.edu", "rundenst@wpi.edu", "emmanuel@wpi.edu"], "keywords": ["Anomaly detection", "outliers", "deep learning", "distribution mapping", "wasserstein autoencoders"], "TL;DR": "An extension of Wasserstein autoencoders such that anomalies in the feature-space remain anomalies in the latent space ", "abstract": " State-of-the-art deep learning methods for outlier detection make the assumption that anomalies will appear far away from inlier data in the latent space produced by distribution mapping deep networks. However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming,  we introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this we leverage the insight that outliers are likely to have a higher reconstruction error than inliers. We thus achieve outlier-preserving distribution mapping through weighting the reconstruction error of individual points by the value of a multivariate Gaussian probability density function evaluated at those points. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions. We show that if the  global minimum of our newly proposed loss function is achieved, \n then our OP-DMA  maps inliers to regions with a Mahalanobis distance less than delta, and outliers to regions past this delta, delta being the inverse Chi Squared CDF evaluated at (1-alpha) with alpha the percentage of outliers in the dataset. Our experiments confirm that OP-DMA  consistently outperforms  the state-of-art  methods on a rich variety of outlier detection benchmark datasets.", "pdf": "/pdf/5cc16b174cf3873400642efe211c20a5e07b31f9.pdf", "paperhash": "gerych|versatile_anomaly_detection_with_outlier_preserving_distribution_mapping_autoencoders", "original_pdf": "/attachment/90fe635406305959cbe10ec548896676e9e2d0c0.pdf", "_bibtex": "@misc{\ngerych2020versatile,\ntitle={Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders},\nauthor={Walter Gerych and Elke Rundensteiner and Emmanuel Agu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxV0RVYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxV0RVYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference/Paper1427/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1427/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1427/Reviewers", "ICLR.cc/2020/Conference/Paper1427/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1427/Authors|ICLR.cc/2020/Conference/Paper1427/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156176, "tmdate": 1576860560975, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference/Paper1427/Reviewers", "ICLR.cc/2020/Conference/Paper1427/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1427/-/Official_Comment"}}}, {"id": "SyxGNNRFiS", "original": null, "number": 2, "cdate": 1573671978400, "ddate": null, "tcdate": 1573671978400, "tmdate": 1573671978400, "tddate": null, "forum": "SkxV0RVYDH", "replyto": "SyxflCHBtH", "invitation": "ICLR.cc/2020/Conference/Paper1427/-/Official_Comment", "content": {"title": "Thank you", "comment": "OUR RESPONSE. Thank you for your time spent reviewing our paper. We have fixed the typos and incorrect figure/table references that have have pointed out in your review. \n\nQUESTION 1: \u201cFig. 2 is also a bit confusing, since it seems like the order in which the diagrams appear should be swapped. Indeed, the text also refers to fig. 2 (b) before (a). The text just below fig. 2 also refers to Figure 1, but I think it should be 2? \u201c\n\nOUR RESPONSE. Thank you for alerting us to this mistake. We have corrected the references to the figure in the paper, so that the text now refers to the correct subfigure. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1427/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders", "authors": ["Walter Gerych", "Elke Rundensteiner", "Emmanuel Agu"], "authorids": ["wgerych@wpi.edu", "rundenst@wpi.edu", "emmanuel@wpi.edu"], "keywords": ["Anomaly detection", "outliers", "deep learning", "distribution mapping", "wasserstein autoencoders"], "TL;DR": "An extension of Wasserstein autoencoders such that anomalies in the feature-space remain anomalies in the latent space ", "abstract": " State-of-the-art deep learning methods for outlier detection make the assumption that anomalies will appear far away from inlier data in the latent space produced by distribution mapping deep networks. However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming,  we introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this we leverage the insight that outliers are likely to have a higher reconstruction error than inliers. We thus achieve outlier-preserving distribution mapping through weighting the reconstruction error of individual points by the value of a multivariate Gaussian probability density function evaluated at those points. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions. We show that if the  global minimum of our newly proposed loss function is achieved, \n then our OP-DMA  maps inliers to regions with a Mahalanobis distance less than delta, and outliers to regions past this delta, delta being the inverse Chi Squared CDF evaluated at (1-alpha) with alpha the percentage of outliers in the dataset. Our experiments confirm that OP-DMA  consistently outperforms  the state-of-art  methods on a rich variety of outlier detection benchmark datasets.", "pdf": "/pdf/5cc16b174cf3873400642efe211c20a5e07b31f9.pdf", "paperhash": "gerych|versatile_anomaly_detection_with_outlier_preserving_distribution_mapping_autoencoders", "original_pdf": "/attachment/90fe635406305959cbe10ec548896676e9e2d0c0.pdf", "_bibtex": "@misc{\ngerych2020versatile,\ntitle={Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders},\nauthor={Walter Gerych and Elke Rundensteiner and Emmanuel Agu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxV0RVYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkxV0RVYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference/Paper1427/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1427/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1427/Reviewers", "ICLR.cc/2020/Conference/Paper1427/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1427/Authors|ICLR.cc/2020/Conference/Paper1427/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156176, "tmdate": 1576860560975, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1427/Authors", "ICLR.cc/2020/Conference/Paper1427/Reviewers", "ICLR.cc/2020/Conference/Paper1427/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1427/-/Official_Comment"}}}, {"id": "SyxflCHBtH", "original": null, "number": 1, "cdate": 1571278314276, "ddate": null, "tcdate": 1571278314276, "tmdate": 1572972470444, "tddate": null, "forum": "SkxV0RVYDH", "replyto": "SkxV0RVYDH", "invitation": "ICLR.cc/2020/Conference/Paper1427/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "This paper proposes a novel outlier detection approach, based on Wasserstein auto encoders. \n\nUnfortunately, I cannot comment on the overall scientific contribution of the paper, as I simply do not possess the expertise to judge it accurately. I will rely on the judgement of the other reviewers, whom I hope will have more experience and will better know the literature. I will report on a few issues with aspects related to the presentation below.\n\nIn fig. 1, the (a) and (b) should probaby appear below each diagram. \"on trained\" is repeated twice in the caption. \n\nIn fig. 2, the WAE acronym is defined only much later in the text. \n\nFig. 2 is also a bit confusing, since it seems like the order in which the diagrams appear should be swapped. Indeed, the text also refers to fig. 2 (b) before (a). The text just below fig. 2 also refers to Figure 1, but I think it should be 2? \n\nIn sec. 4.2, the text mentions table 2 when it should really be table 1. Also, table 1 should appear before table 2 in the body. \n\nIt looks as if the symbols () and [] are inverted? All references are referred to with () and text within parentheses (e.g. references to figures) have []. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1427/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1427/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders", "authors": ["Walter Gerych", "Elke Rundensteiner", "Emmanuel Agu"], "authorids": ["wgerych@wpi.edu", "rundenst@wpi.edu", "emmanuel@wpi.edu"], "keywords": ["Anomaly detection", "outliers", "deep learning", "distribution mapping", "wasserstein autoencoders"], "TL;DR": "An extension of Wasserstein autoencoders such that anomalies in the feature-space remain anomalies in the latent space ", "abstract": " State-of-the-art deep learning methods for outlier detection make the assumption that anomalies will appear far away from inlier data in the latent space produced by distribution mapping deep networks. However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming,  we introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this we leverage the insight that outliers are likely to have a higher reconstruction error than inliers. We thus achieve outlier-preserving distribution mapping through weighting the reconstruction error of individual points by the value of a multivariate Gaussian probability density function evaluated at those points. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions. We show that if the  global minimum of our newly proposed loss function is achieved, \n then our OP-DMA  maps inliers to regions with a Mahalanobis distance less than delta, and outliers to regions past this delta, delta being the inverse Chi Squared CDF evaluated at (1-alpha) with alpha the percentage of outliers in the dataset. Our experiments confirm that OP-DMA  consistently outperforms  the state-of-art  methods on a rich variety of outlier detection benchmark datasets.", "pdf": "/pdf/5cc16b174cf3873400642efe211c20a5e07b31f9.pdf", "paperhash": "gerych|versatile_anomaly_detection_with_outlier_preserving_distribution_mapping_autoencoders", "original_pdf": "/attachment/90fe635406305959cbe10ec548896676e9e2d0c0.pdf", "_bibtex": "@misc{\ngerych2020versatile,\ntitle={Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders},\nauthor={Walter Gerych and Elke Rundensteiner and Emmanuel Agu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxV0RVYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxV0RVYDH", "replyto": "SkxV0RVYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877651084, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1427/Reviewers"], "noninvitees": [], "tcdate": 1570237737554, "tmdate": 1575877651098, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1427/-/Official_Review"}}}, {"id": "ryehE_265r", "original": null, "number": 3, "cdate": 1572878387787, "ddate": null, "tcdate": 1572878387787, "tmdate": 1572972470364, "tddate": null, "forum": "SkxV0RVYDH", "replyto": "SkxV0RVYDH", "invitation": "ICLR.cc/2020/Conference/Paper1427/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes an improved extension of the Wasserstein auto-encoder for anomaly detection. The novelty is in proposing a weighted reconstruction error that penalizes the mapping of data with high reconstruction errors (mostly anomalies) into high probability regions. The idea being that an outlier would have a higher reconstruction error, and hence should be mapped to low-probability region of the latent distribution. \n\nExperimental Results:  As a distribution mapping auto-encoder model, OP-DMA outperforms the deep learning based state-of-the-art models in the same domain. \n\nOverall Assessment: The authors have a nice idea of forcing the latent mappings of inputs to correlate with their reconstruction error. Overall, the method is promising, but I have the following concerns:\n\n* Using the reconstruction error as an anomaly score has been explored many years ago (check replicator neural networks), the novelty here is to enforce that on the latent space in the context of a variational auto-encoder. I am not sure if, from anomaly detection perspective, this is any better than simply using the reconstruction score. Why go the VAE route at all? \n* Is there a possibility that assuming a single multi-variate Gaussian, as a prior, too restrictive? Could it result in a high false alarm rate as well? I guess this could be answered by more experimental results on richer data sets (even synthetic is fine). \n \n* In most score based algorithms, the anomaly score is computed without assuming any prior knowledge about the contamination proportion. However, in the case of OP-DMA, the contamination parameter is used to train the auto-encoder that scores the data. This might result in an optimization that is very specific to the parameter setting. I strongly recommend a sensitivity analysis to study the robustness of the model against different values of contamination parameter.\n\n* Performance on synthetic data-set has not been presented. The set H in theorem 3 has not been defined. \n\n* Additional comparison with other non-distribution mapping state-of-the-art models such as LOF, oc-SVM, KNN would give a clearer idea of the performance. This is important, because in my past experience, non-deep learning methods give much better results on the benchmark data sets that the authors have evaluated their method on. In fact, a comparative analysis (See - https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152173) gives a very nice comparison. However, since the authors provide results using Avg F1-score, instead of AUC curve, it was not possible to compare them myself.\n\n* In figure 3, in the training process, the authors have describe to add the divergence between the latent and prior distribution to the loss function, however, nothing like this is clearly shown in the figure.The references of figures in the text are either out of place or incorrect. Figure 1(a) and (b) in reference to the text are incorrect. Figure 2 is the misleading figure as it doesn't illustrate the anomaly detection process. Figure 3 has not been mentioned anywhere in the text. The authors have mentioned the comparison of their method with Wasserstien and variational auto-encoders in the text, while in table 2 and 4, AAE has also been shown as one of the method for comparison, which is never mentioned or described in the text.\n\n* Typo in caption of Figure 1 and the first line of section 3.3\n\nOverall, I am hesitant to recommend the paper before cross-checking the issue with contamination proportion and learning more about how a VAE framework is indeed important for anomaly detection.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1427/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1427/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders", "authors": ["Walter Gerych", "Elke Rundensteiner", "Emmanuel Agu"], "authorids": ["wgerych@wpi.edu", "rundenst@wpi.edu", "emmanuel@wpi.edu"], "keywords": ["Anomaly detection", "outliers", "deep learning", "distribution mapping", "wasserstein autoencoders"], "TL;DR": "An extension of Wasserstein autoencoders such that anomalies in the feature-space remain anomalies in the latent space ", "abstract": " State-of-the-art deep learning methods for outlier detection make the assumption that anomalies will appear far away from inlier data in the latent space produced by distribution mapping deep networks. However, this assumption fails in practice, because the divergence penalty adopted for this purpose encourages mapping outliers into the same high-probability regions as inliers. To overcome this shortcoming,  we introduce a novel deep learning outlier detection method, called Outlier Preserving Distribution Mapping Autoencoder (OP-DMA), which succeeds to map outliers to low probability regions in the latent space of an autoencoder. For this we leverage the insight that outliers are likely to have a higher reconstruction error than inliers. We thus achieve outlier-preserving distribution mapping through weighting the reconstruction error of individual points by the value of a multivariate Gaussian probability density function evaluated at those points. This weighting implies that outliers will result overall penalty if they are mapped to low-probability regions. We show that if the  global minimum of our newly proposed loss function is achieved, \n then our OP-DMA  maps inliers to regions with a Mahalanobis distance less than delta, and outliers to regions past this delta, delta being the inverse Chi Squared CDF evaluated at (1-alpha) with alpha the percentage of outliers in the dataset. Our experiments confirm that OP-DMA  consistently outperforms  the state-of-art  methods on a rich variety of outlier detection benchmark datasets.", "pdf": "/pdf/5cc16b174cf3873400642efe211c20a5e07b31f9.pdf", "paperhash": "gerych|versatile_anomaly_detection_with_outlier_preserving_distribution_mapping_autoencoders", "original_pdf": "/attachment/90fe635406305959cbe10ec548896676e9e2d0c0.pdf", "_bibtex": "@misc{\ngerych2020versatile,\ntitle={Versatile Anomaly Detection with Outlier Preserving Distribution Mapping Autoencoders},\nauthor={Walter Gerych and Elke Rundensteiner and Emmanuel Agu},\nyear={2020},\nurl={https://openreview.net/forum?id=SkxV0RVYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkxV0RVYDH", "replyto": "SkxV0RVYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1427/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575877651084, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1427/Reviewers"], "noninvitees": [], "tcdate": 1570237737554, "tmdate": 1575877651098, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1427/-/Official_Review"}}}], "count": 8}