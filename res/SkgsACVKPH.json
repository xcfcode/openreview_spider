{"notes": [{"id": "SkgsACVKPH", "original": "BJxgJq5_wH", "number": 1443, "cdate": 1569439442953, "ddate": null, "tcdate": 1569439442953, "tmdate": 1583912048820, "tddate": null, "forum": "SkgsACVKPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "Vb-xRUGnoP", "original": null, "number": 1, "cdate": 1576798723416, "ddate": null, "tcdate": 1576798723416, "tmdate": 1576800913126, "tddate": null, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes a method to improve the training of sparse network by ensuring the gradient is preserved at initialization. The reviewers found that the approach was well motivated and well explained. The experimental evaluation considers challenging benchmarks such as Imagenet and includes strong baselines. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795703826, "tmdate": 1576800251282, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Decision"}}}, {"id": "HkgFzZMcFB", "original": null, "number": 1, "cdate": 1571590417202, "ddate": null, "tcdate": 1571590417202, "tmdate": 1574094413061, "tddate": null, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "\nThe paper proposes a new prunning criterion that performs better than Single-shot Network Pruning (SNIP) in prunning a network at the initalization. This is an important and potentially very impactful research direction, The key idea is to optimize the mask for the loss decrease after an infinimitesal step, rather than for the preservation of loss after prunning. While with the benefit of hindsights it might seem simple, it is a clever innovation. However, I am not convinced by the theoretical explanation and some of the experimental results (see detailed comment below). Based on this I am leaning at the moment towards rejecting the paper. I will be happy to revisit my score if these concerns are addressed.\n\nDetailed comments:\n\n1. I am not sure that NTK based analysis helps explain the efficacy of the method. An increase of the (matrix) norm of the NTK kernel can be achieved by simply scaling up by a constant scalar the logits weights (see for instance https://arxiv.org/abs/1901.08244). Or equivalently (comparing the resulting learning dynamics in NTK, as also can be read from (3)), by just increasing the learning rate. In other words, I could just prune weights randomly, and then scale up logits' weights, and end up with the same effect on the NTK kernel. I think that for this argument to work, NTK kernel should change in a scale-invariant manner. This would correspond to a better conditioning of the loss surface (because Hessian has the same eigenspectrum as the NTK kernel under the NTK assumption), which is a scale invariant property.\n\n2. From the Figure 2 it seems SNIP-prunned network underfits data severly. Could you add training accuracy to the Tables (maybe in the Supplement)? If in all cases when GraSP wins, it is due to underfitting, this should be commented on. Is it common for prunning algorithms to result in underfitting, or is achieving generalization a larger challenge? Could the bad performance at high prunning ratios of SNIP be due to a conflation of two effects: (1) \"good\" prunning, but (2) lowering the effective learning rate (given the gradient norm is low)? Would, for high prunning ratios, a tuned learning rate improve SNIP performance/reduce underfitting? \n\n3. In Table 5 is the batch-size used for training of the network, or only for the computation of the Hessian-vector product in the GraSP procedure? If for training, then the relatively small spread of results is a bit surprising given results by Keskar (https://arxiv.org/abs/1609.04836)\n\nEdit\n\nThank you for the rebuttal. Raise my score. I agree with Reviewer #4 that increasing gradient norm at initialization is a promising direction on its own, which warrants acceptance.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575576685225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Reviewers"], "noninvitees": [], "tcdate": 1570237737318, "tmdate": 1575576685237, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Review"}}}, {"id": "BJlsJthO5S", "original": null, "number": 3, "cdate": 1572550882844, "ddate": null, "tcdate": 1572550882844, "tmdate": 1574090419980, "tddate": null, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "This paper proposes a novel one-shot-pruning algorithm which improves the training of sparse networks by maximizing the norm of the gradient at initialization. The utility of training sparse neural networks and shortcomings of dense-to-sparse algorithms like Pruning, LotteryTicket are nicely motivated at introduction. The pruning criterion is motivated by the first order approximation of the change in the gradient norm when a single connection is removed, though the results show that removing many connections together with GraSP increases the total gradient norm therefore allowing the loss to decrease faster. Experiments suggest employing such pruning algorithm improves final performance over two baselines: random and SNIP. \n\nThough I find the proposed method intriguing and well motivated, experiments section of the paper misses some important sparse training baselines and needs some improvement. I am willing to increase my score given my concerns/questions below are addressed. \n\n(1) The paper doesn't mention some important prior work on the topic. Since the paper focuses on end-to-end sparse training, the following sparse training methods needs to be considered and compared with:\n- Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science [Mocanu, 2018]\n- Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization [Mostafa, 2019]\n- Deep Rewiring: Training very sparse deep networks [Bellec, 2017] \n- There is also few recent work submitted to ICLR2020: https://openreview.net/forum?id=SJlbGJrtDB, https://openreview.net/forum?id=ryg7vA4tPB, https://openreview.net/forum?id=ByeSYa4KPS\n\n(2) Pruning baselines can be improved. I am not convinced that they represent the best achievable sparse training results. I would recommend method proposed by `Prune or Not to Prune` as a strong baseline. You can also check `State of Sparsity` paper to obtain some competitive pruning results.\n\n(3) It's great that the authors are aware of the importance of having experiments on larger datasets. Though, I found the results reported on Imagenet to be limited. Is there a reason why Imagenet-2012 results are missing pruning baselines? I think having other reported pruning results here along with performance of other sparse training methods (SET, DSR) would be useful. Most of these numbers should be readily available in the papers mentioned above, but I guess it is always better to run them using the same settings. \n\n(4) To demonstrate the usefulness of the pruning criteria proposed, it would be nice to do some simple ablations. Some suggestions: (1) Remove weights that would *decrease* the gradient norm most (2) Do random pruning while preserving exact per layer sparsity fractions. (3) sweep over batch size used to calculate the importance scores and evaluate final accuracies or the initial gradient norm. The second experiment would help identifying whether the gains are due to better allocation of sparsities across layers or due to increased gradient norm. Looking at Figure-4 and seeing the per layer sparsities are different, It is not clear to me which one is the underlying reason for improved performance.\n\n(5) (Page 8 / Table 5) Do you aggregate all accuracies in Table 5 using different batch sizes and initialization methods? If, so I am not sure what the intended message here is, since it is difficult to infer how these hyper-parameters affect the result. Do you sweep different batch sizes for estimating importance of units, too? It would be nice to see whether the two batch sizes interact with each other and/or how increased batch size affects the quality of pruned networks. \n\nSome minor comments:\n(a) (Page 1) I found the motivation very intriguing. Though the statement `Recently, F&C (2019) shed light on this...` seems a bit off, given that LT can't find solutions as well as the pruning solution in most practical (larger datasets and architectures) settings. Therefore I would be better to pose this as an `open problem`. \n\n(b) (end of page-1) `However, connection sensitivity is sub-optimal as a criterion because the gradient of each weight might change dramatically after pruning due to complicated interactions between weights`. I think this is still the case for GraSP. Since the criterion it uses assumes independence (i.e. what if we remove a single weight?). It would be nice to see some ablations on this. Does `K=number of weights removed` affect the norm of the sparsified networks?\n\n(c) (Figure 1) I find the comparative illustration between SNIP and GraSP very useful. Though, the architecture presented seems a bit artificial (i.e. I am not aware of any architecture with single hidden layer and a single output unit). I think the same motivation can be made by removing the top unit (therefore having 6-4-1 units) and removing all incoming connections for the output unit until a single connection remains. Then SNIP would remove that single connection whereas GraSP would remove one of the connections in the previous layer.\n\n(d) (Section 2.1) `In contrast, Hessian based algorithms...` Though it is a structured pruning algorithm It might be nice to include the following work, https://arxiv.org/abs/1611.06440. \n\n(e) (Section 2.1) Previous work needs following citations:  [Bellec, 2017], [Mocanu, 2018] and [Mostafa, 2019] \n\n(f) (Section 2.2) Why the initial dynamics affect the final performance? One explanation given in the paper is through recent work on NTK and this is great. Though training settings used at `Lee et al (2019a)` and in the paper are a bit different. Usage of MSE, small datasets, etc\u2026 So it might be nice to point out differences. \n\n(g) (Section 3) At $D = {(x_i, y_i)}_{i=1}^n$, `n`->`N` \n\n(h) (Page 4) `Preserving the loss value motivated several\u2026` -> `motivated by several\u2026`\nI think it is better to use existing terminology whenever available.I think using `One-shot pruning` instead of `Foresight pruning` would be a better choice and would prevent confusion. \n\n(j) (Page 5) `However, it has been observed that different weights are highly coupled \u2026` This has been observed much earlier, too: like in Hassibi, 1993. \n\n(k) (Page 7) Last sentence `and thus hopefully..`: needs to be fixed.\n\n(l) (Page 8) The whole page needs some proof-reading. Some of them: (a) `SNIP and GraSP. We present...` probably connect with comma (b) `aims for preserving` -> `aims to preserve` (c) `In contrast, SNIP are more` `are`->`is` (d) `for ablation study` -> `as an ablation study`... \n\n(m) Is there a specific reason why VGG networks are preferred for experiments? I don't think they are relevant to any practical application anymore and they are massively over-parameterized for tasks in hand. Specifically for Cifar-10. I think focusing on more recent networks and larger datasets would increase the impact of the work. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575576685225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Reviewers"], "noninvitees": [], "tcdate": 1570237737318, "tmdate": 1575576685237, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Review"}}}, {"id": "rJeE_952sS", "original": null, "number": 12, "cdate": 1573853803701, "ddate": null, "tcdate": 1573853803701, "tmdate": 1573853903990, "tddate": null, "forum": "SkgsACVKPH", "replyto": "HJxYY6OhsH", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "We've updated the paper.", "comment": "We've updated the paper to include three DST baselines and separate DST methods from *Pruning during training* ones in related works. However, we haven't finished the experiments of DeepR on Tiny-ImageNet. We will add it to our camera-ready version if our paper gets accepted.\n\nWe thank again for your detailed and constructive comments. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "HyeCnTKnsH", "original": null, "number": 10, "cdate": 1573850549803, "ddate": null, "tcdate": 1573850549803, "tmdate": 1573852749015, "tddate": null, "forum": "SkgsACVKPH", "replyto": "HJxYY6OhsH", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Quick response", "comment": "Thank you for your reply. We sincerely appreciate your valuable comments.\n\nThank you for correcting us in terms of DST algorithms. We will take your suggestions in our new revision.\n\n(A) Currently, we\u2019re not aware of such guarantee. But if the sparsity pattern does not change during training, this might ease the difficulty of designing dedicated hardware for achieve accelerations.\n\n(B) It seems that there is no GPU acceleration library for sparse tensors  (Dettmers & Zettlemoyer, 2019). However, we cannot exclude the possibility of it in the future. CPU is in general not capable of dealing with massive computations, and even for sparse networks, it is still requires a large amount of computations.\n\n(C) We\u2019re now working on a new revision to include those results and will try our best to update to openreview before the rebuttal deadline. \n\nFinally, we\u2019d like to note that our single-shot pruning algorithm may be of independent interest for deep learning theory community. Lottery ticket hypothesis (they showed that there exist winning tickets at the initialization) paper opened a new line of research in understanding neural network dynamics. However, in the original lottery ticket hypothesis paper, they had to use *pruning after training* method to identify the ticket. Our paper is trying to show that we are able to identify them before training and push it to the limit. We believe it may have some inspirations for other researchers in understanding deep learning and neural network training.\n\nWe also believe our pruning criteria has some deep connections with generalization performance of neural networks. Particularly, large gradient norm indicates big stiffness/gradient confusion (assuming we don't change the scale, see the following references), which seems to correlate with good generalization performance across different tasks.\n\nhttps://arxiv.org/pdf/1907.07287.pdf\nhttps://arxiv.org/abs/1901.09491\nhttps://openreview.net/pdf?id=ryeFY0EFwS\n\n\n------------------------------------------------------------------------------------------------------------------------------\nTim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losingperformance.arXiv preprint arXiv:1907.04840, 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "HyezXScnsH", "original": null, "number": 11, "cdate": 1573852441718, "ddate": null, "tcdate": 1573852441718, "tmdate": 1573852500591, "tddate": null, "forum": "SkgsACVKPH", "replyto": "ByxUSu5soH", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Further Response", "comment": "Thank you for your reply and continuing effort to provide constructive feedback until the end of the response/discussion phase.\n\n1) Because GraSP shouldn\u2019t increase the scale, so increasing the gradient norm is more likely to be achieved by aligning larger eigenvalues of the NTK with the target. We strongly agree with the reviewer that large gradient norm might not be desirable, but it can typically be handled with small learning rate. If the reviewer is comfortable with our statement about NTK, we will modify it in our camera-ready version if our paper gets accepted.\n\n2) The goal of pruning is to reduce the model size while with minimal loss in test accuracy, it does not matter the resulting model overfits or underfits the training data. Therefore, the common strategy for comparing pruning algorithms is to compare the trade-off between model size and test accuracy achieved by each algorithm. Also, underfitting is common for high pruning ratios, but different pruning algorithms always result in drastically different empirical performance, and usually better algorithm will achieve better performance. For example, algorithms such as  DSR, SET and DeepR, they all underfit the training data for pruning ratio 98% on CIFAR10 with ResNet32, but DSR can achieve significantly better results than the other two. We totally agree we can design heuristics to improve all of these pruning algorithms.\n\n-------------------------------------------------------------------------------------------------------------------------------------\n\nFinally, we argue that single-shot pruning is promising and offers a new way to speed up network training and inference. This area is new but we believe it will be a very impactful research direction. \nMore importantly, unlike other traditional pruning algorithms, it has a deep connection with neural network training dynamics and our work may be of independent interest for deep learning theory community. Particularly, large gradient norm indicates big stiffness/gradient confusion (assuming we don't change the scale, see the following references), which seems to correlate with good generalization performance across different tasks.\n\nhttps://arxiv.org/pdf/1907.07287.pdf\nhttps://arxiv.org/abs/1901.09491\nhttps://openreview.net/pdf?id=ryeFY0EFwS\n\nAnother potential but promising application of single-shot pruning is to select a big winning ticket (with similar size of standard neural networks) from a gigantic network which cannot fit in our hardware for training. As shown by recent papers in deep learning theory, over-parameterization leads to better generalization, therefore the winning ticket from a gigantic network may perform better than standard neural networks of same size."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "HJxYY6OhsH", "original": null, "number": 9, "cdate": 1573846400999, "ddate": null, "tcdate": 1573846400999, "tmdate": 1573846619105, "tddate": null, "forum": "SkgsACVKPH", "replyto": "BJlsJthO5S", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Few Clarifications Needed", "comment": "Naming Dynamic Sparse Training methods as 'Pruning during Training' can be misleading. Since most of the pruning algorithms start with a dense network and prune the networks during training. I would suggest naming those methods as 'Dynamic Sparse Training'(DST) methods. These methods start with a predefined sparsity, but unlike `One Shot pruning` algorithms, they do change the connectivity of layers during training. This has been shown to improve performance over keeping the connectivity static.  \n\n\"Our goal is to propose an improved pruning algorithm which can conduct pruning before training. In this sense, we don\u2019t think those sparse training baselines relevant enough. \" \n\nI disagree with this. Methods come with the problem they address. DST algorithms are very relevant to your algorithm, since they attack the same problem: *Training sparse neural networks* with the same goal: *Reducing training cost*. They would benefit same improvements in terms of FLOPs as GrasP since they do start from a *predefined sparsity* (term taken from Dey et.al. 2019).\n\nI think the comparison you provided for DST and One-shot-pruning algorithms is a great start. Dey et.al.'s paper is very interesting. They talk about hardware friendly, clash-free, sparse connectivity patterns and show that such patterns perform just as good as any random pattern (static training). There is no guarantee that GrasP would find such clash-free connectivity. Please correct me if I am wrong (A). I am not an expert on hardware, but it is also not obvious to me that FPGA's would be the choice of hardware for training in the future(it is currently not). What about CPU, GPU acceleration of sparse neural network training? (B)\n\nThanks for running extra experiments. I appreciate your work. Results show that GrasP performs worse than some of the DST methods (which is fine). As discussed by the authors there might be some settings where static sparsity is preferred (still not sure about this) and not all papers should get SOTA for the problem they attack. However, they should make a fair comparison with relevant methods.\n\nI don't see any of comparisons and numbers provided below in the revised paper. Are the authors planning adding those results in the next version?  Similarly I believe experiments done for '(4) Usefulness of the pruning criteria' would be a great addition to your work. (C) \n\nA quick response to questions (A), (B), (C) would be helpful. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "ByxUSu5soH", "original": null, "number": 8, "cdate": 1573787709645, "ddate": null, "tcdate": 1573787709645, "tmdate": 1573787709645, "tddate": null, "forum": "SkgsACVKPH", "replyto": "SJg-rWYfiB", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Thank you for your rebuttal", "comment": "Thank you for your rebuttal. \n\nAd 1) I agree that GraSP shouldn't increase the scale. But my argument was actually mainly just to highlight a weird implication of the theory. The theoretical argument is equivalent to saying that large gradient norm is desirable, but this is clearly not true generally speaking; gradient explosion is not desirable in the general case. \n\nI think the argument should be constructed around a standard metric in optimization such as conditioning of the Hessian. The norm of the NTK kernel is clearly not a standard way to argue for an optimization benefit. Maybe GraSP doesn't change the scale, but reduces gradient confusion, or other related metric? Or maybe it improves conditioning of the NTK kernel? Either argument would be more convincing from the optimization perspective.\n\nAd 2) Thank you for checking different learning rates. \n\nIf the primary reason that GraSP outperforms SNP at high prunning ratios is that SNP underfits, I am not sure this is a novel enough contribution. Perhaps there are some simple heuristics that would reduce change that SNP picks some critical connections? \n\nI am not an expert in the field. In the case Reviewer #4 thinks that this is a strong enough contribution, I would be OK accepting the submission."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "S1xyP07isS", "original": null, "number": 7, "cdate": 1573760599373, "ddate": null, "tcdate": 1573760599373, "tmdate": 1573760599373, "tddate": null, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "To all reviewers", "comment": "We thank all reviewers again for your detailed comments and constructive suggestions.\n\nWe've updated our paper and responded to you. We really sorry for the late response to reviewer #4, it took many days for us to run experiments you requested. We hope that our responses address your concerns. If so, it would be great if you can update your review and rating. But if not, we're open to answer more questions and further improve our paper"}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "BJlWGFWsjr", "original": null, "number": 4, "cdate": 1573751048822, "ddate": null, "tcdate": 1573751048822, "tmdate": 1573759139619, "tddate": null, "forum": "SkgsACVKPH", "replyto": "BJlsJthO5S", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Response to reviewer #4 [3/4]", "comment": "# ResNet32 on TinyImageNet\n+-----------+----------------+----------------+----------------+\n|   Ratio  |        85%      |        90%      |       95%      | \n+-----------+----------------+----------------+----------------+\n|    DSR   |      57.08      |      57.19     |       56.08     | \n+-----------+----------------+----------------+----------------+\n|     SET   |      57.02      |      56.92     |       56.18     | \n+-----------+----------------+----------------+----------------+\n|  DeepR |      53.29      |      52.62     |       52.00     | \n+-----------+----------------+----------------+----------------+\n|  Grasp  |  57.25(0.1)  | 55.53(0.1)  |  51.34(0.3)  |\n+-----------+----------------+----------------+----------------+\n\n# VGG19 on TinyImageNet\n+-----------+----------------+----------------+----------------+\n|   Ratio  |        90%      |        95%      |       98%      | \n+-----------+----------------+----------------+----------------+\n|    DSR   |      62.43      |      59.81     |       58.36     | \n+-----------+----------------+----------------+----------------+\n|     SET   |      62.49      |      59.42     |       56.22     | \n+-----------+----------------+----------------+----------------+\n|  Grasp  |  60.76(0.2)  | 59.50(0.3)  |  57.28(0.3) |\n+-----------+----------------+----------------+----------------+\n(Note: DeepR is missing in the above table because it is extremely slow. We will complete once the experiments are finished.)\n\nFurther discussions for the above results:\n\nWe can observe that DSR is the best performing one, and GraSP is quite competitive. In particular, GraSP performs much better than DeepR in most settings, and can outperform SET in more than half of the settings. Furthermore, we would also argue that GraSP has several advantages over these three \u2018Pruning during Training\u2019 methods:\n\n - Simplicity and easy-to-use: GraSP is much simpler than DSR, SET and DeepR, as it only needs to conduct pruning prior to training in a single-shot, and it does not change the sparsity dynamically during training. Moreover, there is almost no hyperparameters to tune for GraSP in comparisons with DSR, SET and DeepR. (i.e. DeepR is extremely slow and not scalable; SET requires manually specified pruning ratio for each layer; DSR requires some specific layers to be dense.)\n\n - Efficiency: GraSP can enjoy training acceleration (5x) by optimization in the hardware level (i.e. mapping the network topology structure to circuits or FPGA pre-programmed wiring). As we mentioned in [A], Dey et al. (2019) show that  with **pre-specified** sparsity, the training can be accelerated by 5x. However, for dynamic sparse training (DSR, SET and DeepR), they need to change the sparse mask during training and thus cannot be optimized in the hardware level, and there is no GPU-accelerated libraries that utilize sparse tensor exist (Dettmers & Zettlemoyer 2019). Also it is almost impractical to optimize the training efficiency of dynamic sparse method in the hardware level, because their topology will change during training and recompile the FPGA program or changing the circuits will make the training even slower.  \n\nIn general, we will not be surprised if DSR, SET, DeepR and other \u2018Pruning during Training\u2019 methods outperform \u2018Pruning before Training\u2019 methods, as they can change the sparsity during training dynamically and thus enjoy more flexibility than GraSP and SNIP. But they also have the disadvantages as we stated in A.a, A.b and the previous paragraph. \n\nThe baselines contained in \u2018State of the sparsity \u2019 are all in the category of \u2018Pruning after Training\u2019, and they all require a pretrained network, which does not save the training cost and cannot be directly compared with GraSP. In our paper, these methods are only used to serve as an upper bound for \u2018Pruning before Training\u2019 methods, and we already included two of them, OBD and MLPrune. Moreover, in the \u2018State of the sparsity\u2019 paper (Gale et al., 2019), they did not include Hessian-based pruning algorithms, such as OBD and MLPrune, into their comparisons with magnitude pruning. We also find that in another ICLR submission ( https://openreview.net/pdf?id=ryl3ygHYDB ), they have demonstrated that OBD can significantly outperform magnitude pruning (See Table 19 and Table 20), which is the best performing one in the \u2018State of the sparsity\u2019 paper. Besides, we would also argue that SNIP is the most related, state-of-the-art baseline for pruning network prior to training. \n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nGale, Trevor, Erich Elsen, and Sara Hooker. \"The state of sparsity in deep neural networks.\" arXiv preprint arXiv:1902.09574 (2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "BJl1D6WiiB", "original": null, "number": 6, "cdate": 1573752151275, "ddate": null, "tcdate": 1573752151275, "tmdate": 1573759046795, "tddate": null, "forum": "SkgsACVKPH", "replyto": "BJlsJthO5S", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Response to reviewer #4 [1/4]", "comment": "Thanks for your detailed reviews. We really appreciate your time for reviewing our paper carefully.\n\nBefore moving on to answer your questions and comments, we\u2019d like to first clarify the focus of this work. Our goal is to propose an improved pruning algorithm which can conduct pruning before training. In this sense, we don\u2019t think those sparse training baselines relevant enough. To the best of our knowledge, the only existing baseline is SNIP (Lee et al., 2019). Other algorithms such as OBD (LeCun et al., 1990) and MLPrune (Zeng & Urtasun, 2019) in our paper are serving as upper bound for single-shot pruning algorithm and the main reason to include them is to inform readers of the gap between pruning before and after training. To address your concerns, we run experiments with the methods you mentioned. However, we note again that those methods only serve as the upper bound and are not really \u201cimportant\u201d baselines.\n\n[A] Regarding the sparse training baselines you mentioned, we would like to clarify the difference between \u2018Pruning before Training\u2019 and \u2018Pruning during Training\u2019.\n- (a) \u2018Pruning during Training\u2019 methods, such as SET (Mocanu et al., 2018) and DSR (Mostafa & Wang, 2019), need to redistribute the weights during training by different heuristics. As shown in Dey et al. (2019), with *pre-defined sparsity*, the training can be accelerated by 5X, and the speedup performance can be optimized in the hardware level by programming the sparse structure using FPGA in advance of training. However, SET and DSR need to change the sparse structures during the whole training process, and thus it is unclear for those methods to enjoy the potential acceleration from hardwares. \n- (b) \u2018Pruning during Training\u2019 methods change the standard training procedure because they need to redistribute the weights during training, which makes it more complicated than \u2018Pruning before Training\u2019.\n- (c) \u2018Pruning during Training\u2019 methods enjoy more flexibility than \u2018Pruning before training\u2019 as they have the freedom to change the sparsity during training. However, this results in the problem of (a) and (b).\nFor other detailed differences, please refer to the ICLR2020 submission at: https://openreview.net/pdf?id=SJem8lSFwB\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n- Lee, Namhoon, Thalaiyasingam Ajanthan, and Philip HS Torr. \"Snip: Single-shot network pruning based on connection sensitivity.\" ICLR 2019.\n- LeCun, Yann, John S. Denker, and Sara A. Solla. \"Optimal brain damage.\" Advances in neural information processing systems. 1990.\n- Zeng, W. and Urtasun, R. MLPrune: Multi-layer pruning for automated neural network compression, 2019. URL https://openreview.net/forum? id=r1g5b2RcKm.\n- Mocanu, Decebal Constantin, et al. \"Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science.\" Nature communications 9.1 (2018): 2383.\n- Mostafa, Hesham, and Xin Wang. \"Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization.\" ICML 2019.\n-Sourya Dey, Kuan-Wen Huang, Peter A Beerel, and Keith M Chugg. \"Pre-defined sparse neural networks with hardware acceleration.\" IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "HJxKfTbisB", "original": null, "number": 5, "cdate": 1573752080661, "ddate": null, "tcdate": 1573752080661, "tmdate": 1573759016162, "tddate": null, "forum": "SkgsACVKPH", "replyto": "BJlsJthO5S", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Response to reviewer #4 [2/4]", "comment": "(1-2) Include more baselines. (DSR, SET, DeepR ); Pruning baselines can be improved. I am not convinced that they represent the best achievable sparse training results.\n\nWe really thank the reviewer for pointing out these sparse training papers and related concurrent submissions. We have updated the paper to include them in the section of related works. We also agree with the reviewer that including those sparse training methods, DSR, SET and DeepR, will greatly improve the experiments section. Therefore, we adopt the public implementation from https://github.com/IntelAI/dynamic-reparameterization for the experiments with DSR, SET and DeepR (Bellec et al., 2018). Specifically, we test them on three datasets (CIFAR-10, CIFAR-100 and TinyImageNet) with two networks (VGG19 and ResNet32). The results are presented in the following:\n\n# ResNet32 on CIFAR10\n+-----------+----------------+----------------+----------------+\n|   Ratio  |        90%      |        95%      |       98%      | \n+-----------+----------------+----------------+----------------+\n|    DSR   |      92.97      |      91.61     |       88.46     | \n+-----------+----------------+----------------+----------------+\n|     SET   |      92.30      |      90.76     |       88.29     | \n+-----------+----------------+----------------+----------------+\n|  DeepR |      91.62      |      89.84     |       86.45     | \n+-----------+----------------+----------------+----------------+\n|  Grasp  |  92.38(0.2)  | 91.39(0.3)  |  88.81(0.1)  |\n+-----------+----------------+----------------+----------------+\n\n# ResNet32 on CIFAR100\n+-----------+----------------+----------------+----------------+\n|   Ratio  |        90%      |        95%      |       98%      | \n+-----------+----------------+----------------+----------------+\n|    DSR   |      69.63      |      68.20     |       61.24     | \n+-----------+----------------+----------------+----------------+\n|     SET   |      69.66      |      67.41     |       62.25     | \n+-----------+----------------+----------------+----------------+\n|  DeepR |      66.78     |      63.90     |       58.47     | \n+-----------+----------------+----------------+----------------+\n|  Grasp  |  69.24(0.2)  | 66.50(0.1)  |  58.43(0.4) |\n+-----------+----------------+----------------+----------------+\n\n# VGG19 on CIFAR10\n+-----------+----------------+----------------+----------------+\n|   Ratio  |        90%      |        95%      |       98%      | \n+-----------+----------------+----------------+----------------+\n|    DSR   |      93.75      |      93.86     |       93.13     | \n+-----------+----------------+----------------+----------------+\n|     SET   |      92.46      |      91.73     |       89.18     | \n+-----------+----------------+----------------+----------------+\n|  DeepR |      90.81      |      89.59     |       86.77     | \n+-----------+----------------+----------------+----------------+\n|  Grasp  |  93.30(0.1)  | 93.04(0.2)  |  92.19(0.1)  |\n+-----------+----------------+----------------+----------------+\n\n# VGG19 on CIFAR100\n+-----------+----------------+----------------+----------------+\n|   Ratio  |        90%      |        95%      |       98%      | \n+-----------+----------------+----------------+----------------+\n|    DSR   |      72.31      |      71.98     |       70.70     | \n+-----------+----------------+----------------+----------------+\n|     SET   |      72.36      |      69.81     |       65.94     | \n+-----------+----------------+----------------+----------------+\n|  DeepR |      66.83     |      63.46     |       59.58     | \n+-----------+----------------+----------------+----------------+\n|  Grasp  |  71.95(0.2)  | 71.23(0.1)  |  68.90(0.4) |\n+-----------+----------------+----------------+----------------+\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n- Guillaume Bellec , David Kappel, Wolfgang Maass, and Robert Legenstein\"Deep Rewiring: Training very sparse deep networks.\" ICLR 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "Syen2_ZjiS", "original": null, "number": 3, "cdate": 1573750964328, "ddate": null, "tcdate": 1573750964328, "tmdate": 1573750964328, "tddate": null, "forum": "SkgsACVKPH", "replyto": "BJlsJthO5S", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Response to reviewer #4 [4/4]", "comment": "(3) ImageNet baselines;\n\nThank you for your kind words, we strongly agree with you that large scale experiments are important and necessary. Our purpose of ImageNet experiments is only for showing that GraSP can beat SNIP consistently even on more challenging and larger datasets. As we mentioned in the beginning of our response, we think the only baseline of single-shot pruning is SNIP. Therefore, we did not include other baselines in this experiment. We agree that including more baselines will make our empirical results stronger, but it won\u2019t change our conclusion that GraSP is better than SNIP.  To have a sense, we provide a rough comparison between the results of SET, DSR, Deep-R and GraSP on ImageNet with ResNet50 referred from their original papers: \n\n+\u2014\u2014\u2014---+\u2014\u2014\u2014\u2014\u2014\u2014+\u2014\u2014\u2014\u2014\u2014\u2014+\u2014\u2014\u2014\u2014\u2014\u2014--+\u2014\u2014\u2014-\u2014\u2014--+\n|   Model   |       SET         |        DSR        |      Deep-R      |      GraSP      |\n+\u2014\u2014\u2014---+\u2014\u2014\u2014\u2014\u2014\u2014+\u2014\u2014\u2014\u2014\u2014\u2014+\u2014\u2014\u2014\u2014\u2014\u2014--+\u2014\u2014\u2014\u2014\u2014---+\n|   80%     |        72.6        |        73.3        |         71.7         |       72.06       |\n+\u2014\u2014\u2014---+\u2014\u2014\u2014\u2014\u2014\u2014+\u2014\u2014\u2014\u2014\u2014\u2014+\u2014\u2014\u2014\u2014\u2014\u2014--+\u2014\u2014\u2014\u2014\u2014---+\n|   90%     |        70.4        |        71.6        |         70.2         |       68.14       |\n+\u2014\u2014\u2014---+\u2014\u2014\u2014\u2014\u2014\u2014+\u2014\u2014\u2014\u2014\u2014\u2014+\u2014\u2014\u2014\u2014\u2014\u2014--+\u2014\u2014\u2014\u2014\u2014---+\nWe can observe that GraSP is still quite competitive in this setting, and it outperforms DeepR at the pruning ratio of 80%, though GraSP is a single-shot pruning algorithm. It is very encouraging that single-shot pruning algorithm can perform as competitively as other \u2018Pruning during Training\u2019 methods.  \n\n\n(4) Usefulness of the pruning criteria.\n\nWe really thank reviewer for proposing some interesting ablation studies. (1) For reducing gradient norm, we found that it will result in disconnected networks for high pruning ratios, and thus correspondingly performs much worse. (2) For \u2018random pruning\u2019, we adopt the sparsity allocation identified by GraSP and then shuffle the sparse mask. We found that for low pruning ratios, shuffling the mask does not degrade the performance much, while for high pruning ratios, i.e., 98%, 99%, it will degrade the performance a lot. We conjecture that for low pruning ratios, the pruned network is still moderately over-parameterized, and thus the shuffling operation will not affect the performance much. Apart from these ablation study, we believe that the best way for showing the usefulness of a pruning criteria is the empirical results in terms of pruning-ratio vs. Test accuracy. \n\n(5) (Page 8 / Table 5) Do you aggregate all accuracies in Table 5 using different batch sizes and initialization methods? \n\nYes, they are averaged over multiple runs. The purpose of them is for sensitivity analysis, so as to show that our pruning criteria is not sensitive to different batch sizes and initialization schemes. \n\n- Response to minor comments.\n\nWe've updated our paper to incorporate your suggestions on writing and citations. In terms of the reason we reported the results of VGG networks, our main purpose is to simulate the case of feedforward networks without skip-connections (we also reported results on ResNet (i.e. with skip-connections) in our paper). We agree that experimenting with more recent networks is good, but we should avoid doing duplicated experiments. \n\nWe really appreciate your valuable comments, and careful assessment of our work. We hope our response can address your concerns well, and if you have any further concerns/questions/suggestions, please let us know!\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "S1eT_9uVjr", "original": null, "number": 2, "cdate": 1573321332561, "ddate": null, "tcdate": 1573321332561, "tmdate": 1573355392582, "tddate": null, "forum": "SkgsACVKPH", "replyto": "S1e9hXy0FS", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Response to reviewer #1", "comment": "Thanks for your detailed comments, and in particular for your valuable suggestions on improving the writing. We've updated our paper to incorporate your suggestions. \n\nResponses to questions/comments:\n\t\n(1) In paragraph below Equation (8): what does \"can be computed by backward twice\" mean?\n\n\u201cBackward twice\u201d means that we first compute the gradient with respect to the weights as $\\mathbf{g} = \\partial \\mathcal{L}/\\partial \\mathbf{\\theta}$ (the first backward), and then we compute the Hessian vector product by simply computing $\\mathbf{Hv} = \\partial (\\mathbf{g}^\\top \\mathbf{v})/\\partial \\mathbf{\\theta}$ (the second backward, and we only differentiate through $\\mathbf{g}$). By doing so, we do not need to compute the Hessian explicitly. \n\t\n(2) Please specify where the equalities in equation (9) are coming from.\n\nFirst of all, $\\nabla \\mathcal{ L}(\\mathbf{\\theta}) =  \\nabla_\\mathbf{\\theta} \\mathcal{Z}^\\top \\nabla_\\mathcal{Z} \\mathcal{L}$, where $\\mathcal{Z}$ is defined in sec 2.2, page 3. Then we can rewrite $\\nabla \\mathcal{L}(\\mathbf{\\theta})^\\top \\nabla \\mathcal{L}(\\mathbf{\\theta})$ as the second term in equation (9). As we reviewed in sec 2.2 (the paragraph below equation (3)), we can decompose the NTK $\\Theta$ as $\\sum_{i=1}^n\\lambda_i\\mathbf{u}_i\\mathbf{u_i}^\\top$, and plug it in the equation (9) can show the equality.\n\n(3) Table 3 & 4: Why are the pruning ratios different for each model?\n\nThe choice of pruning ratios depends on the specific dataset and base network. For ImageNet, we cannot prune as extreme as on Tiny-ImageNet, otherwise the performance of the pruned network will degrade too much and making the comparisons not meaningful. For ResNet32, it is already much more compact than VGG19, so we need to use smaller pruning ratios for ensuring the comparisons are meaningful. \n\n(4) Table 3: Why are values missing for the baseline for 80% and 90%?\n\nIt\u2019s not missing. The baseline is the unpruned network (pruning ratio = 0%), rather the sub-network corresponding to pruning ratios of 60%, 80% or 90%.\n\n(5) Section 5.2: \"We observed that, the main bottleneck or pruned... when deriving the pruning criteria\": it's not clear where this conclusion is coming from.\n\nThe observation comes from Figure 2. We can see that the training error of SNIP-pruned network is far away from 0, which means it cannot fit the training data well, i.e. underfitting.\n\n\nWe hope our response can address your concerns well. If you have any further questions or concerns, please let us know!"}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "SJg-rWYfiB", "original": null, "number": 1, "cdate": 1573191993263, "ddate": null, "tcdate": 1573191993263, "tmdate": 1573250013146, "tddate": null, "forum": "SkgsACVKPH", "replyto": "HkgFzZMcFB", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment", "content": {"title": "Response to reviewer #2", "comment": "Thank you for your detailed comments! It\u2019s really encouraging that you think the research direction we\u2019re working on is important. \n\nIn terms of your concerns, we address them one by one below.\n\n(1) We agree that scaling up the logits weights leads to the same effect on the NTK. Nevertheless, we would like to argue that pruning itself might not have the flexibility to change the scale since it only involves the operation of removing weights. From this perspective, we believe that our algorithm is to align labels with NTK eigenspectrum rather than changing the scale. Indeed, we provide the training loss curve for both SNIP and GraSP in Figure 2, and we can observe that models pruned by GraSP converge much faster than SNIP and also achieve lower training error. To further verify if the difference is caused by using too small a learning rate for SNIP, we conducted experiments with the same setting as in Figure 2, but increased the learning rates for SNIP. We tried learning rates of 0.3, 1.0 and 2.0. The final test accuracies are: \n+------------------------------------------------------------------------+\n|   LR   |           0.3         |           1.0         |           2.0          |\n+------------------------------------------------------------------------+\n|   Acc |  55.5(+/- 1.2)  |  48.7(+/- 1.6)  |  10.95(+/- 6.9) |\n+------------------------------------------------------------------------+\nAll experiments are averaged over three runs. These results show that further performance gain cannot be obtained by simply using larger learning rates.  The corresponding training loss curve can be viewed in https://drive.google.com/file/d/1KUcsGhgj9p1X7rPa7v_0_D5JEWTtVjOR/view . Overall, increasing the learning rate for SNIP does NOT result in better final accuracy or accelerated optimization. \n\n(Minor: Precisely, NTK has the same eigenspectrum as the empirical Fisher matrix rather than the Hessian matrix, though in some cases they are equivalent.)\n\n(2) We have observed that, for large pruning ratios, underfitting is indeed the major problem for pruning algorithms such as SNIP (indicated by the fact that final training error is far away from 0, see Figure 2), because the capacity of the pruned network will be largely affected by the resulting structure, and SNIP will in general result in a bottleneck (prune too many weights) in intermediate layers, whereas it is less severe for GraSP. Besides, we would argue that the bad performance of SNIP for high pruning ratios is not due to a lower effective learning rate based on our results reported in (1) (see above). We didn't observe clear performance improvement by tuning the learning rates for SNIP. Therefore, it\u2019s much more likely that the bad performance of SNIP is due to the pruning strategy induced by the SNIP objective. \n\n(3) No, the batch size is only for the computation of the Hessian vector product in the GraSP. The training procedure is the same as stated in sec 5.1. \n\t\nWe hope our response resolves your concerns well, and if you have any further questions or concerns, please let us know!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgsACVKPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1443/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1443/Authors|ICLR.cc/2020/Conference/Paper1443/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504155937, "tmdate": 1576860546965, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Authors", "ICLR.cc/2020/Conference/Paper1443/Reviewers", "ICLR.cc/2020/Conference/Paper1443/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Comment"}}}, {"id": "S1e9hXy0FS", "original": null, "number": 2, "cdate": 1571840945906, "ddate": null, "tcdate": 1571840945906, "tmdate": 1572972468537, "tddate": null, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "invitation": "ICLR.cc/2020/Conference/Paper1443/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper introduces a method to prune networks at initialization in a way that (mostly) preserves the gradient flow through the resulting pruned network. This is a direct improvement over previous methods (e.g. SNIP) which have no guarantees that pruned connections will break the gradient flow and thereby harm learning.\n\nI quite like this paper, the motivation and results are convincing and it is well presented. The writing is excellent for most of the paper. From section 5 onwards the writing does need quite a bit of editing, as its quality is significantly reduced from what came before.\n\nSome detailed comments:\n- Figure 1 is very nice and really clarifies the idea!\n- In paragraph below Equation (8): what does \"can be computed by backward twice\" mean?\n- Please specify where the equalities in equation (9) are coming from.\n- Table 3 & 4: Why are the pruning ratios different for each model?\n- Table 3: Why are values missing for the baseline for 80% and 90%?\n- Section 5.2: \"We observed that, the main bottleneck or pruned... when deriving the pruning criteria\": it's not clear where this conclusion is coming from.\n- Table 5 has no batch size results, even though you're referencing them in the text.\n\nAnd some minor comments to help with the writing:\n- Intro: \"As shown in Dey et al. (2019) that with pre-specified sparsity, they can achieve\" would read better as \"As shown by Dey et al. (2019), with pre-specified sparsity one can achieve\"\n- Equation (3): Clarify that this is a function of $t$\n- Sentence below Equation (6): \"of the pruned network, and thus our goal\" remove the \"and thus\"\n- Table 1: Specify that you're reporting accuracy.\n- Section 4.1: \"e.g. wide ResNet (Zagaruyko & Komodakis, 2016), and thus we can regard\" remove the \"and thus\"\n- Sentence below equation (9): \"encouraging the eigenspace of \\Theta align\" add a \"to\" before \"align\"\n- Sentence before section 5: \"it will encourage the eigenspace of the NTK distributing large eigenvalues in the direction of Y, which will in turn accelerates the decrease of the loss (Arora et al., 2019) and benefits to the optimization in A\" would read better as \"it will encourage the eigenspace of the NTK to distribute large eigenvalues in the direction of Y, which in turn accelerates the decrease of the loss (Arora et al., 2019) and benefits the optimization in A\"\n- Throughout section 5, write it in present tense rather than past tense. e.g. \"In this section, we conduct various experiments\" instead of \"In this section, we conducted various experiments\"\n- Sentence below table 2: you have \"the the\"\n- Second paragraph of section 5.1: \"We can observe GraSP outperform random pruning clearly\" would read better as \"We can observe GraSP clearly outperforms random pruning\"\n- Second paragraph of section 5.1: \"In the next, we further compared\" remove \"In the next\"\n- Second paragraph of section 5.1: \"Besides, we further experimented with the late resetting\" remove \"Besides\"\n- Paragraph above section 5.2: \"GraSP surpassing SNIP\" use \"surpasses\" instead\n- Paragraph above section 5.2: \"investigate the reasons behind in Section 5.2 for promoting better understanding\" would read better as \"investigate the reasons behind this in Section 5.2 for obtaining a better understanding\"\n- Section 5.2: \"We observed that, the main bottleneck\" -> \"We observe that the main bottleneck\"\n- Section 5.2: \"Besides, we also plotted the the gradient norm of the pruned\", remove \"Besides\" and the extra \"the\"\n- Section 5.2: \"the average of the gradients of the entire dataset\" use \"over the entire dataset\"\n- Section 5.2: \"hopefully more training progress can make as evidenced\" would read better as \"hopefully more training progress can be made as evidenced\"\n- Section 5.3 title would be better using \"Visualizing\" instead of \"Visualize\"\n- Section 5.3: Join the first two sentences with a comma into a single sentence.\n- Section 5.3: \"In contrast, SNIP are more likely\" -> In contrast, SNIP is more likely\"\n- Section 5.4: \"for ablation study\" would read better as \"via ablations\"\n- Section 5.4: \"we tested GraSP with three different initialization methods;\" use a \":\" instead of \";\"\n- Section 6: \"Besides, readers may notice that\", remove the \"Besides\"\n- Section 6: \"traditional pruning algorithms while still enjoy the cheaper training cost. As an evidence,\" would read better as \"traditional pruning algorithms while still enjoying cheaper training costs. As evidence,\"\n- Your citation for Evci et al. (2019) is missing the publication venue/arxiv ID."}, "signatures": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1443/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Picking Winning Tickets Before Training by Preserving Gradient Flow", "authors": ["Chaoqi Wang", "Guodong Zhang", "Roger Grosse"], "authorids": ["cqwang@cs.toronto.edu", "gdzhang@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "keywords": ["neural network", "pruning before training", "weight pruning"], "TL;DR": "We introduced a pruning criterion for pruning networks before training by preserving gradient flow.", "abstract": "Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time.  Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels. Our code is made public\nat: https://github.com/alecwangcq/GraSP.", "pdf": "/pdf/56f2079417e5985446b60ee5b5a3d3e87c7002ad.pdf", "paperhash": "wang|picking_winning_tickets_before_training_by_preserving_gradient_flow", "_bibtex": "@inproceedings{\nWang2020Picking,\ntitle={Picking Winning Tickets Before Training by Preserving Gradient Flow},\nauthor={Chaoqi Wang and Guodong Zhang and Roger Grosse},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgsACVKPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/b919ba486938ebf140cfe33ae0f40e30e629a773.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgsACVKPH", "replyto": "SkgsACVKPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1443/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575576685225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1443/Reviewers"], "noninvitees": [], "tcdate": 1570237737318, "tmdate": 1575576685237, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1443/-/Official_Review"}}}], "count": 17}