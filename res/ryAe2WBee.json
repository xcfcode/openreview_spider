{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396312131, "tcdate": 1486396312131, "number": 1, "id": "BJlsjzIOg", "invitation": "ICLR.cc/2017/conference/-/paper26/acceptance", "forum": "ryAe2WBee", "replyto": "ryAe2WBee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396312722, "id": "ICLR.cc/2017/conference/-/paper26/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryAe2WBee", "replyto": "ryAe2WBee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396312722}}}, {"tddate": null, "tmdate": 1482198889734, "tcdate": 1482198889734, "number": 3, "id": "HJfuJGINx", "invitation": "ICLR.cc/2017/conference/-/paper26/official/review", "forum": "ryAe2WBee", "replyto": "ryAe2WBee", "signatures": ["ICLR.cc/2017/conference/paper26/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper26/AnonReviewer3"], "content": {"title": "not very convinced", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes a semantic embedding based approach to multilabel classification. \nConversely to previous proposals, SEM considers the underlying parameters determining the\nobserved labels are low-rank rather than that the observed label matrix is itself low-rank. \nHowever, It is not clear to what extent the difference between the two assumptions is significant\n\nSEM models the labels for an instance as draws from a multinomial distribution\nparametrized by nonlinear functions of the instance features. As such, it is a neural network.\nThe proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. \n\nThe paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. \n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512723447, "id": "ICLR.cc/2017/conference/-/paper26/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper26/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper26/AnonReviewer1", "ICLR.cc/2017/conference/paper26/AnonReviewer2", "ICLR.cc/2017/conference/paper26/AnonReviewer3"], "reply": {"forum": "ryAe2WBee", "replyto": "ryAe2WBee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512723447}}}, {"tddate": null, "tmdate": 1481825873782, "tcdate": 1481825873654, "number": 2, "id": "SyqLAIg4x", "invitation": "ICLR.cc/2017/conference/-/paper26/official/review", "forum": "ryAe2WBee", "replyto": "ryAe2WBee", "signatures": ["ICLR.cc/2017/conference/paper26/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper26/AnonReviewer2"], "content": {"title": "review", "rating": "4: Ok but not good enough - rejection", "review": "The paper presents the semantic embedding model for multi-label prediction.\nIn my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!\nI was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.\nRegarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.\nOne last question: why is it called \"semantic\" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512723447, "id": "ICLR.cc/2017/conference/-/paper26/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper26/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper26/AnonReviewer1", "ICLR.cc/2017/conference/paper26/AnonReviewer2", "ICLR.cc/2017/conference/paper26/AnonReviewer3"], "reply": {"forum": "ryAe2WBee", "replyto": "ryAe2WBee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512723447}}}, {"tddate": null, "tmdate": 1481824980336, "tcdate": 1481824979864, "number": 1, "id": "rJ20qUlVx", "invitation": "ICLR.cc/2017/conference/-/paper26/official/review", "forum": "ryAe2WBee", "replyto": "ryAe2WBee", "signatures": ["ICLR.cc/2017/conference/paper26/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper26/AnonReviewer1"], "content": {"title": "Weak reject", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.\n\nGiven that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance). I feel like this paper doesn't quite meet the bar.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512723447, "id": "ICLR.cc/2017/conference/-/paper26/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper26/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper26/AnonReviewer1", "ICLR.cc/2017/conference/paper26/AnonReviewer2", "ICLR.cc/2017/conference/paper26/AnonReviewer3"], "reply": {"forum": "ryAe2WBee", "replyto": "ryAe2WBee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512723447}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481742168795, "tcdate": 1477938165607, "number": 26, "id": "ryAe2WBee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryAe2WBee", "signatures": ["~Alex_Gittens1"], "readers": ["everyone"], "content": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481741045415, "tcdate": 1481741045409, "number": 3, "id": "B1TxXz1Ee", "invitation": "ICLR.cc/2017/conference/-/paper26/public/comment", "forum": "ryAe2WBee", "replyto": "ryODwThGl", "signatures": ["~Alex_Gittens1"], "readers": ["everyone"], "writers": ["~Alex_Gittens1"], "content": {"title": "response", "comment": "Eq.(11) is used to calculate the probability that the current instance is assigned the j-th label. By ranking the probabilities along all labels, we can select the top labels for the current instance. Several methods have been proposed in the literature for choosing an appropriate number of labels to predict, but due to space considerations and because that question is orthogonal to the question of ranking the labels, to compare the baselines and our methods, we use the known number of true labels to select the appropriate number of predicted labels in all methods.\n\nJean et al\u2019s paper was published in ACL 2015 for the problem of machine translation, where the input and output neurons represent words from different languages. Similar to our marginalization method, Jean et al. only consider a subset of target words for each source word, but the sampling strategies are different. In SEM, we sample the subset of negative labels for each instance with probability proportional to their frequency of occurrence in all training data, which can be easily and efficiently obtained. However, the approach of Jean et al re-builds a target vocabulary for each source sentence or builds a common candidate list for multiple source sentences, which is time-consuming.\n\nIn experimental sections 4.3 (small label sets) and 4.4 (large label sets), we discuss the choice of the main parameter, r, the latent space dimensionality; a longer discussion of the impact of the choice of r is given in the supplementary material (section A). Throughout we use a minibatch size of 200; for small label sets we use 30 epochs of training, and for large label sets we use 10 epochs of training. These values were selected because they make training efficient across a range of data sets."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287760052, "id": "ICLR.cc/2017/conference/-/paper26/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryAe2WBee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper26/reviewers", "ICLR.cc/2017/conference/paper26/areachairs"], "cdate": 1485287760052}}}, {"tddate": null, "tmdate": 1481739979096, "tcdate": 1481739979089, "number": 2, "id": "S1XCCWJNl", "invitation": "ICLR.cc/2017/conference/-/paper26/public/comment", "forum": "ryAe2WBee", "replyto": "S1RmDv17g", "signatures": ["~Alex_Gittens1"], "readers": ["everyone"], "writers": ["~Alex_Gittens1"], "content": {"title": "response", "comment": "We will correct the taxonomy of linear vs non-linear as you pointed out. \n\nNNML, a previous deep neural network approach showed that, if training time is disregarded, then neural networks can outperform linear algebraic approaches for multi-label learning. Unfortunately, this approach does not apply to large data sets in time comparable to the state-of-the-art linear algebraic approaches. Similarly, ADIOS shows that deep learning can outperform linear approaches, but no comparison of the scalability of ADIOS vs that of the baseline methods is given. In general, it is not hard to believe that as the reviewer suggests, deep nonlinear networks should outperform the current state-of-the-art methods used for multi-label learning. However, the baseline methods were carefully designed to provide good performance with fast training, precisely because the training time limits the size of the data sets to which multi-label learning methods can be fruitfully applied.\n\nThe key point of our paper is that we provided a neural network approach that beats state-of-the-art multi-label learning methods both in terms of performance metrics *and* training time, so can be used in practice on truly large data sets. The question of whether deep learning can reach a similarly competitive ratio of performance and speed is open."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287760052, "id": "ICLR.cc/2017/conference/-/paper26/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryAe2WBee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper26/reviewers", "ICLR.cc/2017/conference/paper26/areachairs"], "cdate": 1485287760052}}}, {"tddate": null, "tmdate": 1481739087514, "tcdate": 1481739087505, "number": 1, "id": "r1v8ibyVl", "invitation": "ICLR.cc/2017/conference/-/paper26/public/comment", "forum": "ryAe2WBee", "replyto": "HkiQ9qk7g", "signatures": ["~Alex_Gittens1"], "readers": ["everyone"], "writers": ["~Alex_Gittens1"], "content": {"title": "response", "comment": "As demonstrated in the paper introducing SLEEC (Bhatia et al. 2015), FastXML (Prabhu & Varma, 2014), etc., extreme multi-label classification data sets exhibit positive label sparsity in that each data point has only a few positive labels associated with it. We believe the P@K metric is a practically important metric for comparison of the methods as it focuses on the accurate prediction of the few positive labels per data point rather than the vast number of negative labels.\n\nWe mention several principal label space transformation methods in the introduction, but due to space constraints, we could not compare to all methods that have been proposed for large-scale multi-label learning. In Section 4, we explain our choice to omit comparisons to ML-CSSP (Bi & Kwok, 2013) and CPLST (Chen & Lin, 2012), which is a principal label space transformation method: prior work has shown SLEEC has better performance (as mentioned by the reviewer, in the P@K metric). Further, the authors of SLEEC argue that \u201ctechniques such as CS, CPLST, ML-CSSP, 1-vs- All could only be trained on the small data sets given standard resources.\u201d\n\nFor the baseline NN model, NNML (Nam et al. 2014), we used the codes provided by the authors, which do use AdaGrad.\n\nThanks for pointing us toward ADIOS. We note that ADIOS does not provide timing results, and uses deep learning. The contribution of our work is proof that shallow neural networks can outperform state-of-the-art linear algebraic methods both in terms of performance metrics as well as, counterintuitively, training time on large multi-label learning problems. It seems that ADIOS outperforms the baseline methods in terms of performance metrics, but timing results are not shown (and it is reasonable to expect that one pays a large cost in terms of training time for this performance). It is future work to determine to what extent ADIOS or other deep-learning methods can maintain a performance gap over baseline methods while having competitive learning times. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287760052, "id": "ICLR.cc/2017/conference/-/paper26/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryAe2WBee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper26/reviewers", "ICLR.cc/2017/conference/paper26/areachairs"], "cdate": 1485287760052}}}, {"tddate": null, "tmdate": 1480727074898, "tcdate": 1480727074894, "number": 3, "id": "HkiQ9qk7g", "invitation": "ICLR.cc/2017/conference/-/paper26/pre-review/question", "forum": "ryAe2WBee", "replyto": "ryAe2WBee", "signatures": ["ICLR.cc/2017/conference/paper26/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper26/AnonReviewer3"], "content": {"title": "prior work and experiments.", "question": "Principle labels space transformation and ML-CSSP are valid competitors even though the authors decided to omit them. Previous work (\"SLEEC\", \"FastXML\") is better than these approaches only on P@K measure, not on MacroF measure. The ICML 2016 paper \"ADIOS: Architectures Deep in Output Space\" is a also not cited.\n\nAlso, did you train the baseline NN model with adagrad?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959504624, "id": "ICLR.cc/2017/conference/-/paper26/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper26/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper26/AnonReviewer2", "ICLR.cc/2017/conference/paper26/AnonReviewer1", "ICLR.cc/2017/conference/paper26/AnonReviewer3"], "reply": {"forum": "ryAe2WBee", "replyto": "ryAe2WBee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959504624}}}, {"tddate": null, "tmdate": 1480714021698, "tcdate": 1480714021694, "number": 2, "id": "S1RmDv17g", "invitation": "ICLR.cc/2017/conference/-/paper26/pre-review/question", "forum": "ryAe2WBee", "replyto": "ryAe2WBee", "signatures": ["ICLR.cc/2017/conference/paper26/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper26/AnonReviewer1"], "content": {"title": "Questions", "question": "Re the prior work description, why are approaches such as Tai & Lin (2010) and Lin et al (2014), which use PCA, not considered to assume a low rank? The performance of PCA-based algorithms often depends on the eigenvalues decaying rather quickly, which is mostly equivalent to a low-rank assumption.\n\nSimilarly, why kernelize the output of SEM instead of using a deeper nonlinear network to produce the predictions to be used?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959504624, "id": "ICLR.cc/2017/conference/-/paper26/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper26/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper26/AnonReviewer2", "ICLR.cc/2017/conference/paper26/AnonReviewer1", "ICLR.cc/2017/conference/paper26/AnonReviewer3"], "reply": {"forum": "ryAe2WBee", "replyto": "ryAe2WBee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959504624}}}, {"tddate": null, "tmdate": 1480542047765, "tcdate": 1480542047759, "number": 1, "id": "ryODwThGl", "invitation": "ICLR.cc/2017/conference/-/paper26/pre-review/question", "forum": "ryAe2WBee", "replyto": "ryAe2WBee", "signatures": ["ICLR.cc/2017/conference/paper26/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper26/AnonReviewer2"], "content": {"title": "Questions", "question": "- How different is the proposed marginal fitting algorithm with [Jean et al, arxiv 1412.2007]?\n- When using for prediction, it seems eq 11 would produce 1 label on average per example; what if the number of labels is higher in expectaction?\n- How were the hyper-parameters chosen for the proposed method (no validation set?)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-label learning with semantic embeddings", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "pdf": "/pdf/d3baddd0392bb8aa948871c918b11cd345b68566.pdf", "TL;DR": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "paperhash": "jing|multilabel_learning_with_semantic_embeddings", "keywords": ["Supervised Learning"], "conflicts": ["berkeley.edu", "bjtu.edu.cn"], "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959504624, "id": "ICLR.cc/2017/conference/-/paper26/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper26/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper26/AnonReviewer2", "ICLR.cc/2017/conference/paper26/AnonReviewer1", "ICLR.cc/2017/conference/paper26/AnonReviewer3"], "reply": {"forum": "ryAe2WBee", "replyto": "ryAe2WBee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper26/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959504624}}}], "count": 11}