{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458149299724, "tcdate": 1458149299724, "id": "jZ9yl678XhnlBG2Xfzjq", "invitation": "ICLR.cc/2016/workshop/-/paper/158/review/10", "forum": "wVqzL1ypocG0qV7mtLqm", "replyto": "wVqzL1ypocG0qV7mtLqm", "signatures": ["ICLR.cc/2016/workshop/paper/158/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/158/reviewer/10"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This paper introduces two model extensions to improve character level recurrent neural network language models. The authors evaluate their approaches on a multilingual language modeling benchmark along with the standard Penn Tree Bank Corpus. Evaluation uses only entropy rather than including the language model in a downstream task but that's okay for a paper of this scope. The paper is clearly written and definitely a sufficient contribution for the workshop track it would be really nice to see how well these methods can improve and more sophisticated recurrent architecture like gru or lstm units. On the PTB Corpus it would be nice to include a state-of-the-art or standard n-gram model to use as a reference point for the reported results.\n\nThe conditioning on words model is an interesting approach. It's unfortunate that such a small word level vocabulary is used with this model. It seems like the small vocabulary restriction is due to the fact that the word level model is jointly trained along with the character models. An alternative approach might be to use as input features the hidden representations from a word level recurrent model already trained when building the Character level language model. I don't have a good sense for how much joint training of both models matters. \n\nWhen conditioning on recent history the authors might think about the NLP context trick of conditioning on a bag of words or bag of characters instead of considering only 10 grams. This would allow for a broader context coverage without expanding the feature dimension too much", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Alternative structures for character-level RNNs", "abstract": "Recurrent neural networks are convenient and efficient models for learning patterns in\nsequential data. However, when applied to signals with very low cardinality such as\ncharacter-level language modeling, they suffer from several problems. In order to success-\nfully model longer-term dependencies, the hidden layer needs to be large, which in turn\nimplies high computational cost. Moreover, the accuracy of these models is significantly\nlower than that of baseline word-level models. We propose two structural modifications\nof the classic RNN LM architecture. The first one consists on conditioning the RNN both\non the character-level and word-level information. The other one uses the recent history\nto condition the computation of the output probability. We evaluate the performance of\nthe two proposed modifications on multi-lingual data. The experiments show that both\nmodifications can improve upon the basic RNN architecture, which is even more visible\nin cases when the input and output signals are represented by single bits. These findings\nsuggest that more research needs to be done to develop general RNN architecture that\nwould perform optimally across wide range of tasks.", "pdf": "/pdf/wVqzL1ypocG0qV7mtLqm.pdf", "paperhash": "bojanowski|alternative_structures_for_characterlevel_rnns", "conflicts": ["inria.fr", "ens.fr", "fb.com"], "authors": ["Piotr Bojanowski", "Armand Joulin", "Tomas Mikolov"], "authorids": ["piotr.bojanowski@inria.fr", "ajoulin@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580026439, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580026439, "id": "ICLR.cc/2016/workshop/-/paper/158/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "wVqzL1ypocG0qV7mtLqm", "replyto": "wVqzL1ypocG0qV7mtLqm", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/158/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458081514046, "tcdate": 1458081514046, "id": "nx9DJNYBNU7lP3z2iopJ", "invitation": "ICLR.cc/2016/workshop/-/paper/158/review/11", "forum": "wVqzL1ypocG0qV7mtLqm", "replyto": "wVqzL1ypocG0qV7mtLqm", "signatures": ["ICLR.cc/2016/workshop/paper/158/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/158/reviewer/11"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This paper explores two augmentations to simple character RNN language models. Closing the performance gap between character and word based language models is an obvious and important research goal. While the aims of this paper interesting, at this point it looks more like a work in progress with a number of significant gaps, particularly in the evaluation.\n\nGeneral Points:\n- Given that simple RNNs struggle to capture long range dependencies, would it not be better to do this study with LSTMs? The assumption that the conclusions drawn for simple RNNs will hold for LSTMs seems flawed.\n- Equation 3 is not a correct likelihood function if the vocabularies of the two mixture components are not equal, which they are not in this instance. Section 5 implies that this function is used to calculate the BPC evaluation metric. This would result in overly optimistic BPC scores for the Mixed model.\n- The conditional output RNN is an interesting way to get direct ngram context into the model. It might be worth noting the similarity to the Multiplicative RNN of Sutskever 2011, where that model uses a conditional tensor contraction for the transition weights. There are many other ways to incorporate ngram conditioning, for example using direct ngram features on the input or feeding more than one character to the input layer. It would be informative to see how these other, easy to implement, approaches compare to the proposed approach.\n- The experimental evaluation is let down by poor choices of data sets. Firstly the datasets are all rather small compared to those usually used for language model evaluation. The processing of the PTB data set is particularly quirky, i.e. lowercased, punctuation removed, and <UNK> symbols for rare words (I assume predicted as < U N K > !). It is not clear why this processing is a good choice for a data set for evaluating a character LM. While the Europarl data set (at least v7, it looks like you used v1?) is a good choice for a multilingual corpus, training on just 60k sentences seems a bit unambitious. Also, if the sentences were randomly selected for test this could mean that test sentences came from the same documents as training sentences? This is not desirable for a LM evaluation.\n- Where did the estimate of '4 times the average entropy for OOVs' come from? This seems a bit random. It would be easy to build a character ngram/RNN estimate for singletons. This practice used to be standard for open vocabulary language models.\n- When reporting BPC metrics on text it is useful to report the performance of standard compression algorithms such as PAQ. These almost always significantly beat ngram and RNN models.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Alternative structures for character-level RNNs", "abstract": "Recurrent neural networks are convenient and efficient models for learning patterns in\nsequential data. However, when applied to signals with very low cardinality such as\ncharacter-level language modeling, they suffer from several problems. In order to success-\nfully model longer-term dependencies, the hidden layer needs to be large, which in turn\nimplies high computational cost. Moreover, the accuracy of these models is significantly\nlower than that of baseline word-level models. We propose two structural modifications\nof the classic RNN LM architecture. The first one consists on conditioning the RNN both\non the character-level and word-level information. The other one uses the recent history\nto condition the computation of the output probability. We evaluate the performance of\nthe two proposed modifications on multi-lingual data. The experiments show that both\nmodifications can improve upon the basic RNN architecture, which is even more visible\nin cases when the input and output signals are represented by single bits. These findings\nsuggest that more research needs to be done to develop general RNN architecture that\nwould perform optimally across wide range of tasks.", "pdf": "/pdf/wVqzL1ypocG0qV7mtLqm.pdf", "paperhash": "bojanowski|alternative_structures_for_characterlevel_rnns", "conflicts": ["inria.fr", "ens.fr", "fb.com"], "authors": ["Piotr Bojanowski", "Armand Joulin", "Tomas Mikolov"], "authorids": ["piotr.bojanowski@inria.fr", "ajoulin@fb.com", "tmikolov@fb.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580026156, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580026156, "id": "ICLR.cc/2016/workshop/-/paper/158/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "wVqzL1ypocG0qV7mtLqm", "replyto": "wVqzL1ypocG0qV7mtLqm", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/158/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455830623892, "tcdate": 1455830623892, "id": "wVqzL1ypocG0qV7mtLqm", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "wVqzL1ypocG0qV7mtLqm", "signatures": ["~Piotr_Bojanowski1"], "readers": ["everyone"], "writers": ["~Piotr_Bojanowski1"], "content": {"CMT_id": "", "title": "Alternative structures for character-level RNNs", "abstract": "Recurrent neural networks are convenient and efficient models for learning patterns in\nsequential data. However, when applied to signals with very low cardinality such as\ncharacter-level language modeling, they suffer from several problems. In order to success-\nfully model longer-term dependencies, the hidden layer needs to be large, which in turn\nimplies high computational cost. Moreover, the accuracy of these models is significantly\nlower than that of baseline word-level models. We propose two structural modifications\nof the classic RNN LM architecture. The first one consists on conditioning the RNN both\non the character-level and word-level information. The other one uses the recent history\nto condition the computation of the output probability. We evaluate the performance of\nthe two proposed modifications on multi-lingual data. The experiments show that both\nmodifications can improve upon the basic RNN architecture, which is even more visible\nin cases when the input and output signals are represented by single bits. These findings\nsuggest that more research needs to be done to develop general RNN architecture that\nwould perform optimally across wide range of tasks.", "pdf": "/pdf/wVqzL1ypocG0qV7mtLqm.pdf", "paperhash": "bojanowski|alternative_structures_for_characterlevel_rnns", "conflicts": ["inria.fr", "ens.fr", "fb.com"], "authors": ["Piotr Bojanowski", "Armand Joulin", "Tomas Mikolov"], "authorids": ["piotr.bojanowski@inria.fr", "ajoulin@fb.com", "tmikolov@fb.com"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}