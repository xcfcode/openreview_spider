{"notes": [{"id": "SJg4Y3VFPS", "original": "H1lcIH13UB", "number": 75, "cdate": 1569438844216, "ddate": null, "tcdate": 1569438844216, "tmdate": 1577168253603, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rHvmfDP_tr", "original": null, "number": 1, "cdate": 1576798686728, "ddate": null, "tcdate": 1576798686728, "tmdate": 1576800948304, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "SJg4Y3VFPS", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose Group Connected Multilayer Perceptron Networks which allow expressive feature combinations to learn meaningful deep representations. They experiment with different datasets and show that the proposed method gives improved performance. \n\nThe authors have done a commendable job of replying to the queries of the reviewers and addresses many of their concerns. However, the main concern still remains: The improvements are not very significant on most datasets except the MNIST dataset. I understand the author's argument that other papers have also reported small improvements on these datasets and hence it is ok to report small improvements. However, the reviewers and the AC did not find this argument very convincing. Given that this is not a theoretical paper and that the novelty is not very high (as pointed out by R1) strong empirical results are accepted.  Hence, at this point, I recommend that the paper cannot be accepted.\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJg4Y3VFPS", "replyto": "SJg4Y3VFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795714156, "tmdate": 1576800263943, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper75/-/Decision"}}}, {"id": "r1lQFeXjiS", "original": null, "number": 6, "cdate": 1573757050992, "ddate": null, "tcdate": 1573757050992, "tmdate": 1573757050992, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "ryeMyViJ5H", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment", "content": {"title": "To Reviewer #1 (1/2)", "comment": "\nThank you for reviewing the manuscript and helpful comments. Please find a point-to-point response to your comments in the following.\n\n-------------------------------------------\n*Comment: \u201c1. The intuition of this approach should be better explained. In equation (1) the features are group together using a binary matrix. Then the authors suggest using a relaxed version in equation (2) involving softmax function. What is the intuition here? If the feature is very high-dimensional, it seems that the normalization factor in equation (2) might be hard to compute. Moreover, what is the relationship between m, k, and d?\u201d\n\nEquation (2) is a continuous relaxation used to make the matrix multiplication operation in Equation (1) differentiable. Here, a softmax function with temperature is used for this purpose.\n\nThe use of softmax functions for the continuous relaxation of discrete parameters is frequent in the variational learning literature. Intuitively, the softmax function guarantees that each row in the $\\psi$ matrix is a valid probability distribution. The temperature is annealed during the training process so that at the very end of the training process the relaxed distribution converges to a discrete distribution. Therefore, the relaxation helps us in turning the discrete objective into a continuous problem. Otherwise, the discrete optimization problem would have been very complex and impractical to be used for end-to-end learning.\n\nBased on our initial experiments, the temperature annealing does not require any extensive adjustment for different experiments and works effectively for high-dimensional problems as long as a reasonable annealing function is used from high temperatures (e.g., 1) to low temperatures (e.g., 0.001).\n\n\nRegarding the relationship between m, k, d:\nm is determining the size of each group for a GMLP architecture\nk is determining the number of groups a GMLP architecture\nd is the number of features for a certain task\n\nIdeally, the values of m and k are related to the number and order of feature interactions for a certain task. Using proper m and k values enables us to reduce the parameter space while maintaining the model complexity required to solve the task. However, finding the ideal m and k directly from a given dataset is a very challenging problem. In this work, we treat m and k as hyperparameters to be found by a hyperparameter search on a validation set, similar to what we have for other network/training hyperparameters. See Appendix A for hyperparameter search spaces and Appendix B for selected architectures for each task.\n\nBased on our experiments, usually m<<d, and m*k has relatively the same order (in some cases, slightly more) as d. For instance, for the CIFAR-10 architecture: m=16, k=1536, and d=3072.\n\nTo address your comment and to clarify for our readers, we added the discussion above to the last paragraph of Section 3.3 in the revised version.\n\n-------------------------------------------\n*Comment: \u201c2. It seems that the neural network architecture is a simple variation of the standard MLP, except that the bottom layer is changed to a linear layer $z=\\Psi x$, where $\\Psi$ is defined using ${\\Psi_{i.j|i \\in [km], j \\in [d]}}$ and a softmax operation. It seems that the contribution of the network structure is rather incremental.\u201d\n\nThe grouping idea at the first layer of the network is one of the main contributions of this paper; however, we believe that the binary-tree like architecture is an equally important contribution. The suggested architecture learns how to utilize the selected groups to control the network complexity. Indeed, we believe that the combination of the first and the second contributions mentioned above is making this paper outstanding compared to the current literature in sparse network architecture and network architecture using expressive feature groups.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg4Y3VFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper75/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper75/Authors|ICLR.cc/2020/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176761, "tmdate": 1576860545889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment"}}}, {"id": "ryxYLlmisr", "original": null, "number": 5, "cdate": 1573757009119, "ddate": null, "tcdate": 1573757009119, "tmdate": 1573757009119, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "ryeMyViJ5H", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment", "content": {"title": "To Reviewer #1 (2/2)", "comment": "\n*Comment: \u201c3. In terms of the experiments, it seems that the results are very similar to that of the MLP, although slightly better. Moreover, the datasets used seem small, with no more than 10^6 data points in all datasets. The largest dataset is the Permutation invariant CIFAR-10, which has 50000 data with 3072 features. It would be interesting to see how this method works for high-dimensional datasets where the number of features is large.\u201d \n\nRegarding the comparison of results with MLP, on CIFAR-10 we have more than 5% improvement over MLP which we believe is a new state-of-the-art for architectures not using pixel coordinates. For the rest of the datasets which are health datasets or UCI benchmarks, even a small percentage of improvement is often very important and frequently used in literature to compare the performance of different methods. From the results presented in Table 2, GMLP is consistently providing more accurate results compared to all other work which we believe is significant given the diversity of the tasks compared here. \n\nMoreover, as demonstrated in Figure 2, the proposed architecture is much more efficient in terms of the number of parameters or model complexity at the prediction time. In other words, GMLP uses an order of magnitude less parameters and still achieves better results compared to the best MLP.\n\nWe agree with the reviewer that the evaluation of the suggested method on larger datasets would be interesting. In the current version, we used 7 different datasets where 5 of which were used to compare with the related work. Our largest task, CIFAR-10, is arguably the largest dataset that other competitors (cited in the related work or in the comparison of results) used for their evaluation. Note that in the literature related to this work, UCI benchmarks, MNIST, etc. are most frequently used for the evaluation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg4Y3VFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper75/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper75/Authors|ICLR.cc/2020/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176761, "tmdate": 1576860545889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment"}}}, {"id": "H1xylgQisS", "original": null, "number": 4, "cdate": 1573756903109, "ddate": null, "tcdate": 1573756903109, "tmdate": 1573756903109, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "BJgm18Gaqr", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment", "content": {"title": "To Reviewer #3 (1/2)", "comment": "\nThank you for reviewing the manuscript and helpful comments. Please find a point-to-point response to your comments in the following.\n\n-------------------------------------------\n*Comment: \u201cThe results do not show much improvement (i.e., < 0.3% improvement for 3 of the datasets, and < 1% for another one), aside from CIFAR-10. Considering that the premise of the paper is that MLP\u2019s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of \u201cthere has been little progress in deep reinforcement learning for domains without a known structure between features.\u201d\u201d\n\nWe understand the reviewer\u2019s concern that the improvements reported in Table 2 are often not larger than 1%. However, we would like to emphasize that for many of these tasks, which are in healthcare or UCI benchmarks, even a small percentage of improvement in accuracy is considered significant. For example, the original paper introduced self-normalizing neural networks (Klambauer et al. 2017) extensively used UCI benchmarks and often reported results that are only a fraction of percentage better than other competitors. \n\nMoreover, as demonstrated in Figure 2, the proposed architecture is much more efficient in terms of the number of parameters or model complexity at the prediction time. In other words, GMLP uses an order of magnitude less parameters and still achieves better results compared to the best MLP.\n\nRegarding the CIFAR-10 experiments, the use of a permuted version as a difficult task to challenge a non-image classifier is not new. For instance, very recent and relevant work by Aydore et al. 2019 used the same benchmark. Indeed, for CIFAR-10, we have more than 5% improvement over MLP which we believe is a new state-of-the-art for architectures not using pixel coordinates. \n\n-------------------------------------------\n*Comment: \u201cThe MNIST visualization of group-select felt informative, but the XOR example for grouping visualizations seemed too easy. It would\u2019ve been good to see visualizations or intuitions regarding grouping for harder datasets, in order to be convinced of the need for more expressive feature representations than standard MLP\u2019s.\n\nTo address your comment, we added a new appendix to the revised paper: \u201cAppendix D: Visual Analysis\u201d. In this appendix, we visualize the selected features for 25 different groups from our CIFAR-10 architecture (see Figure 10) as well as a heatmap showing the overall frequency each pixel is being selected by the group-select layer (see Figure 11).\n\nBased on our initial investigation, it is difficult to objectively analyze the performance of the group-select layer on a large task such as CIFAR-10 which has 1536 groups of size 16 each. Note that often each group is selecting different locations from different channels which makes it challenging to interpret visually. However, one noticeable pattern is that features selected from a certain color channel usually appear in clusters resembling irregularly shaped patches.\n\nWe understand the limitations of the current visualizations on small and simple tasks; however, we think these are still helpful conveying a general intuition on the operation of the suggested method.\n\n-------------------------------------------\n*Comment: I\u2019m not an expert on causality, but it seems like citations from that area are required for this problem statement of dealing with features where the connections between them are unknown but potentially very important.\u201d\n\nAs suggested, we included a discussion of these methods in the paper and added proper citations.\n\nSee the last sentence of Section 5 (discussion):\n\u201cFrom another perspective, the idea of studying feature groups is closely related to causal models such as Bayesian networks and factor graphs (Darwiche, 2009; Neapolitan et al., 2004; Clifford, 1990). These methods are often impractical for large-scale problems because, without a prior over the causal graph, they require an architecture search of the NP-complete complexity or more.\u201d\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg4Y3VFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper75/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper75/Authors|ICLR.cc/2020/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176761, "tmdate": 1576860545889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment"}}}, {"id": "B1xQh1QojH", "original": null, "number": 3, "cdate": 1573756842963, "ddate": null, "tcdate": 1573756842963, "tmdate": 1573756842963, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "BJgm18Gaqr", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment", "content": {"title": "To Reviewer #3 (2/2)", "comment": "\n**Less major:\n-------------------------------------------\n*Comment: \u201cIt would have been nice to include related work on other ways to encourage inter-feature interactions, such as perhaps taking the outer product of the input with itself.\u201d\n\nIn Section 2 (related work), the last two paragraphs are dedicated to the discussion of other work using the feature grouping idea. \nFor instance, we discuss Aydore et al. (2019) which uses feature groups to regularize the network training, or Ke et al. (2018) which exploits expressive feature combinations extracted from a GBDT to learn from tabular data. Please let us know if you have any specific paper in mind and we would be happy to discuss that one as well.\n\n-------------------------------------------\n*Comment: \u201cIt seems like different sizes per group would be a more realistic expectation, and that perhaps this should be worked into the algorithm. Similarly, pooling only 2 groups together (from pre-specified positions) seems like it would be limiting as well. It also seems like the algorithm should account for being able to use a high-level feature from one layer as part of multiple groups in the future (i.e. reuse). Even if any of these options don\u2019t make a difference, it would be good to check/evaluate.\u201d\n\nThank you for suggesting these ideas, we agree with you there are many dimensions that can be explored to extend the proposed method. However, in the current paper, we decided to present the main idea in a simple and clean way which would make the paper more practical.\nWe think that extending the suggested idea to more complicated architectures and ideas such as sharing weights and groups would be a very interesting direction for any future study.\n\n\n**Minor:\n-------------------------------------------\n*Comment: \u201cEquation 8 did not fully make sense to me.\u201d\n\nTo address your comment and to further clarify this for our readers, we added the following explanation to the revised paper (right after equation 8 in text):\n\u201c, where the first term is the work required for the first network layer from $d$ to $km$ neurons, the second term is corresponding to a hidden layer of size $km$, and so forth. The last term is the complexity of the output layer similar to the case of GMLP.\u201d\n\n-------------------------------------------\n*Comment: \u201cWhy were \u201crandom horizontal flips\u201d used as preprocessing for the permutation-invariant CIFAR-10 dataset? This shouldn\u2019t make a difference at all if the pixels become randomly shuffled anyway.\u201d\n\nThe random shuffle is happening in the data pipeline after the standard CIFAR-10 augmentation. Note that the random shuffle is not random across samples, it is merely a fixed random ordering used to remove pixel coordinates for each experiment. \nAlso, we would like to emphasize that while we had the option to ignore any data augmentation for CIFAR-10, it is best for the comparison of results to use the established standard augmentation method for this dataset that is followed by the majority of other work in the literature.\n\nTo address your concern and to prevent any confusion, we added the following clarification to the revised version (see the first paragraph of Section 4.1):\n\u201cNote that the permutation is not changing across samples, it is merely a fixed random ordering used to remove pixel coordinates for each experiment.\u201d\n\nAlso, we changed \u201cpermutation invariant version\u201d to \u201cpermuted version\u201d in Table 2 and 3 captions.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg4Y3VFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper75/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper75/Authors|ICLR.cc/2020/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176761, "tmdate": 1576860545889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment"}}}, {"id": "BJgINkQooH", "original": null, "number": 2, "cdate": 1573756717617, "ddate": null, "tcdate": 1573756717617, "tmdate": 1573756717617, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "HJeu3xwVoS", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment", "content": {"title": "To Reviewer #4", "comment": "Thank you for reviewing the manuscript and helpful comments. Please find a point-to-point response to your comments in the following.\n\n-------------------------------------------\n*Comment: \u201c1. In the modified CIFAR10 dtaset, is the same permutation of pixels applied to all images or is it a different random permutation for every? If it is different, the entire rationale of Group-select breaks down, so I am assuming it is the same, but this must be clarified.\u201d\n\nThe same permutation is used for all images. At the beginning of each experiment, we define a fixed permutation of features and use this to reorder all feature vectors. Here, the idea is to prevent a model to use pixel coordinates: it will make the task very difficult if not impossible for CNNs while it does not impose any challenge for an MLP.\n\nTo address your concern and to prevent any confusion, we added the following clarification to the revised version (see the first paragraph of Section 4.1):\n\u201cNote that the permutation is not changing across samples, it is merely a fixed random ordering used to remove pixel coordinates for each experiment.\u201d\n\nAlso, we changed \u201cpermutation invariant version\u201d to \u201cpermuted version\u201d in Table 2 and 3 captions.\n\n-------------------------------------------\n*Comment: \u201c2. What were the groups selected in CIFAR10? Did the groups selected correspond to nearby pixels or some other meaningful way? A better understanding of why this improvement happened might help a lot.\u201d\n\nTo address your comment, we added a new appendix to the revised paper: \u201cAppendix D: Visual Analysis\u201d. In this appendix, we visualize the selected features for 25 different groups from our CIFAR-10 architecture (see Figure 10) as well as a heatmap showing the overall frequency of each pixel being selected by the group-select layer (see Figure 11).\n\nBased on our initial investigation, it is difficult to objectively analyze the performance of the group-select layer on a large task such as CIFAR-10 for which with have 1536 groups of size 16 each. Note that often each group is selecting different locations from different channels which makes it difficult to interpret visually. However, one noticeable pattern is that features selected from a certain color channel usually appear in clusters resembling irregularly shaped patches.\n\n-------------------------------------------\n*Comment: \u201c3. In figure 5,  learning curves are plotted for different m. But what about k? Is it kept constant? If so what constant? Similarly for figure 6.\u201d\n\nThank you for pointing out this issue. We revised the paper (see captions in Figures 5 and 6) to report the exact m and k used in each case i.e., k=1536 for Figure 5, and m=16 for Figure 6.\n\n-------------------------------------------\n*Comment: \u201c4. The visualisation for MNIST Group-select seems very limited compared to what is there in the architecture. I there any way a selected group can be shown and visualised in a meaningful way?\u201d\n\nAs we indicated in response to your earlier comment, in the revised version, we added a new appendix dedicated to the visual analysis on the group-select layer for our CIFAR-10 model.\n\n-------------------------------------------\n*Comment: \u201c5. The synthetic example is interesting, but this can easily be extended to much larger scales. Do the results hold up in such cases too?\u201d\n\nWe agree with the reviewer that it would be interesting to extend this experiment; however, using a large scale and more complex dataset requires using a deeper GMLP with more groups and a larger group size. Based on our initial experiments, it will be challenging to make visual interpretations in such scenarios as groups start to learn with more redundancy, share information between the groups, and merge information deeper within the network.\n\nWe understand the limitations of the current visualizations on small and simple tasks; however, we think these are still helpful conveying a general intuition on the operation of the suggested method.\n\n-------------------------------------------\n*Comment: \u201cTypos/errors:\n\n1. In equation 4, the i+k/2 part makes sense only for the l=0 layer.\u201d\n\nThank you for pointing out this error. We revised equation (4) to fix this issue:\n\n$$\n    \\bm{z}_{i}^{l+1} = pool(\\bm{z}_{i}^{l},\\bm{z}_{i+k/{2^{l+1}}}^{l})\n$$\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJg4Y3VFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper75/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper75/Authors|ICLR.cc/2020/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176761, "tmdate": 1576860545889, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper75/Authors", "ICLR.cc/2020/Conference/Paper75/Reviewers", "ICLR.cc/2020/Conference/Paper75/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper75/-/Official_Comment"}}}, {"id": "HJeu3xwVoS", "original": null, "number": 3, "cdate": 1573314735999, "ddate": null, "tcdate": 1573314735999, "tmdate": 1573314735999, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "SJg4Y3VFPS", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "Summary:\n\nThis paper gives a new MLP formulation and architecture that is ostensibly suited for data where the label's dependence on the input feature has form of sparsity. The paper reports the performance of this architecture on a few standard datasets and outperforms other baselines.\n\nReview:\n\nThe main contribution arguably in the \"Group select\" matrix which selects features that each Group-FC focusses on. While it is an interesting idea, it is not hugely novel and requires a lot more demonstration. Without the Group-select the architecture in the paper can simply viewed as an MLP with a block diagonal sparsity structure enforced on the weights and max-out pooling as non-linearity.\n\nBased on the above reasoning, I would rate the architectural/theoretical contribution as not significant. The empirical results also do not look particularly convincing, as there is just a few percentage improvement over MLP (except CIFAR10). \n\nQuestions for authors:\n\n1. In the modified CIFAR10 dtaset, is the same permutation of pixels applied to all images or is it a different random permutation for every? If it is different, the entire rationale of Group-select breaks down, so I am assuming it is the same, but this must be clarified.\n\n2. What were the groups selected in CIFAR10? Did the groups selected correspond to nearby pixels or some other meaningful way? A better understanding of why this improvement happened might help a lot.\n\n3. In figure 5,  learning curves are plotted for different m. But what about k? Is it kept constant? If so what constant? Similarly for figure 6.\n\n4. The visualisation for MNIST Group-select seems very limited compared to what is there in the architecture. I there any way a selected group can be shown and visualised in a meaningful way?\n\n5. The synthetic example is interesting, but this can easily be extended to much larger scales. Do the results hold up in such cases too?\n\nTypos/errors:\n\n1. In equation 4, the i+k/2 part makes sense only for the l=0 layer.\n\n\n\n\n\n\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper75/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper75/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg4Y3VFPS", "replyto": "SJg4Y3VFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575715665867, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper75/Reviewers"], "noninvitees": [], "tcdate": 1570237757443, "tmdate": 1575715665883, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper75/-/Official_Review"}}}, {"id": "ryeMyViJ5H", "original": null, "number": 1, "cdate": 1571955673872, "ddate": null, "tcdate": 1571955673872, "tmdate": 1572972641755, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "SJg4Y3VFPS", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper studies supervised classification problems where features are unstructured. For these problems, the authors propose a new neural network architecture that first reorganize the features into groups, then builds feed-forward networks on top each group, and finally aggregate the hidden nodes of each group to produce the final output. Empirical and ablation studies are conducted to show the performance of this approach. \n\nMy detailed comments are as follows. \n\n1. The intuition of this approach should be better explained. In equation (1) the features are group together using a binary matrix. Then the authors suggest using a relaxed version in equation (2) involving softmax function. What is the intuition here? If the feature is very high-dimensional, it seems that the normalization factor in equation (2) might be hard to compute. Moreover, what is the relationship between $k$, $m$, and $d$?\n\n2. It seems that the neural network architecture is a simple variation of the standard MLP, except that the bottom layer is changed to a linear layer $ z = \\Psi x$, where $\\Psi$ is defined using $\\{ \\psi_{ij}, i \\in [km], j\\in [d] \\} $ and a softmax operation. It seems that the contribution of the network structure is rather incremental.\n\n3. In terms of the experiments, it seems that the results are very similar to that of the MLP, although slightly better. Moreover, the datasets used seem small, with no more than 10^6 data points in all datasets. The largest dataset is the Permutation invariant CIFAR-10, which has 50000 data with 3072 features. It would be interesting to see how this method works for high-dimensional datasets where the number of features is large. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper75/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper75/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg4Y3VFPS", "replyto": "SJg4Y3VFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575715665867, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper75/Reviewers"], "noninvitees": [], "tcdate": 1570237757443, "tmdate": 1575715665883, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper75/-/Official_Review"}}}, {"id": "BJgm18Gaqr", "original": null, "number": 2, "cdate": 1572836826640, "ddate": null, "tcdate": 1572836826640, "tmdate": 1572972641712, "tddate": null, "forum": "SJg4Y3VFPS", "replyto": "SJg4Y3VFPS", "invitation": "ICLR.cc/2020/Conference/Paper75/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper addresses the problem of learning expressive feature combinations in order to improve learning for domains where there is no known structure between features. These settings would normally lead to the use of fully-connected MLP networks, which unfortunately have problems with efficient training and generalization after a few layers of depth. The main idea is to use grouping at first, in combination with smaller fully-connected layers for each group, as well as pooling pairs of groups together as the layers go on. Results are shown as comparisons on 5 real-world datasets, and intuitive visualizations on two other datasets. Related work covered MLPs, regularization techniques, sparse networks, random forest models, and other feature grouping. The paper is well written and easy to read. This work did a good job with giving implementation details as well as performing hyperparameter searches and giving the baselines a good effort. \n\nMy current decision is a weak reject, for a well-written paper, but some concerns as follows:\n-The results do not show much improvement (i.e., < 0.3% improvement for 3 of the datasets, and < 1% for another one), aside from CIFAR-10. Considering that the premise of the paper is that MLP\u2019s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of \u201cthere has been little progress in deep reinforcement learning for domains without a known structure between features.\u201d\n-The MNIST visualization of group-select felt informative, but the XOR example for grouping visualizations seemed too easy. It would\u2019ve been good to see visualizations or intuitions regarding grouping for harder datasets, in order to be convinced of the need for more expressive feature representations than standard MLP\u2019s.\n-I\u2019m not an expert on causality, but it seems like citations from that area are required for this problem statement of dealing with features where the connections between them are unknown but potentially very important. \n\nLess major:\n-It would have been nice to include related work on other ways to encourage inter-feature interactions, such as perhaps taking the outer product of the input with itself. \n-It seems like different sizes per group would be a more realistic expectation, and that perhaps this should be worked into the algorithm. Similarly, pooling only 2 groups together (from pre-specified positions) seems like it would be limiting as well. It also seems like the algorithm should account for being able to use a high-level feature from one layer as part of multiple groups in the future (i.e. reuse). Even if any of these options don\u2019t make a difference, it would be good to check/evaluate.\n\nMinor:\n-Equation 8 did not fully make sense to me.\n-Why were \u201crandom horizontal flips\u201d used as preprocessing for the permutation-invariant CIFAR-10 dataset? This shouldn\u2019t make a difference at all if the pixels become randomly shuffled anyway."}, "signatures": ["ICLR.cc/2020/Conference/Paper75/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper75/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Mohammad Kachuee", "Sajad Darabi", "Shayan Fazeli", "Majid Sarrafzadeh"], "title": "Group-Connected Multilayer Perceptron Networks", "abstract": "Despite the success of deep learning in domains such as image, voice, and graphs, there has been little progress in deep representation learning for domains without a known structure between features. For instance, a tabular dataset of different demographic and clinical factors where the feature interactions are not given as a prior. In this paper, we propose Group-Connected Multilayer Perceptron (GMLP) networks to enable deep representation learning in these domains. GMLP is based on the idea of learning expressive feature combinations (groups) and exploiting them to reduce the network complexity by defining local group-wise operations. During the training phase, GMLP learns a sparse feature grouping matrix using temperature annealing softmax with an added entropy loss term to encourage the sparsity. Furthermore, an architecture is suggested which resembles binary trees, where group-wise operations are followed by pooling operations to combine information; reducing the number of groups as the network grows in depth. To evaluate the proposed method, we conducted experiments on five different real-world datasets covering various application areas. Additionally, we provide visualizations on MNIST and synthesized data. According to the results, GMLP is able to successfully learn and exploit expressive feature combinations and achieve state-of-the-art classification performance on different datasets.", "authorids": ["mkachuee@ucla.edu", "sajad.darabi@cs.ucla.edu", "shayan@cs.ucla.edu", "majid@cs.ucla.edu"], "pdf": "/pdf/7aace9196133002419a9bef5c089255122abec42.pdf", "keywords": [], "TL;DR": "An architecture to learn and exploit expressive feature combinations", "paperhash": "kachuee|groupconnected_multilayer_perceptron_networks", "original_pdf": "/attachment/21ad061ecd3abbc45db07ccaaf54afccea8506aa.pdf", "_bibtex": "@misc{\nkachuee2020groupconnected,\ntitle={Group-Connected Multilayer Perceptron Networks},\nauthor={Mohammad Kachuee and Sajad Darabi and Shayan Fazeli and Majid Sarrafzadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SJg4Y3VFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJg4Y3VFPS", "replyto": "SJg4Y3VFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575715665867, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper75/Reviewers"], "noninvitees": [], "tcdate": 1570237757443, "tmdate": 1575715665883, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper75/-/Official_Review"}}}], "count": 10}