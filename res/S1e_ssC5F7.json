{"notes": [{"id": "S1e_ssC5F7", "original": "rklcE4nctQ", "number": 634, "cdate": 1538087839738, "ddate": null, "tcdate": 1538087839738, "tmdate": 1545355438606, "tddate": null, "forum": "S1e_ssC5F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rkeBezxIxE", "original": null, "number": 1, "cdate": 1545105900617, "ddate": null, "tcdate": 1545105900617, "tmdate": 1545354478576, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Meta_Review", "content": {"metareview": "All three reviewers found that the motivation for the proposed method was lacking and recommend rejection. The AC thus recommends the authors to take these comments in consideration when revising their manuscript.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Reject"}, "signatures": ["ICLR.cc/2019/Conference/Paper634/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper634/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353144002, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper634/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper634/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper634/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353144002}}}, {"id": "rygyfBVelV", "original": null, "number": 1, "cdate": 1544729863417, "ddate": null, "tcdate": 1544729863417, "tmdate": 1544729863417, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Public_Comment", "content": {"comment": "Hello,\n\nDuring the implementation of the 2d neural network on MNIST using the proposed algorithm, I got a problem. The initial value of gradient is big (since I put the random values in Ws and they are not probably close to the optimum) so when I use this new learning rate, it doesn't converge. I want to ask if there is an initial value on the learning rate in your algorithm in order to avoid that?\nOne solution: we can do a step of regular gradient descent and after that change the update rules for iterations > 1.\n\nThanks", "title": "initial gradient problem"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311789337, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "S1e_ssC5F7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311789337}}}, {"id": "S1gxz6cfkN", "original": null, "number": 4, "cdate": 1543838984270, "ddate": null, "tcdate": 1543838984270, "tmdate": 1543838984270, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "ryleH4UBA7", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Official_Comment", "content": {"title": "Comment", "comment": "I thank the reviewers for their response, and I keep my score."}, "signatures": ["ICLR.cc/2019/Conference/Paper634/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper634/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper634/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618295, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1e_ssC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper634/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper634/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper634/Authors|ICLR.cc/2019/Conference/Paper634/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618295}}}, {"id": "ryleH4UBA7", "original": null, "number": 3, "cdate": 1542968376164, "ddate": null, "tcdate": 1542968376164, "tmdate": 1542968376164, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "Syeq6mf-3m", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Official_Comment", "content": {"title": "Thanks for your time and insightful comments!", "comment": "1) Our main contribution (or focus) of this paper is to propose a framework for adjusting learning rate adaptively. We offer a novel viewpoint different from previous main approaches like line search and approximate second-order methods (BBstep, Adagrad, etc.).\n\n2) Our idea stems  from  the  work  of  Daubechies  et  al.  (2010),  where  the  authors  adjusted  the weights of the weighted least squares problem by solving an extra objective function which added a regularizer about the weights to origin objective function. \n\n3) Since our framework can derive AdaGrad as a private case, we are more general scene so our bound doesn\u2019t better than AdaGrad. However, our bound are almost same as AdaGrad. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper634/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper634/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618295, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1e_ssC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper634/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper634/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper634/Authors|ICLR.cc/2019/Conference/Paper634/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618295}}}, {"id": "BJeQlWUHRX", "original": null, "number": 1, "cdate": 1542967530631, "ddate": null, "tcdate": 1542967530631, "tmdate": 1542968075619, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "H1g51qT92m", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Official_Comment", "content": {"title": "Thanks for your thoughtful review and your time!", "comment": "1) Our main contribution (or focus) of this paper is to propose a framework for adjusting learning rate adaptively. We offer a novel viewpoint different from previous main approaches like line search and approximate second-order methods (BBstep, Adagrad, etc.).\n2) Compared with Bregman divergence, \\phi-divergence naturally imposes nonnegative constraint about \\beta and \\eta_t, which is necessary for learning rates, while the L_p normalization still can\u2019t guarantee nonnegative condition for learning rate.\n3) Equation (6) is equivalent to (5). We just want to rewrite (5) to a more clear scheme.\n4) In the classical gradient descent algorithm formulated as x_{t+1} = x_t - g_t / \\beta, for small \\beta, more precisely for \\beta < 2 / L, the algorithm has no guarantee for convergence. Our framework gives a upper bound for runtime (O(1 / \\varepsilon)) or regret (O(\\sqrt(T))) for arbitrary \\beta_0. Moreover, like Adagrad or gradient descent, algorithms derived from our framework also can be suggested a best initial learning rate for optimization ( based on our regret bounds).\n5) The proof of Theorem 6 can be found in the proofs of Theorems 21, 22, and 23."}, "signatures": ["ICLR.cc/2019/Conference/Paper634/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper634/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618295, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1e_ssC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper634/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper634/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper634/Authors|ICLR.cc/2019/Conference/Paper634/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618295}}}, {"id": "HJgg6z8H0m", "original": null, "number": 2, "cdate": 1542967992018, "ddate": null, "tcdate": 1542967992018, "tmdate": 1542967992018, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "S1lb8qKMnm", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Official_Comment", "content": {"title": "Thanks for your time and helpful comments!", "comment": "1) Our idea stems  from  the  work  of  Daubechies  et  al.  (2010),  where  the  authors  adjusted  the weights of the weighted least squares problem by solving an extra objective function which added a regularizer about the weights to origin objective function. \n2) We give theoretical analysis for both update rules (see Theorems 6 and 7) not only for the original algorithm.\n3) Compared with Bregman divergence, \\phi-divergence naturally imposes nonnegativity on \\beta and \\veta_t, which is necessary for learning rates.\n\n\nSpecific comments and questions:\n\n1) Since taking a convex regularization term of learning rate and viewing it as a penalty according to current learning rate, we need to regard this as a maximum problem. And there is little difference when adding a regularization and viewing it as a minimum problem. Actually, our paper proposes a framework of adaptive step size learning. \n\n2)Since \\phi-divergence is natural for nonnegative variable, compared with Bregman divergence, and doesn\u2019t have to be a probability (sum is 1) compared to KL-divergence.\n\n3) We have shown some common \\phi-divergence and relevant update rules in Appendix D.1, and most of them seem simple to solve.\n\n4) The objective functions of  problems (5) and (7) are concave for \\beta and convex for x (since \\beta > 0), and let the equations of both partial derivatives equal zero (it is somehow like the equation(12) ), then the solutions satisfy saddle point condition, so the solutions are identical.   \n\n5) We have shown in Lemma 2 that the solution of (5) can easily be extended to constrained cases. Moreover, in practical use, growth clipping is often necessary, which indicates constrained cases. Therefore, we choose maxmin formulation instead of minmax formulation.\n\n6) We would like to adaptively choose the learning rate in the optimization period rather than setting priori \\eta_t for \\beta_t, while not on account of the smoothness.\n\n7) Equation (11), got by alternating update rule, first uses recommended step size \\eta to minimizes x, and then maximizes \\beta to get next step size, next turn back to get x by using this step size. Our theorem showed the convergence bound for both methods. Perhaps, there is no need to show when they coincide.\n\n9) First, we focus on the convergence with different choice of initial learning rate, while many methods, like GD, would fail for extremely large initial learning rate. However, this doesn't bother us at all since we are free of choosing the initial rate. Second, like GD and many other optimization methods, the choice of initial learning rate may need Lipschitz constant or smoothness constant for the sake of convergence, but in our methods, it doesn't.\n\n10) We propose a novel framework on adaptively updating the learning rate with a regularization term, which we take \\phi divergence as an example in the paper. In this way, the bounds are given for \\phi divergence generally. Therefore, we do not make special assumptions on \\alpha or \\beta_t.\n\n11) The bounds are given for general \\phi divergence. Viewed as a special case of our framework with a specialized \\phi divergence, AdaGrad is of no wonder to enjoy lower regret bound than the general regret bound we give.\n\n13) Due to the space limitation, our description may mislead you. Given a \\phi divergence, we totally have three different ways on how to update x_t and \\beta_t alternately, corresponding to Algorithms 1 and 2 in page 5, and Algorithm 3 in Appendix C. Actually, we should not mention Algorithm 1 here, for it is shown in Appendix D.1 that Algorithm 1 is always computationally unfriendly. As shown in Figure 1, Algorithm 2 outperforms Algorithm 3 when the initial learning rate is extremely large. However, at their best initial learning rates, their performance is comparable. Out of the consideration on training stability with large initial learning rate, we finally choose the second update rule corresponding to Algorithm 2.\n\n14) Growth clipping does not affect the theoretical result, since it is only applied to the algorithms in the experiment. The practical optimization problem neither generally guarantees the gradient smoothness, nor guarantees the global strong convexity. So it is necessary to apply growth clipping in case of training collapses due to the stochastic gradient.\n\n15)Figure 2 describes the results of experiments carried out in the full gradient setting. Since there is no randomness, it is of no use to carry out duplicated experiments. "}, "signatures": ["ICLR.cc/2019/Conference/Paper634/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper634/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618295, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1e_ssC5F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper634/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper634/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper634/Authors|ICLR.cc/2019/Conference/Paper634/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper634/Reviewers", "ICLR.cc/2019/Conference/Paper634/Authors", "ICLR.cc/2019/Conference/Paper634/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618295}}}, {"id": "H1g51qT92m", "original": null, "number": 3, "cdate": 1541229026009, "ddate": null, "tcdate": 1541229026009, "tmdate": 1541533821578, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Official_Review", "content": {"title": "minor generalization of AdaGrad style methods", "review": "The paper presents a generalization of the Adagrad type methods using a min-max formulation and then presents two alternate algorithms to solve this formulation. \n\nIt is unclear to me that much extra generalization has been achieved over the original AdaGrad paper. That paper simply presents the choice of hyperparameters as an optimal solution to a proximal primal dual formulation. The formulation presented here appears to be another form of the proximal mapping formulation, and so it is unclear what the advance here is. The AdaGrad paper used a particular Bregman divergence, and different divergences yield slightly different methods, as is observed here by the authors when they use different divergence measures.\n\nThe Bregman divergences do make sense from a primal pual proximal formulation point of view, but why do you use a discrepancy function in your min-max formulation that comes from the \\phi - divergence family? Why not consider an L_p normalization of the discrepancy? \n\nThe difference between formulations (5) and (6) is not clearly specified. Did you mean to drop the constraints that \\beta \\in \\cal{B}_t ? Otherwise, why is (6) , which looks to be a re-write of (5), unconstrained and hence separable?\n\nThe authors claim that the method is free of parameter choices, but the initial \\beta_0 seems to be a crucial parameter here since it forms both a target and a lower bound for subsequent \\beta_t's. How is this parameter chosen and what effect does it have on convergence? From the results (Figs in Sec 5), this choice does significantly impact the final test loss obtained. \n    \nI could not find a proof for Thm 6 in the appendix. Did I over look it or is there a typo?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper634/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Official_Review", "cdate": 1542234414880, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper634/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335768630, "tmdate": 1552335768630, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper634/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lb8qKMnm", "original": null, "number": 2, "cdate": 1540688456810, "ddate": null, "tcdate": 1540688456810, "tmdate": 1541533821375, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Official_Review", "content": {"title": "Novel idea but theoretical guarantees and empirical results are not convincing.", "review": "This paper presents a method for adaptively tuning the learning rate in gradient descent methods. The authors consider the formulation of each gradient descent update as a quadratic minimization problem and they propose adding a phi-divergence between the learning rate that would be used and an auxiliary vector. The authors also propose adding a maximization over all learning rates in the update.  \n\nThe authors study an important problem and propose a novel method. The algorithms suggested by the author are also relatively clear, and it is great that the paper presents both theoretical results as well as numerical experiments.\n\nOn the other hand, I didn't find the main idea of hyper-regularization to be well-justified. It is not clear why adding an additional regularization term for the learning rate makes sense , and it is even less clear why this should be presented as a maxmin problem. This can make the update step much more complicated and is probably why the authors also propose a simpler alternating optimization algorithm as an alternative. Unfortunately, the authors do not discuss how this alternating optimization problem relates to the original one, and the theoretical guarantees are only presented for the original algorithm. The authors also do not justify the choice of phi-divergence as the regularizer for the learning rate. The theoretical guarantees in the paper also do not suggest that the algorithm presented in the paper is better than existing state-of-the-art methods, even in specific situations (i.e. the regret bounds don't appear better than the AdaGrad regret bounds). Moreover, without tests for statistical significance, I also didn't find the experimental results sufficiently compelling.\n\nSpecific comments and questions:\n1) Page 3: Equation (4): The paper would be stronger if the authors motivated why the regularization should be posed as an outer maximization.\n2) Page 3: \"we use the \\phi-divergence as our hyper-regularization\". Why is this a good choice of reuglarizer?\n3) Page 3: \"only a few extra calculations are required for each step\". This is a misleading comment, because the maximization can be hard when phi is complicated, even if the problem splits across dimensions.\n4) Page 4: \"The solution of problem (5) is the same as (7) in unconstrained case\". You should provide a reference for this statement as well as discuss the specific assumptions on the objective that allow you to arrive at this claim.\n5) Page 4: \"while the solution of (7) is more difficult to get. Thus, we choose (5) as our basic problem\". This seems like a very bad motivation for choosing the maxmin formulation. For instance, the problem would be even simpler if  you didn't include this extra phi-divergence at all.\n6) Page 4: \"Although setting \\eta-t=\\beta_t is our main focus...\". Why is smoothness in the learning rate a good property? \n7) Page 5: Equation (11). How do these iterates relate to the ones in equation (5) (e.g. when do they coincide, if ever)?\n8) Page 5: \"influence the efficient of our algorithms.\" Grammatical error.\n9) Page 6. \"our algorithms are robust to the choice of initial learning rates and do not rely on the Lipschitz constant or smoothness constant\". I'm not sure why this is a valuable property, since AdaGrad doesn't rely on these parameters either.\n10) Page 6: Theorems 6 and 7. How do these results depend on alpha and \\beta_t? This paper would be much stronger if the bounds depend on \\phi more clearly and if the authors were able to show that there exist choices of phi that make this algorithm better than existing methods.\n11) Page 6: Theorem 7: The dependence on G in the regret bound actually makes this worse than the AdaGrad regret bound.\n12) Page 7: \"KL_devergence\". Typo.\n13) Page 7: \"different update rules were compared in advance to select the specific one for any phi divergence in the following experiments.\" What does this mean exactly? How much of a difference does the choice of update rule make?\n14) Page 7: \"growth clipping is applied to all algorithms in our framework\". Why is this necessary, and how does it affect the theoretical results?\n14) Page 7-8: Figures 1, 2, and 3. It's hard to interpret the significance of these results without error bars.\n\n\n\n\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper634/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Official_Review", "cdate": 1542234414880, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper634/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335768630, "tmdate": 1552335768630, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper634/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Syeq6mf-3m", "original": null, "number": 1, "cdate": 1540592578053, "ddate": null, "tcdate": 1540592578053, "tmdate": 1541533821128, "tddate": null, "forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "invitation": "ICLR.cc/2019/Conference/-/Paper634/Official_Review", "content": {"title": "Unclear formulation, and Benefit of approach is not Demonstrated", "review": " Summary: \n%%%%%%%%%%%%%%%\nThe paper explores ways to adapt the learning rate rule through a new minimax formulation.\nThe authors provide regret bounds for their method in the online convex optimization setting.\n\nComments:\n%%%%%%%%%%%%%%%\n-I found the motivation of the approach to be very lacking.\nConcretely, it is not clear at all why the minimax formulation even makes sense, and the authors do not explain this issue.\n\n-While the authors provide regret guarantees for their method, the theoretical analysis does not reflect when is their approach  beneficial compared to standard adaptive methods. Concretely, their bounds compare with the well known bounds of AdaGrad. \nIt is nice that their approach enables to extract AdaGrad as a private case. But again, it is not clear what is the benefit of their extension.\n\n-Finally, the experiments do not illustrate almost any benefit of the new approach compared to standard adaptive methods.\n\n\nSummary\n%%%%%%%%%%%%%%%\nThe paper suggests a different approach to adapt the learning rate.\nUnfortunately, the reasoning behind the new approach is not very clear.\nAlso, nor theory neither experiments illustrate the benefit of this new approach over standard methods.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper634/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent", "abstract": "We present a novel approach for adaptively selecting the learning rate in gradient descent methods.  Specifically, we impose a regularization term on the learning rate via a generalized distance, and cast the joint updating process of the parameter and the learning rate into a maxmin problem. Some existing schemes such as AdaGrad (diagonal version) and WNGrad can be rederived from our approach. Based on our approach, the updating rules for the learning rate do not rely on the smoothness constant of optimization problems and are robust to the initial learning rate. We theoretically analyze our approach in full batch and online learning settings, which achieves comparable performances with other first-order gradient-based algorithms in terms of accuracy as well as convergence rate.", "keywords": ["Adaptive learning rate", "novel framework"], "authorids": ["smsxgz@pku.edu.cn", "jin.hao@pku.edu.cn", "lindachao@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Guangzeng Xie", "Hao Jin", "Dachao Lin", "Zhihua Zhang"], "pdf": "/pdf/345c0e2dd15a7399bd29975ac041e0a301e82231.pdf", "paperhash": "xie|hyperregularization_an_adaptive_choice_for_the_learning_rate_in_gradient_descent", "_bibtex": "@misc{\nxie2019hyperregularization,\ntitle={Hyper-Regularization: An Adaptive Choice for the Learning Rate in Gradient Descent},\nauthor={Guangzeng Xie and Hao Jin and Dachao Lin and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=S1e_ssC5F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper634/Official_Review", "cdate": 1542234414880, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1e_ssC5F7", "replyto": "S1e_ssC5F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper634/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335768630, "tmdate": 1552335768630, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper634/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}