{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396361741, "tcdate": 1486396361741, "number": 1, "id": "SyfRsz8ug", "invitation": "ICLR.cc/2017/conference/-/paper108/acceptance", "forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396362251, "id": "ICLR.cc/2017/conference/-/paper108/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396362251}}}, {"tddate": null, "tmdate": 1484609495552, "tcdate": 1484609495552, "number": 7, "id": "BkkyuCc8e", "invitation": "ICLR.cc/2017/conference/-/paper108/public/comment", "forum": "Hy3_KuYxg", "replyto": "rkKawYqLl", "signatures": ["~Alex_Nowak1"], "readers": ["everyone"], "writers": ["~Alex_Nowak1"], "content": {"title": "Answering review 3. Currently improving our results.", "comment": "Dear reviewer 3,\n\nFirst of all, thanks for the comments and questions, they are really helping us to improve both the paper results and readability.\n\nRegarding your question:  The split architecture for the sorting task, in terms of parameters, it is just a model which is replicated at all nodes of the tree, whose input is a set of normalized points and outputs a binary mask on them. In order to train the model to be able to sort, we define a total loss which is the aggregation of the losses at all nodes.\n\nThe loss at a given node is computed using the output mask probabilities and the mask target we assign at this node. However, these mask targets (which points send to the left and which to the right), are not known 'a priori' (we just have the input-output pair unordered-ordered vector) and we have to find a way to generate them in order to train.\n\nThe procedure to train is the following: with the current model parameters, we split recursively the input until we are left with singletons. Every node of the tree defines a 2-partition on its input set of points. Some of the input points at that node are in the correct place (meaning that if you split recursively optimally from that node, it will nail its correct position in the ordered vector). These are the elements that we can supervise, while the other inputs at that node we won't. So the binary mask is created over these supervised points because we know to which side of the partition these must go in order to end up at the good position.\n\nSo the loss at that node will only take into account the elements that can be supervised, and we won't aggregate gradients from the other points because we are not able to create targets for them. Note that at the beginning of the training, we will only supervise the elements at the largest scale (top node), because most of the deeper partitions will be wrong. However, the supervision at deeper nodes will increase automatically with training as the coarse scales start to produce higher accuracy partitions. \nWe also add a regularization term with the goal of minimizing the whole complexity of the architecture by forcing partitions of same cardinality (maximizing the variance of the output probabilities).\n\nThe important fact of this model is its ability to train just using input-output pairs and adjust complexity. Note that our split block is linear, thus we solve the sorting problem with optimal complexity O(n*log(n)).\n\nWe are currently rewriting the paper to incorporate the reviewer\u2019s feedback and hopefully improve readability. We are also improving our results and have added comparisons with state-of-the-art (which we improve on the sorting), as well as extending the experiments to other tasks to highlight the generic nature of our architecture and training procedure. Since these changes are pretty significant, we understand they may require an extra round of review, but will do our best to update the manuscript before the final decision.\n\nThanks again for taking the time to review our work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287724617, "id": "ICLR.cc/2017/conference/-/paper108/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy3_KuYxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper108/reviewers", "ICLR.cc/2017/conference/paper108/areachairs"], "cdate": 1485287724617}}}, {"tddate": null, "tmdate": 1484588993166, "tcdate": 1484588993166, "number": 2, "id": "rkKawYqLl", "invitation": "ICLR.cc/2017/conference/-/paper108/official/comment", "forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "signatures": ["ICLR.cc/2017/conference/paper108/areachair1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper108/areachair1"], "content": {"title": "Author Response to third review?", "comment": "Dear authors,\n\ndo you plan to address the third reviewer's comments? Your responses could help bring some more clarity and improve the confidence for the final decision...\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287724359, "id": "ICLR.cc/2017/conference/-/paper108/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "Hy3_KuYxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper108/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper108/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper108/reviewers", "ICLR.cc/2017/conference/paper108/areachairs"], "cdate": 1485287724359}}}, {"tddate": null, "tmdate": 1482200096729, "tcdate": 1482200096729, "number": 3, "id": "ryYQEzUVl", "invitation": "ICLR.cc/2017/conference/-/paper108/official/review", "forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "signatures": ["ICLR.cc/2017/conference/paper108/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper108/AnonReviewer3"], "content": {"title": "Promising idea but hard-to-reproduce in current state", "rating": "4: Ok but not good enough - rejection", "review": "I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it's still missing. After going over this paper couple of times I'm still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning.\n\nAs an example of the difficulty, take section 4.3. It talks about training \"split block\" which is a function that can assign each element to either partition 0 or partition 1. At this point I'm looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as \"we must create artificial targets at every node of the generated tree from the available final target partition\". What are these artificial targets, and how do they relate to the problem of training the splitter? An example that explicitly goes through this construction would help with understanding.\n", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512694582, "id": "ICLR.cc/2017/conference/-/paper108/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper108/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper108/AnonReviewer2", "ICLR.cc/2017/conference/paper108/AnonReviewer1", "ICLR.cc/2017/conference/paper108/AnonReviewer3"], "reply": {"forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512694582}}}, {"tddate": null, "tmdate": 1481965042990, "tcdate": 1481965042990, "number": 2, "id": "B1slAdMEe", "invitation": "ICLR.cc/2017/conference/-/paper108/official/review", "forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "signatures": ["ICLR.cc/2017/conference/paper108/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper108/AnonReviewer1"], "content": {"title": "Nice problem statement, nut too immature to publish", "rating": "4: Ok but not good enough - rejection", "review": "The basic idea of this contribution is very nice and worth pursuing: how to use the powerful \u201cdivide and conquer\u201d algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.\n\nMy main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of supervision at lower recurrency levels) one can get a partially trainable sorter or convex hull solver.\n\nI found the contribution relatively hard to understand. High level ideas are mixed with low-level tricks required to get the model to work and it is not clear either how the models operate, nor how much of them was actually learned, and how much was designed. The answer to the questions did hep, nut didn't make it into the paper. Mixing the descriptions of the tricks required to solve the two tasks makes things even more confusing. I believe that the paper would be much more accessible if instead of promising a general solution it clearly stated the challenges faced by the authors and the possible solutions.\n\nHighlights:\n+ Proof-of-concept of a partially-trainable implementation of the important \u201cdivide and conquer\u201d paradigm\n++ Explicit reasoning about complexity of induced programs\n- The solution isn\u2019t generic enough to be applicable to unknown problems - the networks require tricks specific to each problem\n- The writing style pictures the method as very general, but falls back on very low level details specific to each task\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512694582, "id": "ICLR.cc/2017/conference/-/paper108/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper108/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper108/AnonReviewer2", "ICLR.cc/2017/conference/paper108/AnonReviewer1", "ICLR.cc/2017/conference/paper108/AnonReviewer3"], "reply": {"forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512694582}}}, {"tddate": null, "tmdate": 1481768061491, "tcdate": 1481768061483, "number": 1, "id": "ryHF2_JEx", "invitation": "ICLR.cc/2017/conference/-/paper108/official/review", "forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "signatures": ["ICLR.cc/2017/conference/paper108/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper108/AnonReviewer2"], "content": {"title": "Interesting but extremely difficult read", "rating": "3: Clear rejection", "review": "I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.\n\nThe paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.\n\nInstead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.\n\nBoth split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training. The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all.\n\nIn general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form. I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together. I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512694582, "id": "ICLR.cc/2017/conference/-/paper108/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper108/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper108/AnonReviewer2", "ICLR.cc/2017/conference/paper108/AnonReviewer1", "ICLR.cc/2017/conference/paper108/AnonReviewer3"], "reply": {"forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512694582}}}, {"tddate": null, "tmdate": 1481062390309, "tcdate": 1481062390298, "number": 4, "id": "Hk0l_hNQl", "invitation": "ICLR.cc/2017/conference/-/paper108/public/comment", "forum": "Hy3_KuYxg", "replyto": "BJlYY-X7x", "signatures": ["~Alex_Nowak1"], "readers": ["everyone"], "writers": ["~Alex_Nowak1"], "content": {"title": "Answers for Reviewer1", "comment": "Question 1: \u201cThe paper begins with great desiderata, but even after several reads (Sorry for the delay) I still fail to see how much is hardcoded and much supervision the models receive. Am I correct that for sorting  you hardcode the architecture to the the extent that a perfect solution only requires the splitter to split by the first element and the merger to select all elements?\u201d\n\n\nAnswer1: Sorting is a task that illustrates how recursion greatly simplifies the learning on problems that are scale invariant. You are right that the only thing that is needed in a recursive architecture is pivoting -- splitting inputs based on a comparison with some arbitrarily chosen input element. Note however that (i) our model does not hardcode that pivoting step, (ii) whereas an arbitrary pivoting will lead to correct output with average-case complexity O(N log N) , it will have significantly worse constants than the optimal pivoting (given by the median). Our experiments show in that simple setting the advantage of regularizing by the complexity, via the tree balancing.\n\n\n\n\nQuestion 2: \u201cPlease describe in more detail the implementation of the network on the tasks:\n- what is the supervision at each level for each task, maybe provide examples for sorting?\u201d\n\n\nAnswer 1: Our main model does NOT supervise each split/merge level; we only see the correct output at the very end of the split tree (in the example of sorting) and at the very end of the merge tree (in the example of convex hull). \n\n\nWe will provide some clarifying examples for small scales both for split and merge in the appendix.\n\n\n\n\nIn the split case, for instance, consider the input vector [7,1,4,5,6,2,3,8] and its corresponding target [1,2,3,4,5,6,7,8]. We forward the input vector to the recursive architecture. At the first scale obtain the partition (1.1)  ->  [1,4,2] and (1.2) -> [7,5,6,3,8]. The supervision at the first scale is \u2018strong\u2019 because given the cardinality of the output 2-partition (3 + 5 = 8) and the sorted vector we know the corresponding target for this split,  which will be the sets (target 1.1) -> {1,2,3} and (target 1.2) -> {4,5,6,7,8}. The first part of the loss at this scale will be the cross-entropy between the output bernouilli probabilities that the 2-partition has been sampled from p  = [p1, p2, \u2026, p8] and [0,1,0,0,0,1,1,0] which encodes the target. The second part of the loss will be the regularizing term enforcing a balanced partition by penalizing the minus empirical variance of the vector p (i.e, maximizing it).\n\n\nSuppose the partitions at next scale are (2.1) -> [1,2], (2.2) -> [4] and (2.3) -> [7,5,3], (2.4) -> [6,8].\nIn this case, we won\u2019t fully supervise the two splits because of the mistakes done at previous scale. As explained in the paper, we first consider the intersection of the set of elements in each partition with the elements at the same set of coordinates in the target. I.e, \nintersection({1,4,2}, {1,2,3}) = {1,2} and intersection({7,5,6,3,8}, {4,5,6,7,8}) = {5,6,7,8} and this will be the elements that we can supervise for each split. \n\n\nThe First part of the loss of first split will be cross-entropy between [p1, p3] and [0,0] (because the partition was 2 + 1 = 3 but we have only been able to supervise the first two coordinates).\nThe First part of the loss of second split will be the cross-entropy between [p4, p6, p7, p8] and [0,1,0,1] (because the partition was 3 + 2 = 5 but we can only supervise the coordinates 4,6,7,8).\n\n\nThe regularization term for each split will be applied to all bernouilli probabilities, not only the ones we can supervise. Observe that if the split block outputs random 2-partitions, the supervision at smaller scales will be considerably weak. However, the supervision will increase during learning because the splits at large scale will gradually perform correctly.\n\n\n\n\n\n\nQuestion 3: - \u201cfor sorting do you prevent the model from creating overlapping or incomplete partitions (the paper says that you sample, but you can essentially put an element to x1 or x2 depending on a binary variable, or make two tests, one whether the sample is in x1 and another that it is in x2). The first variant simplifies the task a lot.\u201d\n\n\nAnswer 3: Indeed, our initial implementation enforces non-overlapping split. We encode that by producing for each input i to the split block a probability p( i \\in x_1) that said input will be present at x_1. \n\n\nQuestion 4: - \u201cdo you train the basis of the recursion? You train the sorter up to 8 splits, not until sets of 1 element are produced. Do you simply fire quicksort and sort the subsets? Does this mean that if the net provides 8 very unbalanced splits you will feed it with a properly sorted sequence of 256-8 = 246 numbers?\u201d\n\n\nAnswer 4: We will clarify this important point. We keep applying the split module until all the sets are broken down to singletons. \nThe forward pass does NOT sort any subset based on some oracle information. \nThe gradients are only accumulated on the J largest scales (J=8 in our experiments). \nIf the tree for a given input is highly unbalanced (as your example), the loss will disregard any decision taken in the large chunk and promote that (i) the few elements that are split are indeed extrema, and (ii) that split balancing is increased.\n\n\nQuestion 5: - \u201ccan you explain why the weakly supervised convex hull network seems to be better than the strongly supervised one? \u201c\n\n\nAnswer 5: An interesting side-effect of training with weak supervision is that the model is trained under partial errors -- that is each merge block is likely to receive inputs which are not always convex hulls. This is effectively the regime in which the model runs at test time. However, strong supervision learns how to merge of two correct convex hulls into the convex hull of their union, so it has not been trained on inputs which are not convex hulls. This disadvantage could be corrected by \u2018augmenting\u2019 the training in strong supervision, but we emphatically focus on the weak supervision route thanks to its obvious advantages.\n\n\nQuestion 6: - \u201cfor sorting you implement the merger in a way that favors quicksort. Can you compare the performance on partially sorted data (in both directions) which is the true adversarial case for quicksort, rather than test the uniform and exponential distributions, that do not change the expected complexity of quicksort? The trivial solution of splitting on the first element of the set would work optimally under your dataset generation.\u201d\n\n\nAnswer 6: Note that our split model is set-to-set, that is, it is by construction invariant to permutations of the input. This means that the output of the split block does not depend on the the ordering of the input, this is the main reason we haven\u2019t performed these experiments. The feature of the input that can change the complexity of our recursive architecture is its distribution, because the ability to generate correct balanced trees (i.e, estimating the median) can be different depending on the input distribution shape. You can construct another trivial baseline by simply pivoting based on the mean, but for skewed distributions this will be far less efficient than splitting based on the median."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287724617, "id": "ICLR.cc/2017/conference/-/paper108/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy3_KuYxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper108/reviewers", "ICLR.cc/2017/conference/paper108/areachairs"], "cdate": 1485287724617}}}, {"tddate": null, "tmdate": 1480952183977, "tcdate": 1480952183970, "number": 2, "id": "BJlYY-X7x", "invitation": "ICLR.cc/2017/conference/-/paper108/pre-review/question", "forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "signatures": ["ICLR.cc/2017/conference/paper108/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper108/AnonReviewer1"], "content": {"title": "Please clearly state what parts of the models are trained", "question": "The paper begins with great desiderata, but even after several reads (Sorry for the delay) I still fail to see how much is hardcoded and much supervision the models receive. Am I correct that for sorting  you hardcode the architecture to the the extent that a perfect solution only requires the splitter to split by the first element and the merger to select all elements?\n\nPlease describe in more detail the implementation of he network on the tasks:\n- what is the supervision at each level for each task, maybe provide examples for sorting?\n- for sorting do you prevent the model from creating overlapping or incomplete partitions (the paper says that you sample, but you can essentially put an element to x1 or x2 depending on a binary variable, or make two tests, one whether the sample is in x1 and another that it is in x2). The first variant simplifies the task a lot.\n- do you train the basis of the recursion? You train the sorter up to 8 splits, not until sets of 1 element are produced. Do you simply fire quicksort and sort the subsets? Does this mean that if the net provides 8 very unbalanced splits you will feed it with a properly sorted sequence of 256-8 = 246 numbers?\n- can you explain why the weakly supervised convex hull network seems to be better than the strongly supervised one? \n- for sorting you implement the merger in a way that favors quicksort. Can you compare the performance on partially sorted data (in both directions) which is the true adversarial case for quicksort, rather than test the uniform and exponential distributions, that do not change the expected complexity of quicksort? The trivial solution of splitting on the first element of the set would work optimally under your dataset generation.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959457527, "id": "ICLR.cc/2017/conference/-/paper108/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper108/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper108/AnonReviewer2", "ICLR.cc/2017/conference/paper108/AnonReviewer1"], "reply": {"forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959457527}}}, {"tddate": null, "tmdate": 1480741578154, "tcdate": 1480741578150, "number": 1, "id": "BJzCfAJ7g", "invitation": "ICLR.cc/2017/conference/-/paper108/pre-review/question", "forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "signatures": ["ICLR.cc/2017/conference/paper108/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper108/AnonReviewer2"], "content": {"title": "Backpropagation through the sampling operation", "question": "How do you backpropagate through the sampling procedures? Are the gradient estimates biased?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959457527, "id": "ICLR.cc/2017/conference/-/paper108/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper108/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper108/AnonReviewer2", "ICLR.cc/2017/conference/paper108/AnonReviewer1"], "reply": {"forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper108/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959457527}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1479014126779, "tcdate": 1478228340234, "number": 108, "id": "Hy3_KuYxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Hy3_KuYxg", "signatures": ["~Joan_Bruna1"], "readers": ["everyone"], "content": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1478574336319, "tcdate": 1478574336314, "number": 3, "id": "HJubZpAel", "invitation": "ICLR.cc/2017/conference/-/paper108/public/comment", "forum": "Hy3_KuYxg", "replyto": "Bk2vxpClg", "signatures": ["~Tara_N_Sainath1"], "readers": ["everyone"], "writers": ["~Tara_N_Sainath1"], "content": {"title": "pdf upload", "comment": "yes pls upload a revised pdf"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287724617, "id": "ICLR.cc/2017/conference/-/paper108/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy3_KuYxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper108/reviewers", "ICLR.cc/2017/conference/paper108/areachairs"], "cdate": 1485287724617}}}, {"tddate": null, "tmdate": 1478574180464, "tcdate": 1478574180457, "number": 2, "id": "Bk2vxpClg", "invitation": "ICLR.cc/2017/conference/-/paper108/public/comment", "forum": "Hy3_KuYxg", "replyto": "ByQnnICge", "signatures": ["~Joan_Bruna1"], "readers": ["everyone"], "writers": ["~Joan_Bruna1"], "content": {"title": "Fixed", "comment": "Dear PC,\nWe have updated the main pdf file (sorry it was a spurious package lurking). \n\nShould I create a revision?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287724617, "id": "ICLR.cc/2017/conference/-/paper108/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy3_KuYxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper108/reviewers", "ICLR.cc/2017/conference/paper108/areachairs"], "cdate": 1485287724617}}}, {"tddate": null, "tmdate": 1478554514879, "tcdate": 1478548650656, "number": 1, "id": "ByQnnICge", "invitation": "ICLR.cc/2017/conference/-/paper108/public/comment", "forum": "Hy3_KuYxg", "replyto": "Hy3_KuYxg", "signatures": ["~Tara_N_Sainath1"], "readers": ["everyone"], "writers": ["~Tara_N_Sainath1"], "content": {"title": "ICLR Paper Format", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct margin spacing for your submission to be considered. Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Divide and Conquer with Neural Networks", "abstract": "We consider the learning of algorithmic tasks by mere observation of input-output pairs. \nRather than studying this as a black-box discrete regression problem with no assumption whatsoever \non the input-output mapping, we concentrate on tasks that are amenable to the principle of divide and conquer, and study what are its implications in terms of learning. \n\nThis principle creates a powerful inductive bias that we exploit with neural architectures that are defined recursively, by learning two scale-invariant atomic operators: how to split a given input into two disjoint sets, and how to merge two partially solved tasks into a larger partial solution. The scale invariance creates parameter sharing across all stages of the architecture, and the dynamic design creates architectures whose complexity can be tuned in a differentiable manner.\n\nAs a result, our model is trained by backpropagation not only to minimize the errors at the output, but also to do so as efficiently as possible, by enforcing shallower computation graphs. Moreover, thanks to the scale invariance, the model can be trained only with only input/output pairs, removing the need to know oracle intermediate split and merge decisions. As it turns out, accuracy and complexity are not independent qualities, and we verify empirically that when the learnt complexity matches the underlying complexity of the task, this results in higher accuracy and better generalization in two paradigmatic problems: sorting and finding planar convex hulls.", "pdf": "/pdf/5fc8ed0ce293f8444896021bdacd823dcb8cea37.pdf", "TL;DR": "learn dynamic programming with neural networks", "paperhash": "nowak|divide_and_conquer_with_neural_networks", "keywords": ["Deep learning"], "authors": ["Alex Nowak", "Joan Bruna"], "conflicts": ["nyu.edu", "berkeley.edu", "fb.com"], "authorids": ["anv273@nyu.edu", "bruna@cims.nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287724617, "id": "ICLR.cc/2017/conference/-/paper108/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Hy3_KuYxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper108/reviewers", "ICLR.cc/2017/conference/paper108/areachairs"], "cdate": 1485287724617}}}], "count": 13}