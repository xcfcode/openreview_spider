{"notes": [{"id": "BylTta4YvB", "original": "HkgFRrpvwH", "number": 687, "cdate": 1569439109184, "ddate": null, "tcdate": 1569439109184, "tmdate": 1577168219221, "tddate": null, "forum": "BylTta4YvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "How Well Do WGANs Estimate the Wasserstein Metric?", "authors": ["Anton Mallasto", "Guido Mont\u00fafar", "Augusto Gerolin"], "authorids": ["anton.mallasto@gmail.com", "montufar@math.ucla.edu", "augustogerolin@gmail.com"], "keywords": ["Optimal Transport", "Wasserstein Metric", "Generative Adversial Networks"], "TL;DR": "A study of how well different methods used to compute the 1-Wasserstein distance in the GAN setting actually perform.", "abstract": "Generative modelling is often cast as minimizing a similarity measure between a data distribution and a model distribution. Recently, a popular choice for the similarity measure has been the Wasserstein metric, which can be expressed in the Kantorovich duality formulation as the optimum difference of the expected values of a potential function under the real data distribution and the model hypothesis. In practice, the potential is approximated with a neural network and is called the discriminator. Duality constraints on the function class of the discriminator are enforced approximately, and the expectations are estimated from samples. This gives at least three sources of errors: the approximated discriminator and constraints, the estimation of the expectation value, and the optimization required to find the optimal potential. In this work, we study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform. We consider, in particular, the $c$-transform formulation, which eliminates the need to enforce the constraints explicitly. We demonstrate that the $c$-transform allows for a more accurate estimation of the true Wasserstein metric from samples, but surprisingly, does not", "pdf": "/pdf/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "paperhash": "mallasto|how_well_do_wgans_estimate_the_wasserstein_metric", "original_pdf": "/attachment/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "_bibtex": "@misc{\nmallasto2020how,\ntitle={How Well Do {\\{}WGAN{\\}}s Estimate the Wasserstein Metric?},\nauthor={Anton Mallasto and Guido Mont{\\'u}far and Augusto Gerolin},\nyear={2020},\nurl={https://openreview.net/forum?id=BylTta4YvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "4r93aL6xQo", "original": null, "number": 1, "cdate": 1576798703303, "ddate": null, "tcdate": 1576798703303, "tmdate": 1576800932723, "tddate": null, "forum": "BylTta4YvB", "replyto": "BylTta4YvB", "invitation": "ICLR.cc/2020/Conference/Paper687/-/Decision", "content": {"decision": "Reject", "comment": "There is insufficient support to recommend accepting this paper.  Generally the reviewers found the technical contribution to be insufficient, and were not sufficiently convinced by the experimental evaluation.  The feedback provided should help the authors improve their paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Well Do WGANs Estimate the Wasserstein Metric?", "authors": ["Anton Mallasto", "Guido Mont\u00fafar", "Augusto Gerolin"], "authorids": ["anton.mallasto@gmail.com", "montufar@math.ucla.edu", "augustogerolin@gmail.com"], "keywords": ["Optimal Transport", "Wasserstein Metric", "Generative Adversial Networks"], "TL;DR": "A study of how well different methods used to compute the 1-Wasserstein distance in the GAN setting actually perform.", "abstract": "Generative modelling is often cast as minimizing a similarity measure between a data distribution and a model distribution. Recently, a popular choice for the similarity measure has been the Wasserstein metric, which can be expressed in the Kantorovich duality formulation as the optimum difference of the expected values of a potential function under the real data distribution and the model hypothesis. In practice, the potential is approximated with a neural network and is called the discriminator. Duality constraints on the function class of the discriminator are enforced approximately, and the expectations are estimated from samples. This gives at least three sources of errors: the approximated discriminator and constraints, the estimation of the expectation value, and the optimization required to find the optimal potential. In this work, we study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform. We consider, in particular, the $c$-transform formulation, which eliminates the need to enforce the constraints explicitly. We demonstrate that the $c$-transform allows for a more accurate estimation of the true Wasserstein metric from samples, but surprisingly, does not", "pdf": "/pdf/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "paperhash": "mallasto|how_well_do_wgans_estimate_the_wasserstein_metric", "original_pdf": "/attachment/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "_bibtex": "@misc{\nmallasto2020how,\ntitle={How Well Do {\\{}WGAN{\\}}s Estimate the Wasserstein Metric?},\nauthor={Anton Mallasto and Guido Mont{\\'u}far and Augusto Gerolin},\nyear={2020},\nurl={https://openreview.net/forum?id=BylTta4YvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BylTta4YvB", "replyto": "BylTta4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728812, "tmdate": 1576800281293, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper687/-/Decision"}}}, {"id": "rklP3uT8ir", "original": null, "number": 3, "cdate": 1573472431085, "ddate": null, "tcdate": 1573472431085, "tmdate": 1573472431085, "tddate": null, "forum": "BylTta4YvB", "replyto": "SygEdKHnKS", "invitation": "ICLR.cc/2020/Conference/Paper687/-/Official_Comment", "content": {"title": "Author feedback", "comment": "Thank you for pointing out where we could improve. Below, we will address those comments, for which the explanations will also be added to the paper. Furthermore, we give arguments to why we view the contribution as a fit for ICLR.\n\nWe utilize POT\u2019s ot.emd method to compute the ordinary optimal transport quantity, which is implemented using the method in [1]. We apologize for not explaining which method is used in the entropic case, where the ot.sinkhorn method is utilized with the \u2018sinkhorn\u2019 option.\n\nThe sacrifice due to only applying one Sinkhorn-Knopp iteration for the unbiasing terms is slight, as can be seen from the resulting errors with respect to the ground truth in the experiments.\n\n\nWe view the main contributions of this paper to be the study of the approximation and estimation (stability) of the WGAN methods, which unfortunately has not been considered to its rightful extend in the literature yet. On the technical side, we are the first to consider the smooth c-transform in the WGAN setting, although it relates closely to [2], where the primal formulation of entropic optimal transport is considered. We are happy to hear that you consider the experimentation extensive. On top of new methodological contributions, it is important to also consider comparative studies to guide the development of new methodologies. For example, it is important to consider which functional classes the discriminator should belong to, which reflects on the representation part of ICLR. From our experiments, it seems that, when evaluating optimal transport quantities in the general setting (independent of generation), the $c$-transform provides a meaningful class. In the generative setting the task gets more complicated due to the minimax nature of the game.\n\n[1] Bonneel, N., Van De Panne, M., Paris, S., & Heidrich, W. (2011, December). Displacement interpolation using Lagrangian mass transport. In\u00a0ACM Transactions on Graphics (TOG)\u00a0(Vol. 30, No. 6, p. 158). ACM.\n\n[2] Genevay, A., Peyre, G. & Cuturi, M.. (2018). Learning Generative Models with Sinkhorn Divergences. Proceedings of the Twenty-Fi"}, "signatures": ["ICLR.cc/2020/Conference/Paper687/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Well Do WGANs Estimate the Wasserstein Metric?", "authors": ["Anton Mallasto", "Guido Mont\u00fafar", "Augusto Gerolin"], "authorids": ["anton.mallasto@gmail.com", "montufar@math.ucla.edu", "augustogerolin@gmail.com"], "keywords": ["Optimal Transport", "Wasserstein Metric", "Generative Adversial Networks"], "TL;DR": "A study of how well different methods used to compute the 1-Wasserstein distance in the GAN setting actually perform.", "abstract": "Generative modelling is often cast as minimizing a similarity measure between a data distribution and a model distribution. Recently, a popular choice for the similarity measure has been the Wasserstein metric, which can be expressed in the Kantorovich duality formulation as the optimum difference of the expected values of a potential function under the real data distribution and the model hypothesis. In practice, the potential is approximated with a neural network and is called the discriminator. Duality constraints on the function class of the discriminator are enforced approximately, and the expectations are estimated from samples. This gives at least three sources of errors: the approximated discriminator and constraints, the estimation of the expectation value, and the optimization required to find the optimal potential. In this work, we study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform. We consider, in particular, the $c$-transform formulation, which eliminates the need to enforce the constraints explicitly. We demonstrate that the $c$-transform allows for a more accurate estimation of the true Wasserstein metric from samples, but surprisingly, does not", "pdf": "/pdf/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "paperhash": "mallasto|how_well_do_wgans_estimate_the_wasserstein_metric", "original_pdf": "/attachment/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "_bibtex": "@misc{\nmallasto2020how,\ntitle={How Well Do {\\{}WGAN{\\}}s Estimate the Wasserstein Metric?},\nauthor={Anton Mallasto and Guido Mont{\\'u}far and Augusto Gerolin},\nyear={2020},\nurl={https://openreview.net/forum?id=BylTta4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylTta4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference/Paper687/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper687/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper687/Reviewers", "ICLR.cc/2020/Conference/Paper687/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper687/Authors|ICLR.cc/2020/Conference/Paper687/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167742, "tmdate": 1576860559450, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference/Paper687/Reviewers", "ICLR.cc/2020/Conference/Paper687/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper687/-/Official_Comment"}}}, {"id": "SylRMdaIoH", "original": null, "number": 2, "cdate": 1573472277955, "ddate": null, "tcdate": 1573472277955, "tmdate": 1573472277955, "tddate": null, "forum": "BylTta4YvB", "replyto": "H1llP9f6tB", "invitation": "ICLR.cc/2020/Conference/Paper687/-/Official_Comment", "content": {"title": "Author feedback", "comment": "Thank you for the comments, we especially appreciate your point on whether evaluating the 'batchwise' distance is meaningful, as this is also something we had to spend some time thinking on. We will address the comments below.\n\nYour criticism on the approximation example is point on, which is exactly why we also considered the stability experiment, where we compared the \u2018batchwise\u2019 and \u2018full\u2019 evaluations, i.e., <f_\\star, \\mu-\\nu>, <f_star, \\mu_I-\\nu_I>, <f,\\mu-\\nu> and <f,\\mu_I-\\nu_I>. Even in these cases, the weight-clipping and gradient penalty methods are far off. On the other hand, there are successful \u2018batchwise\u2019 training algorithms for GANs, such as the Sinkhorn based[1], and for example GAN with quadratic transport [2], which motivates the study of the batchwise approximation.\n\n\nBased on the current methodology, it seems that for general cost functions, the \u2018batchwise\u2019 evaluation is the way to go, as is supported by the theoretical results (i.e. the max in the Kantorovich duality can be achieved by the pair (f,f^c)). Additionally, including the Kantorovich potentials in other cases than the fortunate 1-Wasserstein case becomes highly non-trivial. As examples of applications with more general cost functions, we would like to point out an example in quantum chemistry, where the Coulomb cost (related to Coulomb potential) is natural [3]. On the other hand, maximum-likelihood deconvolutions with respect to different noise distributions relates to the minimization of entropic optimal transport with different cost functions[4].\n\n[1] Genevay, A., Peyre, G. & Cuturi, M.. (2018). Learning Generative Models with Sinkhorn Divergences. Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics, in PMLR 84:1608-1617\n\n[2] Liu, H., Gu, X., & Samaras, D. (2019). Wasserstein GAN with Quadratic Transport Cost. In\u00a0Proceedings of the IEEE International Conference on Computer Vision\u00a0(pp. 4832-4841).\n\n[3] Cotar, C., Friesecke, G., & Kl\u00fcppelberg, C. (2013). Density functional theory and optimal transportation with Coulomb cost.\u00a0Communications on Pure and Applied Mathematics,\u00a066(4), 548-599.\n\n[4] Rigollet, P., & Weed, J. (2018). Entropic optimal transport is maximum-likelihood deconvolution.\u00a0Comptes Rendus Mathematique,\u00a0356(11-12), 1228-1235."}, "signatures": ["ICLR.cc/2020/Conference/Paper687/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Well Do WGANs Estimate the Wasserstein Metric?", "authors": ["Anton Mallasto", "Guido Mont\u00fafar", "Augusto Gerolin"], "authorids": ["anton.mallasto@gmail.com", "montufar@math.ucla.edu", "augustogerolin@gmail.com"], "keywords": ["Optimal Transport", "Wasserstein Metric", "Generative Adversial Networks"], "TL;DR": "A study of how well different methods used to compute the 1-Wasserstein distance in the GAN setting actually perform.", "abstract": "Generative modelling is often cast as minimizing a similarity measure between a data distribution and a model distribution. Recently, a popular choice for the similarity measure has been the Wasserstein metric, which can be expressed in the Kantorovich duality formulation as the optimum difference of the expected values of a potential function under the real data distribution and the model hypothesis. In practice, the potential is approximated with a neural network and is called the discriminator. Duality constraints on the function class of the discriminator are enforced approximately, and the expectations are estimated from samples. This gives at least three sources of errors: the approximated discriminator and constraints, the estimation of the expectation value, and the optimization required to find the optimal potential. In this work, we study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform. We consider, in particular, the $c$-transform formulation, which eliminates the need to enforce the constraints explicitly. We demonstrate that the $c$-transform allows for a more accurate estimation of the true Wasserstein metric from samples, but surprisingly, does not", "pdf": "/pdf/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "paperhash": "mallasto|how_well_do_wgans_estimate_the_wasserstein_metric", "original_pdf": "/attachment/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "_bibtex": "@misc{\nmallasto2020how,\ntitle={How Well Do {\\{}WGAN{\\}}s Estimate the Wasserstein Metric?},\nauthor={Anton Mallasto and Guido Mont{\\'u}far and Augusto Gerolin},\nyear={2020},\nurl={https://openreview.net/forum?id=BylTta4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylTta4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference/Paper687/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper687/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper687/Reviewers", "ICLR.cc/2020/Conference/Paper687/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper687/Authors|ICLR.cc/2020/Conference/Paper687/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167742, "tmdate": 1576860559450, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference/Paper687/Reviewers", "ICLR.cc/2020/Conference/Paper687/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper687/-/Official_Comment"}}}, {"id": "ryg63wpIjH", "original": null, "number": 1, "cdate": 1573472181339, "ddate": null, "tcdate": 1573472181339, "tmdate": 1573472181339, "tddate": null, "forum": "BylTta4YvB", "replyto": "H1gjYpiaFH", "invitation": "ICLR.cc/2020/Conference/Paper687/-/Official_Comment", "content": {"title": "Author feedback", "comment": "Thank you for the valuable comments, which we will address below.\n\nSentence after (17): The range of the discriminator in the WGAN setting can attain any value, and thus its range is not constrained to lie inside of [0,1], unlike in the vanilla GAN case, where the attained value reflects on a probability of a picture being from the given dataset.\n\nPenalization term: Thank you for pointing out the mistake on the discussion about the penalization term. Although 1-Lipschitzness does imply that the gradient norm should lie inside of [0,1], due to the properties of the optimal matching, on the support of the two measures considered the norm should equal to one.\n\n Subjective error: The error given in (24) only applies to the smooth c-transform case. Therefore, it does give a bias towards the smooth c-transform, but not the vanilla c-transform. The error was described as subjective, and not absolute, as the POT values can err, too.\n\nDependence on architecture: We agree, that the generative results do seem to depend on the architecture. However, it seems that the key difference is convolutional vs. non-convolutional architectures. We are currently running experiments using the popular resnet architecture, and in the case of the smooth and vanilla c-transforms, the resulting images look very similar to the DCGAN case.\n\nContribution: We see that the outcomes of the experiments would be valuable to the optimal transport and WGAN communities due to the following aspects: we show that the c-transform methods, which are able to incorporate any cost function, provide a a meaningful functional family for the discriminator. This then brings forth the surprising result we emphasized in the paper, that efficient computation of the optimal transport quantity might be suboptimal for generative purposes. "}, "signatures": ["ICLR.cc/2020/Conference/Paper687/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Well Do WGANs Estimate the Wasserstein Metric?", "authors": ["Anton Mallasto", "Guido Mont\u00fafar", "Augusto Gerolin"], "authorids": ["anton.mallasto@gmail.com", "montufar@math.ucla.edu", "augustogerolin@gmail.com"], "keywords": ["Optimal Transport", "Wasserstein Metric", "Generative Adversial Networks"], "TL;DR": "A study of how well different methods used to compute the 1-Wasserstein distance in the GAN setting actually perform.", "abstract": "Generative modelling is often cast as minimizing a similarity measure between a data distribution and a model distribution. Recently, a popular choice for the similarity measure has been the Wasserstein metric, which can be expressed in the Kantorovich duality formulation as the optimum difference of the expected values of a potential function under the real data distribution and the model hypothesis. In practice, the potential is approximated with a neural network and is called the discriminator. Duality constraints on the function class of the discriminator are enforced approximately, and the expectations are estimated from samples. This gives at least three sources of errors: the approximated discriminator and constraints, the estimation of the expectation value, and the optimization required to find the optimal potential. In this work, we study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform. We consider, in particular, the $c$-transform formulation, which eliminates the need to enforce the constraints explicitly. We demonstrate that the $c$-transform allows for a more accurate estimation of the true Wasserstein metric from samples, but surprisingly, does not", "pdf": "/pdf/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "paperhash": "mallasto|how_well_do_wgans_estimate_the_wasserstein_metric", "original_pdf": "/attachment/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "_bibtex": "@misc{\nmallasto2020how,\ntitle={How Well Do {\\{}WGAN{\\}}s Estimate the Wasserstein Metric?},\nauthor={Anton Mallasto and Guido Mont{\\'u}far and Augusto Gerolin},\nyear={2020},\nurl={https://openreview.net/forum?id=BylTta4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BylTta4YvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference/Paper687/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper687/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper687/Reviewers", "ICLR.cc/2020/Conference/Paper687/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper687/Authors|ICLR.cc/2020/Conference/Paper687/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167742, "tmdate": 1576860559450, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper687/Authors", "ICLR.cc/2020/Conference/Paper687/Reviewers", "ICLR.cc/2020/Conference/Paper687/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper687/-/Official_Comment"}}}, {"id": "SygEdKHnKS", "original": null, "number": 1, "cdate": 1571735916420, "ddate": null, "tcdate": 1571735916420, "tmdate": 1572972564663, "tddate": null, "forum": "BylTta4YvB", "replyto": "BylTta4YvB", "invitation": "ICLR.cc/2020/Conference/Paper687/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper has studied the efficiency and stability of computing the Wasserstein metric through its dual formulation under weight clipping, gradient penalty, c-transform and (c- \u03f5)-transform. The results show that (c- \u03f5)-transform and c-transform give more estimation of the Wasserstein distances than the gradient penalty and weight clipping methods in the given experiments, but the gradient penalty method produces more compelling samples in the generative setting.\n\nThe paper is well written and the experiment section is extensive. However, it is more like an extended experiment report to me which is very valuable but lacks sufficient technical novelty expected at ICLR.\n\nAnother comment is that when the authors mentioned \"... are compared to ground truth values d_ground computed by POT\", there needs more explanations on what that library actually does to compute the Wasserstein distance to make the paper self-contained, e.g. what exact algorithms it uses as there are also dozens of different algorithms implemented in that library.\n\nIn Section 3.1, the authors state that \"it tends to converge within couple of iterations of the symmetric Sinkhorn-Knopp algorithm. For efficiency, we approximate these terms with one Sinkhorn-Knopp iteration\". What is the extent of sacrifice in accuracy due to this approximation? The authors should provide more evidences to justify the approximation."}, "signatures": ["ICLR.cc/2020/Conference/Paper687/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper687/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Well Do WGANs Estimate the Wasserstein Metric?", "authors": ["Anton Mallasto", "Guido Mont\u00fafar", "Augusto Gerolin"], "authorids": ["anton.mallasto@gmail.com", "montufar@math.ucla.edu", "augustogerolin@gmail.com"], "keywords": ["Optimal Transport", "Wasserstein Metric", "Generative Adversial Networks"], "TL;DR": "A study of how well different methods used to compute the 1-Wasserstein distance in the GAN setting actually perform.", "abstract": "Generative modelling is often cast as minimizing a similarity measure between a data distribution and a model distribution. Recently, a popular choice for the similarity measure has been the Wasserstein metric, which can be expressed in the Kantorovich duality formulation as the optimum difference of the expected values of a potential function under the real data distribution and the model hypothesis. In practice, the potential is approximated with a neural network and is called the discriminator. Duality constraints on the function class of the discriminator are enforced approximately, and the expectations are estimated from samples. This gives at least three sources of errors: the approximated discriminator and constraints, the estimation of the expectation value, and the optimization required to find the optimal potential. In this work, we study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform. We consider, in particular, the $c$-transform formulation, which eliminates the need to enforce the constraints explicitly. We demonstrate that the $c$-transform allows for a more accurate estimation of the true Wasserstein metric from samples, but surprisingly, does not", "pdf": "/pdf/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "paperhash": "mallasto|how_well_do_wgans_estimate_the_wasserstein_metric", "original_pdf": "/attachment/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "_bibtex": "@misc{\nmallasto2020how,\ntitle={How Well Do {\\{}WGAN{\\}}s Estimate the Wasserstein Metric?},\nauthor={Anton Mallasto and Guido Mont{\\'u}far and Augusto Gerolin},\nyear={2020},\nurl={https://openreview.net/forum?id=BylTta4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylTta4YvB", "replyto": "BylTta4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575773394289, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper687/Reviewers"], "noninvitees": [], "tcdate": 1570237748542, "tmdate": 1575773394310, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper687/-/Official_Review"}}}, {"id": "H1llP9f6tB", "original": null, "number": 2, "cdate": 1571789399970, "ddate": null, "tcdate": 1571789399970, "tmdate": 1572972564630, "tddate": null, "forum": "BylTta4YvB", "replyto": "BylTta4YvB", "invitation": "ICLR.cc/2020/Conference/Paper687/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "\n[Summary]\nThis paper provides an empirical evaluation of commonly used discriminator training strategies in estimating the Wasserstein distance between distributions. The paper finds that methods motivated from optimal transport theory, e.g. c-transform and (c,\\eps)-transform, perform better in evaluating the Wasserstein distance than methods commonly used in WGAN practice such as weight clipping and gradient penalty. However, when deployed in WGANs as the discriminator training strategy, these methods do not generate images as high-quality as the gradient penalty method. \n\n[Pros]\nThe question considered in this paper, i.e. how well does various discriminator strategies (as proxies of the infeasible all 1-Lipschitz discriminator) perform for *evaluating* the Wasserstein distributions, is important for strengthening our understanding of generative models. The main result that methods that are better at computing W may not be better at generating images is interesting, and agrees with the theoretical insight (e.g. Arora et al. 2017) that *non-parametric* minimization of W may not be a good explanation for the generative power of WGANs.\n\n[Cons]\nI have concerns about the specific setting of \u201cmini-batch distance\u201d considered in this paper, in that whether it is really a sensible task (and can say anything about computing W) given the curse of dimensionality of estimating W from samples. The paper did not really discuss this issue, and from my own thoughts I don\u2019t think the task avoids this issue. \n\nFrom my understanding, the \u201cApproximation\u201d experiment does the following:\n(1) Set (\\mu, \\nu) to be a random split of a dataset and consider them to be the populations. Let\u2019s let f_\\star denote the (ground truth) optimal discriminator between (\\mu, \\nu).\n(2) Train discriminators (f_wc, f_gp, f_c, f_ceps) (using the different algorithms) from training batches from (\\mu, \\nu).\n(3) Evaluate <f, \\mu\u2019_l - \\nu\u2019_l> where (\\mu\u2019_l, \\nu\u2019_l) are fresh test batches from (\\mu, \\nu) and f is one of the above trained discriminators.\nIn comparison, the \u201cground truth\u201d computes W(\\mu\u2019_l, \\nu\u2019_l) = <f_l, \\mu\u2019_l, \\nu\u2019_l> from the POT package (though not necessarily through explicitly computing f_l.)\n\nThe issue with this is that we expect the method to perform well if f ~= f_l, which can be achievable if all the f_l\u2019s are similar (and hopefully they\u2019re all approximately equal to f_\\star.) However I don\u2019t think this is true -- as (\\mu\u2019_l, \\nu\u2019_l) are samples, and because of the curse of dimensionality, we should expect the f_l\u2019s to be quite different from each other. (Otherwise if they\u2019re really just ~= f_\\star, then we can use standard concentration to show the W(\\mu\u2019_l, \\nu\u2019_l) ~= W(\\mu, \\nu), which we know is not true from curse of dim.)\n\nGiven this, I don\u2019t think the task of comparing <f, \\mu\u2019_l, \\nu\u2019_l> with the POT results really says anything about their power in computing W. I would be glad though to hear back from the authors to see if my understanding is accurate, and adjust my evaluation from there.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper687/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper687/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Well Do WGANs Estimate the Wasserstein Metric?", "authors": ["Anton Mallasto", "Guido Mont\u00fafar", "Augusto Gerolin"], "authorids": ["anton.mallasto@gmail.com", "montufar@math.ucla.edu", "augustogerolin@gmail.com"], "keywords": ["Optimal Transport", "Wasserstein Metric", "Generative Adversial Networks"], "TL;DR": "A study of how well different methods used to compute the 1-Wasserstein distance in the GAN setting actually perform.", "abstract": "Generative modelling is often cast as minimizing a similarity measure between a data distribution and a model distribution. Recently, a popular choice for the similarity measure has been the Wasserstein metric, which can be expressed in the Kantorovich duality formulation as the optimum difference of the expected values of a potential function under the real data distribution and the model hypothesis. In practice, the potential is approximated with a neural network and is called the discriminator. Duality constraints on the function class of the discriminator are enforced approximately, and the expectations are estimated from samples. This gives at least three sources of errors: the approximated discriminator and constraints, the estimation of the expectation value, and the optimization required to find the optimal potential. In this work, we study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform. We consider, in particular, the $c$-transform formulation, which eliminates the need to enforce the constraints explicitly. We demonstrate that the $c$-transform allows for a more accurate estimation of the true Wasserstein metric from samples, but surprisingly, does not", "pdf": "/pdf/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "paperhash": "mallasto|how_well_do_wgans_estimate_the_wasserstein_metric", "original_pdf": "/attachment/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "_bibtex": "@misc{\nmallasto2020how,\ntitle={How Well Do {\\{}WGAN{\\}}s Estimate the Wasserstein Metric?},\nauthor={Anton Mallasto and Guido Mont{\\'u}far and Augusto Gerolin},\nyear={2020},\nurl={https://openreview.net/forum?id=BylTta4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylTta4YvB", "replyto": "BylTta4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575773394289, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper687/Reviewers"], "noninvitees": [], "tcdate": 1570237748542, "tmdate": 1575773394310, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper687/-/Official_Review"}}}, {"id": "H1gjYpiaFH", "original": null, "number": 3, "cdate": 1571827074930, "ddate": null, "tcdate": 1571827074930, "tmdate": 1572972564588, "tddate": null, "forum": "BylTta4YvB", "replyto": "BylTta4YvB", "invitation": "ICLR.cc/2020/Conference/Paper687/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper empirically evaluates different variants of WGAN (with weight clipping, gradient penalty, c-transform, and the generalized c-transform under entropy relaxation). The experiments, mainly performed over three datasets (MNIST, CIFAR10, CelebA), are designed to evaluate how well the Wasserstein distance is approximated, how much these approximations depend on batch sizes, and how good are the obtained generative models.\n\nI find the presentation of the different background work and models to be excellent, especially for someone who's not expert on WGANs like me. However, they may want to check the writing, like the sentence just after (17) or the penalization term between (18) and (19).\n\nThe contributions of the paper are experimental. The authors argue that they obtain a surprising observation, which is that \"the method best approximating the Wasserstein distance does not produce the best looking images in the generative setting \". \nHowever, the goodness of the approximation is measured with (24), which the authors called \"subjective error\". I think the authors may want to comment more on this measure, which seems to favor the different transforms.\nAlso, the quality of the generative models seems to strongly depend on the architectures used in WGAN. The authors' conclusions are based on DCGAN. However, the results obtained with simple MLP and presented in the appendix have not the same clear distinction as with DCGAN.\n\nOverall, although I liked the presentation very much, I feel the experimental results may be a bit too light for a publication in a venue such as ICLR."}, "signatures": ["ICLR.cc/2020/Conference/Paper687/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper687/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How Well Do WGANs Estimate the Wasserstein Metric?", "authors": ["Anton Mallasto", "Guido Mont\u00fafar", "Augusto Gerolin"], "authorids": ["anton.mallasto@gmail.com", "montufar@math.ucla.edu", "augustogerolin@gmail.com"], "keywords": ["Optimal Transport", "Wasserstein Metric", "Generative Adversial Networks"], "TL;DR": "A study of how well different methods used to compute the 1-Wasserstein distance in the GAN setting actually perform.", "abstract": "Generative modelling is often cast as minimizing a similarity measure between a data distribution and a model distribution. Recently, a popular choice for the similarity measure has been the Wasserstein metric, which can be expressed in the Kantorovich duality formulation as the optimum difference of the expected values of a potential function under the real data distribution and the model hypothesis. In practice, the potential is approximated with a neural network and is called the discriminator. Duality constraints on the function class of the discriminator are enforced approximately, and the expectations are estimated from samples. This gives at least three sources of errors: the approximated discriminator and constraints, the estimation of the expectation value, and the optimization required to find the optimal potential. In this work, we study how well the methods, that are used in generative adversarial networks to approximate the Wasserstein metric, perform. We consider, in particular, the $c$-transform formulation, which eliminates the need to enforce the constraints explicitly. We demonstrate that the $c$-transform allows for a more accurate estimation of the true Wasserstein metric from samples, but surprisingly, does not", "pdf": "/pdf/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "paperhash": "mallasto|how_well_do_wgans_estimate_the_wasserstein_metric", "original_pdf": "/attachment/e0262e13f0a56c423ad7a66d3bfd364fdc7b7363.pdf", "_bibtex": "@misc{\nmallasto2020how,\ntitle={How Well Do {\\{}WGAN{\\}}s Estimate the Wasserstein Metric?},\nauthor={Anton Mallasto and Guido Mont{\\'u}far and Augusto Gerolin},\nyear={2020},\nurl={https://openreview.net/forum?id=BylTta4YvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BylTta4YvB", "replyto": "BylTta4YvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper687/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575773394289, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper687/Reviewers"], "noninvitees": [], "tcdate": 1570237748542, "tmdate": 1575773394310, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper687/-/Official_Review"}}}], "count": 8}