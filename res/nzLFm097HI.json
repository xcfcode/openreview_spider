{"notes": [{"id": "nzLFm097HI", "original": "MaDu03G33Ah", "number": 1882, "cdate": 1601308207441, "ddate": null, "tcdate": 1601308207441, "tmdate": 1614985725356, "tddate": null, "forum": "nzLFm097HI", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "fst_BZHbGLT", "original": null, "number": 1, "cdate": 1610040414975, "ddate": null, "tcdate": 1610040414975, "tmdate": 1610474012949, "tddate": null, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers are in agreement that this paper could benefit further improvement. There are several areas: novelty of the proposed approach and evaluation on real-world datasets (beyond just CLEVR)."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"forum": "nzLFm097HI", "replyto": "nzLFm097HI", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040414962, "tmdate": 1610474012933, "id": "ICLR.cc/2021/Conference/Paper1882/-/Decision"}}}, {"id": "wbhmMu7HwXc", "original": null, "number": 2, "cdate": 1603848008681, "ddate": null, "tcdate": 1603848008681, "tmdate": 1606797926615, "tddate": null, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Official_Review", "content": {"title": "Missing critical comparison", "review": "**PAPER SUMMARY**\n\nThe paper presents a method for visual question answering (VQA) that makes use of a differentiable program executor that softly approximates the execution of neural modules that read and write to a stack. Experiments on CLEVR demonstrate that the model can achieve high performance using small amounts of supervision (varying the amount of question/answer pairs, image/object annotations, and question/program annotation used during training).\n\n**STRENGTHS**\n- The method achieves differentiable program execution without resorting to training with policy gradients\n- Experiments demonstrate high accuracy on CLEVR, even using very little training data\n\n**WEAKNESSES**\n- Critical missing citation / comparison: Prob-NMN (Vedantam et al, ICML 2018)\n- The model hardcodes a lot of task-specific knowledge; is this useful?\n- The writing is very unclear in some places\n\n**UNCLEAR WRITING**\n\nI find the paper to be quite unclear about exactly what type of supervision is used where. Section 3.4 states that \u201cWe pre-train our models with a very small percentage of visual and textual data\u201d -- but what kind of data is this exactly? If I understood the following discussion, I think that \u201cimage data\u201d refers to (image, object) annotations used to train the Mask R-CNN model, while \u201ctextual data\u201d refers to (question, program) pairs used to supervise the program executor. Then in Table 3, \u201cX% pre-train\u201d refers to the amount of (image, object) and (question, program) annotations used to supervise the model, and \u201cY% of Training Data\u201d refers to the number of (question, answer) pairs used to supervise the model. I think that the clarity of the paper would be much improved by making it more clear as to what types of supervision are used at different stages.\n\nI similarly found Section 4.2 and Tables 4 and 5 very unclear, and I\u2019m not entirely sure what experiments are being performed; what does it mean to \u201cfix vision and train text from 0.1% pretraining\u201d in Table 4, or \u201cfix text and train vision from scratch\u201d in Table 5?\n\n**MISSING COMPARISON: PROB-NMN**\n\nOne of the main draws of the proposed method is that it is extremely data-efficient on CLEVR, e.g. achieving 99.1% accuracy when supervised with 10% of (question, answer) pairs and 1% of (image, object) and (question, program) pairs.\n\nThis sort of data-efficient learning is also a key feature of Prob-NMN [1]; this is a critical piece of prior work which is not cited; the proposed method be discussed in light of this prior work, and the method should be compared against it. From Table 2 of [1], Prob-NMN achieves 97.73% test-set accuracy on CLEVR when supervised with 100% of (question, answer) pairs and 0.143% of (question, program) pairs; this outperforms the 93.1% reported for DePe in Table 3 of the submission which uses 100% of (question, answer) pairs and 0.1% of (question, program) and (image, object) pairs. However this comparison is not fully valid, since Prob-NMN uses slightly more program annotations but uses no explicit object detector. The authors should set up a more controlled experiment to compare their method against Prob-NMN.\n\n[1] Vedantam et al, \u201cProbabilistic Neural-symbolic Models for Interpretable Visual Question Answering\u201d, ICML 2018\n\n**HARDCODED REASONING**\n\nThe proposed model achieves data-efficiency by encoding a huge amount of task-specific prior knowledge into the model architecture:\n- The use of any ground-truth programs assumes that the set of primitive operators needed to answer questions are known\n- The module designs in Table 2 go a step further, and assume that the implementation of all of these primitive operators is known -- the operators in Table 2 are effectively a differentiable approximation to the ground-truth CLEVR program executor\n- The use of an object detector assumes that all questions can be answered in terms of object positions and properties; that the vocabulary of semantic concepts needed to answer questions is known; and that furthermore the mapping of images -> object locations and properties can be supervised during training\n\nTaken together, the model essentially hardcodes the data-generation process of the CLEVR dataset into the model. This is evident from Table 3: if I\u2019ve read and understood the experimental setup, training with 1% of (question, program) annotations and (image, object) annotations and zero (question, answer) annotations achieves >90% test-set accuracy on CLEVR! To me this is not a positive result -- instead it signifies that the model architecture itself is overfit to the dataset.\n\nThe higher-level research goal of building neuro-symbolic models that elegantly blend prior knowledge with learning is extremely exciting. But testing this kind of model on CLEVR is not. CLEVR is a synthetic dataset, and its questions are not natural: they are generated programmatically, and there is a lossless and deterministic mapping between question text and semantics (encoded as functional programs). CLEVR images are likewise not natural, and the image semantics necessary for answering any CLEVR question can likewise be losslessly encoded into a scene graph. As such, the fact that this model achieves data-efficient learning on CLEVR is nearly as uninteresting as the fact that the ground-truth CLEVR program execution engine achieves perfect accuracy when fed with ground-truth scene graphs: it is nearly guaranteed by construction, and only works by exploiting the synthetic nature of the benchmark.\n\nMy biggest concern with this line of work is that I don\u2019t see how it could possibly be used to achieve visual reasoning on real-world data, where none of the above can be exploited: for true natural language and images, we do not know a vocabulary of symbolic primitives that can losslessly represent the world and we certainly can\u2019t hardcode the implementation of all symbolic primitives necessary for performing real-world reasoning.\n\nSo although this method may indeed achieve extremely data-efficient learning on CLEVR, this is not an interesting achievement on its own unless there is some hope that the method used to do so can also be used for non-synthetic, real-world data. And the submission presents no evidence, or even an argument, for how this might be done.\n\n**OVERALL**\n\nOn the whole, I think that this paper is not ready for publication due to unclear writing, and the fact that it is missing a critical comparison to Prob-NMN.\n\nI also take issue with the underlying approach, since I think the methods used to achieve data-efficiency on CLEVR cannot scale to real-world data. However I can accept that different people in the community feel differently from me on this issue (as evidenced by excitement over papers like the Neuro-Symbolic Concept Learner), so I would not necessarily consider this as a disqualifier if it were my only concern with the paper.\n\n\n**AFTER REBUTTAL**\n\nI appreciate the author's efforts to improve the clarity of the submission and to add a comparison with Prob-NMN. However I remain unconvinced that this style of model can possibly scale to more realistic data. I've upgraded my score from 4 to 5, but I still generally lead toward rejection.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1882/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108667, "tmdate": 1606915776796, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1882/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1882/-/Official_Review"}}}, {"id": "pHz4UC6taRu", "original": null, "number": 7, "cdate": 1606201884193, "ddate": null, "tcdate": 1606201884193, "tmdate": 1606201884193, "tddate": null, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment", "content": {"title": "Redrafted the Paper as an Exploration", "comment": "Dear Reviewers and AC,\n\nWe have made major revisions to the paper draft to improve the readability and to address your specific comments. We encourage you to take a look! In general we spend more time exploring and analyzing different VQA components, reflected in these high level changes:\n\n1) There is a new section that describes the vision, language, reasoning components in more detail.\n\n2) The order of which we present the experiments is updated. At each experiment we arrive at clear a conclusion for which method is most efficient for each component.\n\n3) After experimenting on individual components we show that we need a new framework to handle object-level representations and soft program execution. We highlight two key challenges in the main paper to address this integration: an updated memory and optimization procedure. The rest of previous model details have been moved to the appendix. \n\n4) Our final experiments now also compare the model efficiencies across vision, language, and reasoning jointly to show that this model integration works. We also have comparison results to other models as before.\n\nWe are still working on GQA experiments, and we hope that the current revision provides interesting insights.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nzLFm097HI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1882/Authors|ICLR.cc/2021/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854701, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment"}}}, {"id": "UvNec4wcLhd", "original": null, "number": 5, "cdate": 1605759671012, "ddate": null, "tcdate": 1605759671012, "tmdate": 1605759938211, "tddate": null, "forum": "nzLFm097HI", "replyto": "wbhmMu7HwXc", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment", "content": {"title": "Our Response to Reviewer 3", "comment": "Thank you for the detailed feedback and suggestions! We have updated the overall paper so the writing flows better in general. Here we address specific comments as well.\n\n### Training Data:\n\nWhen we indicate pre-training on a small amount of data, we indicate that we train our text and vision classification models on a fraction of the fine grained function program and scene graph labels provided in the dataset. So when we say 1% pre-trained we pre-train these models on 1% of the 700k QA function programs and their corresponding scene graphs (7k each). We thus test ranges from 0.1% - 1% pre-training, and our updated results show that the method still scales well with smaller pre-training percentages (0.1%, 700 program and scene graphs).\n\nThe vision pre-training is done using the scene graphs to train the MLP models corresponding to object centric attributes (shape, color, etc.) and relations between objects. Here it is assumed that the general object bounding boxes can be estimated, as used in previous work such as NS-CL. While this is a hard assumption to make, there has been more progress towards unsupervised detection and segmentation, which remains out of the scope of the current work. The vision pre-training is done as you indicated, by the text and program pairs.\n\nThe training data is the number of question and answer pairs used for end-to-end supervision, where no detailed program annotations or scene graphs are provided. We have made this more clear in our updated revision.\n\n### Experiments:\n\nPlease view our global comment above for more details regarding the experiments.\n\n### Prob-NMN:\n\nProb-NMN is a good comparison method to our methods with its pre-training and fine tuning setup, and we will add it to our paper. Compared to our original scores for our 0.1% pre-training 93.1% accuracy on 100% QA pairs while Prob-NMN achieves 97.7% on 0.14% pre-training. However we noticed that our $\\alpha$ and $\\beta$ hyperparameters (described in the optimization section 3.4) were not set, which led to a significant dip in our accuracies. After setting these correctly for 0.1% pre-training we achieve **99%** and 98% validation accuracy using 100% and 10% QA pairs respectively. \n\n### Hardcoded Reasoning: \n\nThe use of primitive operators that we use follow the CLEVR convention, but we believe that they are general enough to use. Primitives such as filter, relate, count can be generally used for different datasets and even different domains such as text. The high level reasoning to parse the natural question to a traversal over the subject domain, images in our case. The CLEVR functions defined are just an implementation choice, or a good bias, which can be used on other image datasets. We are currently working on experiments that show that we can leverage these same definitions on a more real-world dataset such as GQA.\n\nCLEVR is indeed a baseline dataset for neuro-symbolic reasoning, and we use it to emphasize various vision and text model configuration efficiencies, in addition to the end-to-end accuracy. Based on the experiments we have a concrete path to testing such a method on real image data efficiently. As for out of distribution vocabulary in GQA we use the attributes corresponding to the top-k primitives per attribute for practical purposes. Similarly a method can be extended to use a dense embedding representation for the attributes with cosine metric to softly match primitives, instead of the current sparse one-hot selection. In such a setting, if new primitives are detected, their dense embeddings can be added to an attribute set without retraining the entire pipeline.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nzLFm097HI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1882/Authors|ICLR.cc/2021/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854701, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment"}}}, {"id": "o8U-HC9ximY", "original": null, "number": 4, "cdate": 1605759444090, "ddate": null, "tcdate": 1605759444090, "tmdate": 1605759912179, "tddate": null, "forum": "nzLFm097HI", "replyto": "tPVKmIR-xV-", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment", "content": {"title": "Our Response to Reviewer 2", "comment": "Thank you for the constructive comments. We are updating the overall paper so the writing flows better in general. Here we address specific comments as well.\n\n### Novelty:\n\nIndeed pieces of the model are motivated from NS-CL, Stack-NMN, MAC and others. The key contribution here is first looking at a combination of these methods which empirically show to be sample and statistically efficient, which are the probabilistic program executions and the object level detections. \n\nPlease view our global comment above for more details regarding the motivation.\n\n### Real World Data:\n\nLike previous approaches we are curious to see if our method can scale on more real world datasets. We are currently experimenting on the GQA dataset to test our performance against other methods. \n\n### Mask-RCNN:\n\nRegarding the mask R-CNN we followed the convention laid out by NS-VQA and NS-CL. We emphasize that the R-CNN is just used to identify the object bounding boxes, but the class labels are not used. Instead the object feature based MLP models are used to classify object attributes and relationships. In practice for a task like this, faster R-CNN works about the same as Mask RCNN for identifying bounding boxes of the objects. The high level idea is we can use such pipelines on more natural image tasks. These images have R-CNN methods that already are pre-trained and the idea is that we should be able to drop them into our object-centric vision pipelines. While this is not directly comparable to pixel wise methods such as MAC, we show that such object based architectures are important for efficiency and are working to show that they work for real image datasets.\n\n### Takeaway:\n\nThroughout our experiments we observe that methods that can continuously optimize end-to-end perform the most efficiently. More specifically for VQA this requires leveraging object-centric representations, an intermediate soft logic representation, and a probabilistic way of representing soft logic programs from text. While at a high level this makes sense, we empirically show this in our vision and text training experiments. This motivates future work that requires reasoning paths to retain probabilistic program distributions over object centric representations to compute the expected answer, such as in GQA.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nzLFm097HI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1882/Authors|ICLR.cc/2021/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854701, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment"}}}, {"id": "UVysiqRqc2j", "original": null, "number": 6, "cdate": 1605759809059, "ddate": null, "tcdate": 1605759809059, "tmdate": 1605759809059, "tddate": null, "forum": "nzLFm097HI", "replyto": "mW0MLgq153h", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment", "content": {"title": "Our Response to Reviewer 1", "comment": "We thank the reviewer for the constructive feedback. Below we address specific comments.\n\n### Writing:\n\nWe are updating the overall paper to provide more analysis and so the writing flows better in general. More specifically we will modify the writing to explore these efficiency tradeoffs of existing models and then propose our model architecture based on these results. \n\n### Motivation:\n\nFor the motivation we show that to handle the vision, language, and reasoning there are many core components that VQA models look at and compare and contrast these components with respect to sample and statistical efficiency in the experiments. \n\nPlease view our global comment above for more details regarding the motivation.\n\n### Experiments:\n\nPlease view our global comment above for more details regarding the experiments.\n\n### Real Datasets:\n\nWe see great benefits from the MAC and Stack-NMN operating on the real world domains. Such models show the benefit of pixel wise attention and general text neural modules when operating on these open domains. We are currently working on experiments on GQA to show that our method can scale to more real world datasets as well.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nzLFm097HI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1882/Authors|ICLR.cc/2021/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854701, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment"}}}, {"id": "xNhlcqCGdNw", "original": null, "number": 3, "cdate": 1605759147917, "ddate": null, "tcdate": 1605759147917, "tmdate": 1605759224849, "tddate": null, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment", "content": {"title": "General Update and Common Questions", "comment": "We thank the reviewers for their feedback so far. We currently focusing on two main directions with the current work: \n\n1) We are restructuring the paper to primarily be an exploration of current models and methods for efficient VQA, this will motivate the design of final model we propose and test.\n2) We currently running experiments on a more real-world dataset: GQA\n\nSome clarifications useful for all reviewers are regarding the following topics:\n\n### Experiment Explanation\n\nIn section 4.2 we explore the different model components from Table 1 given different sample complexities. Here we want to explore which model architecture would be more sample efficient for the vision and language components. The table 4 shows that if the models were provided perfect vision predictions, we can isolate the effects of the text training. Here the train ratio is what percentage of the CLEVR question answer training pairs are provided for end-to-end training and the corresponding validation QA accuracies. This tests the sample efficiency of symbolic methods which choose a discrete program trace versus ours that leverage soft probabilistic programs. For the vision we fine tune the program parsers perfectly and train the vision models from scratch to test abduction, pixel-wise, and object centric representations. The takeaways from these experiments is that the soft program layouts and the object-level predictors provide the best sample complexities for the text and vision models respectively.\n\nSimilarly in 4.3 we follow the same setup as in 4.2, but instead of varying the sample complexity we measure the computational complexity of how long these approaches take to converge. In our experience these REINFORCE and abductive methods for NS-VQA, NS-CL text and NGS vision take many samples and iterations to converge. Moreover due to the high variance of the REINFORCE methods, it is very difficult to reproduce the numbers reported in the original papers, motivating our continuous approach. \n\nIn the updated paper we will clearly break down these components and test them on existing models, which will motivate the design of the proposed model.\n\n### New Model Motivation\n\nThe motivation for the new model builds on the works of NS-CL and Stack-NMN. NS-CL leverages **soft logic** and **object-level** vision models, which has proven key for sample efficient and accurate results. For the text it executes a discrete program layout, making it statistically inefficient due to the REINFORCE to optimization procedure and discards uncertainty regarding the program parse. \n\nWe also leverage Stack-NMN, which uses a **soft program layout** to capture the uncertainty of the text to program conversion, but uses a less accurate pixel-wise image model and corresponding neural modules. They leverage this pixel-wise approach to store the image embedding of the same dimension in its intermediate stack. This makes it stack architecture simpler and the corresponding neural modules only have to deal with one type of input.\n\nBased on these observations, our proposed method leverages object-level vision, and soft program layouts, and soft logic. This led to key architectural differences in how the stack was defined and manipulated, as well as how the final model inference is carried out. This is because the stack needs to store the intermediate results of all probabilistic program executions of the soft logic, whose function signatures and output types. \n\nFurthermore we explore challenges of jointly optimizing program and vision prediction in a true **end-to-end** fashion. In comparison NS-CL iteratively tunes its program parser and vision models, while Stack-NMN leverages coarser neural modules instead of more interpretable programs. This leads to the name **Differentiable end-to-end Program execution (DePe)**.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1882/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "nzLFm097HI", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1882/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1882/Authors|ICLR.cc/2021/Conference/Paper1882/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854701, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1882/-/Official_Comment"}}}, {"id": "mW0MLgq153h", "original": null, "number": 1, "cdate": 1602975292512, "ddate": null, "tcdate": 1602975292512, "tmdate": 1605024336630, "tddate": null, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Official_Review", "content": {"title": "Interesting direction but the paper really needs more work", "review": "This paper studies visual question-answering (VQA). The authors proposed an end-to-end differentiable framework that performs \"soft\" logical reasoning based on object-centric scene representations and attention-based language parsing. The authors claimed that the new model is more data-efficient.\n\nWhile I think this direction is interesting and useful, I think the submission really needs more work before it may be published at a top-tier conference. The current manuscript is hard to understand and the results are not convincing.\n\nFirst, the method is not well-motivated. The intro mostly discussed related papers and their differences, so I was expecting an analysis paper, but it turns out the rest of the paper is about a new model. There is no justification for why we need a new model, what is new about it, and why we expect it to do better. \n\nApart from this, the writing is in general unclear. For example, the model has an acronym DePe, but the full name was never fully told. The experiment section is also poorly written. In 4.2, it's unclear what \"Train Ratio\" means in Table 4 or 5, and these tables were never referred to in the main text.  I also don't understand the setup in Section 4.3, either, as essential descriptions are completely missing.\n\nThis makes it hard to assess the results.  In addition, all results are on the CLEVR dataset. One selling point of methods such as MAC, NS-CL, and Stack-NMN is that they can naturally be applied to real images such as the GQA dataset. Without experiments on real images, the analyses presented in this paper are unlikely to convince researchers in this area.\n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1882/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108667, "tmdate": 1606915776796, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1882/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1882/-/Official_Review"}}}, {"id": "tPVKmIR-xV-", "original": null, "number": 3, "cdate": 1603890622937, "ddate": null, "tcdate": 1603890622937, "tmdate": 1605024336503, "tddate": null, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "invitation": "ICLR.cc/2021/Conference/Paper1882/-/Official_Review", "content": {"title": "Review", "review": "The paper proposes a neuro-symbolic model for sample-efficient VQA, which turns each question into a probabilistic program which is then softly executed. The problem explored in the paper and its background and context presented clearly and it does a good job in motivating its importance and trade-offs between possible solutions. While the use of a probabilistic program to represent the questions might be too stiff / inflexible in my opinion and may not generalize well to less constrained natural language, this direction is still of course important and interesting. It also does a great job in presenting the existing approaches and comparing their properties. The writing is good and the model is presented clearly with a very useful diagram. \n\nHowever, the novelty of the paper seems limited to me, as it mainly combines together ideas that have been extensively explored in many prior works which are mentioned by the paper. Turning the question into a series of attentions over semantic factors appears in the NSM and partially in MAC models. The iterative memory updates appeared in MAC. Combining together small operations and functions defined by hand as dictated by programs as in page 6 of the model is the main idea of Module network. End-to-end differentiability for VQA models has also been extensively explored and multiple solutions have been proposed: relations networks, soft variants of NMN, and also MAC and NSM, etc. The use of stacks has been explored too in the stack-NMN model. I therefore feel the paper mostly recombines and tunes together these ideas rather than offering one particular new idea or insight. \n\nThe paper presents results on CLEVR only, which goes back into my concern about the inflexibility of the probabilistic programs. Especially for this type of models, it will be useful to explore it on tasks beyond CLEVR such as VQA/GQA to show whether it can work for natural or richer language. The use of Mask R-CNN on CLEVR is also quite unreasonable in my opinion: the task has meant to be visually simple, so using a very strong visual model on it nullifies the visual aspect of it completely, making the model working on perfect semantic scene graph inputs rather than on \"real/natural\" uncertain and more noisy inputs. It also gives unfair advantage to the model when comparing to baselines which didn\u2019t use object detectors on CLEVR but rather work directly with the image, e.g. MAC and others (presented in the table in the experiments section).\n\nAt the same time, it is important to mention the model does get quantitative improvements in scores and especially sample efficiency, but the paper doesn\u2019t make it clear what is the particular property or part of the model that allows for the improved numbers, and so the paper doesn\u2019t leave the reader with a clear new takeaway message.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1882/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1882/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How to Design Sample and Computationally Efficient VQA Models", "authorids": ["~Karan_Samel2", "~Zelin_Zhao1", "~Kuan_Wang1", "robin1997@gatech.edu", "~Binghong_Chen1", "~Le_Song1"], "authors": ["Karan Samel", "Zelin Zhao", "Kuan Wang", "Robin Luo", "Binghong Chen", "Le Song"], "keywords": ["vqa", "visual question answering", "neural modules", "probabilistic logic"], "abstract": "In multi-modal reasoning tasks, such as visual question answering (VQA), there have been many modeling and training paradigms tested. Previous models propose different methods for the vision and language tasks, but which ones perform the best while being sample and computationally efficient? Based on our experiments, we find that representing the text as probabilistic programs and images as object-level scene graphs best satisfy these desiderata. We extend existing models to leverage these soft programs and scene graphs to train on question answer pairs in an end-to-end manner. Empirical results demonstrate that this differentiable end-to-end program executor is able to maintain state-of-the-art accuracy while being sample and computationally efficient.", "one-sentence_summary": "An exploration of which individual components of VQA methods are most efficient, and show that the combination of all these components provides the most efficient model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "samel|how_to_design_sample_and_computationally_efficient_vqa_models", "pdf": "/pdf/993b247d28594eb01131e24f59a1eea94bbfa5f8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=04g3mBawvt", "_bibtex": "@misc{\nsamel2021how,\ntitle={How to Design Sample and Computationally Efficient {\\{}VQA{\\}} Models},\nauthor={Karan Samel and Zelin Zhao and Kuan Wang and Robin Luo and Binghong Chen and Le Song},\nyear={2021},\nurl={https://openreview.net/forum?id=nzLFm097HI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "nzLFm097HI", "replyto": "nzLFm097HI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1882/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108667, "tmdate": 1606915776796, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1882/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1882/-/Official_Review"}}}], "count": 10}