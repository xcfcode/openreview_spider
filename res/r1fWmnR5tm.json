{"notes": [{"id": "r1fWmnR5tm", "original": "SJed-otcFm", "number": 1331, "cdate": 1538087960881, "ddate": null, "tcdate": 1538087960881, "tmdate": 1545355412205, "tddate": null, "forum": "r1fWmnR5tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning to Search Efficient DenseNet with Layer-wise Pruning", "abstract": "Deep neural networks have achieved outstanding performance in many real-world applications with the expense of huge computational resources. The DenseNet, one of the recently proposed neural network architecture, has achieved the state-of-the-art performance in many visual tasks. However, it has great redundancy due to the dense connections of the internal structure, which leads to high computational costs in training such dense networks. To address this issue,  we design a reinforcement learning framework to search for efficient DenseNet architectures with layer-wise pruning (LWP) for different tasks, while retaining the original advantages of DenseNet, such as feature reuse, short paths, etc. In this framework, an agent evaluates the importance of each connection between any two block layers, and prunes the redundant connections. In addition, a novel reward-shaping trick is introduced to make DenseNet reach a better trade-off between accuracy and float point operations (FLOPs). Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives.  ", "keywords": ["reinforcement learning", "DenseNet", "neural network compression"], "authorids": ["xuanyang91.zhang@gmail.com", "uestcliuhao@gmail.com", "zhanxing.zhu@pku.edu.cn", "zenglin@gmail.com"], "authors": ["Xuanyang Zhang", "Hao liu", "Zhanxing Zhu", "Zenglin Xu"], "pdf": "/pdf/6e55047547d4f5968ef74ad2955621e858d6222a.pdf", "paperhash": "zhang|learning_to_search_efficient_densenet_with_layerwise_pruning", "_bibtex": "@misc{\nzhang2019learning,\ntitle={Learning to Search Efficient DenseNet with Layer-wise Pruning},\nauthor={Xuanyang Zhang and Hao liu and Zhanxing Zhu and Zenglin Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1fWmnR5tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJeoEo3A1E", "original": null, "number": 1, "cdate": 1544633139236, "ddate": null, "tcdate": 1544633139236, "tmdate": 1545354502239, "tddate": null, "forum": "r1fWmnR5tm", "replyto": "r1fWmnR5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1331/Meta_Review", "content": {"metareview": "The paper proposes to apply Neural Architecture Search for pruning DenseNet. \n\nThe reviewers and AC note the potential weaknesses of the paper in various aspects, and decided that the authors need more works to publish. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Limited contribution"}, "signatures": ["ICLR.cc/2019/Conference/Paper1331/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1331/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Search Efficient DenseNet with Layer-wise Pruning", "abstract": "Deep neural networks have achieved outstanding performance in many real-world applications with the expense of huge computational resources. The DenseNet, one of the recently proposed neural network architecture, has achieved the state-of-the-art performance in many visual tasks. However, it has great redundancy due to the dense connections of the internal structure, which leads to high computational costs in training such dense networks. To address this issue,  we design a reinforcement learning framework to search for efficient DenseNet architectures with layer-wise pruning (LWP) for different tasks, while retaining the original advantages of DenseNet, such as feature reuse, short paths, etc. In this framework, an agent evaluates the importance of each connection between any two block layers, and prunes the redundant connections. In addition, a novel reward-shaping trick is introduced to make DenseNet reach a better trade-off between accuracy and float point operations (FLOPs). Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives.  ", "keywords": ["reinforcement learning", "DenseNet", "neural network compression"], "authorids": ["xuanyang91.zhang@gmail.com", "uestcliuhao@gmail.com", "zhanxing.zhu@pku.edu.cn", "zenglin@gmail.com"], "authors": ["Xuanyang Zhang", "Hao liu", "Zhanxing Zhu", "Zenglin Xu"], "pdf": "/pdf/6e55047547d4f5968ef74ad2955621e858d6222a.pdf", "paperhash": "zhang|learning_to_search_efficient_densenet_with_layerwise_pruning", "_bibtex": "@misc{\nzhang2019learning,\ntitle={Learning to Search Efficient DenseNet with Layer-wise Pruning},\nauthor={Xuanyang Zhang and Hao liu and Zhanxing Zhu and Zenglin Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1fWmnR5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1331/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352876959, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1fWmnR5tm", "replyto": "r1fWmnR5tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1331/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1331/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1331/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352876959}}}, {"id": "rJgkzj5Rn7", "original": null, "number": 3, "cdate": 1541479174944, "ddate": null, "tcdate": 1541479174944, "tmdate": 1541533227564, "tddate": null, "forum": "r1fWmnR5tm", "replyto": "r1fWmnR5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1331/Official_Review", "content": {"title": "Straightforward Idea with Limited Contribution", "review": "This paper proposes to apply Neural Architecture Search (NAS) for connectivity pruning to improve the parameter efficiency of DenseNet. The idea is straightforward and the paper is well organized and easy to follow.\n\nMy major concern is the limited contribution. Applying deep reinforcement learning (DRL) and following the AutoML framework for architecture/parameter pruning has been extensively investigated during the past two years. For instance, this work has a similar motivation and design \"AMC: AutoML for Model Compression and Acceleration on Mobile Devices.\"\n\nThe experimental results also show a limited efficiency improvement according to Table 1. Although this is a debatable drawback compared with the novelty/contribution concern, it worth to reconsider the motivation of the proposed method given the fact that the AutoML framework is extremely expensive due to the DRL design. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1331/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Search Efficient DenseNet with Layer-wise Pruning", "abstract": "Deep neural networks have achieved outstanding performance in many real-world applications with the expense of huge computational resources. The DenseNet, one of the recently proposed neural network architecture, has achieved the state-of-the-art performance in many visual tasks. However, it has great redundancy due to the dense connections of the internal structure, which leads to high computational costs in training such dense networks. To address this issue,  we design a reinforcement learning framework to search for efficient DenseNet architectures with layer-wise pruning (LWP) for different tasks, while retaining the original advantages of DenseNet, such as feature reuse, short paths, etc. In this framework, an agent evaluates the importance of each connection between any two block layers, and prunes the redundant connections. In addition, a novel reward-shaping trick is introduced to make DenseNet reach a better trade-off between accuracy and float point operations (FLOPs). Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives.  ", "keywords": ["reinforcement learning", "DenseNet", "neural network compression"], "authorids": ["xuanyang91.zhang@gmail.com", "uestcliuhao@gmail.com", "zhanxing.zhu@pku.edu.cn", "zenglin@gmail.com"], "authors": ["Xuanyang Zhang", "Hao liu", "Zhanxing Zhu", "Zenglin Xu"], "pdf": "/pdf/6e55047547d4f5968ef74ad2955621e858d6222a.pdf", "paperhash": "zhang|learning_to_search_efficient_densenet_with_layerwise_pruning", "_bibtex": "@misc{\nzhang2019learning,\ntitle={Learning to Search Efficient DenseNet with Layer-wise Pruning},\nauthor={Xuanyang Zhang and Hao liu and Zhanxing Zhu and Zenglin Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1fWmnR5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1331/Official_Review", "cdate": 1542234253210, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1fWmnR5tm", "replyto": "r1fWmnR5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1331/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923636, "tmdate": 1552335923636, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1331/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJe-bk0on7", "original": null, "number": 2, "cdate": 1541295864938, "ddate": null, "tcdate": 1541295864938, "tmdate": 1541533227344, "tddate": null, "forum": "r1fWmnR5tm", "replyto": "r1fWmnR5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1331/Official_Review", "content": {"title": "RL based method for pruning a pre-trained network", "review": "This paper proposes a layer-based pruning method based on reinforment learning for pre-train networks.\n\nThere are several major issues for my rating:\n\n- Lack of perspective. I do not understand where this paper sits compared to other compression methods. If this is about RL great, if this is about compression, there is a lack of related work and proper comparisons to existing methods (at least concenptual)\n- Claims about the benefits of not needed expertise are not clear to me as, from the results, seems like expertise is needed to set the hyperparameters.\n\n- experiments are not convincing. I would like to see something about computational costs. Current methods aim at minimizing training / finetuning costs while maintaining the accuracy. How does this stands in that regard? How much time is needed to prune one of these models? How many resources?\n\n- Would it be possible to add this process into a training from scratch method?\n\n- how would this compare to training methods that integrate compression strategies?\n- Table 1 shows incomplete results, why? Also, there is a big gap between accuracy/number of parameters trade-of between this method and other presented in that table. Why?\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1331/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Search Efficient DenseNet with Layer-wise Pruning", "abstract": "Deep neural networks have achieved outstanding performance in many real-world applications with the expense of huge computational resources. The DenseNet, one of the recently proposed neural network architecture, has achieved the state-of-the-art performance in many visual tasks. However, it has great redundancy due to the dense connections of the internal structure, which leads to high computational costs in training such dense networks. To address this issue,  we design a reinforcement learning framework to search for efficient DenseNet architectures with layer-wise pruning (LWP) for different tasks, while retaining the original advantages of DenseNet, such as feature reuse, short paths, etc. In this framework, an agent evaluates the importance of each connection between any two block layers, and prunes the redundant connections. In addition, a novel reward-shaping trick is introduced to make DenseNet reach a better trade-off between accuracy and float point operations (FLOPs). Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives.  ", "keywords": ["reinforcement learning", "DenseNet", "neural network compression"], "authorids": ["xuanyang91.zhang@gmail.com", "uestcliuhao@gmail.com", "zhanxing.zhu@pku.edu.cn", "zenglin@gmail.com"], "authors": ["Xuanyang Zhang", "Hao liu", "Zhanxing Zhu", "Zenglin Xu"], "pdf": "/pdf/6e55047547d4f5968ef74ad2955621e858d6222a.pdf", "paperhash": "zhang|learning_to_search_efficient_densenet_with_layerwise_pruning", "_bibtex": "@misc{\nzhang2019learning,\ntitle={Learning to Search Efficient DenseNet with Layer-wise Pruning},\nauthor={Xuanyang Zhang and Hao liu and Zhanxing Zhu and Zenglin Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1fWmnR5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1331/Official_Review", "cdate": 1542234253210, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1fWmnR5tm", "replyto": "r1fWmnR5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1331/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923636, "tmdate": 1552335923636, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1331/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkllOklK3Q", "original": null, "number": 1, "cdate": 1541107559624, "ddate": null, "tcdate": 1541107559624, "tmdate": 1541533227135, "tddate": null, "forum": "r1fWmnR5tm", "replyto": "r1fWmnR5tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1331/Official_Review", "content": {"title": "The evaluation could be improved.", "review": "The paper introduces RL based approach to prune layers in a DenseNet. This work extends BlockDrop to DenseNet architecture making the controller independent form the input image. The approach is evaluated on CIFAR10 and CIFAR100 datasets as well as on ImageNet showing promising results.\n\nIn order to improve the paper, the authors could take into consideration the following points:\n\n1. Given the similarity of the approach with BlockDrop, I would suggest to discuss it in the introduction section clearly stating the similarities and the differences with the proposed approach. \n2. BlockDrop seems to introduce a general framework of policy network to prune neural networks. However, the authors claim that BlockDrop \"can only be applied to ResNets or its variants\". Could the authors comment on this? \n3. In the abstract, the authors claim: \"Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives\". It is hard to asses if the statement is correct given the evidence presented in the experimental section. It is not clear if the method is more efficient and compact than others, e. g.  CondenseNet. \n4. In the experimental section, addressing the following questions would make the section stronger: What is more important FLOPs or number of parameters? What is the accuracy drop we should allow to pay for reduction in number of parameters or FLOPs?\n5. For the evaluation, I would suggest to show that the learned policy is better than a random one: e. g. not using the controller to define policy (in line 20 of the algorithm) and using a random random policy instead.\n6. In Table 1, some entries for DenseNet LWP are missing. Is the network converging for this setups? \n7. \\sigma is not explained in section 3.3. What is the intuition behind this hyper parameter?\n8. I'd suggest moving related work section to background section and expanding it a bit.\n9. In the introduction: \"... it achieved state-of-the-art results across several highly competitive datasets\". Please add citations accordingly.\n\nAdditional comments:\n1. It might be interesting to compare the method introduced in the paper to a scenario where the controller is conditioned on an input image and adaptively selects the connections/layers in DenseNet at inference time.\n2. It might be interesting to report the number of connections in Table 1 for all the models.\n\nOverall, I liked the ideas presented in the paper. However, I think that the high degree of overlap with BlockDrop should be addressed by clearly stating the differences in the introduction section. Moreover, I encourage the authors to include missing results in Table 1 and run a comparison to random policy. In the current version of the manuscript, it is hard to compare among different methods, thus, finding a metric or a visualization that would clearly outline the \"efficiency and compactness\" of the method would make the paper stronger.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1331/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Search Efficient DenseNet with Layer-wise Pruning", "abstract": "Deep neural networks have achieved outstanding performance in many real-world applications with the expense of huge computational resources. The DenseNet, one of the recently proposed neural network architecture, has achieved the state-of-the-art performance in many visual tasks. However, it has great redundancy due to the dense connections of the internal structure, which leads to high computational costs in training such dense networks. To address this issue,  we design a reinforcement learning framework to search for efficient DenseNet architectures with layer-wise pruning (LWP) for different tasks, while retaining the original advantages of DenseNet, such as feature reuse, short paths, etc. In this framework, an agent evaluates the importance of each connection between any two block layers, and prunes the redundant connections. In addition, a novel reward-shaping trick is introduced to make DenseNet reach a better trade-off between accuracy and float point operations (FLOPs). Our experiments show that DenseNet with LWP is more compact and efficient than existing alternatives.  ", "keywords": ["reinforcement learning", "DenseNet", "neural network compression"], "authorids": ["xuanyang91.zhang@gmail.com", "uestcliuhao@gmail.com", "zhanxing.zhu@pku.edu.cn", "zenglin@gmail.com"], "authors": ["Xuanyang Zhang", "Hao liu", "Zhanxing Zhu", "Zenglin Xu"], "pdf": "/pdf/6e55047547d4f5968ef74ad2955621e858d6222a.pdf", "paperhash": "zhang|learning_to_search_efficient_densenet_with_layerwise_pruning", "_bibtex": "@misc{\nzhang2019learning,\ntitle={Learning to Search Efficient DenseNet with Layer-wise Pruning},\nauthor={Xuanyang Zhang and Hao liu and Zhanxing Zhu and Zenglin Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=r1fWmnR5tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1331/Official_Review", "cdate": 1542234253210, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1fWmnR5tm", "replyto": "r1fWmnR5tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1331/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335923636, "tmdate": 1552335923636, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1331/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}