{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392664560000, "tcdate": 1392664560000, "number": 4, "id": "jArLXVnW-4AIQ", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YDXrDdbom9YCi", "replyto": "YDXrDdbom9YCi", "signatures": ["Jinseok Nam"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks for the helpful reviews. We are currently working on improving our paper along your suggestions. We noticed that we have not been clear enough about a few important points, which we would like to clarify in this first reply (more details will follow later):\r\n\r\n1. learning-to-rank vs. multi-label classification\r\nAlthough we frequently talk about ranking in our paper, our objective is not learning- to-rank. Learning-to-rank aims at ranking a set of objects (such as documents), whereas our goal is to assign a set of labels to a given document. (as opposed to conventional multi-class classification, where only a single label is assigned to the document). Mutli-label classification is often framed as a ranking problem, but in this case the labels need to be ranked for a given document (as opposed to ranking the documents themselves). Many commonly used loss functions for multi-label classifi- cation focus on a good ranking of the labels (a good ranking is one where all relevant labels tend to be ranked before all irrelevant labels).\r\n\r\n2. The pairwise hinge loss and the pairwise exponential loss to minimize ranking loss\r\nIt is said that the pairwise hinge loss in RankSVM [2] and the pairwise expo- nential loss in BP-MLL are natural choices as the surrogate loss for the ranking loss [4]. However, these surrogate losses are not consistent with the ranking loss that we want to minimize.\r\n\r\n3. Are BP-MLL and BR (linear SVMs) reasonable choices for the baselines?\r\nNeural Network-based algorithms are particularly interesting for multilabel classifi- cation because they allow to model dependencies between the occurrence of labels, whereas the standard binary relevance approach assumes that the occurrence of a la- bel for an example is independent of the occurrence of other labels for this examples, an assumption that is typically wrong in practice. BP-MLL is a well-known NN archi- tecture which exploits a pairwise error function instead of the traditional cross entropy loss.\r\nThe main claim that we want to make in this paper is that Neural-Network based approaches to multilabel classification may benefit from several recent advancements that have been developed in Deep Learning, as well as that the minimization of the pairwise error function may be replaced with something simpler such the cross entropy loss.\r\nThus BP-MLL is a natural benchmark, because this is the prototypical Neural- Network-based multilabel classification algorithm, which is often used as a baseline [5].\r\nBinary relevance, on the other hand, is in many domains not a strong baseline for the reasons discussed above, but in particular in text domains it is still commonly used and has shown very good results. Several recent works have shown that the BR approach may outperform their counterparts which consider label dependencies between labels. Also note that recent analyses has shown that ranking losses may be minimized by loss functions on the individual labels [1, 4], which may be part of an explanation why binary relevance with SVMs as base classifiers tends to often performs well in practice even though it does not take label dependencies into account.\r\n\r\nReviewer 1\r\n\r\n- Section 3: this is a nice idea, for choosing data dependent thresholds - is it novel (for multi-label classification)?:\r\nThe basic idea of this sort of threshold has been discussed in several papers [2, 8, 6]. Instead of minimizing bipartite misclassification errors, which is sum of the number of false positives and false negatives, we use F1 score as a reference measure.\r\n\r\n- Given the use of ReLU transfer functions, what different did L1 regularization on the hidden activations make?:\r\nWe follow the recent findings from [8] where deep neural networks are constructed by using ReLUs at the hidden layers, together with L1 regularization on the activations. Even though, as we expected, stronger L1 regularization makes averaged value of positive hidden activations decrease, we did not find meaningful differences.\r\n\r\n- Figs 3a is great. Fig 3b is a little misleading, though - you chose the one dataset where adding dropout helped. What does that curve look like on a dataset where adding dropout hurt?:\r\nThe performance of NNs with dropout or without dropout is highly correlated with properties of datasets. When we train NNs on EUR-Lex and Delicious dataset, the networks tend to overfit severely such as the red dashed lines in Figure 3 (b). Both datasets have a relatively large number of labels compared to the number of training documents and unique terms. Precisely, on the Delicious dataset, the number of distinct labels is larger than the number of unique words, and each document is associated with nearly 20 labels on average. Additionally, one thirds of labels on EUR-Lex dataset does not appear in the training data split, that is the label distribution of training data and that of test data are different. To prevent overfitting due to such characteristics of datasets, we tried to regularize the models with L1 and L2 penalty as well as Dropout, but only works Dropout. On the rest of the datasets, even though we increase the number of units in the hidden layer up to 2000 and 4000, no such severe overfitting observed. We conjecture that this is because why dropout does not help training. In that case, otherwise, it introduces undesirable noise to the models.\r\nWe will include figures for the cases where dropout does not help.\r\n\r\nReviewer 2\r\n\r\n- the comparison made in the paper between the proposed network and BP-MLL is not valid: influences of architecture, choice of transfer functions and loss are all mixed. In the end of the 'Plateaus' paragraph, it is suggested that ReLUs allow to outperform BP-MLL but no experience with BP-MLL with ReLU has been conducted. Perhaps the loss of BP-MLL (PWE) could be as efficient as cross entropy with ReLU. Is BP-MLL used as a benchmark as a network or simply as a loss (exponential)? What are the results with hinge loss?:\r\nWe used BP-MLL as it has been proposed where the hidden units and the output units are tanh, and the error function is the pairwise error function. In comparison of ReLU and tanh in the hidden layer of BP-MLL, tanh often performs better than ReLU. We will add additional experimental results with respect to the type of hidden units in BP-MLL.\r\n\r\n- The curves form Figure 2 are more original but I don\u2019t agree with its creation. What justifies that what is observed with such a weird networks (a single hidden unit) can transfer to more generic feed-forward NNs?:\r\nIt is hard to draw error as a function of parameters in general neural networks as in Figure 2 because the number of all possible configurations of parameters in NN equals to '1 + the number of hidden layers' assuming that each hidden layer has a single unit. \r\n\r\n- the conclusions regarding the \u201dplateaus\u201d are not really obvious: it seems that there are plateaus for all settings.:\r\nWe can only say given all the same settings such as input data, output targets and weight initialization methods, a curve is more steep than the other. What we want to show in Figure 2 is that the use of different cost functions yields the different curves where PWE consists of larger plateaus compared to CE. Obviously, using the ReLU unit in the hidden layer gives rise to absolutely flat region as in Figure 2(b), which implies it is impossible to escape from such a region once the hidden unit\u2019s activation is determined to zero. That is a downside of the use of ReLUs as Reviewer 1 pointed out.\r\n\r\n- Couldn\u2019t it be interesting to try to learn thresholds using the class probabilities outputted by the network?:\r\nTo learn instance-wise threshold predictor, the class probabilities are used to estimate the best performing threshold on training instances from which we learn the threshold predictor. If one wants to train label-wise threshold predictor, a variant of ScutFBR might be considered [7, 3].\r\n\r\n\r\nReferences\r\n[1] Krzysztof Dembczynski, Wojciech Kotlowski, and Eyke H\u00fcllermeier. Consistent multilabel ranking through univariate losses. In ICML, 2012.\r\n[2] Andr\u00e9 Elisseeff and Jason Weston. A kernel method for multi-labelled classification. In NIPS, 2001.\r\n[3] Rong-En Fan and Chih-Jen Lin. A study on threshold selection for multi-label classification. Technical Report, National Taiwan University, 2007.\r\n[4] Wei Gao and Zhi-Hua Zhou. On the consistency of multi-label learning. In COLT, 2011.\r\n[5] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas. Random k-Labelsets for Multilabel Classi- fication. IEEE Trans. Knowl. Data Eng., 23(7):1079\u20131089, 2011.\r\n[6] Yiming Yang and Siddharth Gopal. Multilabel classification with meta-level features in a learning-to-rank framework. Machine Learning, pages 1\u201322, 2011.\r\n[7] Yiming Yang. A study of thresholding strategies for text categorization. In SIGIR, 2001.\r\n[8] Min-Ling Zhang Min-Ling Zhang and Zhi-Hua Zhou Zhi-Hua Zhou. Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization. IEEE Trans. Knowl. Data Eng., 18(10): 1338\u20131351, 2006."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks", "decision": "submitted, no decision", "abstract": "Large-scale datasets with multi-labels are becoming readily available, and the demand for large-scale multi-label classification algorithm is also increasing. In this work, we propose to utilize a single-layer Neural Networks approach in large-scale multi-label text classification tasks with recently proposed learning techniques. We carried out experiments on six textual datasets with varying characteristics and size, and show that a simple Neural Networks model equipped with recent advanced techniques for Neural Networks components such as an activation layer, optimization, and generalization techniques performs as well as or even outperforms the previous state-of-the-art approaches on large-scale datasets with diverse characteristics.", "pdf": "https://arxiv.org/abs/1312.5419", "paperhash": "nam|largescale_multilabel_text_classification_revisiting_neural_networks", "keywords": [], "conflicts": [], "authors": ["Jinseok Nam", "Jungi Kim", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "authorids": ["jinseok.n@gmail.com", "kim@ukp.informatik.tu-darmstadt.de", "gurevych@ukp.informatik.tu-darmstadt.de", "juffi@ke.informatik.tu-darmstadt.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391695620000, "tcdate": 1391695620000, "number": 3, "id": "alkAlHVICypit", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YDXrDdbom9YCi", "replyto": "YDXrDdbom9YCi", "signatures": ["anonymous reviewer 9528"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Large-scale Multi-label Text Classification - Revisiting Neural Networks", "review": "The paper describes a series of experiments for multi-labeled text classification using neural networks. The classification model is a simple one hidden layer NN integrating rectifier units and dropout.  A comparison of this model with baselines (Binary relevance and a ranking NN) is performed on 6 multi-labeled datasets with different characteristics.\r\nThis is an experimental paper. The NN model is a classical MLP and the only new algorithmic contribution concerns the prediction of decision thresholds for binary decisions. On the other hand, the experimental comparison is extensive and allows the authors to examine the benefits of recent improvements for NNs on the task of text classification. The datasets characteristics are representative of different text classification problems (dataset and vocabulary size, label cardinality). Different loss functions are also used for the evaluation. The paper could then be useful for popularizing NNs for text classification.\r\nI am not sure that the BP-MLL is a reference algorithm for learning to rank, and there are probably better candidates. On the other hand it is true that the simple binary framework (BR in the paper) is a strong baseline despite its simplicity, so that the results could be considered as significant. An extension of this work could be to consider large scale problems (both in the vocabulary size and in the number of categories, since several benchmarks are now available with a very large number of classes \u2013 see for example the LSHTC challenges). A deeper discussion on the respective complexity of the different approaches and of their behavior when the number of training examples varies (learning curves) would strengthen the paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks", "decision": "submitted, no decision", "abstract": "Large-scale datasets with multi-labels are becoming readily available, and the demand for large-scale multi-label classification algorithm is also increasing. In this work, we propose to utilize a single-layer Neural Networks approach in large-scale multi-label text classification tasks with recently proposed learning techniques. We carried out experiments on six textual datasets with varying characteristics and size, and show that a simple Neural Networks model equipped with recent advanced techniques for Neural Networks components such as an activation layer, optimization, and generalization techniques performs as well as or even outperforms the previous state-of-the-art approaches on large-scale datasets with diverse characteristics.", "pdf": "https://arxiv.org/abs/1312.5419", "paperhash": "nam|largescale_multilabel_text_classification_revisiting_neural_networks", "keywords": [], "conflicts": [], "authors": ["Jinseok Nam", "Jungi Kim", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "authorids": ["jinseok.n@gmail.com", "kim@ukp.informatik.tu-darmstadt.de", "gurevych@ukp.informatik.tu-darmstadt.de", "juffi@ke.informatik.tu-darmstadt.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391466660000, "tcdate": 1391466660000, "number": 2, "id": "1QRFgLal6wk-f", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YDXrDdbom9YCi", "replyto": "YDXrDdbom9YCi", "signatures": ["anonymous reviewer 1ddd"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Large-scale Multi-label Text Classification - Revisiting Neural Networks", "review": "This paper tackles the problem of multi-label classification using a single-layer neural network. Several options are considered such as thresholding, using various transfer functions or dropouts.\r\n\r\nThe paper is full of imprecisions, writing errors that make it hard to read (the paragraph 'Computational Expenses' of Section 2.2 is perhaps the worse from this point of view), even if it is possible to get most of the content.\r\n\r\nThe whole paper is based on a comparison with BP-MLL, which is quite problematic for two reasons. First, it is not clear that BP-MLL is the best suitable baseline. What motivated this choice? Their pairwise exponential loss function is not standard, most ranking methods instead choose the hinge loss). What is their architecture? We need more arguments to assess this as a strong benchmark. Besides, the comparison made in the paper between the proposed network and BP-MLL is not valid: influences of architecture, choice of transfer functions and loss are all mixed. In the end of the 'Plateaus' paragraph, it is suggested that ReLUs allow to outperform BP-MLL but no experience with BP-MLL with ReLU has been conducted. Perhaps the loss of BP-MLL (PWE) could be as efficient as cross entropy with ReLU. Is BP-MLL used as a benchmark as a network or simply as a loss (exponential)? What are the results with hinge loss?\r\n\r\nMost results about neural networks are not particularly new. It is already quite well known that (1) Dropout prevent overfitting and (2) networks with ReLUs are easier to optimize than those with Tanh. In that sense, Figure 3, which is quite nice and sound, or Section 5 do not bring much new results to practitioners already used to Deep Learning (such as the ICLR audience I guess). \r\n\r\nThe curves form Figure 2 are more original but I don't agree with its creation. What justifies that what is observed with such a weird networks (a single hidden unit) can transfer to more generic feed-forward NNs? Besides, the conclusions regarding the 'plateaus' are not really obvious: it seems that there are plateaus for all settings. Is is expected that the curves with tanh appear to be symetric w.r.t. the origin?\r\n\r\nOther comments:\r\n- I don't see what means 'Revisiting Neural Networks' in the title of the paper.\r\n- In Equation (2), what is the definition of $y_l$?\r\n- Legend of Fig 1(b): CE w/ ReLU -> CE W/ tanh\r\n- ReLU is used from the beginning of Section 2 but defined in Section 2.3.\r\n- How is defined Delta for ADAGRAD?\r\n- Couldn't it be interesting to try to learn thresholds using the class probabilities outputted by the network?\r\n- The metrics used in experiments could be described.\r\n- Discussion on the training efficiency of linear SVMs in Section 6.2 seems irrelevant here."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks", "decision": "submitted, no decision", "abstract": "Large-scale datasets with multi-labels are becoming readily available, and the demand for large-scale multi-label classification algorithm is also increasing. In this work, we propose to utilize a single-layer Neural Networks approach in large-scale multi-label text classification tasks with recently proposed learning techniques. We carried out experiments on six textual datasets with varying characteristics and size, and show that a simple Neural Networks model equipped with recent advanced techniques for Neural Networks components such as an activation layer, optimization, and generalization techniques performs as well as or even outperforms the previous state-of-the-art approaches on large-scale datasets with diverse characteristics.", "pdf": "https://arxiv.org/abs/1312.5419", "paperhash": "nam|largescale_multilabel_text_classification_revisiting_neural_networks", "keywords": [], "conflicts": [], "authors": ["Jinseok Nam", "Jungi Kim", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "authorids": ["jinseok.n@gmail.com", "kim@ukp.informatik.tu-darmstadt.de", "gurevych@ukp.informatik.tu-darmstadt.de", "juffi@ke.informatik.tu-darmstadt.de"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391405160000, "tcdate": 1391405160000, "number": 1, "id": "FMBUveVoQjvA1", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "YDXrDdbom9YCi", "replyto": "YDXrDdbom9YCi", "signatures": ["anonymous reviewer 3ccf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Large-scale Multi-label Text Classification - Revisiting Neural Networks", "review": "The authors claim that a simple two-layer fully connected neural net can outperform or meet state of the art accuracies on large, multi-label classification tasks, using rectified linear units and dropout.  They make novel use of L2 regression to compute a data-dependent threshold to map model outputs to class labels.\r\n\r\nWhile the paper is interesting, the baselines seem weak, in particular, the comparison against a neural net ranker with exponential versus cross-entropy loss.  The authors make a point several times in the paper with which I disagree - that ranking approaches do not scale to large datasets.  The existence of effective web search engines is strong evidence to the contrary.\r\n\r\n\r\nSpecific comments\r\n*****************\r\n\r\nThere are many typos and grammatical errors but few that change the meaning, so below I only mention the latter.  \r\n\r\nEq. (2): For clarity's sake it might be worth mentioning that f_0 has range [0,1] and that the labels y are in {0,1}.\r\n\r\nFig. (2): Yes, but the high slope of ReLU can also cause problems if there is noise in the data (which appears as outliers).\r\n\r\nI think that a much better comparison than the one with BP-MLL would be to compare with RankNet, a neural net ranker that uses a cross entropy error function and thus is much closer to the cost function used in the paper.  It does seem to make sense to rank all outputs corresponding to positive labels above all outputs corresponding to negative; the question left open by your comparison is whether the problem lies with the use of the exponential cost in BP-MLL.  Unfortunately there are at least two confounding factors that you have not separated in comparing BP-MLL versus your method: the use of ranking at all, and the choice of ranking cost function.  So I think it's important to compare against a cross entropy ranking function. \r\n\r\n'The ReLU disables negative activation so that the number of parameters to be learned decreases during the training.' - what do you mean?\r\n\r\nSection 2.3: define the Delta used in Adagrad\r\n\r\nSection 3: this is a nice idea, for choosing data dependent thresholds - is it novel (for multi label classification)?\r\n\r\nSec. 4.1: it would be useful to give brief definitions of the ranking measures used ('Rank loss, one-error, etc.'), since, except for MAP they (or at least, these names for them) are not well known.\r\n\r\nSec 4.2: my impression is that your choice of SVM baseline is weak (also, few details are given). Binary relevance seems like a poor choice for the multilabel task.  Why not compare to SVM rankers?  And since you're comparing to a nonlinear system, reporting results using nonlinear kernels would be good to be more complete (although linear SVMs usually do work well on text classification, this is a different task).\r\n\r\nTable 2: 'B and R followed by BR are to represent' should read 'B and R following BR represent' and would be even better written as simply 'B and R subscripts represent'\r\n\r\nThe results in Table 2 seem to be mixed. Sometimes dropout helps, sometimes it hurts. Sometimes the SVMs win, often not.  The only take-away that seems safe to me, is that if you want the best performing system, then try all these methods (and other, stronger baselines - see above) on your data, and pick the best.  I think the most interesting result is that NNs (lumping with and without dropout together) beat linear SVMs, which is usually the strongest performing method for text classification.  But this task is different, and so it would have made more sense to compare against SVM rankers.\r\n\r\nGiven the use of ReLU transfer functions, what different did L1 regularization on the hidden activations make?\r\n\r\nFigs 3a is great.  Fig 3b is a little misleading, though - you chose the one dataset where adding dropout helped.  What does that curve look like on a dataset where adding dropout hurt?\r\n\r\nSection 6.1:  'the presence of a specific label may suppress or exhibit the probability of presence of other labels' - I think you just mean that some sets of labels tend to occur together, and this is ignored in binary relevance methods, but please explain this more clearly."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks", "decision": "submitted, no decision", "abstract": "Large-scale datasets with multi-labels are becoming readily available, and the demand for large-scale multi-label classification algorithm is also increasing. In this work, we propose to utilize a single-layer Neural Networks approach in large-scale multi-label text classification tasks with recently proposed learning techniques. We carried out experiments on six textual datasets with varying characteristics and size, and show that a simple Neural Networks model equipped with recent advanced techniques for Neural Networks components such as an activation layer, optimization, and generalization techniques performs as well as or even outperforms the previous state-of-the-art approaches on large-scale datasets with diverse characteristics.", "pdf": "https://arxiv.org/abs/1312.5419", "paperhash": "nam|largescale_multilabel_text_classification_revisiting_neural_networks", "keywords": [], "conflicts": [], "authors": ["Jinseok Nam", "Jungi Kim", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "authorids": ["jinseok.n@gmail.com", "kim@ukp.informatik.tu-darmstadt.de", "gurevych@ukp.informatik.tu-darmstadt.de", "juffi@ke.informatik.tu-darmstadt.de"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387791000000, "tcdate": 1387791000000, "number": 30, "id": "YDXrDdbom9YCi", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "YDXrDdbom9YCi", "signatures": ["jinseok.n@gmail.com"], "readers": ["everyone"], "content": {"title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks", "decision": "submitted, no decision", "abstract": "Large-scale datasets with multi-labels are becoming readily available, and the demand for large-scale multi-label classification algorithm is also increasing. In this work, we propose to utilize a single-layer Neural Networks approach in large-scale multi-label text classification tasks with recently proposed learning techniques. We carried out experiments on six textual datasets with varying characteristics and size, and show that a simple Neural Networks model equipped with recent advanced techniques for Neural Networks components such as an activation layer, optimization, and generalization techniques performs as well as or even outperforms the previous state-of-the-art approaches on large-scale datasets with diverse characteristics.", "pdf": "https://arxiv.org/abs/1312.5419", "paperhash": "nam|largescale_multilabel_text_classification_revisiting_neural_networks", "keywords": [], "conflicts": [], "authors": ["Jinseok Nam", "Jungi Kim", "Iryna Gurevych", "Johannes F\u00fcrnkranz"], "authorids": ["jinseok.n@gmail.com", "kim@ukp.informatik.tu-darmstadt.de", "gurevych@ukp.informatik.tu-darmstadt.de", "juffi@ke.informatik.tu-darmstadt.de"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 5}