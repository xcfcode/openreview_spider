{"notes": [{"id": "HylloR4YDr", "original": "rkxg7gKOPH", "number": 1306, "cdate": 1569439384092, "ddate": null, "tcdate": 1569439384092, "tmdate": 1577168294622, "tddate": null, "forum": "HylloR4YDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "fAg2ck3d9B", "original": null, "number": 1, "cdate": 1576798719955, "ddate": null, "tcdate": 1576798719955, "tmdate": 1576800916601, "tddate": null, "forum": "HylloR4YDr", "replyto": "HylloR4YDr", "invitation": "ICLR.cc/2020/Conference/Paper1306/-/Decision", "content": {"decision": "Reject", "comment": "Solid, but not novel enough to merit publication.  The reviewers agree on rejection, and despite authors' adaptation, the paper requires more work and broader experimentation for publication.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HylloR4YDr", "replyto": "HylloR4YDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723467, "tmdate": 1576800274946, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1306/-/Decision"}}}, {"id": "B1goP3P7oH", "original": null, "number": 4, "cdate": 1573252194762, "ddate": null, "tcdate": 1573252194762, "tmdate": 1573252194762, "tddate": null, "forum": "HylloR4YDr", "replyto": "HJlHJ3v7iS", "invitation": "ICLR.cc/2020/Conference/Paper1306/-/Official_Comment", "content": {"title": "Author Response to Reviewer #1 - Point 2", "comment": "\"limited to navigation environments - The proposed methods do not seem to be directly applicable to tasks other than navigation, where a very task-specific goal position can be provided.\"\n\nWe have shown results on tasks that have been the focus of many recent works in goal-conditioned RL ([4], [5], [6]). At the same time, the key definition of e.m.o. experiences is not restricted to this setting, because they can be used whenever symmetry in data can be exploited. \n\n\n\n[4] Vitchyr Pong*, Shixiang Gu*, Murtaza Dalal, and Sergey Levine. Temporal difference models: Model-free deep RL for model-based control. In International Conference on Learning Representations, 2018.\n[5] Dibya Ghosh, Abhishek Gupta, and Sergey Levine. Learning actionable representations with goalconditioned policies. CoRR, abs/1811.07819, 2018.\n[6] Florensa, C., Held, D., Geng, X. and Abbeel, P., 2017. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366."}, "signatures": ["ICLR.cc/2020/Conference/Paper1306/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylloR4YDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference/Paper1306/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1306/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1306/Reviewers", "ICLR.cc/2020/Conference/Paper1306/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1306/Authors|ICLR.cc/2020/Conference/Paper1306/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158023, "tmdate": 1576860529274, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference/Paper1306/Reviewers", "ICLR.cc/2020/Conference/Paper1306/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1306/-/Official_Comment"}}}, {"id": "HJlHJ3v7iS", "original": null, "number": 3, "cdate": 1573252061215, "ddate": null, "tcdate": 1573252061215, "tmdate": 1573252061215, "tddate": null, "forum": "HylloR4YDr", "replyto": "rkehqHWYFH", "invitation": "ICLR.cc/2020/Conference/Paper1306/-/Official_Comment", "content": {"title": "Author Response to Reviewer #1 - Point 1", "comment": "We thank the reviewer for the constructive feedback. We hope that our revision and the following comments answer the questions raised:\n\n\"The proposed algorithm lack novelty. Goal conditioned reinforcement learning, where a generalized inverse dynamics is used, has been widely studied in [2, 3].\"\n\nGoal-conditioned reinforcement learning involves maximizing the return under a goal-specific reward function. Our work, in contrast, leverages experience already collected while training an agent for a locomotion task, to enable the agent to navigate to the goal. Our approach is different in that we do not introduce the goal while training the locomotion policy. We train an agent to perform well on the locomotion task, and then utilize the data collected in this process to learn a model (IDM) that maps the current state of the agent and the goal, to the required action. Thus, our method is different from goal-conditioned reinforcement learning methods.\n[2] develops an algorithm for goal-conditioned RL from the imitation learning perspective, i.e. a set of expert trajectories is provided to the learning agent, whereas our method does not have an expert demonstrator. In addition to this, the data augmentation technique used in [2] leverages intermediate states in the expert trajectory as goals, whereas our method leverages symmetry in the trajectories collected by the agent while optimizing for a locomotion policy.\n[3] has a very different setting from our work: a large number of experts that perform single skills well are available, and the focus is on learning a shared policy that can perform well on single skills, or a composition of skills, to eventually achieve a system capable of performing one-shot imitation. Also, the embedding space learned in [3] represents short-term motor behaviour, and is used to pick behavioral modes, whereas the embedding space we learn, captures orientation-invariant features that help the agent generalize its actions for the state-goal input space.\nThe main challenge in existing approaches for goal-conditioned RL is the need to sample goals, resulting in low generalization capability (as we show in VGCP, HER-Sp and HER-De). Thus, we propose learning latent representations for encoding information shared between similar experiences, and learn an inverse dynamics model that takes as input these latent representations and outputs the desired actions. We compare our results (GE, LR), with methods that use goal-conditioned policies/value functions (VGCP, HER-Sp, HER-De), and show that our method is much superior both qualitatively, and quantitatively.\n\n\n\"And the use of symmetry has also been studied [1].\"\n\nIt is true that symmetry has been studied in [1]; however, [1] imposes symmetry in the action space of the locomotion agent, by penalizing paired limbs for learning different actions, to learn good locomotion gaits. Our work utilizes symmetry in state-goal pairs in different orientations, to achieve navigation to arbitrary goals in a sample-efficient manner.\n\n\n\"The augmentation of data by considering symmetry is relatively straight-forward.\"\n\nWe have shown that merely augmenting the data by adding symmetric experiences is not enough to achieve SOTA performance (refer to GE results in Table 1). It is essential to learn a common embedding that is invariant to orientation, and can control the agent effectively in any orientation to reach the goal (refer to LR in experiments). The results of our ablation studies clearly indicate that LR (architecture shown in Fig. 4) outperforms GE (using generalized experiences to train the IDM) in each environment, showing that the design of the Latent Feature Model with shared weights for e.m.o. experiences leads to a boost in performance over using only symmetric experiences, for all agents.\n\n\n\n[1] Yu, Wenhao, Greg Turk, and C. Karen Liu. \"Learning symmetric and low-energy locomotion.\" ACM Transactions on Graphics (TOG) 37.4 (2018): 144.\n[2] Ding, Yiming, Carlos Florensa, Mariano Phielipp, and Pieter Abbeel. \"Goal-conditioned Imitation Learning.\" arXiv preprint arXiv:1906.05838 (2019).\n[3] Merel, Josh, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, and Nicolas Heess. \"Neural probabilistic motor primitives for humanoid control.\" arXiv preprint arXiv:1811.11711 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1306/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylloR4YDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference/Paper1306/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1306/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1306/Reviewers", "ICLR.cc/2020/Conference/Paper1306/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1306/Authors|ICLR.cc/2020/Conference/Paper1306/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158023, "tmdate": 1576860529274, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference/Paper1306/Reviewers", "ICLR.cc/2020/Conference/Paper1306/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1306/-/Official_Comment"}}}, {"id": "BkgORYD7jS", "original": null, "number": 2, "cdate": 1573251535915, "ddate": null, "tcdate": 1573251535915, "tmdate": 1573251535915, "tddate": null, "forum": "HylloR4YDr", "replyto": "rkgnfN29YB", "invitation": "ICLR.cc/2020/Conference/Paper1306/-/Official_Comment", "content": {"title": "Author Response to Reviewer #3", "comment": "We thank the reviewer for the constructive feedback. We hope that our revision and the following comments answer the questions raised:\n\n1. The setting of our IDM is that it takes in waypoints, or intermediate goals, and outputs the action required to reach that goal. These waypoints are typically provided by a planning algorithm. Our approach is agnostic to the planning algorithm; in our methods, we use Model Predictive Control, i.e. we replan at each step. We have added these details to A.1.3.\n\n\n2. Thank you for pointing this out. We have modified the paragraph on \u201cCollecting Generalized Experiences (GE) in 5.2 to make the data collection process clearer. In short, data collection for our methods happens in the following manner:\na) An agent learns to walk to the right. We collect the trajectories observed during this training procedure.\nb) For each trajectory (this varies according to the environment, exact details in A.1.2), we rotate the initial state of the agent by a random angle, and repeat the actions taken in the trajectory.\nc) These two trajectories (the original one, and the augmented one), consist of e.m.o. experience samples (Def. 3). These samples are used to train our models (GE and LR).\n\nThe reason that the agent is able to turn while walking across different waypoints as seen in Fig. 7 is that thought the agent is rewarded for walking to the right, there is no penalty on it drifting from the X-axis. This diversity of samples benefits the robustness of the learned IDM. Thus, the agent will encounter a number of trajectories, in which it is not strictly going to the right, but along a curve, and still gets high rewards. These kinds of experience samples allow our IDM to learn actions that can enable the agent to turn in a certain direction, resulting in the trajectories that you see in Fig. 7.\nFor the baseline algorithm VGCP used in Fig. 7, at training time, goals are randomly sampled from all directions, and so, the agent is able to learn policies to navigate to any goal in the plane.\n\n\n3. We have updated the main text to include the exact input to our models (please refer to Remark 2). Instead of providing (s, o, o\u2019) to the model, we input (s, d), where d is the unit vector in the direction of the goal (represents \\theta). Since we replan at each step, we believe we do not need to include the distance to the goal as an input to the model. Thus, the experiment requested (running the proposed method without generalized experience and latent representation) is the same as RL in the paper (see 5.2)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1306/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylloR4YDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference/Paper1306/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1306/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1306/Reviewers", "ICLR.cc/2020/Conference/Paper1306/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1306/Authors|ICLR.cc/2020/Conference/Paper1306/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158023, "tmdate": 1576860529274, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference/Paper1306/Reviewers", "ICLR.cc/2020/Conference/Paper1306/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1306/-/Official_Comment"}}}, {"id": "SkgQMKDQiH", "original": null, "number": 1, "cdate": 1573251339310, "ddate": null, "tcdate": 1573251339310, "tmdate": 1573251339310, "tddate": null, "forum": "HylloR4YDr", "replyto": "r1ejU8BQqr", "invitation": "ICLR.cc/2020/Conference/Paper1306/-/Official_Comment", "content": {"title": "Author Response to Reviewer #2", "comment": "We thank the reviewer for the detailed feedback. We hope that our revision and the following comments answer the questions raised:\n\n1. For the Mujoco Ant environment, our results show that naive sampling of the goal space gives models that are still far from matching the performance shown by our models, after 10 million samples of environment interaction (our models use only 2 million samples). Thus, we have focused on comparing with more sophisticated baseline methods as shown in Section 5. But this is a good point and we can add more details to the paper. \nIntuitively, we can think about this as: if (s1, g1) and (s2, g2) have the same angle between the agent\u2019s orientation and the goal, a policy learned using goal-conditioned RL could learn different actions to reach the goal in each case, which would be more expensive in terms of the number of samples used to learn these actions. However, in our case, by modeling the equivalence between these (state, goal) pairs into a common latent embedding, we are generalizing the same actions across all such pairs. Thus, our method would be more sample-efficient than random goal sampling.\n\n\n2. Thank you for raising this point. Although it is not the focus of our paper, adding generalized experiences would likely help learning the goal-conditioned policy as well. The setting in our paper assumes that the IDM takes pairs of waypoints and outputs actions, where the waypoints can be given by a higher level planning module. This is the typical setting in locomotion robots, but using generalized experiences to directly handle goal-conditioned RL by combining the two modules is an interesting avenue for future work.\nThe point discussed in (1) still holds in this case: using these generalized experiences, the goal-conditioned policies may enable the agent to navigate to a larger range of goals, but it would not be generalizable like our methods, since it does not attempt to learn the symmetries in these experiences.\n\n\n3. We do not modify the LFM in any way. The set of 2 e.m.o. states and goals in Fig. 4 share weights i.e. they are passed through the same neural network to generate the latent representations. As mentioned in Remark 1 (page 6), at test time, only the current state and goal are passed through this network to generate the latent representation.\n\n\n4. Although the same number of samples is used for training all models, GE and LR use the first half of the samples collected while training the agent to walk to the right. These samples are \u201cinferior\u201d to the latter half of samples, as the agent achieves lower rewards in the first half. In other words, as the agent interacts more with the environment and updates its policy, it learns better locomotion policies. In this sense, the samples used to train GE and LR are collected by running an inferior locomotion policy, compared to the other models.\n\n\n5. Thank you for pointing this out. We have corrected these references in the revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper1306/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylloR4YDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference/Paper1306/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1306/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1306/Reviewers", "ICLR.cc/2020/Conference/Paper1306/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1306/Authors|ICLR.cc/2020/Conference/Paper1306/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158023, "tmdate": 1576860529274, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1306/Authors", "ICLR.cc/2020/Conference/Paper1306/Reviewers", "ICLR.cc/2020/Conference/Paper1306/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1306/-/Official_Comment"}}}, {"id": "rkehqHWYFH", "original": null, "number": 1, "cdate": 1571521940432, "ddate": null, "tcdate": 1571521940432, "tmdate": 1572972486042, "tddate": null, "forum": "HylloR4YDr", "replyto": "HylloR4YDr", "invitation": "ICLR.cc/2020/Conference/Paper1306/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences\n\nIn this paper, the authors propose to utilize the symmetry property in locomotion problems (more specifically navigation problems), and more efficiently generate additional training data from existing data, and learns a more efficient representation.\n\nI tend to vote for rejection for this paper mostly because, while it seems to me to be a very efficient and practical engineering project, but relatively lack the novelty in terms of the algorithm.\n\nPros:\n- The experiments are of good quality, providing a lot of ablation studied and hyper-parameter specifications.\n- The proposed idea is combined with some of the state-of-the-art algorithms, showing it\u2019s compatibility and good practical performance.\n\nCons:\n\n- The proposed algorithm lack novelty.\nGoal conditioned reinforcement learning, where a generalized inverse dynamics is used, has been widely studied in [2, 3].\nAnd the use of symmetry has also been studied [1].\nThe augmentation of data by considering symmetry is relatively straight-forward.\n\n- limited to navigation environments\nThe proposed methods do not seem to be directly applicable to tasks other than navigation, where a very task-specific goal position can be provided.\n\n\n[1] Yu, Wenhao, Greg Turk, and C. Karen Liu. \"Learning symmetric and low-energy locomotion.\" ACM Transactions on Graphics (TOG) 37.4 (2018): 144.\n[2] Ding, Yiming, Carlos Florensa, Mariano Phielipp, and Pieter Abbeel. \"Goal-conditioned Imitation Learning.\" arXiv preprint arXiv:1906.05838 (2019).\n[3] Merel, Josh, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu Pham, Greg Wayne, Yee Whye Teh, and Nicolas Heess. \"Neural probabilistic motor primitives for humanoid control.\" arXiv preprint arXiv:1811.11711 (2018)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1306/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1306/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylloR4YDr", "replyto": "HylloR4YDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666550547, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1306/Reviewers"], "noninvitees": [], "tcdate": 1570237739301, "tmdate": 1575666550560, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1306/-/Official_Review"}}}, {"id": "rkgnfN29YB", "original": null, "number": 2, "cdate": 1571632147705, "ddate": null, "tcdate": 1571632147705, "tmdate": 1572972486006, "tddate": null, "forum": "HylloR4YDr", "replyto": "HylloR4YDr", "invitation": "ICLR.cc/2020/Conference/Paper1306/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a method to learn locomotion and navigation to a goal location or through a set of waypoints for simulated legged robots. The contributions of this paper include 1) generalized experience, which is a data-augmentation technique to add more orientation-invariant experience, and 2) a latent representation to encode the state, the current location and the goal location. The paper compares the proposed method with a few baselines and demonstrates better performance.\n\nMy recommendation of this paper is Weak Reject. Although the method seems reasonable and the evaluation shows good results, I think that the paper can be improved for the following three reasons. \n\nFirst, it is not clear to me how the inference works if the goal is reasonably far away (e.g. can be reached in 10 steps) from the current position of the robot? Since the inverse dynamics model only outputs an action given the current position and the immediate goal (in the next time-step), how is the 10-step action sequence planned using the 1-step immediate goals? \n\nSecond, the details of data collection are unclear to me. I believe that the policy \\pi used for data collection plays an important role. Which \\pi is used? It would be clearer to present this part in the main text, not Appendix. If the data is collected from an RL agent which learns to walk to the right, how does the robot learns to turn when walking across different waypoints (Figure 7)?  \n\nThird, in this paper, the generalized experience is to add different initial orientations of the robot. I think that the similar effect can be achieved by reparameterizing (s, o, o') into polar coordinates: (s, \\theta, r), where (\\theta, r) is the intermediate goal location relative to the robot's current orientation and position. \\theta=0 means the goal is in front of the robot, and r is the distance between the goal and the robot. In this representation, all the generalized experience will reduce to a single (s, \\theta, r), which is invariant to the robot's orientation. This would already be a good latent space, without any learning. For this reason, I would suggest adding one more baseline to compare the proposed method against: using (s_t, \\theta_t, r_t, a_t, s_{t+1}, \\theta_{t+1}, r_{t+1}) to represent experience, then run the proposed method without generalized experience and latent representation. Will this baseline achieve similar or even better results? "}, "signatures": ["ICLR.cc/2020/Conference/Paper1306/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1306/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylloR4YDr", "replyto": "HylloR4YDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666550547, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1306/Reviewers"], "noninvitees": [], "tcdate": 1570237739301, "tmdate": 1575666550560, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1306/-/Official_Review"}}}, {"id": "r1ejU8BQqr", "original": null, "number": 3, "cdate": 1572193875183, "ddate": null, "tcdate": 1572193875183, "tmdate": 1572972485972, "tddate": null, "forum": "HylloR4YDr", "replyto": "HylloR4YDr", "invitation": "ICLR.cc/2020/Conference/Paper1306/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a method for exploiting structure in locomotive tasks for efficiently learning low-level control policies that pass through waypoints while achieving some goal (typically 3D Cartesian position). This is in contrast to goal-conditioned RL policies that sample random goals during training and are thus sample inefficient, which are trained to execute one policy at a time. In particular, the paper proposes the notion of generalized experiences, where new trajectories are generated from existing trajectories, in such as a way that they are equivalent to each other (in this case translation and orientation invariant) with respect to actions.  \n\nThe idea proposed here, of exploiting environmental structures in order better generalize previous knowledge to new, unseen situations is an interesting direction for achieving sample efficiency in practical RL, such as in robotics. \n\nI have the following comments/questions.\n\n1. If I understand correctly, rather than randomly sampling the environment, the paper proposes starting off with trajectories generated while learning some single-goal policy, and generate from these new ones that the agent must execute in the environment. In which case, the question is how much less interaction does the agent have with the environment compared to random goal sampling, to achieve the same performance?\n\n2. What happens if you use a standard goal-conditioned RL with the Generalized Experiences, without training an IDM? For example, using VGCP with GE, where the goals for VGCP are teminating states of each trajectory. In other words, can a vanilla goal-conditioned RL benefit from the proposed trajectory sampling technique (GE), and how does that compare with random sampling and using the proposed latent representation technique?\n\n3. How do you modify the LFM so that it only accepts the current state and goal, instead of a set of two e.m.o equivalent states and goals? Is the same query treated as two queries that are e.m.o equivalent?\n\n4. In A.1.2, the paper mentions that the first half of the data generated using the RL algorithm is used for generating the second half of data for GE and LR, right? Which means GE and LR are trained from the same number of steps as the baselines, except that GE and LR make use of generalized experiences. However, the paper states in A2 that GE and LR learn from inferior samples. Can the authors please clarify this? My understanding is that GE are generated by modifying existing trajectories and letting the agent apply the same actions by interating with environment. Meaning that, given 2M samples, GE will generate 2M more samples that are e.m.o equivalent by interacting with the environment. \n\n5. Some of the references in the text do not have year of publication, e.g., Kulkarni et al."}, "signatures": ["ICLR.cc/2020/Conference/Paper1306/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1306/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Latent Representations for Inverse Dynamics using Generalized Experiences", "authors": ["Aditi Mavalankar", "Sicun Gao"], "authorids": ["amavalan@eng.ucsd.edu", "sicung@ucsd.edu"], "keywords": ["deep reinforcement learning", "continuous control", "inverse dynamics model"], "TL;DR": "We show that the key to achieving good performance with IDMs lies in learning latent representations to encode the information shared between equivalent experiences, so that they can be generalized to unseen scenarios.", "abstract": "Many practical robot locomotion tasks require agents to use control policies that can be parameterized by goals. Popular deep reinforcement learning approaches in this direction involve learning goal-conditioned policies or value functions, or Inverse Dynamics Models (IDMs). IDMs map an agent\u2019s current state and desired goal to the required actions. We show that the key to achieving good performance with IDMs lies in learning the information shared between equivalent experiences, so that they can be generalized to unseen scenarios. We design a training process that guides the learning of latent representations to encode this shared information. Using a limited number of environment interactions, our agent is able to efficiently navigate to arbitrary points in the goal space. We demonstrate the effectiveness of our approach in high-dimensional locomotion environments such as the Mujoco Ant, PyBullet Humanoid, and PyBullet Minitaur. We provide quantitative and qualitative results to show that our method clearly outperforms competing baseline approaches.", "pdf": "/pdf/53ea301f34105f09f9c3f551f771a2943cd571d7.pdf", "paperhash": "mavalankar|learning_latent_representations_for_inverse_dynamics_using_generalized_experiences", "original_pdf": "/attachment/41b5289b3aeff068d7ab64c059c6bcd38cc4ae4f.pdf", "_bibtex": "@misc{\nmavalankar2020learning,\ntitle={Learning Latent Representations for Inverse Dynamics using Generalized Experiences},\nauthor={Aditi Mavalankar and Sicun Gao},\nyear={2020},\nurl={https://openreview.net/forum?id=HylloR4YDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylloR4YDr", "replyto": "HylloR4YDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1306/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575666550547, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1306/Reviewers"], "noninvitees": [], "tcdate": 1570237739301, "tmdate": 1575666550560, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1306/-/Official_Review"}}}], "count": 9}