{"notes": [{"id": "DGttsPh502x", "original": "GUoi_kIrDn8", "number": 3438, "cdate": 1601308381620, "ddate": null, "tcdate": 1601308381620, "tmdate": 1614985778843, "tddate": null, "forum": "DGttsPh502x", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "nEseTYD8ZJj", "original": null, "number": 1, "cdate": 1610040351883, "ddate": null, "tcdate": 1610040351883, "tmdate": 1610473940927, "tddate": null, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a simple method to discover latent manipulations in trained text VAEs. Compared to random and coordinate directions, the authors found that by performing PCA on the latent code to find directions that maximize variance, more interpretable text manipulations can be achieved. \n\nThis paper receives 4 reject recommendations with an average score of 3.75. The reviewers have raised many concerns regarding the paper. (i) The idea is straightforward with limited novelty. (ii) There are only mostly qualitative results presented. More in-depth analysis and more solid evaluations are needed. (iii) Human evaluation is too small to draw any reliable conclusion. (iv) The proposed method is only tested on one text VAE, how well it can be generalized to other models remains unclear.\n\nThe rebuttal unfortunately did not address the reviewers' main concerns. Therefore, the AC regrets that the paper cannot be recommended for acceptance at this time. The authors are encouraged to consider the reviewers' comments when revising the paper for submission elsewhere. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"forum": "DGttsPh502x", "replyto": "DGttsPh502x", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040351868, "tmdate": 1610473940909, "id": "ICLR.cc/2021/Conference/Paper3438/-/Decision"}}}, {"id": "ZF1DUm4mdXI", "original": null, "number": 1, "cdate": 1602875876540, "ddate": null, "tcdate": 1602875876540, "tmdate": 1606614393213, "tddate": null, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Official_Review", "content": {"title": "Simple method but unclear results", "review": "-------------------\nSummary\n-------------------\nThis paper proposes a simple approach to discover interpretable latent manipulations in trained text VAEs. The method essentially involves performing PCA on the latent representations to find directions that maximize variance. The authors argue that this results in more interpretable directions. The method is applied on top of a VAE model (OPTIMUS), and the authors argue that different directions discovered by PCA correspond to interpretable concepts.\n\n-------------------\nStrengths\n-------------------\n- The method is simple, and can be applied on top of existing text VAEs.\n- Learning interpretable and controllable generative models of text is an important research area, and this paper contributes to this important field.\n\n-------------------\nWeaknesses\n-------------------\n- There are only mostly qualitative results presented. While I agree that performing quantitative results is difficult with this style of work, the authors could have (for example) adopted methods from the style transfer literature to show quantitative results. These metrics include perplexity (to see how fluent the generations are), reverse perplexity, and style transfer accuracy (this may not be applicable since there is no ground truth \"style\" in this work, but the ground truth style could be heuristically defined for some transformations, e.g. for singular/plural transformations).\n- Human evaluation seems nonideal since it is only tested on 12 people.\n- The generations are actually not so good in my opinion? E.g. many of the generations in the appendix are ungrammatical and/or semantically nonsensical.  Again, metrics such as perplexity could quantify the fluency of generated text.\n- The method is only applied to one text VAE mode which specifically uses BERT/GPT-2 , so it is not clear if this will generalize to other models (e.g. models trained from scratch).\n\n-------------------\nQuestions/Comments\n-------------------\n- In Figure 2, are these the top 4 principal directions? If not, how were these directions discovered?\n- \"It is known that variational autoencoders trained with a schedule for the KL weight parameter (equation 1) obtain disentangled representations (Higgins et al., 2016; Sikka et al., 2019; John et al., 2019). Since OPTIMUS is also trained with KL annealing, canonical coordinates in its latent space are likely to be disentangled.\" I believe this is only valid for beta > 1 so it is not really applicable here.\n-----------------------\nEdit after rebuttal: Thank you for the rebuttal and clarifying some of my questions. I have decided to keep the original score.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3438/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075824, "tmdate": 1606915758907, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3438/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3438/-/Official_Review"}}}, {"id": "rx7V4ECBVmZ", "original": null, "number": 5, "cdate": 1606305255316, "ddate": null, "tcdate": 1606305255316, "tmdate": 1606305255316, "tddate": null, "forum": "DGttsPh502x", "replyto": "ZF1DUm4mdXI", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for a detailed review! We address your concerns below: \n\nWeaknesses:\n1. Thank you for this suggestion! We will look into the evaluation of style transfer models and add more quantitative results in the next revision.\n2. To our knowledge, having few annotators for evaluation of text attribute manipulation (style transfer in particular) is quite common. For example, in [1] and [2] there are 6 and 10 annotators respectively.\n3. The generation artifacts are caused not only by the latent shifts, but also by the model itself. We will evaluate this in more detail in the next revision of the paper.\n4. We also evaluated the method on two models trained from scratch on smaller datasets, namely we trained CP-VAE [3] and the model from [4] on Yelp and Amazon datasets respectively. While some directions are also identifiable (e.g., sentence length and sentiment in case of CP-VAE), the overall generation quality is significantly lower. This decline in fluency is expected: the highest-quality generative models for language have millions of parameters and are trained on massive datasets. To our knowledge, OPTIMUS is the only high-capacity language VAE trained on a large dataset with openly available weights, which is why we mostly evaluate our method on this model. \n\nQuestions:\n1. These are examples of directions obtained with the SNLI model that were manually chosen from a set of generated sentences to highlight the differences between word categories.\n2. Thank you for the correction! We replaced this motivation with a description of the fully factorized prior distribution of standard Gaussian prior VAEs.\n\n[1] Disentangled Representation Learning for Non-Parallel Text Style Transfer. Vineet John, Lili Mou, Hareesh Bahuleyan, Olga Vechtomova. ACL 2019 \n\n[2] Improving Disentangled Text Representation Learning with Information-Theoretic Guidance. Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, Lawrence Carin. ACL 2020 \n\n[3] On Variational Learning of Controllable Representations for Text without Supervision. Peng Xu, Jackie Chi Kit Cheung, Yanshuai Cao. ICML 2020 \n\n[4] Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation. Ke Wang, Hang Hua, Xiaojun Wan. NeurIPS 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3438/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DGttsPh502x", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3438/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3438/Authors|ICLR.cc/2021/Conference/Paper3438/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3438/-/Official_Comment"}}}, {"id": "Bl0s-kVQQeL", "original": null, "number": 4, "cdate": 1606304165515, "ddate": null, "tcdate": 1606304165515, "tmdate": 1606304165515, "tddate": null, "forum": "DGttsPh502x", "replyto": "YA_fvk0vmP0", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for a thorough evaluation of our paper and constructive feedback! We address your concerns below: \n\nCons: \n1. Regarding the novelty: to the best of our knowledge, previous works on unsupervised discovery have not attempted to reveal interpretable latent directions in generative models for language. We believe this is a challenging yet important task that will bring forth more applications as the field develops, similarly to what we observe in image GANs. In addition, our method exploits the availability of the encoder network in VAEs and works directly in the model's latent space. Previous works were applied only to image GANs and required sampling from the latent distribution (which is not necessary for encoder-decoder models) or backpropagation through generated samples (which is not possible with discrete outputs).\n2. Regarding the baselines: we agree that they are not as strong as one would prefer. However, the choice of baselines here is restricted: the task of unsupervised latent discovery in text generation models has not yet been approached, so the field itself is not quite established. If you have any suggestions on additional methods that would fit the setting of the paper, we would be happy to evaluate them.\n3. Regarding the modification procedure description: we have updated the text to highlight that each interpretable direction is a vector in the latent space. As a result, applying the shift corresponds to adding this vector to the encoder output. \n\nQuestions: \n1. We believe you are referring to Figure 1: it was meant to give an intuitive explanation of our method; both dimensions correspond to coordinates in an example two-dimensional latent space. The actual method works with 768-dimensional representations of sentences, which are much harder to visualize. \n2. As suggested by Reviewers 3 and 4, it is possible to measure the fluency of generated outputs (in terms of perplexity) and attribute change quality (in terms of heuristic metrics when we can express the manipulation in simple words)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3438/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DGttsPh502x", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3438/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3438/Authors|ICLR.cc/2021/Conference/Paper3438/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3438/-/Official_Comment"}}}, {"id": "sLfWkBPDA3C", "original": null, "number": 3, "cdate": 1606302714266, "ddate": null, "tcdate": 1606302714266, "tmdate": 1606302738757, "tddate": null, "forum": "DGttsPh502x", "replyto": "hmHHN99TBv", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your review! Please allow us to address your concerns below: \n\n1. In our preliminary experiments, we trained CP-VAE [1] and the model from [2] on Yelp and Amazon datasets respectively. While some directions are also identifiable (e.g., sentence length and sentiment in case of CP-VAE), the overall generation quality is significantly lower. This decline in fluency is expected: the highest-quality generative models for language have millions of parameters and are trained on massive datasets. To our knowledge, OPTIMUS is the only high-capacity language VAE trained on a large dataset with openly available weights, which is why we mostly evaluate our method on this model. \n\n2. Regarding the applicability of the method to AE models instead of just VAEs. Indeed, one can apply the technique to AEs as well; in fact, one of the models we evaluated was a regular autoencoder (SNLI, $\\beta=0$ in Table 1). However, a regular autoencoder is not a proper generative model because its sampling process is not well-defined. Hence, we focus only on variational autoencoders in our work. \n\n[1] On Variational Learning of Controllable Representations for Text without Supervision. Peng Xu, Jackie Chi Kit Cheung, Yanshuai Cao. ICML 2020 \n\n[2] Controllable Unsupervised Text Attribute Transfer via Editing Entangled Latent Representation. Ke Wang, Hang Hua, Xiaojun Wan. NeurIPS 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper3438/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DGttsPh502x", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3438/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3438/Authors|ICLR.cc/2021/Conference/Paper3438/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3438/-/Official_Comment"}}}, {"id": "S5uNTnEtojx", "original": null, "number": 2, "cdate": 1606302084708, "ddate": null, "tcdate": 1606302084708, "tmdate": 1606302084708, "tddate": null, "forum": "DGttsPh502x", "replyto": "MWu4hOJKhcx", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for your review! We address your points below.\n\n1. For a list of directions, we have added several examples of latent manipulations that were discovered by our method on the SNLI model ($\\beta=1$) and separated them into several categories to help the reader understand the differences between groups. These are available in the new section of the appendix.\n\n2. Regarding the sentences with non-applicable directions: such directions still change the sentences, although the content changes are not as drastic. The degree of content change depends on the magnitude of a shift and is a property of the model: if a particular VAE has latent shifts that can be used only for a subset of texts, any method can reveal them.\n\n3. On human and automatic evaluation: first, although we use 20 sentences for manipulation, we apply 20 manipulations to each sentence, which gives us 400 initial examples. We reuse these 20 sentences across different manipulations to observe how different latent shifts affect the same sentence. After filtering unchanged sentences, we show 5 examples of each manipulation, which corresponds to 100 transformation examples; for each shift, we have 5 degrees of intensity.\nSecond, we agree that testing the interpretability of each manipulation with automatic metrics would strengthen the results. We will measure the fluency of generated sentences and the success of simple transformations in the next revision of the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3438/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "DGttsPh502x", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3438/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3438/Authors|ICLR.cc/2021/Conference/Paper3438/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837538, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3438/-/Official_Comment"}}}, {"id": "YA_fvk0vmP0", "original": null, "number": 2, "cdate": 1603831810932, "ddate": null, "tcdate": 1603831810932, "tmdate": 1605024000731, "tddate": null, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Official_Review", "content": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "review": "This paper presents a PCA-based latent variable language model for unsupervised latent variable interpretation.\n\nPros:\n1. The authors propose to use PCA to extract the principal components of the results and claim them to be interpretable latent variables.\n\nCons:\n1. The novelty is quite limited. Applying an existing well-known technique to obtain interpretable latent variables is not advancing this domain in the right direction.\n2. The explanation of latent variable in this paper is self-justified. The self-defined baselines cannot be convincingly conveyed that latent variable are interpreted. And the baselines are quite weak.\n3. In the quality evaluation, the authors do not show how clearly to modify the discovered latent variable to alter the sentences.\n\nQuestion:\n1. How do you encode a sentence in a two-dimensional space? Are both dimension probability?\n2. Other than the current quantitative and qualitative analysis, do you think any other quantitative evaluation will be helpful?", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3438/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075824, "tmdate": 1606915758907, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3438/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3438/-/Official_Review"}}}, {"id": "hmHHN99TBv", "original": null, "number": 3, "cdate": 1604101324275, "ddate": null, "tcdate": 1604101324275, "tmdate": 1605024000673, "tddate": null, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Official_Review", "content": {"title": "An interesting examination and exploration of the OPTIMUS VAE model", "review": "The author propose to use PCA-like method on latent space of VAE models to unsupervisedly detect interpretable direction. The idea is reasonable and practically useful for large-scale pretrained VAE model, i.e. OPTIMUS. This paper has a clear idea and a thorough discussion with related works. \n\nI have some concerns about the model. The proposed model seems requiring a large-scale pretrained model. If the VAE model is just trained on SNIL level, is method still valid?  From the PCA side, it does not require a Gaussian space. So why specifically targeting on VAE model, not just AE model is another confusion. Since the direction is computed based on training data, I kind of feeling of no need of using VAE model.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3438/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075824, "tmdate": 1606915758907, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3438/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3438/-/Official_Review"}}}, {"id": "MWu4hOJKhcx", "original": null, "number": 4, "cdate": 1604333411160, "ddate": null, "tcdate": 1604333411160, "tmdate": 1605024000616, "tddate": null, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "invitation": "ICLR.cc/2021/Conference/Paper3438/-/Official_Review", "content": {"title": "Straightforward idea, hasty  experiments", "review": "This paper studies latent manipulations in text autoencoders. The authors propose that compared to random and coordinate directions, moving in the PCA directions of encodings of training examples will produce more interpretable text manipulations.\n\nAs the idea is straightforward, I'd like to see more in-depth analysis and more solid evaluations. The authors characterize the effects of PCA directions into four types (length, word change, word insertion, and structure enforcement), but for each type only one example is provided. What are the changed/inserted words and what are the enforced structures? Can you give a comprehensive list of them? When are these latent directions applicable and when are they not? For sentences that are not applicable, what effects will they bring?\n\nThe only evaluation in the paper is human evaluation of whether a latent direction shift produces interpretable generations. It's conducted on 20 sentences, which is too small to draw any conclusions. The results on the Wikipedia dataset are very poor. You may test the success rate of manipulations in a specific direction (such as word insertion) through automatic evaluation. This can also reveal which manipulations are easier to implement and which are more difficult.\n\nI think with these changes, the paper will be more substantial, instead of spending 4 out of 8 pages on the background like in the current submission. Also, it's more suitable for NLP conferences than ICLR.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3438/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3438/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Discovery of Interpretable Latent Manipulations in Language VAEs", "authorids": ["~Max_Ryabinin1", "~Artem_Babenko1", "~Elena_Voita1"], "authors": ["Max Ryabinin", "Artem Babenko", "Elena Voita"], "keywords": ["interpretability", "unsupervised interpretable directions", "controllable text generation"], "abstract": "Language generation models are attracting more and more attention due to their constantly increasing quality and remarkable generation results. State-of-the-art NLG models like BART/T5/GPT-3 do not have latent spaces, therefore there is no natural way to perform controlled generation. In contrast, less popular models with explicit latent spaces have the innate ability to manipulate text attributes by moving along latent directions. For images, properties of latent spaces are well-studied: there exist interpretable directions (e.g. zooming, aging, background removal) and they can even be found without supervision. This success is expected: latent space image models, especially GANs, achieve state-of-the-art generation results and hence have been the focus of the research community. For language, this is not the case: text GANs are hard to train because of non-differentiable discrete data generation, and language VAEs suffer from posterior collapse and fill the latent space poorly. This makes finding interpetable text controls challenging. In this work, we make the first step towards unsupervised discovery of interpretable directions in language latent spaces. For this, we turn to methods shown to work in the image domain. Surprisingly, we find that running PCA on VAE representations of training data consistently outperforms shifts along the coordinate and random directions. This approach is simple, data-adaptive, does not require training and discovers meaningful directions, e.g. sentence length, subject age, and verb tense. Our work lays foundations for two important areas: first, it allows to compare models in terms of latent space interpretability, and second, it provides a baseline for unsupervised latent controls discovery.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ryabinin|unsupervised_discovery_of_interpretable_latent_manipulations_in_language_vaes", "one-sentence_summary": "We propose the first method for unsupervised discovery of interpretable attribute manipulations in text variational autoencoders; this method is very simple, fast, and outperforms the baselines by a large margin.", "supplementary_material": "/attachment/63059e11947f578028fe72efe15fea699e4a08a3.zip", "pdf": "/pdf/46aa1ab23832fe111723276d57c5a058caf25fa9.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=w4crqw42hZ", "_bibtex": "@misc{\nryabinin2021unsupervised,\ntitle={Unsupervised Discovery of Interpretable Latent Manipulations in Language {\\{}VAE{\\}}s},\nauthor={Max Ryabinin and Artem Babenko and Elena Voita},\nyear={2021},\nurl={https://openreview.net/forum?id=DGttsPh502x}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "DGttsPh502x", "replyto": "DGttsPh502x", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3438/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075824, "tmdate": 1606915758907, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3438/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3438/-/Official_Review"}}}], "count": 10}