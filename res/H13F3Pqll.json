{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396582336, "tcdate": 1486396582336, "number": 1, "id": "B10shMUdl", "invitation": "ICLR.cc/2017/conference/-/paper433/acceptance", "forum": "H13F3Pqll", "replyto": "H13F3Pqll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "All three reviewers point to significant deficiencies (particularly the lack of quantitative evaluation and clarity problems). No response or engagement from the authors. I see no basis for supporting this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inverse Problems in Computer Vision using  Adversarial  Imagination Priors", "abstract": "Given an image,  humans  effortlessly run the image formation process backwards in their minds: they can tell albedo from shading,  foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three  Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer.  Data driven adversarial imagination priors  effectively guide inversion,  minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.", "pdf": "/pdf/3c3449d94f4a81cddaaabe2b21ee20f81d2276d1.pdf", "TL;DR": "We present a model that given a visual image learns to generate imaginations of  complete scenes, albedo, shading etc, by using adversarial data driven priors on the imaginations spaces.", "paperhash": "tung|inverse_problems_in_computer_vision_using_adversarial_imagination_priors", "authorids": ["htung@cs.cmu.edu", "katef@cs.cmu.edu"], "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cmu.edu", "ntu.edu", "google.com", "berkeley.edu"], "authors": ["Hsiao-Yu Fish Tung", "Katerina Fragkiadaki"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396582810, "id": "ICLR.cc/2017/conference/-/paper433/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H13F3Pqll", "replyto": "H13F3Pqll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396582810}}}, {"tddate": null, "tmdate": 1481969597332, "tcdate": 1481969597332, "number": 3, "id": "r1H61czNl", "invitation": "ICLR.cc/2017/conference/-/paper433/official/review", "forum": "H13F3Pqll", "replyto": "H13F3Pqll", "signatures": ["ICLR.cc/2017/conference/paper433/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper433/AnonReviewer3"], "content": {"title": "", "rating": "3: Clear rejection", "review": "In this work, the authors propose to use a (perhaps deterministic) retrieval function to replace uniform sampling over the train data in training the discriminator of a GAN.\nAlthough I like the basic idea, the experiments are very weak.  There are essentially no quantitative results, no real baselines, and only a small amount of not especially convincing qualititative results.   It is honestly hard to review the paper- there isn't any semblance of normal experimental validation.\n\nNote:  what is happening with the curves in fig. 6?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inverse Problems in Computer Vision using  Adversarial  Imagination Priors", "abstract": "Given an image,  humans  effortlessly run the image formation process backwards in their minds: they can tell albedo from shading,  foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three  Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer.  Data driven adversarial imagination priors  effectively guide inversion,  minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.", "pdf": "/pdf/3c3449d94f4a81cddaaabe2b21ee20f81d2276d1.pdf", "TL;DR": "We present a model that given a visual image learns to generate imaginations of  complete scenes, albedo, shading etc, by using adversarial data driven priors on the imaginations spaces.", "paperhash": "tung|inverse_problems_in_computer_vision_using_adversarial_imagination_priors", "authorids": ["htung@cs.cmu.edu", "katef@cs.cmu.edu"], "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cmu.edu", "ntu.edu", "google.com", "berkeley.edu"], "authors": ["Hsiao-Yu Fish Tung", "Katerina Fragkiadaki"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512587137, "id": "ICLR.cc/2017/conference/-/paper433/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper433/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper433/AnonReviewer2", "ICLR.cc/2017/conference/paper433/AnonReviewer1", "ICLR.cc/2017/conference/paper433/AnonReviewer3"], "reply": {"forum": "H13F3Pqll", "replyto": "H13F3Pqll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper433/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper433/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512587137}}}, {"tddate": null, "tmdate": 1481943949123, "tcdate": 1481940763541, "number": 2, "id": "BkQQJmMVe", "invitation": "ICLR.cc/2017/conference/-/paper433/official/review", "forum": "H13F3Pqll", "replyto": "H13F3Pqll", "signatures": ["ICLR.cc/2017/conference/paper433/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper433/AnonReviewer1"], "content": {"title": "The proposed model has potential merits, but lack of quantitative evaluation and paper clarity issues put the paper below the bar.", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a model that generates a latent representation of input image(s) and optimizes a reconstruction loss with an adversarial loss (Eq (1)) over nearest neighbors from a bank of images (\u201cmemory\u201d).   The framework is adapted to three tasks: (i) image in-painting, (ii) intrinsic image decomposition, (iii) figure-ground layer extraction.  Qualitative results are shown for all three tasks.\n\nI think the proposed model has potential merits.  I particularly like the fact that it seems to be reasoning over image composites via matching against a bank of images (somewhat similar to \u201cSegmenting Scenes by Matching Image Composites\u201d work in NIPS 2009).  However, I won\u2019t champion the paper as the overall clarity and evaluation could be improved.\n\nMore detailed comments:\n\nI believe the fatal flaw of the paper is there is no quantitative evaluation of the approach.  At the very least, there should be a comparison against prior work on intrinsic image decomposition (e.g., SIRFS, maybe benchmark on \"intrinsic images in the wild\u201d dataset).\n\nI found the writing vague and confusing throughout.  For instance, \u201cmemory database\u201d could mean a number of things, and in the end it seems that it\u2019s simply a set of images.  \u201cImagination\u201d is also vague.  On page 4, R(M,x) has the database and input image as arguments, but Fig 2 doesn\u2019t show the input image as an input to R.  The contributions listed on page 3 should be tightened (e.g., it\u2019s not clear what \u201cRelevant memory retrieval for informative adversarial priors\u201d means).  Fig 3 seems inconsistent with Fig 2 as the module for \u201cmemory database\u201d is not present.  The fully-convolutional discriminator could use more details; one possibility is to provide a cost function.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inverse Problems in Computer Vision using  Adversarial  Imagination Priors", "abstract": "Given an image,  humans  effortlessly run the image formation process backwards in their minds: they can tell albedo from shading,  foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three  Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer.  Data driven adversarial imagination priors  effectively guide inversion,  minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.", "pdf": "/pdf/3c3449d94f4a81cddaaabe2b21ee20f81d2276d1.pdf", "TL;DR": "We present a model that given a visual image learns to generate imaginations of  complete scenes, albedo, shading etc, by using adversarial data driven priors on the imaginations spaces.", "paperhash": "tung|inverse_problems_in_computer_vision_using_adversarial_imagination_priors", "authorids": ["htung@cs.cmu.edu", "katef@cs.cmu.edu"], "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cmu.edu", "ntu.edu", "google.com", "berkeley.edu"], "authors": ["Hsiao-Yu Fish Tung", "Katerina Fragkiadaki"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512587137, "id": "ICLR.cc/2017/conference/-/paper433/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper433/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper433/AnonReviewer2", "ICLR.cc/2017/conference/paper433/AnonReviewer1", "ICLR.cc/2017/conference/paper433/AnonReviewer3"], "reply": {"forum": "H13F3Pqll", "replyto": "H13F3Pqll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper433/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper433/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512587137}}}, {"tddate": null, "tmdate": 1481891617824, "tcdate": 1481891617824, "number": 1, "id": "Hyc7kPWNg", "invitation": "ICLR.cc/2017/conference/-/paper433/official/review", "forum": "H13F3Pqll", "replyto": "H13F3Pqll", "signatures": ["ICLR.cc/2017/conference/paper433/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper433/AnonReviewer2"], "content": {"title": "The proposed architecture seems novel and potentially interesting, but experiments are only proof-of-concept and clarity can be improved", "rating": "6: Marginally above acceptance threshold", "review": "The paper describes a network architecture for inverse problems in computer vision. Example inverse problems considered are image inpainting, computing intrinsic image decomposition and foreground/background separation.\nThe architecture is composed of (i) a generator that produces target (latent) output (such as foreground / background regions), \n(ii) renderer that composes that latent output back to the image that can be compared with the input to measure reconstruction error, \nand (iii) adversarial prior that ensures the target output (latent) image respects a certain image statistics.\n\nStrong  points.\n- The proposed architecture with memory database is interesting and appears to be novel. \n\nWeak points:\n- Experimental results are only proof-of-concept in toy set-ups and do not clearly demonstrate benefits of the proposed architecture.\n- It is unclear whether the memory retrieval engine that retrieves images based on L2 distance on pixel values is going generalize to other more realistic scenarios. \n- Clarity. The clarity of explanation can be also improved (see below).\n\n\nDetailed evaluation.\n\nOriginality:\n- The novelty of this work lies in the (iii) adversarial prior that places an adversarial loss between the generated latent output and a single image retrieved from a large unlabelled database of target output examples (called memory). The adversarial prior has a convolutional form matching local image statistics, rather than the entire image.  The particular form of network architecture with the memory-based fully convolutional adversarial loss appears to be novel and potentially interesting.\n\n- Motivation for the Architecture. The weakest point of the proposed architecture is the \"Memory retrieval engine\" R (section 2.4),\nwhere images are retrieved from the memory by measuring L2 distance on pixel intensities. While this maybe ok for simple problems considered in this work, it is unclear how this can generalize to other more complicated datasets and problems.  \nThis should be better discussed, better justified and ideally results in some more realistic set-up shown (see below).\n\n\nQuality:\n- Experiments. Results are shown for inpainting of MNIST digits, intrinsic image decomposition on the MIT intrinsic image database, and figure/ground layer extraction on the synthesized dataset of 3D chairs rendered onto background from real photographs.  \n The experimental validation of the model is not very strong and proof-of-concept only. All the experiments are performed in simplified toy set-ups. The MNIST digit inpainting is far from current state-of-the-art on image inpainting in real photographs (see e.g. Pathak et al., 2016). The foreground background separation is done on  only synthetically generated test data. Even for intrinsic image demposition problem there is now relatively large-scale dataset of (Bell et al., 2014), see the citation below.  \n\nWhile this is probably ok for the ICLR paper, it diminishes the significance of the work. Is this model going to be useful in a real settings? One possibility to address this would be to focus on one of the problems and show results on a challenging state-of-the-art data. It would be great to see the benefits of the memory database. \n\nS. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild.\nACM Transactions on Graphics, 33(4):159, 2014.\n\nClarity:\n- The clarity of the writing can be improved. I found some of the terminology of the paper, specially the \u201cimagination\u201d and \u201cmemory\u201d confusing. From figure 2, it is not clear how the \u201cmemories\u201d for the given input image are obtained, which also took me some time to understand.\n\n- To help understand the proposed architecture, it would be useful to draw an illustration of what is happening in the \"feature space\u201d, similar in spirit e.g. to figure 2 in https://arxiv.org/pdf/1612.02136.pdf. Specially, it would be interesting to understand the role of the memory database in this way.\n\n\nSignificance:\n- The paper describes potentially interesting architecture. Given the only proof-of-concept results in toy set-ups, the significance, in the current version, appears only limited. Rather than addressing many different problems, it would be interesting to see benefits of the proposed architecture on realistic challenging data for one of the problems.\n\nOverall:\n- The proposed architecture seems novel and potentially interesting, but experiments are only proof-of-concept and clarity can be improved. It is unclear whether the memory matching engine will generalize to other more complicated datasets and problems.  Overall, I am on the edge with this paper, giving the authors the benefit of doubt with a score slightly above the threshold.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Inverse Problems in Computer Vision using  Adversarial  Imagination Priors", "abstract": "Given an image,  humans  effortlessly run the image formation process backwards in their minds: they can tell albedo from shading,  foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three  Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer.  Data driven adversarial imagination priors  effectively guide inversion,  minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.", "pdf": "/pdf/3c3449d94f4a81cddaaabe2b21ee20f81d2276d1.pdf", "TL;DR": "We present a model that given a visual image learns to generate imaginations of  complete scenes, albedo, shading etc, by using adversarial data driven priors on the imaginations spaces.", "paperhash": "tung|inverse_problems_in_computer_vision_using_adversarial_imagination_priors", "authorids": ["htung@cs.cmu.edu", "katef@cs.cmu.edu"], "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cmu.edu", "ntu.edu", "google.com", "berkeley.edu"], "authors": ["Hsiao-Yu Fish Tung", "Katerina Fragkiadaki"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512587137, "id": "ICLR.cc/2017/conference/-/paper433/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper433/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper433/AnonReviewer2", "ICLR.cc/2017/conference/paper433/AnonReviewer1", "ICLR.cc/2017/conference/paper433/AnonReviewer3"], "reply": {"forum": "H13F3Pqll", "replyto": "H13F3Pqll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper433/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper433/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512587137}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478377544894, "tcdate": 1478290563855, "number": 433, "id": "H13F3Pqll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "H13F3Pqll", "signatures": ["~Hsiao-Yu_Tung1"], "readers": ["everyone"], "content": {"title": "Inverse Problems in Computer Vision using  Adversarial  Imagination Priors", "abstract": "Given an image,  humans  effortlessly run the image formation process backwards in their minds: they can tell albedo from shading,  foreground from background, and imagine the occluded parts of the scene behind foreground objects. In this work, we propose a weakly supervised inversion machine trained to generate similar imaginations that when rendered using differentiable, graphics-like decoders, produce the original visual input. We constrain the imagination spaces by providing exemplar memory repositories in the form of foreground segmented objects, albedo, shading, background scenes and imposing adversarial losses on the imagination spaces. Our model learns to perform such inversion with weak supervision, without ever having seen paired annotated data, that is, without having seen the image paired with the corresponding ground-truth imaginations. We demonstrate our method by applying it to three  Computer Vision tasks: image in-painting, intrinsic decomposition and object segmentation, each task having its own differentiable renderer.  Data driven adversarial imagination priors  effectively guide inversion,  minimize the need for hand designed priors of smoothness or good continuation, or the need for paired annotated data.", "pdf": "/pdf/3c3449d94f4a81cddaaabe2b21ee20f81d2276d1.pdf", "TL;DR": "We present a model that given a visual image learns to generate imaginations of  complete scenes, albedo, shading etc, by using adversarial data driven priors on the imaginations spaces.", "paperhash": "tung|inverse_problems_in_computer_vision_using_adversarial_imagination_priors", "authorids": ["htung@cs.cmu.edu", "katef@cs.cmu.edu"], "keywords": ["Unsupervised Learning", "Deep learning"], "conflicts": ["cmu.edu", "ntu.edu", "google.com", "berkeley.edu"], "authors": ["Hsiao-Yu Fish Tung", "Katerina Fragkiadaki"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 5}