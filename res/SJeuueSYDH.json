{"notes": [{"id": "SJeuueSYDH", "original": "HkgIbaeKvS", "number": 2404, "cdate": 1569439856198, "ddate": null, "tcdate": 1569439856198, "tmdate": 1577168234655, "tddate": null, "forum": "SJeuueSYDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "YIcgBqa2U", "original": null, "number": 1, "cdate": 1576798748335, "ddate": null, "tcdate": 1576798748335, "tmdate": 1576800887697, "tddate": null, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Decision", "content": {"decision": "Reject", "comment": "The paper introduces a distributed algorithm for training deep nets in clusters with high-latency (i.e. very remote) nodes. While the motivation and clarity are the strengths of the paper, the reviewers have some concerns regarding novelty and insufficient theoretical analysis. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715042, "tmdate": 1576800264873, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Decision"}}}, {"id": "BkgiDncnjr", "original": null, "number": 7, "cdate": 1573854306905, "ddate": null, "tcdate": 1573854306905, "tmdate": 1573854306905, "tddate": null, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment", "content": {"title": "Our General Response", "comment": "We sincerely thank all reviewers for their comments. All reviewers agree that the paper is clearly written and has certain contributions to against latency. R2 & R3 mainly concern about the convergence guarantees, which we have justified through proof in the appendix (guarantee to converge and no slower than SGD). For unclear parts in writing, we have revised accordingly in the updated version.\n\nWe have added experiments on NLP tasks and DTS well-preserves the accuracy while tolerating a high latency.  Some experiments (e.g., transformer and BERT) takes a longer time than the rebuttal period. We will update these results later.\n\nOur code is available on : https://drive.google.com/drive/folders/1XdneMoRPNooN3-dj6yFwFcQRQO6OhLB1?usp=sharing\n\nIf there are any additional comments/suggestions on paper/code,  please let us know!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeuueSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2404/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2404/Authors|ICLR.cc/2020/Conference/Paper2404/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141874, "tmdate": 1576860553389, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment"}}}, {"id": "HJgi2xt3sB", "original": null, "number": 6, "cdate": 1573847218610, "ddate": null, "tcdate": 1573847218610, "tmdate": 1573847218610, "tddate": null, "forum": "SJeuueSYDH", "replyto": "BJxcVFy2oS", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment", "content": {"title": "Re: Thank you for the detailed answer", "comment": "Thank you for your comment! We would try to address your concern on gradient coherence.\n\nFirst, let us consider the standard SGD algorithm (on a single machine). When we sample a mini-batch, there is no guarantee that the specific stochastic gradient on this specific mini-batch will point to the optimum. What we could guarantee is in expectation they are pointing to the optimum. \n\nNow let us consider the delay update with delay t. It is true that the iterates on each machine are different  (i.e. not synchronized), but our update strategy guarantees that they are not far from each other. This is characterized by Lemma A.1 in the appendix, showing that the difference of local iterates are bounded by a constant proportional to t. This implies that the stale gradient does not deviate too much from the current gradients in expectation when t is reasonable (because of Lipschitz smoothness). \n\nWe hope this clarifies your concern. Finally, our method is 1) theoretically sound; 2) effective on tolerating high latency and achieving high scalability, which we believe has a lot of potential in large-scaled applications. We will appreciate if you could reconsider the score based on this contribution. Thank you! \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeuueSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2404/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2404/Authors|ICLR.cc/2020/Conference/Paper2404/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141874, "tmdate": 1576860553389, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment"}}}, {"id": "BJxcVFy2oS", "original": null, "number": 5, "cdate": 1573808433805, "ddate": null, "tcdate": 1573808433805, "tmdate": 1573808433805, "tddate": null, "forum": "SJeuueSYDH", "replyto": "BJxCgGFcjH", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment", "content": {"title": "Thank you for the detailed answer", "comment": "Dear authors, I acknowledge your detailed answer and believe this paper do have some merits. Glad that the comment on t inspired that warm up strategy. The theoretical analysis answers some of my concerns, but I remain doubtful that it adresses the question of gradient coherence, this is a concern I have not with your paper alone but with a few other on asynchronous SGD: the hypothesis that current and stale gradients will somehow point to the same half-space and decrease the loss needs much more exploration than what the community did so far (assuming it holds almost for free). I would highly suggest you explore this in any future iteration of this (relevant) work."}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeuueSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2404/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2404/Authors|ICLR.cc/2020/Conference/Paper2404/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141874, "tmdate": 1576860553389, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment"}}}, {"id": "ryxOXZKcsH", "original": null, "number": 2, "cdate": 1573716256131, "ddate": null, "tcdate": 1573716256131, "tmdate": 1573741150955, "tddate": null, "forum": "SJeuueSYDH", "replyto": "HJeMe67H5S", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment", "content": {"title": "Response to reviewer #1", "comment": "Thanks for your reviewer. Please see point-to-point response below:\n\n>>> The definition of scalability\n\nThe ideal case in distributed training is that N machines bring N times speedup. However, this is usually not achievable in practice because of communication costs (latency and bandwidth). If a system can achieve M times speedup on N machines, the scalability is M / N, which demonstrates how scalable the system is. We will make this point clear in the revision.\n\n>>> Evaluation only for ResNet-50 on ImageNet\n\nWe agree that ResNet-50 is a more compute-intensive model. However, as shown in  Fig1, 4.a, and 4.b, even for models with high compute/network ratio, the training efficiency of conventional approaches are still seriously affected by latency. It could be only worse when dealing with communication-intensive models.\n\nWe have added more experiments on NLP tasks as suggested by the reviewer. We experimented DTS on 2 layer LSTM on the Penn Treebank corpus (PTB) dataset. The results are attached below. DTS well-preserves the accuracy up to 20 delay steps and temporal sparsity of 1/20.\n\n\n                     Perplexity |                                                  Perplexity |\nOriginal          72.30      |   Original                                   72.30       |\nDelay(t)=4      72.28      |   Temporal Sparse (p)=4         72.27       |\nDelay(t)=10    72.29      |   Temporal Sparse (p)=10       72.31       |\nDelay(t)=20    72.27      |   Temporal Sparse (p)=20       72.28       |\n\n>>> Experiment settings \n\nApology for the confusion. We experiment on 16 eight-card GPU servers and obtain scalability of 0.008. We intend to emphasize how poor the scalability of SSGD is, that even with 100 workers (assume the same scalability 0.008), it is still slower than a single machine. We will clarify this point.\n\n>>>  Further comments\n\nWe have added convergence analysis of our algorithm in the appendix. This justifies the theoretical soundness of our algorithm. Moreover, we bridge a new connection between our delayed variant and the well-known variance reduction technique in convex optimization, we believe this provides more intuition and insight on the empirical effectiveness of our method.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeuueSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2404/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2404/Authors|ICLR.cc/2020/Conference/Paper2404/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141874, "tmdate": 1576860553389, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment"}}}, {"id": "BJlVOGFcjS", "original": null, "number": 4, "cdate": 1573716587941, "ddate": null, "tcdate": 1573716587941, "tmdate": 1573741052969, "tddate": null, "forum": "SJeuueSYDH", "replyto": "rJlp29pRtS", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment", "content": {"title": "Response to reviewer #3", "comment": "Thanks for your comments!  For the theoretical parts, we attach proof of convergence and the connection between our algorithm and variance reduction (please see Appendix updated PDF). We show that our proposed delayed update and temporally sparse update have the same convergence rate as original synchronous distributed SGD.\n\n>>>  none convergence /  convergence analysis / compared with SSGD\n\nBoth R2 and you have raised this concern. To address it, we have added a convergence analysis of our algorithm. Under the non-convex smooth optimization setting, we show that our algorithms enjoy the same convergence rate as SSGD. Moreover, we make a new connection of our methods to the well-known variance reduction technique [4]. We believe that this connection provides intuition and insights on the effectiveness of our algorithm. All the details are included in appendix.\n\nThe convergence rate of our method is given by\n\n$$ O(\\frac{1}{\\sqrt{NJ}}) + O{\\frac{t^2J}{N}} $$ \n\nwhere $t$ is the delay parameter, $N$ is the number of iterations, $J$ is the number of workers. When $t$ is reasonable ( $t < O(N^{\\frac{1}{4}} J^{-\\frac{3}{4}})$ ), the first term dominates and the convergence rate is identical to the convergence rate of SSGD. \n\n>>> delaying the updates opens up for the same problems as asynchronous SGD (ASGD)\n\nThe biggest known problem for ASGD is the stale gradients, though it is also where the scalability benefits from. The main challenge is to reduce the noise brought by staleness. There are studies on (1) dampening the stale gradients [1] (2) correct the gradients [2] (3) limit the maximum staleness [3]. In order to deal with the stale gradient, our work incorporates the well-known variance reduction technique, which naturally reduces the noise. Our theoretical analysis justifies the soundness of the proposed strategy.\n\nMoreover, the latency grows, ASGD suffers from performance degrade because of increasing staleness of gradients. On ImageNet, \n The strength of our method is to allow high scalability with negligible loss on performance.\n\n>>> Using only one dataset/network to evaluate the approach is too little\n\nOn one hand, ImageNet is a large scale dataset and ResNet is a widely adapted models in both industry and research. We believe the improvements on ResNet / ImageNet will consistently translate into other models and tasks. \n\nOn the other hand, we have performed additional experiments on NLP tasks. We experiment DTS on 2 layer LSTM on the Penn Treebank corpus (PTB) dataset. The results are attached below. DTS well-preserves the accuracy up to 20 delay steps and temporal sparsity of 1/20.\n\n\n                     Perplexity |                                                  Perplexity |\nOriginal          72.30      |   Original                                   72.30       |\nDelay(t)=4      72.28      |   Temporal Sparse (p)=4         72.27       |\nDelay(t)=10    72.29      |   Temporal Sparse (p)=10       72.31       |\nDelay(t)=20    72.27      |   Temporal Sparse (p)=20       72.28       |\n\n>>> Definition of scalability\n\nThe idea case in distributed training is that N machines bring N times speedup. However, this is usually not achievable in practice because of communication costs (latency and bandwidth). If a system can achieve M times speedup on N machines, the scalability is M / N, which demonstrates how scalable the system is. We will clarify this point in the revision.\n\n>>> Related work / Novelty\n\nWe agree with the reviewer that sparse communication has been studied previously and our temporal sparsity variant is similar to them as a strategy to amortize latency. We will add more related works including [5, 6, 7]. Please kindly advise if we miss any. However, to tolerate the latency, we have proposed a variant with delayed update, which is completely novel and new, according to the best of our knowledge. \n\nExisting methods with sparse communication do not scale well when latency grows since they only amortize it. Conventional ASGD based approaches, though scales well with growth of latency, suffer from performance drop because of increasing staleness. DTS is the first work to both maintain good scalability (0.72) and achieve promising results (no drop on accuracy) on modern models (ResNet-50) and large scale dataset (ImageNet).\n\n[1] Communication compression for decentralized training.\n[2] Staleness-aware async-sgd for distributed deep learning.\n[3] More effective distributed ml via a stale synchronous parallel parameter server.\n[4] Accelerating stochastic gradient descent using predictive variance reduction.\n[5] Communication-efficient learning of deep networks from decentralized data.\" \n[6] Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning.\n[7] Deep learning with elastic averaging SGD.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeuueSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2404/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2404/Authors|ICLR.cc/2020/Conference/Paper2404/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141874, "tmdate": 1576860553389, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment"}}}, {"id": "BJxCgGFcjH", "original": null, "number": 3, "cdate": 1573716470129, "ddate": null, "tcdate": 1573716470129, "tmdate": 1573716789462, "tddate": null, "forum": "SJeuueSYDH", "replyto": "HyeMuX_J9H", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment", "content": {"title": "Response to reviewer #2", "comment": "Thanks for your detailed and constructive comments! For the theoretical parts, we attach proof of convergence and the connection between our algorithm and variance reduction (please see Appendix updated PDF). We show that our proposed delayed update and temporally sparse update have the same convergence rate as original synchronous distributed SGD.\n\n>>> Experimental settings\n\nWe are using homogeneous infrastructure for training: All servers are communicated through allreduce and there is no central parameter server. We will make it clear in revision.\n\n>>> Theoretical soundness \n\nBoth R3 and you have raised this concern. To address it, we have added a convergence analysis of our algorithm. Under the non-convex smooth optimization setting, we show that our algorithms enjoy the same convergence rate as SSGD. Moreover, we make a new connection of our methods to the well-known variance reduction technique [4]. We believe that this connection provides intuition and insights on the effectiveness of our algorithm. All the details are included in appendix.\n\n>>> Limitation on value of t / Arbitrarily old gradients when close to convergence \n\nThe convergence rate of our method is given by\n\n$$ O(\\frac{1}{\\sqrt{NJ}}) + O{\\frac{t^2J}{N}} $$ ,\n\nwhere $t$ is the delay parameter, $N$ is the number of iterations, $J$ is the number of workers. When $t$ is in a reasonable range ( $t < O(N^{\\frac{1}{4}} J^{-\\frac{3}{4}})$ ), the first term dominates and the convergence rate is identical to the convergence rate of SSGD. \n\nWhen $t=0$, there is no staleness, the update is identical to SSGD. \nWhen $t=N$, the algorithm becomes identical as local SGD and does not converge.\n\nAs shown in the convergence analysis, $t$ has to be a reasonable value and workers cannot take any arbitrary old gradient. However, $t$ can be set to larger when training close to convergence (the first term in convergence rate dominates). Your observation inspires us to design a warm-up strategy: $t$ is initially set to a small value and grows during optimization. With this technique, we can double the maximum staleness on CIFAR (no loss on accuracy). \n\n>>> Intuitions / Why no dampening / Difference between ASGD\n\nThe biggest known problem for ASGD is the stale gradients, though it is also where the scalability benefits from. The main challenge is to reduce the noise brought by staleness. There are studies on (1) dampening the stale gradients [1] (2) correct the gradients [2] (3) limit the maximum staleness [3]. In order to deal with the stale gradient, our work incorporates the well-known variance reduction technique, which naturally reduces the noise. Our theoretical analysis justifies the soundness of the proposed strategy.\n\nOne thing worth noting is that though the scalability of ASGD variants does not vary much with the growth of latency, the accuracy will drop because increasing staleness, which is not we want. That is why we mainly compare with SSGD in our initial submission. The strength of our method is to allow high scalability with a negligible loss of accuracy. \n\nWe agree with the reviewer that we should have compared to ASGD and it is an omission from our part. We now have added comparisons to vanilla ASGD and ECD-PSGD (an improved version of ASGD) in Fig 1. When training across the world, the performance of ECD-PSGD drops quickly where DTS has no drop on accuracy while maintaining good scalability.\n\n>>> Sparse Communication \n\nDistributed training has been focusing on training inside a cluster where latency is also promised to be low. Most previous works consider more about bandwidth rather than latency. Though conventional approaches like sparse communication can be also applied to amortize latency, it cannot fully cover the communication by computation especially when the latency is high (e.g., federated learning).\n\nWe agree with the reviewer that sparse communication has been studied previously and our temporal sparsity variant is similar to them. However, the variant on the delayed update is completely novel, according to the best of our knowledge.\n\nMore importantly, as shown in Fig.4, delayed update is more effective than sparse communication (temporal sparse update) when against latency. It is worth noting that delayed update is NOT a sparse communication: information are sent received at every iteration just as SSGD. The main difference is that the received information is processed in a delayed manner. While sparse communication can only amortize the latency, our delayed technique can fully cover the cost of communication by computation.\n\n[1] Tang, Hanlin, et al. \"Communication compression for decentralized training.\"\n[2] Zhang, Wei, et al. \"Staleness-aware async-sgd for distributed deep learning.\" \n[3] Ho, Qirong, et al. \"More effective distributed ml via a stale synchronous parallel parameter server.\"\n[4] Johnson, Rie, and Tong Zhang. \"Accelerating stochastic gradient descent using predictive variance reduction.\" \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeuueSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2404/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2404/Authors|ICLR.cc/2020/Conference/Paper2404/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141874, "tmdate": 1576860553389, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment"}}}, {"id": "rJlp29pRtS", "original": null, "number": 1, "cdate": 1571900084856, "ddate": null, "tcdate": 1571900084856, "tmdate": 1572972342565, "tddate": null, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents an approach - Delayed and Temporally Sparse Update (DTS) - to do distributed training on nodes that are very distant from each other in terms of latency. The approach relies on delayed updates to tolerate latencies and temporally sparse updates to reduce traffic. The approach is implemented in a synchronous stochastic gradient decent (SGD) scheme. \n\nThe paper's approach with delayed updates, i.e., delaying gradient updates to other nodes, sends the updates in a later iteration. In this way, very long latencies can be hidden (covered by computation) since the gradient updates can be postponed to an arbitrary iteration (barrier synchronization) in the future. \n\nUnfortunately, delaying the updates opens up for the same problems as asynchronous SGD (ASGD), i.e., slower (none) convergence or staleness problems. These problems are coped with using a compensation factor, that adjusts the momentum and weights accordingly. \n\nA critical aspect of ASGD is to provide convergence guarantees, and DTS is similar in that aspect. However, the paper does not provide any convergence analysis or convergence guarantees. \n\nUsing only one dataset / network to evaluate the approach is too little. In order to show the generality, more benchmarks need to be evaluated.\n\nIn general, I like the paper. It is well written and easy to read. However, the novelty and contribution is relatively low. A lot of work has been done in the HPC and distributed systems communities over decades on how to tolerate latencies, trade-offs between communication and computation, proper synchronization points, etc. The ideas presented in this paper are well-known and extensively applied in those communities. None of that work is referensed or acknowledged in this paper. \n\nThe term \"scalability\" is used as an evaluation metric, but never clearly defined in the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576555574084, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Reviewers"], "noninvitees": [], "tcdate": 1570237723302, "tmdate": 1576555574097, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Review"}}}, {"id": "HyeMuX_J9H", "original": null, "number": 2, "cdate": 1571943273838, "ddate": null, "tcdate": 1571943273838, "tmdate": 1572972342520, "tddate": null, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper tackles the issue of scalability in distributed SGD over a high-latency network.\nAlong with experiments, this paper contributes two ideas: delayed updates and temporally sparse updates.\n\n- strengths: \n\n\u00b7 Clear-looking overall presentation.\n  Good efforts to explain the network problems (latency, congestion) and how they are tackled by delayed updates and temporally sparse updates.\n\n- weaknesses:\n\n\u00b7 While the overall presentation looks clean, the paper does not talk about the setting (one parameter server and many workers, only workers, etc).\n  This is arguably basic information, and the readers is left to understand by themselves that the setting consists in many workers without a parameter server.\n\n\u00b7 There is no theoretical analysis of the soundness of the proposed algorithm.\n  For instance in ASGD (that the authors cite), stale gradients are dampened before being used, which in turn is used to guarantee convergence.\n  In the proposed algorithm, there is no dampening nor apparent limit of the maximum value of t; such a difference with prior art should entail a serious (theoretical) analysis.\n\n\u00b7 Finally, using SSGD for comparison is not very \"fair\", as communication-efficient algorithms have already been published for quite some time [1, 2, and follow-ups (e.g. searching for \"local sgd\")].\n  At the very least a comparison with ASGD (cited) is necessary, as in a realistic setting latency is indeed a problem but arguably not bandwidth\n  (plus, orthogonal gradient compression techniques do exist, e.g. as in \"Federated Learning: Strategies for Improving Communication Efficiency\").\n\nquestions to the authors: \n\n- Can you clarify the setting?\n\n- Can you give at least an intuition why accepting stale gradient is correct (i.e. does not impede convergence)?\n  There is no theoretical limit on the value of t; can workers take any arbitrary old gradient?\n  So when the training is close to convergence (if it reaches it), i.e., when the norm of the gradients are close to 0,  the algorithm could then use very old gradients (which norms could be orders of magnitude larger) to \"correct the mismatch\" caused by the local training; is this correct?\n  To solve that issue, ASGD introduces a simple dampening mechanism (which is necessary in the convergence proof), which your algorithm does not have.\n\n\nThe related work section focuses on gradient compression techniques (which tackle low bandwidth, not latency) and asynchronous SGD (which is more prone to congestion, with a single parameter server),\nbut seems to overlook that sparse communication techniques already exist (this fact should at least be mentioned).\n\nThe idea of sparse communication for SGD exists since at least 2012 [1, Algorithm 3].\nA first mention of the use of such techniques for \"communication efficiency\" dates from (at least) 2015 [2].\n\n[1] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.\n    Optimal distributed online prediction using mini-batches.\n    J. Mach. Learn. Res., 13(1):165\u2013202, January 2012.\n\n[2] Sixin Zhang, Anna E. Choromanska, Yann LeCun.\n    Deep learning with Elastic Averaging SGD.\n    NeurIPS, 2015.\n\n\nI do not see a clear novelty, nor a proof (or even intuitions) that the proposed algorithm is theoretically sound.\nThe comparison with SSGD is arguably unfair, since SSGD is arguably not at all the state-of-the-art in the proposed setting (hence the claimed speedup of x90 can be very misleading)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576555574084, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Reviewers"], "noninvitees": [], "tcdate": 1570237723302, "tmdate": 1576555574097, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Review"}}}, {"id": "HJeMe67H5S", "original": null, "number": 3, "cdate": 1572318441831, "ddate": null, "tcdate": 1572318441831, "tmdate": 1572972342470, "tddate": null, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Authors provide a technique to compensate for error introduced by stale or infrequent synchronization updates in distributed deep learning.\n\nTheir solution is to use local gradient information to update the model, and once delayed gradient information from other workers arrives, the use it to provide a correction which would give an equivalent result to \"no delay\" synchronization in the case of linear model training.\n\nThe three approaches are:\n1. Delayed update -- use local gradients for immediate update, apply correction when stale averaged update arrives. \n2. Sparse update -- only update once every p iterations, the averaged update includes p steps\n3. Combined -- 1. and 2. combined\n\nAuthors evaluate on ImageNet, showing some improvement, and promise to release their implementation for PyTorch/Horovod. Their technique, combined with a reference implementation in a popular framework stands a good chance of having impact. Given the increase in cloud training workloads, even a small improvement in this setting is significant.\n\nComments:\n- \"scalability\" is never defined. I would recommend defining it, or referencing a paper which defines it. I assume it refers to training throughput divided by ideal training throughput.\n- Evaluation is one on resnet-50. Because it's mostly convolutions, such network has high compute/network ratio and is not frequently bottlenecked by network. A more convincing experiment would rely on lower computation intensity architecture such as Transformer/BERT training.\n- Section 1 states that without their technique, they expect SGD to exhibit 0.008 scalability for 100 servers, compared to 0.72 for their method. However, the number 0.72 was not supported by data, their largest experiment used 16 servers.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576555574084, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Reviewers"], "noninvitees": [], "tcdate": 1570237723302, "tmdate": 1576555574097, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Review"}}}, {"id": "r1eT_7l55r", "original": null, "number": 1, "cdate": 1572631413196, "ddate": null, "tcdate": 1572631413196, "tmdate": 1572631413196, "tddate": null, "forum": "SJeuueSYDH", "replyto": "SJxf5f3pDr", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment", "content": {"title": "Thanks for your interest!", "comment": "1. Yes. It is required to store previous gradients for calculating error compensation. Notably it just stores locally and does not bring any extra cost to the communication.\n2. We use \u201cw\u2019 = w + lambda * grad\u201d as the update formula. We will rewrite the equations to be consistent with the mainstream. Apologize for the confusion.\n3. Yes, it works for NLP task. We have experimented DTS on 2 layer LSTM on the Penn Treebank corpus (PTB) dataset. The results are attached below. DTS well-preserves the accuracy up to 20 delay steps and temporal sparsity of 1/20.\n\n\n                     Perplexity |                                                  Perplexity |\nOriginal          72.30      |   Original                                   72.30       |\nDelay(t)=4      72.28      |   Temporal Sparse (p)=4         72.27       |\nDelay(t)=10    72.29      |   Temporal Sparse (p)=10       72.31       |\nDelay(t)=20    72.27      |   Temporal Sparse (p)=20       72.28       |"}, "signatures": ["ICLR.cc/2020/Conference/Paper2404/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeuueSYDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2404/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2404/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2404/Authors|ICLR.cc/2020/Conference/Paper2404/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141874, "tmdate": 1576860553389, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Official_Comment"}}}, {"id": "SJxf5f3pDr", "original": null, "number": 1, "cdate": 1569731210436, "ddate": null, "tcdate": 1569731210436, "tmdate": 1569731210436, "tddate": null, "forum": "SJeuueSYDH", "replyto": "SJeuueSYDH", "invitation": "ICLR.cc/2020/Conference/Paper2404/-/Public_Comment", "content": {"comment": "Very interesting work!\nA few questions:\n1. Does this method require to remember t(delay) previous gradients to compute compensation error?\n2. It looks like there is typo in  equations (1),(2)...: it should be \"w - lambda*grad\" instead of \"w + lambda*grad\"?\n3. Have you tried  to apply this method to other tasks (NLP, speech...)? \n", "title": "Very interesting work!"}, "signatures": ["~Boris_Ginsburg1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Boris_Ginsburg1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ligeng@mit.edu", "luyao11175@gmail.com", "yujunlin@mit.edu", "songhan@mit.edu"], "title": "Distributed Training Across the World", "authors": ["Ligeng Zhu", "Yao Lu", "Yujun Lin", "Song Han"], "pdf": "/pdf/20bb9f0dd3087e7851a6710e5ce57765087793a1.pdf", "TL;DR": "Conventional distributed learning is only performed inside cluster because of latency requirements. We scale the distributed training across the world under high latency network.", "abstract": "Traditional synchronous distributed training is performed inside a cluster, since it requires high bandwidth and low latency network (e.g. 25Gb Ethernet or Infini-band). However, in many application scenarios, training data are often distributed across many geographic locations, where physical distance is long and latency is high.  Traditional synchronous distributed training cannot scale well under such limited network conditions. In this work, we aim to scale distributed learning un-der high-latency network. To achieve this, we propose delayed and temporally sparse (DTS) update that enables synchronous training to tolerate extreme network conditions without compromising accuracy.  We benchmark our algorithms on servers deployed across three continents in the world: London (Europe), Tokyo(Asia), Oregon (North America) and Ohio (North America). Under such challenging settings, DTS achieves90\u00d7speedup over traditional methods without loss of accuracy on ImageNet.", "keywords": ["Distributed Training", "Bandwidth"], "paperhash": "zhu|distributed_training_across_the_world", "original_pdf": "/attachment/938b66c277d65784784af26362132b2f81d3f0af.pdf", "_bibtex": "@misc{\nzhu2020distributed,\ntitle={Distributed Training Across the World},\nauthor={Ligeng Zhu and Yao Lu and Yujun Lin and Song Han},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeuueSYDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeuueSYDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504180768, "tmdate": 1576860586568, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2404/Authors", "ICLR.cc/2020/Conference/Paper2404/Reviewers", "ICLR.cc/2020/Conference/Paper2404/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2404/-/Public_Comment"}}}], "count": 13}