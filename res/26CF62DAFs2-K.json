{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392755460000, "tcdate": 1392755460000, "number": 5, "id": "DT9EX7jt-1Xwm", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "26CF62DAFs2-K", "replyto": "26CF62DAFs2-K", "signatures": ["Tamara Polajnar"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have uploaded an updated version which addresses some of the points raised by the reviewers.  The new version will be visible after  Wed, 19 Feb 2014 01:00:00 GMT."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Type-Driven Tensor-Based Meaning Representations", "decision": "submitted, no decision", "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "pdf": "https://arxiv.org/abs/1312.5985", "paperhash": "polajnar|learning_typedriven_tensorbased_meaning_representations", "keywords": [], "conflicts": [], "authors": ["Tamara Polajnar", "Luana Fagarasan", "Stephen Clark"], "authorids": ["tamara.polajnar@gmail.com", "luana.fagarasan@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392728940000, "tcdate": 1392728940000, "number": 1, "id": "4iL2qSHRBri5h", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "26CF62DAFs2-K", "replyto": "9ECjsHg-MXsWH", "signatures": ["Stephen Clark"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "> > In fact, for non-toy tasks, the number of parameters will be too\r\n> > many. Realistically, to capture cooccurence counts of single words,\r\n> > single vectors need to be at least 100 dimensions, which will then\r\n> > require 1000000 parameters to represent a simple adverb.\r\n> \r\n\r\nIt is not clear that at least 100 dimensions are needed to encode nouns. For example, Socher et al use 50-dimensional word vectors to encode all word types in their large-scale experiments. We're only seeking to encode nouns using context vectors so it is likely that we can effectively use fewer dimensions.\r\n\r\nNevertheless you make a valid point that encoding complex types using higher order tensors leads to a large number of parameters. We are currently looking into methods for remaining true to the framework while reducing the estimation, and storage, requirements for more complex types. However we reserve this research for future publications.\r\n\r\n> > Since the MEN dataset has just been introduced 2 years ago and it is\r\n> > not very well known, it would be great if you described it in your\r\n> > paper.\r\n> \r\n> > How was your SVD word vector representations cross validated? Did\r\n> > you try alternative word vectors?\r\n> \r\n\r\nThe results in Figure 2 support the fact that the way we apply SVD results\r\nwith only a slight reduction in performance with 20 dimensional vectors\r\n(0.71) from the full 10,000 dimensional vectors (0.75). We will add a\r\nfew sentences describing MEN data and also give results for 20\r\ndimensional and full 10,000 vectors on the WS353 data which\r\nreaders may be more familiar with.\r\n\r\n> > Furthermore, one can argue that this particular tensor based\r\n> > framework is actually less flexible in that it could not compare the\r\n> > semantic similarity between simple phrases such as 'a rainy day',\r\n> > 'raining all day' and 'sunshine' or 'swim', 'swimmer' and\r\n> > 'swimming'. People are able to easily determine how similar or\r\n> > related these concepts are and a framework that assigns them\r\n> > representations of different dimensionality is not flexible enough\r\n> > to do this.\r\n> \r\n\r\nThis type of comparison is possible with some existing representations\r\n(such as some versions of the one by Socher et al.), but is not\r\npossible with other type-driven representations (e.g. the convolution\r\ntree kernels by Zanzotto) which purposely encode different parts of a\r\ntree in different subspaces to ensure that both syntactic and semantic\r\nsimilarity are encoded in one model. But you are right that there are\r\ntrade-offs involved in having a fully type-driven semantics.\r\n\r\n> > Regarding: 'plausibility space to have two dimensions, one\r\n> > corresponding to plausible and one corresponding to implausible.'\r\n> > This could be simply captured by a single number that is high for\r\n> > plausible and low for implausible? In fact, in standard machine\r\n> > learning a 2 class softmax (which is being used in this paper) is\r\n> > usually collapsed to a single sigmoid unit.\r\n\r\nWe addressed this question in the response to the first two reviewers.\r\n\r\n> > Section 5.1 mentions problems with rare words. How could this\r\n> > approach be extended to learn representations for rare combinations\r\n> > or words?\r\n\r\nA couple of simple ways of dealing with this problem are to seed the\r\ntraining of a new low frequency word by a prototype tensor of similar\r\nwords, or by backing off to a tensor representing all words of the same\r\ntype.\r\n\r\n> On a related note, could this approach ever learn representations for\r\n> all syntactic types in setups that move beyond trigrams? Presumably, \r\n> the\r\n> counts for most\r\n> such n-tuples will be extremely sparse.\r\n\r\nWe only use frequent n-tuples to generate plausible examples for this\r\ndataset. We don't need highly frequent tuples for training as we only\r\never pass each tuple to the learning method once."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Type-Driven Tensor-Based Meaning Representations", "decision": "submitted, no decision", "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "pdf": "https://arxiv.org/abs/1312.5985", "paperhash": "polajnar|learning_typedriven_tensorbased_meaning_representations", "keywords": [], "conflicts": [], "authors": ["Tamara Polajnar", "Luana Fagarasan", "Stephen Clark"], "authorids": ["tamara.polajnar@gmail.com", "luana.fagarasan@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392631920000, "tcdate": 1392631920000, "number": 4, "id": "9ECjsHg-MXsWH", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "26CF62DAFs2-K", "replyto": "26CF62DAFs2-K", "signatures": ["anonymous reviewer 6020"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Type-Driven Tensor-Based Meaning Representations", "review": "This paper introduces a method to train very low 2-dimensional representations for different syntactic types and evaluates it on a benchmark of selectional preferences.\r\n\r\n\r\nRegarding\r\n'assume that words, phrases and sentences all live in the\r\nsame vector space. The tensor-based semantic framework is more flexible, in that it allows different\r\nspaces for different grammatical types, which results from it being tied more closely to a type driven\r\nsyntactic description; however, this flexibility comes at a price, since there are many more\r\nparamaters to learn.'\r\nIn fact, for non-toy tasks, the number of parameters will be too many. Realistically, to capture cooccurence counts of single words, single vectors need to be at least 100 dimensions, which will then require 1000000 parameters to represent a simple adverb.\r\n\r\nFurthermore, one can argue that this particular tensor based framework is actually less flexible in that it could not compare the semantic similarity between simple phrases such as 'a rainy day', 'raining all day' and 'sunshine' or 'swim', 'swimmer' and 'swimming'. People are able to easily determine how similar or related these concepts are and a framework that assigns them representations of different dimensionality is not flexible enough to do this.\r\n\r\nRegarding:\r\n'plausibility space to have two dimensions, one corresponding to plausible and one corresponding to implausible.'\r\nThis could be simply captured by a single number that is high for plausible and low for implausible?\r\nIn fact, in standard machine learning a 2 class softmax (which is being used in this paper) is usually collapsed to a single sigmoid unit.\r\n\r\nSince the MEN dataset has just been introduced 2 years ago and it is not very well known, it would be great if you described it in your paper.\r\n\r\nSection 5.1 mentions problems with rare words.\r\nHow could this approach be extended to learn representations for rare combinations or words? People can certainly understand a word from very few examples.\r\nOn a related note, could this approach ever learn representations for all syntactic types in setups that move beyond trigrams? Presumably, the counts for most such n-tuples will be extremely sparse.\r\n\r\nSince you used techniques from neural networks to train your model, why not compare to neural networks to predict the same statistics?\r\nHow was your SVD word vector representations cross validated? Did you try alternative word vectors?\r\n\r\nThis paper works in the interesting area of combining discrete syntactic information with distributed representations.\r\nIt is well written but some doubts remain about the approach.\r\n\r\nWhile the paper is not very novel (approach A + approach B) it may expose the ml community to type and syntactic approaches."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Type-Driven Tensor-Based Meaning Representations", "decision": "submitted, no decision", "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "pdf": "https://arxiv.org/abs/1312.5985", "paperhash": "polajnar|learning_typedriven_tensorbased_meaning_representations", "keywords": [], "conflicts": [], "authors": ["Tamara Polajnar", "Luana Fagarasan", "Stephen Clark"], "authorids": ["tamara.polajnar@gmail.com", "luana.fagarasan@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391712720000, "tcdate": 1391712720000, "number": 3, "id": "Hore_5SLB4_m2", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "26CF62DAFs2-K", "replyto": "26CF62DAFs2-K", "signatures": ["Tamara Polajnar"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear Reviewers,\r\n\r\nThank you very much for your insightful comments. We respond directly\r\nbelow to the comments of Reviewer 1, but these also cover most of the\r\npoints made by Reviewer 2.\r\n\r\n> Why map to a two dimensional space, one axis of which models\r\n> plausibility, and the other, implausibility? How is the point (1,1)\r\n> in this space different from the point (0.5, 0.5) (i.e. what does\r\n> simultaneously being 'more plausible' and 'less plausible' mean)? Wouldn't it be more natural, and simpler, to just map to the real\r\n> line? (These questions also apply to Clark's paper from which this\r\n> idea is inherited.)\r\n\r\nIn the compositional framework that we base our experiments on\r\n(Coecke et al.), the sentence space is represented as a vector\r\nresulting in a tensor representation for the verb, so part of the\r\nreason we chose an overparameterised representation is to remain\r\nfaithful to this framework.\r\n\r\nBoth reviewers ask why we did not represent the verb as a matrix, with\r\nthe sentence space being a scalar. In fact we did try these\r\nexperiments, with the verb as a matrix and the output a scalar value\r\ntransformed by a sigmoid, resulting in a single-output two-class\r\nclassifier. This approach was not as effective as the softmax\r\noverparameterised version, and to make the paper more concise we\r\nomitted these results. We can add a line or two to the final version\r\nto clarify this point.\r\n\r\n> Also the baseline seems quite weak, as it is constructed from only\r\n> positive examples. Furthermore the generalization performance of the\r\n> resulting system was split by sentence, not by noun - that is, nouns\r\n> that appear in the training set can also appear in the test set,\r\n> suggesting other, simpler ways of constructing baselines (see\r\n> below).\r\n\r\nThis baseline was chosen as it has precedent in the paper of\r\nGrefenstette and Sadrzadeh (2011), the first paper discussing\r\nexperimental approaches for this compositional framework. In general,\r\nmethods in distributional semantics typically use positive-only data,\r\nand hence we consider this to be a reasonable baseline. Furthermore,\r\nwe have implemented the alternative verb-as-a-matrix model, as\r\ndiscussed above, and can present results for it if necessary.\r\n\r\n> It seems a bit of a stretch to call the simple gradient descent\r\n> method used here, 'deep learning'.\r\n\r\nIn the context of this simple experiment with the single logistic\r\nregression node, we agree that the claim that this is deep learning is\r\noverstated. Nevertheless, the aim of this work is just an initial\r\ninvestigation, the ultimate goal of which is to investigate longer\r\nsentences whose more complicated sentence structures would necessitate\r\nthe use of 'deep learning' techniques with more complex networks.\r\n\r\n> Clarity: I think it would help the reader's understanding to explain\r\n> how the string 'with' in 'eat with a fork' maps to the category\r\n> ((SNP)(SNP))/NP.\r\n\r\nWe will clarify this point.\r\n\r\n> The problem appears to decompose into very small decoupled\r\n> optimization problems (an objective function for each verb) since\r\n> the verb parameters are independent and the noun vectors are fixed\r\n> (right?). It would have been interesting to allow the noun vectors\r\n> to be learned also, perhaps starting from your initial values (but a\r\n> much harder optimization problem).\r\n\r\nThis is an interesting suggestion, and has been explored in related\r\nwork (Andreas and Gharamani, 2013) and anecdotally by Socher.\r\n\r\n> A little more clarification is needed here. From what I understand,\r\n> you took, for each noun, the top N most relevant context words, each\r\n> of which is also a vector, thus forming the 'noun context\r\n> matrix'. But how were the vectors for the context words computed? In\r\n> the same way as the nouns, thus giving a 10,000 dimensional vector\r\n> for each context word? Also, when you do the SVD, you will wind up\r\n> with a matrix representation (constructed from either the left\r\n> singular vectors or the right, depending on how the problem was set\r\n> up). How do you wind up with a 20 dimensional vector? If you just\r\n> used the top 20 singular values, that would seem to throw away\r\n> crucial information.\r\n\r\nWe constructed the vectors in a standard way for distributional\r\nsemantics, so we did not describe the method in great detail. The\r\ncontext words are just used as indices (or rather basis vectors) of\r\nthe target word vectors. Furthermore, using SVD is a standard\r\ntechnique to reduce noise and uncover the latent dimensions (e.g. see\r\nTurney and Pantel). 20 is perhaps a little low compared with other\r\nwork (e.g. Socher uses 50), but with additional techniques including\r\ncontext selection and normalisation (see our EACL-14 paper) we can get\r\naway with 20 dimensions. We found that adding more dimensions did not\r\nconsistently improve results.\r\n\r\n> There appears to be a flaw in the experimental protocol: no attempt\r\n> was made (or at least, mentioned) to keep the nouns that appear in\r\n> the test set, different from those that appear in the training\r\n> set. If this is so it makes the results weaker, as methods that\r\n> leverage such coincidences may do much better.  For example, we\r\n> could construct a simple baseline for a given N1-V-N2 test triple by\r\n> saying that if N1-V appears in the training set, and also if V-N2\r\n> does too, then output '1', else output '0'. At least I think it's\r\n> important to break results out by such a 'did noun occur in training\r\n> set' stratification.\r\n\r\nWe ensure that the subject-verb-object *triples* are unique, but not\r\nallowing any subject or object nouns in the test data to occur in the\r\ntraining data we think is too restrictive (and would most likely\r\nresult in a small training set). Such a scenario would also be\r\nunrealistic when moving to longer sentences.\r\n\r\nFinally, we wish to stress the point that this paper is a first\r\nattempt at implementing the tensor-based compositional framework, with\r\na simple sentence space. In future work our intention is to move to\r\nmore complex sentences and sentence spaces, for which the learning\r\nmethods used here will become more indispensable."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Type-Driven Tensor-Based Meaning Representations", "decision": "submitted, no decision", "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "pdf": "https://arxiv.org/abs/1312.5985", "paperhash": "polajnar|learning_typedriven_tensorbased_meaning_representations", "keywords": [], "conflicts": [], "authors": ["Tamara Polajnar", "Luana Fagarasan", "Stephen Clark"], "authorids": ["tamara.polajnar@gmail.com", "luana.fagarasan@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391639160000, "tcdate": 1391639160000, "number": 2, "id": "RBmD3BhWN4Bwg", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "26CF62DAFs2-K", "replyto": "26CF62DAFs2-K", "signatures": ["anonymous reviewer 0365"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Type-Driven Tensor-Based Meaning Representations", "review": "This paper presents a method for learning 3-modes tensor for representing meaning of transitive verbs. The choice of this representation for verbs is motivated by the Combinatory Categorical Grammar framework; nouns are represented by low-dimensional continuous vectors obtained by a modified SVD on a co-occurrence matrix. Verbs tensors are learned for 10 verbs using subject-verb-object triples extracted from Wikipedia  and Google N-grams. Evaluation is carried out by asking the system to predict the verb given a pair (subject, object).\r\n\r\n---\r\n\r\nThe paper is clearly written and tackles an important and hot topic. The introduction and section 2 are actually very interesting to motivate this field of research: attempting to combine distributional and combinatorial semantics. However, I find the message a bit too ambitious and misleading w.r.t. the content of the remainder of the paper, and hence its actual contributions. Indeed, the paper ends up by learning representations for 10 verbs (nouns representations being obtained using methods developed in previous work). This might be an important first step, but this is not exactly what the introduction and title indicate.\r\n\r\nThe sentence of the introduction 'One goal of this paper is to introduce the problem of learning tensor-based semantic representations to the machine learning community' is a bit strong, since there exist many tensor learning work, perhaps not exactly in the same context but still, as well as the paper by Krishnamurthy and Mitchell from last year (cited in the paper). \r\n\r\nBesides, the learned tensors are quite strange since one slice has only 2 dimensions for predicting plausible/implausible probabilities. What is different from using a single value to predict the plausibility only? This would imply simply representing verbs by matrices, but I wonder if the result would be much different. In test, I assume that the ranking of verbs given pairs of (subject, object) is carried out by sorting using the plausibility probability. The role of the second value destined to encode for implausibility is unclear (and hence the role of tensors with a mode of dimension 2).\r\n\r\nEven if using such tensors was fully justified, the paper needs to propose a comparison with a baseline encoding verbs as matrices (the current baseline is too weak). For instance, the paper by Jenatton et al. 'A latent factor model for highly multi-relational data' (NIPS 2012) proposes very similar experiments by also training on subject-verb-object extracted from Wikipedia to learn matrices for representing verb meanings. I think this system or an adaptation of it would make a much better baseline.\r\n\r\nOther remarks:\r\n- Which norm is used in Equation 1? L2-norm for tensors or matrix is ambiguous. How was set lambda?\r\n- In Equation 2, f_{w_i , c_j} should be defined.\r\n- It seems that there is a single N (and not 1 per noun). This should be made more clear.\r\n- Figure 2 is rather unclear. What is K? The size of the noun vectors?\r\n- In the beginning of Section 5, for the second experiments shouldn't it be 52 instead of 42?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Type-Driven Tensor-Based Meaning Representations", "decision": "submitted, no decision", "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "pdf": "https://arxiv.org/abs/1312.5985", "paperhash": "polajnar|learning_typedriven_tensorbased_meaning_representations", "keywords": [], "conflicts": [], "authors": ["Tamara Polajnar", "Luana Fagarasan", "Stephen Clark"], "authorids": ["tamara.polajnar@gmail.com", "luana.fagarasan@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391496300000, "tcdate": 1391496300000, "number": 1, "id": "11ic1LsAz6LUk", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "26CF62DAFs2-K", "replyto": "26CF62DAFs2-K", "signatures": ["anonymous reviewer 67d9"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Learning Type-Driven Tensor-Based Meaning Representations", "review": "The authors investigate learning tensor representations of verbs using fixed vector representations of nouns, building on Steedman's CCGs and Coecke's work on modeling meaning representations using linear maps (so that, for example, a verb 3-tensor is right-contracted with its object noun vector and left-contracted with its subject noun vector, to output a sentence living in another vector space). In particular, they learn the noun/verb/noun mapping, turning the problem into a binary classification problem (where the binary labels correspond to legal combinations, and impostor combinations), using L2 regularized logistic regression.  Results are given on ten verbs, chosen for varying levels of frequency and 'concreteness', and using up to 2000 positive and 2000 negative N/V/N training sentences, constructed using the Google N-Gram corpus and Wikipedia.  The results are compared to a baseline where the verb tensors are constructed using the outer product of the vector representations of their subject and object nouns, for legal uses of that verb. Throughout the work the nouns are represented by fixed vectors using a statistical co-occurrence technique based on that of Turney and Pantel. The proposed technique beats the proposed baseline in accuracy.\r\n\r\nThis is an interesting and useful direction for research: to try to construct linear maps that more faithfully represent the functional aspects of language (e.g. a verb is a function that takes two nouns and outputs a sentence).  I wonder, however, why the authors didn't just treat the problem as a standard classification problem.  Why map to a two dimensional space, one axis of which models plausibility, and the other, implausibility? How is the point (1,1) in this space different from the point (0.5, 0.5) (i.e. what does simultaneously being 'more plausible' and 'less plausible' mean)?  Wouldn't it be more natural, and simpler, to just map to the real line?  (These questions also apply to Clark's paper from which this idea is inherited.)  Also the baseline seems quite weak, as it is constructed from only positive examples.  Furthermore the generalization performance of the resulting system was split by sentence, not by noun - that is, nouns that appear in the training set can also appear in the test set, suggesting other, simpler ways of constructing baselines (see below).  I do think the paper has some value as a step towards investigating more general linear maps to model language; the link between CCGs and linear maps seems intuitive.\r\n\r\n\r\nSpecific Comments\r\n*****************\r\n\r\nNumbers are section numbers.\r\n\r\n1.0\r\n\r\nIt seems a bit of a stretch to call the simple gradient descent method used here, 'deep learning'.\r\n\r\n2.0\r\n\r\nClarity: I think it would help the reader's understanding to explain how the string 'with' in 'eat with a fork' maps to the category ((SNP)(SNP))/NP.\r\n\r\n3.0\r\n\r\nAgain, why not use a one dimensional representation for plausibility?  What is the intuition for using two dimensions?\r\n\r\n3.1\r\n\r\nThe problem appears to decompose into very small decoupled optimization problems (an objective function for each verb) since the verb parameters are independent and the noun vectors are fixed (right?).  It would have been interesting to allow the noun vectors to be learned also, perhaps starting from your initial values (but a much harder optimization problem).\r\n\r\n3.2\r\n\r\nThe baseline - cosine similarity on the matrix elements of the Kronecker product matrix and that of the target subject/object pair - seems very ad-hoc, and so it's hard to see what we learn by comparing against it.\r\n\r\n4.2\r\n\r\nA little more clarification is needed here.  From what I understand, you took, for each noun, the top N most relevant context words, each of which is also a vector, thus forming the 'noun context matrix'.  But how were the vectors for the context words computed?  In the same way as the nouns, thus giving a 10,000 dimensional vector for each context word? Also, when you do the SVD, you will wind up with a matrix representation (constructed from either the left singular vectors or the right, depending on how the problem was set up). How do you wind up with a 20 dimensional vector?  If you just used the top 20 singular values, that would seem to throw away crucial information. \r\n\r\n5.0\r\n\r\nThere appears to be a flaw in the experimental protocol: no attempt was made (or at least, mentioned) to keep the nouns that appear in the test set, different from those that appear in the training set.  If this is so it makes the results weaker, as methods that leverage such coincidences may do much better.  For example, we could construct a simple baseline for a given N1-V-N2 test triple by saying that if N1-V appears in the training set, and also if V-N2 does too, then output '1', else output '0'.  At least I think it's important to break results out by such a 'did noun occur in training set' stratification."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Type-Driven Tensor-Based Meaning Representations", "decision": "submitted, no decision", "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "pdf": "https://arxiv.org/abs/1312.5985", "paperhash": "polajnar|learning_typedriven_tensorbased_meaning_representations", "keywords": [], "conflicts": [], "authors": ["Tamara Polajnar", "Luana Fagarasan", "Stephen Clark"], "authorids": ["tamara.polajnar@gmail.com", "luana.fagarasan@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387812240000, "tcdate": 1387812240000, "number": 35, "id": "26CF62DAFs2-K", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "26CF62DAFs2-K", "signatures": ["tamara.polajnar@gmail.com"], "readers": ["everyone"], "content": {"title": "Learning Type-Driven Tensor-Based Meaning Representations", "decision": "submitted, no decision", "abstract": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "pdf": "https://arxiv.org/abs/1312.5985", "paperhash": "polajnar|learning_typedriven_tensorbased_meaning_representations", "keywords": [], "conflicts": [], "authors": ["Tamara Polajnar", "Luana Fagarasan", "Stephen Clark"], "authorids": ["tamara.polajnar@gmail.com", "luana.fagarasan@cl.cam.ac.uk", "stephen.clark@cl.cam.ac.uk"]}, "writers": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 7}