{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488561588456, "tcdate": 1478284693363, "number": 296, "id": "HyTqHL5xg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HyTqHL5xg", "signatures": ["~Maximilian_Karl1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396488000, "tcdate": 1486396488000, "number": 1, "id": "H1lInzIug", "invitation": "ICLR.cc/2017/conference/-/paper296/acceptance", "forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper provides a clear application of variational methods for learning non-linear state-space models, which is of increasing interest, and of general relevance to the community.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396488501, "id": "ICLR.cc/2017/conference/-/paper296/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396488501}}}, {"tddate": null, "tmdate": 1484327969766, "tcdate": 1484327969766, "number": 7, "id": "Bk9X2K88g", "invitation": "ICLR.cc/2017/conference/-/paper296/public/comment", "forum": "HyTqHL5xg", "replyto": "ryv7lUbVg", "signatures": ["~Maximilian_Soelch1"], "readers": ["everyone"], "writers": ["~Maximilian_Soelch1"], "content": {"title": "Added to revised draft.", "comment": "Thanks a lot for pointing this out to us. We have added it to our current draft."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633999, "id": "ICLR.cc/2017/conference/-/paper296/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyTqHL5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper296/reviewers", "ICLR.cc/2017/conference/paper296/areachairs"], "cdate": 1485287633999}}}, {"tddate": null, "tmdate": 1484327872047, "tcdate": 1484327872047, "number": 6, "id": "Sk_TsFLUx", "invitation": "ICLR.cc/2017/conference/-/paper296/public/comment", "forum": "HyTqHL5xg", "replyto": "S1EQUMMEe", "signatures": ["~Maximilian_Soelch1"], "readers": ["everyone"], "writers": ["~Maximilian_Soelch1"], "content": {"title": "Reply to your review", "comment": "Thank you for your kind feedback.\n\nWe would like to address two points you raised in your review:\n1. Relation to SVAE: The main overlap between SVAE and DVBF is that graphical models are exploited. One of the key abilities we are trying to achieve when using SGVB is efficient inference, a feature where DVBF and SVAE differ significantly. SVAE aims to solve inference on arbitrary graphical models (which needn\u2019t be Markovian) through message-passing subroutines. We indeed ran their code on their given example, essentially a simplified version of Bouncing Ball (with 1 ground truth latent instead of 4). As of now, it is written in pure python (including automatic differentiation of pure python code), it is non-trivial to port to, e.g., tensorflow. The current implementation, however, prohibited application to our data set---even the simple example took several days to train, as compared to a few hours for DVBF on our more intricate data set.\nDVBF aims to solve inference for and of dynamical systems, a very particular but nonetheless important subclass of graphical models. Hence, we can exploit streamlined architectures for follow-up control-related tasks, which allow \u201creal-time\u201d inference. We make it possible to apply the recently very successful approximate inference algorithms via neural architectures. On this very important subclass, we show unprecedented results.\n\n2. Weight uncertainty on v: Firstly, we would like to stress that it is up to user and application which parts of the transition are assigned to v or w, respectively. The locally linear transition in section 3.3 is only one possible instance. In this particular case, it works as follows: v gathers M matrix triplets (A^(i), B^(i), C^(i)). You can consider these as M basic linear systems, independent of a specific data point. It is possible to learn them via maximum likelihood, as you suggest. To prevent overfitting, however, they are regularized in a Bayesian way as in, e.g., [*]. As you also point out correctly, it makes sense to have the dynamics for a specific sequence be dependent on the sequence. This is achieved by making the mixture weights \\alpha data-dependent (and hence part of w). Long story short: We learn probabilistically regularized, data-independent basic systems, and then mix them depending on data. \n\n[*] Blundell et al., 2015, Weight Uncertainty in Neural Networks, http://jmlr.org/proceedings/papers/v37/blundell15.pdf\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633999, "id": "ICLR.cc/2017/conference/-/paper296/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyTqHL5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper296/reviewers", "ICLR.cc/2017/conference/paper296/areachairs"], "cdate": 1485287633999}}}, {"tddate": null, "tmdate": 1484327799390, "tcdate": 1484327799390, "number": 5, "id": "SyyKiY8Lg", "invitation": "ICLR.cc/2017/conference/-/paper296/public/comment", "forum": "HyTqHL5xg", "replyto": "ByLhqXN4l", "signatures": ["~Maximilian_Soelch1"], "readers": ["everyone"], "writers": ["~Maximilian_Soelch1"], "content": {"title": "Reply to your review", "comment": "Thank you very much for your feedback.\n\nWe would like to address several points raised in your review:\n1. We are not sure whether we agree on what you call a \u201cfairly specific problem\u201d. System identification is *not* a fairly specific problem. On the contrary, it remains an unsolved challenge, and is of utter importance in Reinforcement Learning/Control of, e.g., robots.\nWe agree that this can be stated more clearly, and we have adjusted the introduction accordingly.\n\n2. Thank you for pointing out the glitch in section 2.1. In order to avoid misunderstandings, we would like to point out that F_t and B_t are explicitly given a Bayesian treatment in our algorithm. As section 3.3 shows, the locally linear matrices are gathered in v_t, which is itself a component of \\beta_t. This is a difference to classical filtering-only like the Kalman filter does.\n\n3. It is unclear to us why forcing the latent space to fit the transition is trivial. To the best of our knowledge, previous approaches have tried the opposite direction, and only the reparametrization enabled us to reverse the direction. Our experiments confirm that this greatly improves model performance. To elaborate, we refer to our answer given to a pre-review question by Reviewer3, part of which has found its way into the revised paper: \u201cWe set off with an important observation: Previous methods such as DKF aim at fitting the latent dynamics to a latent state z that is beneficial for reconstruction. This encourages learning of a stationary auto-encoder with focus of extracting as much from a single image as possible, as we see with DKF. Importantly, velocity is not necessary for excellent reconstruction---after all, the original data set renderer from ground truth latents to observations discards velocity. Once the latent states are set, it is hard to adjust the transition to them. This would require changing the latent states slightly, and that comes at a cost of decreasing the reconstruction (temporarily). The learning algorithm is stuck in a local optimum with good compression only. The model choice prohibits finding the global optimum with a good generative model via optimization, not the problem itself.\u201d\nWe have adjusted our related work section to clearly state this more clearly.\n\n4. Indeed, the factorization of the posterior is not \u201cimplied\u201d in a rigorous mathematical sense, just that it is a natural factorization given the interpretation of v and w, respectively. We have adapted the respective section accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633999, "id": "ICLR.cc/2017/conference/-/paper296/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyTqHL5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper296/reviewers", "ICLR.cc/2017/conference/paper296/areachairs"], "cdate": 1485287633999}}}, {"tddate": null, "tmdate": 1484327617536, "tcdate": 1484327617536, "number": 4, "id": "HJYp5KU8e", "invitation": "ICLR.cc/2017/conference/-/paper296/public/comment", "forum": "HyTqHL5xg", "replyto": "Hk3UZs-Ne", "signatures": ["~Maximilian_Soelch1"], "readers": ["everyone"], "writers": ["~Maximilian_Soelch1"], "content": {"title": "Reply to your review", "comment": "Thank you very much for your feedback.\n\nWe would like to address several points raised in your review:\n- Linear Regression: As footnote 3 indicates, we did test nonlinear regression via neural networks from inferred latent states to ground truth latents. The result was the same: Angle reconstruction works for both, velocity reconstruction only for DVBF.\nWe chose linear regression because a) it allows for looking at and reporting measures such as goodness of fit in a way neural networks cannot, and b) the fact that a simple linear regression allows for perfect recovery indicates the strength of the latent embedding found by DVBF.\n\n- E2C (Watter et al., 2015): E2C crucially depends on the user stacking data such that all temporal information is present in one sample. While technically working on raw pixels, this enhancement of data significantly simplifies finding temporal derivatives. E2C requires a qualitatively different data set to work on, to the extent that E2C cannot serve as a fair baseline.\nAs stated in our paper, the E2C objective function is not rigorously implemented within the SGVB framework. It averages two loosely connected lower bounds and adds another KL term with the sole purpose of regularizing the transition. For DVBF the transition is a natural part of the lower bound, which fixes this theoretical flaw.\nE2C works on data that uniformly samples control *and* state space. This assumption is very strong in itself. Moreover, such data acquisition is virtually impossible for any real application as it would require a priori insight into the control of the system. It has not been shown that E2C works in a more realistic data regime, while DVBF does.\n\n- Bouncing Ball/Checkerboard: Firstly, please note that there are random control inputs present, so that the trajectory is not fully determined by the initial state. Moreover, there is noise present in the simulator. Apart from that, our algorithm relies on the system being deterministic up to process noise and given control inputs, and identify the underlying system. Bouncing Ball is no more or less random than most control problems in, e.g., OpenAI Gym.\nSecondly, the checkerboard is quite a remarkable result. It is designed to show how well DVBF reconstructs the unknown ground truth: The ground truth position of the ball lies within the 2D unit square, the bounding box. Again, we wanted to visualize how ground truth reappears in the learned latent states. To do so, we wanted to show how the ground truth bounding box is warped into the latent space. To this end, we partitioned (discretized) the ground truth unit square into a regular 3x3 checkerboard with respective coloring. Remarkably, there is almost no warping present! That means that DVBF not only learned to extract the 2D position from the 256 pixels, but also aligned them in two dimensions of the latent space almost exactly as they do in the physical system. The algorithm does the exact same pixel-to-2D inference that a human observer automatically does when we looking at the image---which is why at first glance it seems this would be an easy task, which it isn\u2019t.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633999, "id": "ICLR.cc/2017/conference/-/paper296/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyTqHL5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper296/reviewers", "ICLR.cc/2017/conference/paper296/areachairs"], "cdate": 1485287633999}}}, {"tddate": null, "tmdate": 1484327417313, "tcdate": 1484327417313, "number": 3, "id": "BkZW5F88l", "invitation": "ICLR.cc/2017/conference/-/paper296/public/comment", "forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "signatures": ["~Maximilian_Soelch1"], "readers": ["everyone"], "writers": ["~Maximilian_Soelch1"], "content": {"title": "Comment to all reviewers", "comment": "Dear reviewers,\n\nThank you for your kind and constructive feedback. According to your reviews, our paper is \u201cwell-written\u201d and \u201cwell-motivated\u201d. We agree that applying SGVB to identify and filter latent state-space models and \u201c[forcing] the latent state to maintain all the information relevant for predictions\u201d, is a \u201cnovel\u201d model and \u201can interesting application\u201d. Our experimental results are \u201cillustrative\u201d and \u201cshow promise\u201d, as \u201c[DVBF] is better able to learn meaningful representations of sequence data\u201d---an \u201cinteresting and potentially useful\u201d contribution.\n\nWe have replied to each of your reviews individually to address your specific remarks and to clarify open questions and misunderstandings. These have found their way into a revised draft, which we will upload to OpenReview tomorrow (Feb 14). Notable improvements include:\n- We have greatly reworked introduction and background. It is crisper and more to the point.\n- At the same time, we have added more emphasis on specifying the two tasks we are solving (system identification and inference in the learned system), and their significance for controlled systems.\n- Section 3 has seen minor changes to clarify misconceptions raised in your reviews.\n\nWe hope to have understood and addressed your reviews in sufficient detail. Thank you for your time and consideration.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633999, "id": "ICLR.cc/2017/conference/-/paper296/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyTqHL5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper296/reviewers", "ICLR.cc/2017/conference/paper296/areachairs"], "cdate": 1485287633999}}}, {"tddate": null, "tmdate": 1482074797761, "tcdate": 1482074797761, "number": 3, "id": "ByLhqXN4l", "invitation": "ICLR.cc/2017/conference/-/paper296/official/review", "forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "signatures": ["ICLR.cc/2017/conference/paper296/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper296/AnonReviewer2"], "content": {"title": "Rather incremental, but interesting experiments", "rating": "6: Marginally above acceptance threshold", "review": "This is mainly a (well-written) toy application paper. It explains SGVB can be applied to state-space models. The main idea is to cast a state-space model as a deterministic temporal transformation, with innovation variables acting as latent variables. The prior over the innovation variables is not a function of time. Approximate inference is performed over these innovation variables, rather the states. This is a solution to a fairly specific problem (e.g. it doesn't discuss how priors over the beta's can depend on the past), but an interesting application nonetheless. The ideas could have been explained more compactly and more clearly; the paper dives into specifics fairly quickly, which seems a missed opportunity.\n\nMy compliments for the amount of detail put in the paper and appendix.\n\nThe experiments are on toy examples, but show promise.\n\n- Section 2.1: \u201cIn our notation, one would typically set beta_t = w_t, though other variants are possible\u201d -> It\u2019s probably better to clarify that if F_t and B_t and not in beta_t, they are not given a Bayesian treatment (but e.g. merely optimized).\n\n- Section 2.2 last paragraph: \u201cA key contribution is [\u2026] forcing the latent space to fit the transition\u201d. This seems rather trivial to achieve.\n\n- Eq 9: \u201cThis interpretation implies the factorization of the recognition model:..\u201d\nThe factorization is not implied anywhere: i.e. you could in principle use q(beta|x) = q(w|x,v)q(v)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512633257, "id": "ICLR.cc/2017/conference/-/paper296/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper296/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper296/AnonReviewer1", "ICLR.cc/2017/conference/paper296/AnonReviewer3", "ICLR.cc/2017/conference/paper296/AnonReviewer2"], "reply": {"forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper296/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper296/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512633257}}}, {"tddate": null, "tmdate": 1481938459733, "tcdate": 1481938459733, "number": 2, "id": "S1EQUMMEe", "invitation": "ICLR.cc/2017/conference/-/paper296/official/review", "forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "signatures": ["ICLR.cc/2017/conference/paper296/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper296/AnonReviewer3"], "content": {"title": "interesting way of learning nonlinear state space models", "rating": "7: Good paper, accept", "review": "This paper presents a variational inference based method for learning nonlinear dynamical systems. Unlike the deep Kalman filter, the proposed method learns a state space model, which forces the latent state to maintain all of the information relevant to predictions, rather than leaving it implicit in the observations. Experiments show the proposed method is better able to learn meaningful representations of sequence data.\n\nThe proposed DVBF is well motivated, and for the most part the presentation is clear. The experiments show interesting results on illustrative toy examples. I think the contribution is interesting and potentially useful, so I\u2019d recommend acceptance.\n\nThe SVAE method of Johnson et al. (2016) deserves more discussion than the two sentences devoted to it, since the method seems pretty closely related. Like the DVBF, the SVAE imposes a Markovianity assumption, and it is able to handle similar kinds of problems. From what I understand, the most important algorithmic difference is that the SVAE q network predicts potentials, whereas the DVBF q network predicts innovations. What are the tradeoffs between the two?  Section 2.2 says they do the latter in the interest of solving control-related tasks, but I\u2019m not clear why this follows. \n\nIs there a reason SVAEs don\u2019t meet all the desiderata mentioned at the end of the Introduction?\n\nSince the SVAE code is publicly available, one could probably compare against it in the experiments. \n\nI\u2019m a bit confused about the role of uncertainty about v. In principle, one could estimate the transition parameters by maximum likelihood (i.e. fitting a point estimate of v), but this isn\u2019t what\u2019s done. Instead, v is integrated out as part of the marginal likelihood, which I interpret as giving the flexibility to model different dynamics for different sequences. But if this is the case, then shouldn\u2019t the q distribution for v depend on the data, rather than being data-independent as in Eqn. (9)?\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512633257, "id": "ICLR.cc/2017/conference/-/paper296/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper296/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper296/AnonReviewer1", "ICLR.cc/2017/conference/paper296/AnonReviewer3", "ICLR.cc/2017/conference/paper296/AnonReviewer2"], "reply": {"forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper296/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper296/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512633257}}}, {"tddate": null, "tmdate": 1481908564373, "tcdate": 1481908564373, "number": 1, "id": "Hk3UZs-Ne", "invitation": "ICLR.cc/2017/conference/-/paper296/official/review", "forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "signatures": ["ICLR.cc/2017/conference/paper296/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper296/AnonReviewer1"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes to use the very standard SVGB in a sequential setting like several previous works did. However, they proposes to have a clear state space constraints similar to Linear Gaussian Models: Markovian latent space and conditional independence of observed variables given the latent variables. However the model is in this case non-linear. These assumptions are well motivated by the goal of having meaningful latent variables.\nThe experiments are interesting but I'm still not completely convinced by the regression results in Figure 3, namely that one could obtain the angle and velocity from the state but using a function more powerful than a linear function. Also, why isn't the model from (Watter et al., 2015) not included ?\nAfter rereading I'm not sure I understand why the coordinates should be combined in a 3x3 checkerboard as said in Figure 5a. \nThen paper is well motivated and the resulting model is novel enough, the bouncing ball experiment is not quite convincing, especially in prediction, as the problem is fully determined by its initial velocity and position. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512633257, "id": "ICLR.cc/2017/conference/-/paper296/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper296/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper296/AnonReviewer1", "ICLR.cc/2017/conference/paper296/AnonReviewer3", "ICLR.cc/2017/conference/paper296/AnonReviewer2"], "reply": {"forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper296/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper296/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512633257}}}, {"tddate": null, "tmdate": 1481887774857, "tcdate": 1481887774857, "number": 2, "id": "ryv7lUbVg", "invitation": "ICLR.cc/2017/conference/-/paper296/public/comment", "forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "signatures": ["~Antti_Honkela1"], "readers": ["everyone"], "writers": ["~Antti_Honkela1"], "content": {"title": "Missing relevant prior work", "comment": "Thanks for the interesting paper!\n\nIt seems you may have missed some very relevant early prior work on variational inference for nonlinear state-space models in Valpola and Karhunen (Neural Computation 2002, doi:10.1162/089976602760408017) further developed in Honkela et al. (JMLR 2010)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633999, "id": "ICLR.cc/2017/conference/-/paper296/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyTqHL5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper296/reviewers", "ICLR.cc/2017/conference/paper296/areachairs"], "cdate": 1485287633999}}}, {"tddate": null, "tmdate": 1480509888046, "tcdate": 1480509888038, "number": 1, "id": "r1daYBnMg", "invitation": "ICLR.cc/2017/conference/-/paper296/public/comment", "forum": "HyTqHL5xg", "replyto": "H1fQk6uGe", "signatures": ["~Maximilian_Soelch1"], "readers": ["everyone"], "writers": ["~Maximilian_Soelch1"], "content": {"title": "Reconstruction error backprop through transition is the key, achieved by reparametrization", "comment": "Thank you for your question. We agree that this is one of the most important points in our paper.\n\nCompression (i.e., likelihood) and a realistic generative model are not orthogonal, there is no inherent trade-off. In fact, state-of-the-art generative models for images are trained via log-likelihood, e.g., PixelRNNs.\n\nIn our case, this means in a global optimum of log-likelihood or lower bound, the transition should be recovered perfectly, even for DKF. The questions is: Why it is so hard to extract latent dynamics?\n\nWe set off with an important observation: Previous methods such as DKF aim at fitting the latent dynamics to a latent state z that is beneficial for reconstruction. This encourages learning of a stationary auto-encoder with focus of extracting as much from a single image as possible, as we see with DKF. Importantly, velocity is not necessary for excellent reconstruction---after all, the original data set renderer from ground truth latents to observations discards velocity. Once the latent states are set, it is hard to adjust the transition to them. This would require changing the latent states slightly, and that comes at a cost of decreasing the reconstruction (temporarily). The learning algorithm is stuck in a local optimum with good compression only. The model choice prohibits finding the global optimum with a good generative model via optimization, not the problem itself.\n\nThis leads directly to our main contribution, the reparametrization of the transition. The effect is that the transition is now part of p(x|w) = \\int p(x|z)p(z|w) dz in the generative model p(x,w), whereas for DKF, it is only part of the prior p(z) in p(x,z). An immediate consequence is that the transition becomes part of the reconstruction term of the lower bound, E_q[log p(x|w)]. \n\nThis way, we achieve reconstruction error backprop directly through the *transition* (DKF also does backprop of reconstruction error through *time*, but only in the recognition model RNN. This leads to the transition information being hidden in the recognition model, which is not used when generating new samples). DVBF's gradient flow strongly couples transitions at different points in time. Reconstruction errors in observation space later in time can be tracked down to latent transition errors much earlier in time. \n\nAs our experiments show, this encourages recovery of the full latent state and state-space dynamics---good compression and also a good generative model."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287633999, "id": "ICLR.cc/2017/conference/-/paper296/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyTqHL5xg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper296/reviewers", "ICLR.cc/2017/conference/paper296/areachairs"], "cdate": 1485287633999}}}, {"tddate": null, "tmdate": 1480277786498, "tcdate": 1480277786493, "number": 1, "id": "H1fQk6uGe", "invitation": "ICLR.cc/2017/conference/-/paper296/pre-review/question", "forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "signatures": ["ICLR.cc/2017/conference/paper296/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper296/AnonReviewer3"], "content": {"title": "why state space assumption helps with prediction?", "question": "From Section 2.2: \"The recognition model [of a non-state-space model] becomes a powerful compression algorithm, prohibiting plausible long-term generative prediction.\"\n\nAn important point of this paper is the idea that enforcing the state-space assumption helps learn a better model, but I think this could be spelled out more clearly. In particular, why does having a good compressor prohibit long-term prediction?  In principle, modeling the distribution better ought to help with compression.\n\nIn particular, suppose we extend any of the proposed models so that the generative and recognition networks can each make use of x values from previous time steps. This would be a strictly more powerful model, so it ought to achieve higher likelihood (at least on the training set). What would go wrong?  Are we facing a tradeoff between quantitative log-likelihood scores and interpretability?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data", "abstract": "We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n", "pdf": "/pdf/6734b367969e365aa39c22f7101c259ca0efebbd.pdf", "paperhash": "karl|deep_variational_bayes_filters_unsupervised_learning_of_state_space_models_from_raw_data", "conflicts": ["tum.de", "tu-darmstadt.de", "iit.it", "fortiss.org"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Maximilian Karl", "Maximilian Soelch", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["karlma@in.tum.de", "m.soelch@tum.de", "bayer.justin@googlemail.com", "smagt@brml.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959355044, "id": "ICLR.cc/2017/conference/-/paper296/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper296/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper296/AnonReviewer3"], "reply": {"forum": "HyTqHL5xg", "replyto": "HyTqHL5xg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper296/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper296/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959355044}}}], "count": 13}