{"notes": [{"id": "HkzZBi0cFQ", "original": "HJxCng_fYm", "number": 62, "cdate": 1538087736919, "ddate": null, "tcdate": 1538087736919, "tmdate": 1545355432801, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1lbngMreE", "original": null, "number": 1, "cdate": 1545048232680, "ddate": null, "tcdate": 1545048232680, "tmdate": 1545354483691, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "HkzZBi0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Meta_Review", "content": {"metareview": "This paper proposes an 8-bit quantization strategy for rapid DNN deployment. 3 reviewers all rated this paper as marginally below acceptance threshold due to lack of novelty. 8 bit quantization (including channel-wise) is a well studied task. The paper lacks comparison with peer work. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "lacks novelty"}, "signatures": ["ICLR.cc/2019/Conference/Paper62/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper62/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353351147, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzZBi0cFQ", "replyto": "HkzZBi0cFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper62/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper62/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper62/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353351147}}}, {"id": "rJlAQp8Zk4", "original": null, "number": 6, "cdate": 1543757094087, "ddate": null, "tcdate": 1543757094087, "tmdate": 1543757094087, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "S1gnBtlFRX", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "content": {"title": "Thank you very much for the valuable comments.", "comment": "Thank you very much for the valuable comments. We will improve the manuscript in the final version by adding further analysis and results as you suggested."}, "signatures": ["ICLR.cc/2019/Conference/Paper62/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617484, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzZBi0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper62/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper62/Authors|ICLR.cc/2019/Conference/Paper62/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617484}}}, {"id": "S1gnBtlFRX", "original": null, "number": 5, "cdate": 1543207236211, "ddate": null, "tcdate": 1543207236211, "tmdate": 1543207249161, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "HygNgn6eRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "content": {"title": "Thank you for the updates", "comment": "Thank you for the updates. These results indicate more consistent results for the non-MAX approaches for layer-wise methods. Overall, channel-wise methods do outperform layer-wise, but the gap is smaller now. The take-away is that channel-wise methods (with Laplace, S. Cauchy, or PDF-aware modes) will reliably improve classification accuracy loss over layer-wise methods (as well as AP in object detection) since large (>4%) degradation is not present in channel-wise settings.\n\nOverall, I think this is a nice result, but it is lacking in presentation and analysis. The overall presentation can be cleaned up and improved with respect to the plots and graphics to help better guide the reader through the paper.\n\nFor example:\n- Further analysis in the way of plots or tables to compare relevant network statistics (to better contextualize those which exhibited such awful degradation in the layer-wise/channel-wise MAX cases)\n- The algorithm and figure on page 4 are made less effective by being isolated from the flow of the paper. It is understandable that all the text related to a figure may not be on the same page, but in this case there is opportunity to improve the formatting to allow better flow and readability.\n- The plot on pages 5 would be more powerful if it showed statistics from other networks as well to give a better picture of the variability between the networks\n- The plot on page 7 could be more informative if it showed both layer-wise and channel-wise cases as well as depicted more networks\n- Minor: including top-5 accuracies as well (since differences in top-1 accuracy degradation are often <0.5%)\n\nOther things that I think would be interesting but understandably hard to add would be exploration of other tasks (as done with object detection/YOLO system), even more networks, and including some results from systems that use methods with retraining such that those numbers can be seen for reference when making the trade-off."}, "signatures": ["ICLR.cc/2019/Conference/Paper62/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper62/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617484, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzZBi0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper62/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper62/Authors|ICLR.cc/2019/Conference/Paper62/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617484}}}, {"id": "B1eFz3plRX", "original": null, "number": 3, "cdate": 1542671376774, "ddate": null, "tcdate": 1542671376774, "tmdate": 1542671619580, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "HkltNd_637", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "content": {"title": "Response to Reviewer", "comment": "Thank you very much for your comments. Competing methods in other papers require retraining or needs to cope with high accuracy loss when quantized in a layer-wise fashion. The proposed method is the first of its kind to resolve these issues by incorporating channel-wise quantization and moment-analysis method which DOES NOT require retraining or the training dataset. Na\u00efve channel-wise quantization requires adding huge number of HW shifters and providing values for them which make it unrealistic for implementation (please see Figure 1 (b) in the revised manuscript). The biggest contribution of our paper is the HW-friendly channel-wise quantization by manipulating the kernels prior to inference. For your reference, Figure 1 has been modified to make the distinction clearer."}, "signatures": ["ICLR.cc/2019/Conference/Paper62/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617484, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzZBi0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper62/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper62/Authors|ICLR.cc/2019/Conference/Paper62/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617484}}}, {"id": "r1xlSn6l0X", "original": null, "number": 4, "cdate": 1542671415708, "ddate": null, "tcdate": 1542671415708, "tmdate": 1542671541926, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "ryebGUDO2X", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "content": {"title": "Response to Reviewer", "comment": "Thank you very much for your helpful comments and suggestions. \n1. Notations have been added to Algorithm 1 and modified Figure 1 along with its description, as suggested.\n2. The channel-wise quantization algorithm is calculated statically as show in Algorithm 1 and exemplified in Figure 1. Algorithm 1 demonstrates all aspects of \u201cpre-coordinating the fractional lengths of the weights.\u201d Therefore, should be able to be reproduced by anyone and didn\u2019t think it would be necessary to write it out in words. \n3. The layer-wise moment-analysis results have been added to the paper in Table 1 and 2. Channel-wise out-performs layer-wise in most cases without the need for additional HW. The overhead introduced in channel-wise quantization is just a simple pre-processing step of determining the fraction lengths by executing Algorithm 1.\nWe would like to emphasize that our paper proposes a practical solution for channel-wise quantization. Na\u00efve channel-wise quantization requires adding huge number of HW shifters and providing values for them which make it unrealistic for implementation (please see Figure 1 in the revised manuscript). The proposed method does not require such cost by manipulating the kernels prior to inference. For your reference, Figure 1 has been modified to make the distinction clearer.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper62/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617484, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzZBi0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper62/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper62/Authors|ICLR.cc/2019/Conference/Paper62/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617484}}}, {"id": "HygNgn6eRQ", "original": null, "number": 2, "cdate": 1542671340456, "ddate": null, "tcdate": 1542671340456, "tmdate": 1542671340456, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "Hklvh2J_6m", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "content": {"title": "Response to Reviewer", "comment": "Thank you very much for your helpful comments. We have addressed them as follows:\n\n1. Results of layer-wise quantization with moment-analysis method were added\n: The layer-wise quantization with moment-analysis method could remove the effect of outliers. To show this clearly, we added the results of applying moment-analysis method to layer-wise quantization in Table 1 and 2 in the revised manuscript. However, even if outliers were to be removed, channel-wise method consistently outperforms layer-wise one. Channel-wise method in comparison to the layer-wise method both with moment-analysis (or MAX) clearly demonstrates that channel-wise has the advantage.\n\n2. Novelty of the paper\n: The main contribution of the paper is manipulation of the weights prior to inference for channel-wise quantization as shown in Figure 1 and Algorithm 1. Na\u00efve channel-wise implementation requires shifters to be in place for each channel which would cause huge hardware cost (please see Figure 1 (b) in the revised manuscript). Thus, it is not a practical solution at all in HW perspective. The proposed method performs channel-wise quantization WITHOUT the need for any additional HW and we believe there\u2019s novelty in that it enables channel-wise quantization for real-world HW. We modified Figure 1 and improved manuscript to reflect this point clearly. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper62/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617484, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkzZBi0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper62/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper62/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper62/Authors|ICLR.cc/2019/Conference/Paper62/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper62/Reviewers", "ICLR.cc/2019/Conference/Paper62/Authors", "ICLR.cc/2019/Conference/Paper62/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617484}}}, {"id": "Hklvh2J_6m", "original": null, "number": 3, "cdate": 1542089903038, "ddate": null, "tcdate": 1542089903038, "tmdate": 1542089903038, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "HkzZBi0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Official_Review", "content": {"title": "promising method for 8-bit quantization without sensitivity to outliers, limited in novelty and presentation clarity", "review": "The paper proposes channel-wise 8-bit quantization rather than layer-wise. It further takes advantage of work using moment analysis instead of just MAX values to avoid susceptibility to outliers. The main take-away seems to be that channel-wise set ups limit the need for outlier removal and the care with which you select your data subset when performing quantization.\n\nPros:\n- using channel-wise quantization (with MAX values or moment-analysis) yields improvement over layer-wise MAX approaches\n- limits the amount of care that is needed to be taken when applying quantization (e.g. size of data subset used)\n- shows differences in degradation when blindly applying quantization methods to different networks; with less (but still some) variation in degradation when applying channel-wise quantization\n\nCons:\n- unclear how much is gained over layer-wise and MAX value methods with careful tuning/removal of outliers; would be good to see if careful tuning closes the gap or if channel-wise methods are the clear winner\n- unclear if the layer-wise set up with moment-analysis could help to avoid the need for outlier removal altogether and (potentially) offer similar improvements to the channel-wise set up; a few more experiments are important to determine specifically if improvement is with respect to channel-wise or moment-analysis since only layer-wise MAX results are presented\n- clarity, presentation, and organization can be improved to help with flow, avoid confusion, and improve readability\n\nOverall:\nThe paper offers nice empirical results regarding the relative ease with which one can quantize networks when considering channel-wise quantization (and moment-analysis), but the overall novelty is limited. With the limited novelty, the primary benefits appear to be the ease of quantization for rapid deployment and channel-wise setups. Comparisons with stronger baseline numbers when using layer-wise methods would give a more complete picture. In addition, having these stronger tuned baseline numbers on even more networks would be great to show that the channel-wise method has clear impact across the board, even with respect to well-tuned layer-wise baselines. These results could give better support for the importance of the novelty.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper62/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Official_Review", "cdate": 1542234546502, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkzZBi0cFQ", "replyto": "HkzZBi0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper62/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335640610, "tmdate": 1552335640610, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper62/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkltNd_637", "original": null, "number": 2, "cdate": 1541404721399, "ddate": null, "tcdate": 1541404721399, "tmdate": 1541534318306, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "HkzZBi0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Official_Review", "content": {"title": "Novelty is limited", "review": "This paper proposes an new 8-bit quantization strategy for rapid deployment. \n\n8-bit quantization has attracted many attentions recently. And it is already well used in GPU servers (cudnn), phones, ARM chips and various ASIC neural network chips. In these situations, almost no performance drop is observed for classification and detection tasks.\n\nSo, the novelty of this paper is limited.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper62/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Official_Review", "cdate": 1542234546502, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkzZBi0cFQ", "replyto": "HkzZBi0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper62/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335640610, "tmdate": 1552335640610, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper62/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryebGUDO2X", "original": null, "number": 1, "cdate": 1541072392770, "ddate": null, "tcdate": 1541072392770, "tmdate": 1541534318096, "tddate": null, "forum": "HkzZBi0cFQ", "replyto": "HkzZBi0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper62/Official_Review", "content": {"title": "Interesting approach to channel-wise CNN quantization with adaptive bit allocation, with evaluation on eleven modern CNNs, comparison with simple layer-wise baseline", "review": "This paper proposes a technique for channel-wise quantization of CNNs\nto 8-bit, fixed point precision. The authors propose several\ntechniques for analyzing the statistical properties of output channel\nactivations in order to select the best fractional bit length for each\nchannel. Experimental results on eleven different CNN architectures\ndemonstrate that the approaches proposed result in significantly less\naccuracy loss when compared to a layer-wise baseline.\n\nThe paper has the following strengths:\n\n 1. The experimental results on eleven different architectures (of\n    varying depth and breadth) are convincing, and are consistently\n    better than layer-wise MAX for choosing fractional bit length.\n\nThe paper has the following weak points:\n\n 1. There is not much coherence between the description of the\n    approach in section 2.1, Figure 1, Algorithm 1, and\n    Figure 2. Notation is used in Algorithm 1 which is never defined.\n 2. Related to the previous point, the proposed technique has a lot of\n    moving parts and I don't feel that it would be easy to reproduce\n    the results of the paper. There are some vague statements, like\n    \"We resolve this complication by pre-coordinating the fractional\n    lengths of the weights\", which require significantly more\n    precision. This issue -- one of the main issues with channel-wise\n    versus layer-wise quantization -- is never returned to in the\n    definition of the method.\n 3. The experimental comparison with layer-wise quantization is\n    somewhat lacking. Is layer-wise MAX the state-of-the-art in CNN\n    quantization? The results comparing channel-wise and layer-wise\n    MAX are already convincing, but are the moment-analysis approaches\n    not equally applicable to layer-wise quantization?\n    State-of-the-art results that are less sensitive to outliers\n    should be included in Table 1. A comparison with layer-wise\n    approaches would be nice to have also in Figure 4 to show\n    sensitivity to profiling set size.\n\nThe experimental results in the paper are impressive, and the analysis\nmotivating the approach is convincing. However, there are presentation\nand clarity issues in the technical development, and the comparative\nanalysis is lacking broader comparisons with the state-of-the-art (to\nbe fair, the authors recognize that layer-wise MAX as a baseline is\nparticularly susceptible to outliers). These two aspects combined,\nhowever, lead me to the opinion that this work is just not quite ready\nfor publication at ICLR.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper62/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantization for Rapid Deployment of Deep Neural Networks", "abstract": "This paper aims at rapid deployment of the state-of-the-art deep neural networks (DNNs) to energy efficient accelerators without time-consuming fine tuning or the availability of the full datasets.  Converting DNNs in full precision to limited precision is essential in taking advantage of the accelerators with reduced memory footprint and computation power. However, such a task is not trivial since it often requires the full training and validation datasets for profiling the network statistics and fine tuning the networks to recover the accuracy lost after quantization. To address these issues, we propose a simple method recognizing channel-level distribution to reduce the quantization-induced accuracy loss and minimize the required image samples for profiling. We evaluated our method on eleven networks trained on the ImageNet classification benchmark and a network trained on the Pascal VOC object detection benchmark. The results prove that the networks can be quantized into 8-bit integer precision without fine tuning.", "paperhash": "lee|quantization_for_rapid_deployment_of_deep_neural_networks", "authorids": ["junhaeng2.lee@samsung.com", "sw815.ha@samsung.com", "sincere.choi@samsung.com", "w-j.lee@samsung.com", "seungw.lee@samsung.com"], "authors": ["Jun Haeng Lee", "Sangwon Ha", "Saerom Choi", "Won-Jo Lee", "Seungwon Lee"], "keywords": [], "pdf": "/pdf/5864e94427975d97800a5d8618480224312c914d.pdf", "_bibtex": "@misc{\nlee2019quantization,\ntitle={Quantization for Rapid Deployment of Deep Neural Networks},\nauthor={Jun Haeng Lee and Sangwon Ha and Saerom Choi and Won-Jo Lee and Seungwon Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=HkzZBi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper62/Official_Review", "cdate": 1542234546502, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkzZBi0cFQ", "replyto": "HkzZBi0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper62/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335640610, "tmdate": 1552335640610, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper62/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}