{"notes": [{"id": "ryeoxnRqKQ", "original": "HJgxQic9F7", "number": 1106, "cdate": 1538087922682, "ddate": null, "tcdate": 1538087922682, "tmdate": 1545355407541, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 33, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bk2e8tklE", "original": null, "number": 1, "cdate": 1544685043517, "ddate": null, "tcdate": 1544685043517, "tmdate": 1545354506322, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Meta_Review", "content": {"metareview": "Although one review is favorable, it does not make a strong enough case for accepting this paper. Thus there is not sufficient support in the reviews to accept this paper.\n\nI am recommending rejecting this submission for multiple reasons.\n\nGiven that this is a \"black box\" attack formalized as an optimization problem, the method must be compared to other approaches in the large field of derivative-free optimization. There are many techniques including: Bayesian optimization, (other) evolutionary algorithms, simulated annealing, Nelder-Mead, coordinate descent, etc. Since the method of the paper does not use anything about the structure of the problem it can be applied to other derivative-free optimization problems that had the same search constraint. However, the paper does not provide evidence that it has advanced the state of the art in derivative-free optimization.\n\nThe method the paper describes does not need a new name and is an obvious variation of existing evolutionary algorithms. Someone facing the same problem could easily reinvent the exact method of the paper without reading it and this limits the value of the contribution.\n\nFinally, this paper amounts to breaking already broken defenses, which is not an activity of high value to the community at this stage and also limits the contribution of this work.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "the only favorable review does not make a convincing argument to accept the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1106/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352963436, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352963436}}}, {"id": "ryxoR3ptyE", "original": null, "number": 25, "cdate": 1544309970939, "ddate": null, "tcdate": 1544309970939, "tmdate": 1544309970939, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "rJxJgUsYkE", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Steps 1--4 generate valid adversarial examples", "comment": "Not sure why you are obsessed with the minimal adversarial examples. If those are what you are looking for, our paper does not provide a direct answer though you probably can derive one based on our work. \n\nKindly check Steps 1--4 in the paper which generate valid adversarial examples whose differences from the original images are imperceptible up to the thresholds $\\tao_p, p=2 or \\infty$."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "rJxJgUsYkE", "original": null, "number": 24, "cdate": 1544300007439, "ddate": null, "tcdate": 1544300007439, "tmdate": 1544300007439, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "SklmUtNFJE", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "minimal adversarial examples?", "comment": "Sampling from this distribution would have little to do with minimal adversarial examples: if you succeeded in modelling all adversarials, this would include the vast majority that are far away from the clean image. Sampling would mainly return large adversarial perturbations, not small ones."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "SklmUtNFJE", "original": null, "number": 23, "cdate": 1544272203454, "ddate": null, "tcdate": 1544272203454, "tmdate": 1544272203454, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "r1eZ4M7N14", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Mode is not the mean", "comment": "If you referred to a parametric distribution by the \"more powerful distribution\", we learn the parameters to specify the distribution as we described earlier. Once we reach such a distribution, we are supposed to either sample from it or use the mode to generate an adversarial example following Steps 1--4 in the paper. The mean of Gaussian overlaps with the Gaussian's mode."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "r1eZ4M7N14", "original": null, "number": 21, "cdate": 1543938601107, "ddate": null, "tcdate": 1543938601107, "tmdate": 1543938601107, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "BylsJD_90X", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "you don't want to model the distribution of adversarials", "comment": "You don't want to model the distribution of adversarial examples. Say you use a more powerful distribution that is really capable of capturing the distribution of adversarial examples. This distribution would comprise the whole input space for which the model decision is different from the label of the original image, right? You would then take the mean of that distribution (at least that's what you do right now). But that point would certainly be far away from the minimum adversarial perturbation that you seek."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "BylsJD_90X", "original": null, "number": 20, "cdate": 1543304931410, "ddate": null, "tcdate": 1543304931410, "tmdate": 1543305298447, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ByeQ2E_dAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Understanding our approach", "comment": "Regarding the \"whole\" wording, we are fine to remove the \"whole\" because \n\"the whole population of adversarial examples per image\" \nis actually equivalent to \n\"the population of adversarial examples per image\". \nGiven an image, all its adversarial examples comprise the population. We use a Gaussian distribution to model this population in this work. In the future, other multi-variate continuous distributions, like GMM or uniform, may be found a better fit to the population. Additionally, one may also consider to capture the population by non-parametric distributions. No matter which one --- including the Gaussian, what it models is the population per image and not the local region. \n\nThanks to the above, we formalize our problem as minimizing the expected loss under the Gaussian distribution. This problem formulation is different from any problem formulations of the existing white-box attack methods. In contrast, BPDA and QL are built upon PGD (and CW for BPDA). PGD is the basic framework for them and they only (and yet non-trivially) replace the true gradients by the estimated ones. In this sense, we said we did not employ any white-box attack methods. We also agree with the reviewer that there is no sharp contrast between BIM and MIM because both are based on the similar principle of solving the following optimization problem: $min_{perturbation} Loss$. In contrast, ours is $min_{Gaussian mean} Expectation Loss$. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "BJe1JZucAm", "original": null, "number": 19, "cdate": 1543303383352, "ddate": null, "tcdate": 1543303383352, "tmdate": 1543303383352, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ByeQ2E_dAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Regarding the experiments", "comment": "We have tuned the hyper-parameters of the competing methods (BPDA, QL, ZOO, D-based) in order to achieve the best performances they could have. The ES part is to approximate an expectation by a sample mean, so we believe it is fair to fix the sample size for QL and our algorithm --- we did tune the other hyper-parameters in QL such as the learning rate and number of iterations. (QL actually doubles the sample size by reversing the signs of the samples.)\n\nFor all of the competing methods but the decision-based, we used the code released by the original authors in the experiments. We did not find the official implementation of the decision-based method (D-based) due to the deadline rush; instead, this implementation (https://github.com/greentfrapp/boundary-attack) was employed. Thanks to the reviewer's question, we tested this implementation using the evaluation metric reported in the original publication and only found it failed to re-produce the reported results. Upon a second search, we found the \"foolbox\" implementation (https://github.com/bethgelab/foolbox) of D-based. With it, we re-produced the reported results and obtained 66% success rate on attacking INPUT-TRANS (ours: 100%, BPDA: 100%, QL: 66.5%, and ZOO: 38.3%). Regarding the D-based experiments for generating the $\\ell_infty$ bounded adversarial examples, we re-wrote the norm function in the two implementations and did not observe any good results. We agree with the reviewer that, since D-based was particularly tailored for the $\\ell_2$ bounded adversarial examples, that simple change of norm is not good enough to improve D-based for handling the $\\ell_infty$ metric. More careful work has to be done to modify the D-based method to fit the $\\ell_infty$ context; for example, the projection to the $\\ell_2$ sphere has to be updated by the projection to the $\\ell_infty$ polygon. We have updated the PDF."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "ByeQ2E_dAQ", "original": null, "number": 17, "cdate": 1543173290935, "ddate": null, "tcdate": 1543173290935, "tmdate": 1543173290935, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "rygk8TgICX", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "The \"sharp contrast\" you are trying to construct does not exist", "comment": "Dear authors,\nfirst of all let me say that I do appreciate the effort you put into revising the manuscript and the rebuttal. I have, however, a couple of questionmarks behind your responses:\n\n1) Bad performance of decision- and score-based attacks\n\nFirst, [2] is an L2-based attack but you are applying it in an L-infinity scenario, that doesn't make sense. Second, the decision-based attack [2] and other black-box attacks based on score-based methods do perform very well on e.g. Madry et al. (MNIST) or the analysis by synthesis model [3], see results in [3]. In fact, on Madry et al. [2] performs much better than e.g. gradient-based BIM. I hence strongly doubt the results related to [2].\n\n2) \"We run QL using the same hyper-parameters as N ATTACK for the ES part\"\n\nThat's not a fair comparison because the optimal parameters for QL are likely different (e.g. because of the clipping) than for the N Attack. Please compare the attacks with hyper parameters optimised for each attack.\n\n3) On the contrary, we do not employ any white-box attack methods at all in developing our algorithm.\n\nI think you are confusing what is meant by \"white-box\": white-box refers to whether or not you are using the backpropagated gradient (which requires you to have access to the internal structure and weights of the model). PGD is white-box if you use the exact gradient and score-based if you use estimated gradients. Similarly, your method is performing a gradient descent using an estimated gradient. I fail to see how that fundamentally differs from QL.\n\n3) \"this work is the first to capture the whole population of adversarial examples per image\"\n\nYou are not capturing the whole distribution, you are capturing a local Gaussian region. I really do not like the whole wording around \"populations\" and find that confusing and misleading. Your motivation in the end is exactly equivalent to ES, you are just using the gradient in a different way.\n\n4) In the updated manuscript you write: \"In sharp contrast, we do not employ any white-box attack methods at all and, instead, provide a novel perspective to the adversarial attack by modeling the whole population of adversarial examples for every single image. This change alleviates the dependence on the gradients and leads to big differences in terms of the attack results.\"\n\nIn line with what I wrote above I find this part extremely misleading. Of course your method is based on a \"white-box attack method\" - it's called gradient descent. There is no sharp contrast, just as there is no sharp contrast between BIM and MIM, both are based on a similar principle. I think it would be much better if you would tune your paper to say that you have developed a more effective ES-based attack and show that an PGD-based ES attack doesn't work as well on defended networks."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "Hyxdoxb8CX", "original": null, "number": 16, "cdate": 1543012511737, "ddate": null, "tcdate": 1543012511737, "tmdate": 1543014809114, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Summary of changes in the new PDF", "comment": "Denote by QL (Ilyas et al., 2018)\u2019s approach. We have added to the revised PDF \n+ new results on attacking Adv-Train (Madry et al., 2018) (Table 1), \n+ a new paragraph to draw readers\u2019 attention to QL upfront (cf. the highlighted text in the introduction), \n+ new results of QL on attacking the 10 defense methods (Table 1), \n+ a new section (Section 3.1.3) to carefully investigate the factors that contribute to the inferior performance of QL algorithm. The results reveal that, in order to improve its attack success rates, it is vital to get rid of PGD (projection and the sign of the gradients), which is the foundation upon which QL is built, and meanwhile to couple the $\\ell_infty$ clip with the tanh transformation. \n\nThanks to the careful experimental investigation, we make the following conclusion. \n\n1) QL is hinged on the white-box PGD attack --- in terms of methodology, it is actually closer to [1] than ours because both QL and [1] essentially approximate the gradients for PGD. As a result, the quality of the estimated gradients in QL is a big deal. Unfortunately, ES does not give rise to stable gradients due to the sampling step and PGD\u2019s projection and sign functions, especially when the gradients are \"obfuscated\". On the contrary, we do not employ any white-box attack methods at all in developing our algorithm. The Gaussian mean is more important than the gradients in our approach. Whereas ES is a natural choice to search for the Gaussian mean, some derivative-free methods [2] are also good alternatives. \n\n2) The seemingly subtle algorithmic distinction between QL and ours actually leads to significantly different attack success rates. In order to improve QL\u2019s performance, it is vital to remove PGD, the foundation upon which QL is built. \n\n[1] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. \n[2] Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization:  a review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3):1247\u20131293,2013."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "rygk8TgICX", "original": null, "number": 14, "cdate": 1543011654960, "ddate": null, "tcdate": 1543011654960, "tmdate": 1543013824380, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "SyepS_mP3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "PDF updated & responses to questions", "comment": "Q: the attack introduced here is actually equivalent to (Ilyas et al., 2018). \n\nA: We believe the above is a mis-interpretation of our responses. Denote by QL (Ilyas et al., 2018). QL is hinged on the white-box PGD attack --- in terms of the methodology, it is actually closer to [1] than ours because both QL and [1] essentially approximate the gradients for PGD. As a result, the quality of the estimated gradients in QL is a big deal. Unfortunately, ES does not give rise to stable gradients due to the sampling step and PGD\u2019s projection and sign functions. Indeed, after we remove the PGD step in QL, there is a significant performance boost (cf. Table 2 in the revised PDF). On the contrary, we do not employ any white-box attack methods at all in developing our algorithm. The Gaussian mean is more important than the gradients in our approach. Whereas ES is a natural choice to search for the Gaussian mean, some derivative-free methods [2] are also good alternatives. \n\nPlease see Section 3.1.3 for a more detailed investigation about QL.\n\n\nQ: \"existing black-box attacks are weaker than their white-box counterparts\u201d is simply not true.\n\nA: It is actually unclear how strong the existing black-box methods are on attacking the defended neural networks --- most experiments reported in the original publications are conducted on vanilla neural networks. Our own experiments do show that ZOO [1] and the decision-based attack [2] fail to perform well on attacking all the 10 defense methods (cf. Table 1 in the PDF) --- as the decision-based attack consumes a lot of run time, we have to include the complete results later. We have toned down the description about prior black-box attack in the introduction (cf. the text highlighted in the blue color). \n\n[1] Chen, Pin-Yu, et al. \"Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models.\" Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017.\n\n[2] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.\n\n\nQ: The concept of adversarial distributions is not new\n\nA: We have to point out the distribution over adversarial examples per image is different from the distribution over the transformations for a physical adversarial in the real world. In order to photograph a real-world adversarial, it is natural to consider all the conditions (location, background, lighting, etc.) as a distribution of transformations. In contrast, it is not so obvious to model by a distribution the adversarial examples for every single image. To the best of our knowledge, this work is the first to capture the whole population of adversarial examples per image.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "HyxHfJWUCm", "original": null, "number": 15, "cdate": 1543012108576, "ddate": null, "tcdate": 1543012108576, "tmdate": 1543012108576, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "SklRyhS5nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Results", "comment": "Here are the success rates on attacking the vanilla PGD training (Madry et al., 2018) on CIFAR10: \nBPDA: 46.9%\nOurs: 47.9%\n\nThe classification accuracy of the PGD-defended network is 87.3% on CIFAR10.\n\nConclusion 1: The vanilla PGD training is strong.\nConclusion 2: The vanilla PGD training sacrifices the performance to certain degree on the original classification task, so do some other defense techniques (cf. Table 1 in the PDF)."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "rkxO_clI07", "original": null, "number": 13, "cdate": 1543010927868, "ddate": null, "tcdate": 1543010927868, "tmdate": 1543010927868, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "S1g3Hhj_hm", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "PDF revised with extensive discussion on [1]; Curves updated", "comment": "Q: The paper would have to change significantly in order to relate it properly to (Ilyas et al., 2018).\n\nA: Denote by QL (Ilyas et al., 2018)\u2019s approach. We have added to the revised PDF \n+ a new paragraph (in the Introduction section) to draw readers\u2019 attention to QL upfront, \n+ new results of QL on attacking the 10 defense methods (Table 1), and \n+ a new section (Section 3.1.3) to carefully investigate the factors that contribute to the inferior performance of QL algorithm. The results reveal that, in order to improve its attack success rates, it is vital to get rid of PGD (projection and the sign of the gradients), which is the foundation upon which QL is built, and meanwhile to couple the $\\ell_infty$ clip with the tanh transformation. \n\nGiven the above changes, it seems like feasible to extensively discuss QL and yet not completely re-write the paper. \n\nAdditionally, we wanted to emphasize that the Gaussian mean is more important than the gradients in our approach. Whereas ES is a natural choice to search for the Gaussian mean, some derivative-free methods [2] are also good afternatives. In sharp contrast, QL is hinged on the white-box PGD attack --- in terms of the methodology, it is actually closer to [1] than ours because both QL and [1] essentially approximate the gradients for PGD. As a result, the quality of the estimated gradients in QL is a big deal. Unfortunately, ES does not give rise to stable gradients due to the sampling step and PGD\u2019s projection and sign functions. Indeed, after we remove the PGD step in QL, there is a significant performance boost (cf. Table 2). \n\n[1]\tAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018. \n[2] Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization:  a review of algorithms and comparison of software implementations. Journal of Global Optimization, 56(3):1247\u20131293,2013.\n\n\nQ: Example adversarial examples to baseline the figure:\n\nA: Sorry for the confusion about Figure 1. First of all, we did not include all the defense methods in Figure 1 due to the heavy run time on ImageNet. Besides, for each attack method, we had removed all the examples of which it failed to change the labels. Our intention was to compare the relative convergences when their last steps are aligned. Upon reading your comments, however, we think this alignment is actually unnecessary and should be removed. In the revised PDF submission, you can see that some of the attack methods fail to reach 100% success rate.\n\nWe will add some example adversarial examples in the appendix, but the adversarial examples in $\\ell_\\infty = 0.031$ are hardly differentiable from the benign ones. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "BkeY7txIR7", "original": null, "number": 12, "cdate": 1543010593112, "ddate": null, "tcdate": 1543010593112, "tmdate": 1543010593112, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "r1lSC_3T2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Appreciated; Have re-named the curve", "comment": "Thank you for the encouraging comments! Regarding the name of the curve, we have removed \u201cROC\u201d and now simply call it the curve of success rate vs. number of evolution iterations. We will continue to polish the text."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "r1lSC_3T2m", "original": null, "number": 3, "cdate": 1541421261304, "ddate": null, "tcdate": 1541421261304, "tmdate": 1541533417891, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Review", "content": {"title": "original", "review": "In this paper, authors propose a \"universal\" Gaussian balck-box adversarial attack.\nOriginal and well-written (although there are a few grammar mistakes that would require some revision) and structured. Having followed the comments and discussion I am convinced that the proposed method is state of the art and interesting enough fro ICLR.\nTo the best of my knowledge, the study is technically sound.\nIt fairly accounts for recent literature in the field.\nExperiments are convincing.\nOne thing I am not so convinced about is the naming of the evaluation curve as \"a new ROC curve\". I understand the appeal of pairing the proposed evaluation curve with the ROC curve but, beyond an arguable resemblance, they have no much in common, really.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Review", "cdate": 1542234304788, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335873884, "tmdate": 1552335873884, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1g3Hhj_hm", "original": null, "number": 2, "cdate": 1541090371911, "ddate": null, "tcdate": 1541090371911, "tmdate": 1541533417686, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Review", "content": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "review": "Summary: In this paper the authors discuss a black-box method to learn\nadversarial inputs to DNNs which are \"close\" to some nominal example\nbut nevertheless get misclassified. The algorithm essentially tries to\nlearn the mean of a joint Gaussian distribution over image\nperturbations so that the perturbed image has high likelihood of being\nmisclassified. The method takes the form of zero-th order gradient\nupdates on an objective measuring to what degree the perturbed example\nis misclassified. The authors test their method against 10 recent DNN\ndefense mechanisms, which showed higher attack-success rates than\nother methods. Additionally the authors looked at transferrability of\nthe learned adversarial examples.\n\nFeedback: As noted before, this paper shares many similarities with\n\n[1] \"Black-box Adversarial Attacks with Limited Queries and Information\" (https://arxiv.org/abs/1804.08598)\n\nand the authors have responded to those similarities in two follow-ups. I have reviewed these results and their \nmethod does appear to improve over [1]. However, I am still reluctant to admit these additions to the original submission, \nmainly because dropping [1] in the original submission seems to be a fairly major omission of one of the most relevant competitors out there. In its current form, the apparent redundancies distract significantly from the paper, and to remedy this, the paper would have to change significantly in order to relate it properly to [1] clear is needed. I'd be curious on the ACs thoughts on this. \n\nI appreciate the authors' claim that their method can breach many of the popular defense methods out there, but we \nalso see that many of the percentages in Figure 1 converge  to 1. On the one hand this suggests that all defense methods \nare in some sense equally bad, but on the other, it could also just reflect on the fact that the thresholds are chosen \n\"too large\". I understand that many of the thresholds were inherited from previous work, but it would nevertheless help if the authors showed some example adversarial images to help baseline this Figure.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Review", "cdate": 1542234304788, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335873884, "tmdate": 1552335873884, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyepS_mP3m", "original": null, "number": 1, "cdate": 1540991045317, "ddate": null, "tcdate": 1540991045317, "tmdate": 1541533417481, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Review", "content": {"title": "Good evaluation but important prior work was missed which substantially reduces novelty and makes a major rewrite necessary", "review": "In this work the authors use a score-based adversarial attack (based on the natural evolution strategy (NES)) to successfully attack a multitude of defended networks, with success rates rivalling the best gradient-based attacks.\n\nAs confirmed by the authors in a detailed and very open response to a question of mine, the attack introduced here is actually equivalent to [1]. While the attack itself is not novel (which will require a major revision of the manuscript), the authors point out the following contributions over [1]:\n\n* Attack experiments here go way beyond Ilyas et al. in terms of Lp metrics, different defense models, different datasets and transferability.\n* Different motivation/derivation of NES.\n* Concept of adversarial distributions.\n* Regression network for good initialization.\n* Introduction of accuracy-iterations plots.\n\nMy main concerns are as follows:\n* The review of the prior literature, in particular on score-based and decision-based defences (the latter of which are not even mentioned), is very limited and is framed wrongly. In particular, the statement \u201cHowever, existing black-box attacks are weaker than their white-box counterparts\u201d is simply not true: as an example, the most prominent decision-based attack [2] rivals white-box attacks on vanilla DNNs as well as defended networks [3].\n* The concept of adversarial distributions is not new but is common in the literature of real-world adversarials that are robust to transformations and perturbations (like gaussian noise), check for example [4]. In [4] the concept of _Expectation Over Transformation (EOT)_ is introduced, which is basically the generalised concept of the expectation over gaussian perturbations introduced in this work.\n* While I like the idea of accuracy-iterations plots, the idea is not new, see e.g. the accuracy-iterations plot in [2] (sample-based, Figure 6), the loss-iterations plot in [5] or the accuracy-distortion plots in [3]. However, I agree that these type of visualisation or metric is not as widespread as it should be.\n\nHence, in summary the main contribution of the paper is the application of NES against different defence models, datasets and Lp metrics as well as the use of a regression network for initialisation. Along this second point it would be great if the authors would be able to demonstrate substantial gains in the accuracy-query metric. In any case, in the light of previous literature a major revision of the manuscript will be necessary.\n\n[1] Ilyas et al. (2018) \u201cBlack-box Adversarial Attacks with Limited Queries and Information\u201d (https://arxiv.org/abs/1804.08598) \n[2] Brendel et al. (2018) \u201cDecision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models\u201d (https://arxiv.org/abs/1712.04248)\n[3] Schott et al. (2018) \u201cTowards the first adversarially robust neural network model on MNIST\u201d (https://arxiv.org/abs/1805.09190)\n[4] Athalye et al. (2017) \u201cSynthesizing Robust Adversarial Examples\u201d (https://arxiv.org/pdf/1707.07397.pdf)\n[5] Madry et al. {2017) \u201cTowards Deep Learning Models Resistant to Adversarial Attacks\u201d (https://arxiv.org/pdf/1706.06083.pdf)", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Review", "cdate": 1542234304788, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335873884, "tmdate": 1552335873884, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklRyhS5nQ", "original": null, "number": 10, "cdate": 1541196774352, "ddate": null, "tcdate": 1541196774352, "tmdate": 1541196774352, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ByeeVjH92m", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Got it", "comment": "Got it. Thanks for the pointers! "}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "ByeeVjH92m", "original": null, "number": 7, "cdate": 1541196584319, "ddate": null, "tcdate": 1541196584319, "tmdate": 1541196584319, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "HJe1uvS52Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "content": {"comment": "FYI, Madry et al. released publicly available pre-trained weights.\n\nhttps://github.com/MadryLab/mnist_challenge\nhttps://github.com/MadryLab/cifar10_challenge", "title": "Public code"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311677268, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeoxnRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311677268}}}, {"id": "HJe1uvS52Q", "original": null, "number": 9, "cdate": 1541195622879, "ddate": null, "tcdate": 1541195622879, "tmdate": 1541195622879, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "H1eCOHrqhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Will do", "comment": "Oops, I misunderstood your earlier question.. We are running experiments against the vanilla PGD defended CNN. As Athalye et al. (2018) did not release this very strong model, we had to train it ourselves. Actually, we will ask them for that model by email now.. Stay tuned please."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "H1eCOHrqhQ", "original": null, "number": 6, "cdate": 1541195125782, "ddate": null, "tcdate": 1541195125782, "tmdate": 1541195125782, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "S1ekDzBc2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "content": {"comment": "You don't have to use the Thermometer encoding anymore. I mean the pure vanilla PGD training which Athalye et al. (2018) were unable to defeat.", "title": "standard adversarial training"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311677268, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeoxnRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311677268}}}, {"id": "S1ekDzBc2m", "original": null, "number": 8, "cdate": 1541194326811, "ddate": null, "tcdate": 1541194326811, "tmdate": 1541194326811, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "SkleNxHqhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Response to \"Vanilla PGD training\"", "comment": "Since Therm discretizes the input, it prevents one from simply applying the gradient projection in PGD, not mentioning that the gradients are estimated through other methods like BPDA or DGA (Buckman et al., 2018). Did you mean that DGA is a more faithful application of (Madry et al., 2018)'s defense to Therm than LS-PGA? However, DGA has been shown a weak attack so it likely cannot lead to strong defense (e.g., called Therm-Adv-DGA) either."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "SkleNxHqhQ", "original": null, "number": 5, "cdate": 1541193767907, "ddate": null, "tcdate": 1541193767907, "tmdate": 1541193793055, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "BylxbaE9nX", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "content": {"comment": "Thanks for the answer. I still have a question. Vanilla PGD training is more robust than Therm-Adv.\nFor (2), why do you not use vanilla PGD training instead of Therm-Adv? ", "title": "Vanilla PGD training"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311677268, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeoxnRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311677268}}}, {"id": "BylxbaE9nX", "original": null, "number": 7, "cdate": 1541192952415, "ddate": null, "tcdate": 1541192952415, "tmdate": 1541192952415, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "S1gc1YNc3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Answers to (1) & (2)", "comment": "Regarding (2), thank @Nicholas for the catch! We will cite both (Buckman et al., 2018) and (Madry et al., 2018) in the revised paper. It is worth noting that vanilla PGD does not apply to Therm. As a result, the LS-PGA enhanced Therm-Adv is probably one of the best one can do in order to apply Madry et al. (2018)'s defense principle to Therm. \n\nRegarding (1), you may consider the Gaussian distribution along with the steps 1--4 in the paper as the threat model. We do not use any substitute network in our approach. The black-box setting: We query a black-box network by an input and obtain its output probability vector.  This setting is as standard as many existing works'."}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "S1gc1YNc3Q", "original": null, "number": 4, "cdate": 1541191906316, "ddate": null, "tcdate": 1541191906316, "tmdate": 1541191939064, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "B1xPOxN527", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "content": {"comment": "Thanks for your comment!\n\nI clarify my concern more clear. I was trying to say that THERM-ADV of Buckman et al. (2018)  should not be cited as Madry et al. (2018) for evaluation since standard adversarial training, Madry et al. (2018), is more robust than THERM-ADV. ", "title": "Clarification"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311677268, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeoxnRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311677268}}}, {"id": "BkeM4gQq2X", "original": null, "number": 2, "cdate": 1541185577633, "ddate": null, "tcdate": 1541185577633, "tmdate": 1541191549140, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "content": {"comment": "Hi, it looks very interesting.\n\nHowever, I have a few questions.\n\n(1) Could you specify the threat model? For example, I could not find what substitute models are used to generate adversarial examples. What black-box setting did you use?\n\n(2) I think you don't actually evaluate your attack on Madry et al. (2018). THERM-ADV did not technically use PGD adversarial examples described in Madry et al. (2018), but use LS-PGA examples described in Buckman et al. (2018). In addition, Athalye et al. (2018) argued that THERM-ADV is significantly weaker than Madry et al. (2018) since it is trained against the LS-PGA attacks.  Therefore, The argument in your paper, \"Athalye et al. (2018) find that the adversarial robust training (Madry et al., 2018) can significantly improve the defense strength of THERM.\" may be wrong.", "title": "Questions"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311677268, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeoxnRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311677268}}}, {"id": "B1xPOxN527", "original": null, "number": 3, "cdate": 1541189742932, "ddate": null, "tcdate": 1541189742932, "tmdate": 1541189742932, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "BkeM4gQq2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "content": {"comment": "The statement as it is written is technically correct. Thermometer encoding by itself is no more robust than a standard neural network. Adding adversarial training to thermometer encoding confers some amount of robustness, but less than standard adversarial training.\n\nSo whether or not adversarial training can \"significantly improve the defense strength of THERM\" depends I guess on your definition of \"significantly\". In Athalye et al. (2018) we find this difference to be ~20% at eps=8 and ~40% at eps=4. ", "title": "Regarding thermometer encoding"}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["~Nicholas_Carlini1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311677268, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeoxnRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311677268}}}, {"id": "BkgZeTW5hX", "original": null, "number": 6, "cdate": 1541180648694, "ddate": null, "tcdate": 1541180648694, "tmdate": 1541180648694, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "Bkx94ECt2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Clarification", "comment": "Thanks for asking. We will clarify our previous responses (mainly the paragraph below) by answering three of your questions.\n\n----------------------------------------\nOn the algorithmic aspect, both ours and Ilyas et al. (2018)\u2019s employ NES as the optimization algorithm. However, we arrive at it via different routes and for different purposes. We assume a probabilistic generation process of the adversarial examples (Steps 1\u20134, Section 2), which finds an adversarial example by a one-step addition to the input. In contrast, Ilyas et al. (2018)\u2019s modeling assumption is that an adversarial example can be found by PGD, which iteratively updates the original input with a small learning rate until it becomes adversarial. To this end, we use NES to estimate the parameters of the distribution, while Ilyas et al. (2018) use NES to replace the true (stochastic) gradients in PGD. We contend that, due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients \u2014 we are running experiments to empirically verify if this is true or not.  \n--------------------------------------------------------------\n\n== Q1: specify exactly what the difference between NES and your attack is? ==\n\nUsing our notation, the pseudo code below sketches our algorithm and Ilyas et al. (2018)\u2019s.\n\nOurs, which searches for the Gaussian from which more than one adversarial examples can be generated.\nIterate until convergence:\n1. Draw a sample {\\epsilon} from the normal distribution\n2. Transform it to a sample of Gaussian by {z=\\theta + \\sigma * \\epsilon}\n3. Generate current adversarial examples from {z} by steps 1\u20134\n4. Compute the losses {J(z)}\n5. Compute the search gradients {g} by equation (5)\n6. Update the Gaussian mean: \\theta = \\theta - r * g \nReturn \\theta\n\nIlyas et al. (2018)\u2019s, which searches for a single adversarial example.\nIterate until convergence:\n1. Draw a sample {e} from the normal distribution\n2. Transform it to a sample of zero-mean Gaussian by {z=0 + \\sigma * \\epsilon}\n3. Generate current adversarial examples by {x + z} and {x - z}\n4. Compute the losses {J(z)}\n5. Compute the search gradients {g} by equation (5)\n6. x = Projection(x - r * sign(g))\nReturn x\n\n\nThe differences start from the second line, where we transform the normal sample to a sample of the Gaussian N(\\theta, sigma^2) while Ilyas et al. (2018) transform it following a zero-mean Gaussian N(\\theta, sigma^2). \n\nLine 3: The difference is on how to generate the adversarial examples. \n\nLine 4: Slightly different loss functions are used in the two methods. This is not vital. \n\nLine 5 is the same for the two methods. \n\nLine 6: While we update the Gaussian mean by a gradient descent step, Ilyas et al. (2018) update the adversarial example by PGD.\n\n== Q2: a smaller standard deviation for sampling ==\nBy using the same setting for the NES component of our algorithm and Ilyas et al. (2018)\u2019s, including the same sample size and standard deviation, we obtain the comparison results below. Ours still performs better. We will complete the experiments with all the defense methods studied in our paper. \n\nTable 1: Success rate on attacking Randomization (ImageNet)\n# of iterations          30       90       150      210       270    300     360     400\nours                         21.54  78.58  90.02   95.41   95.5    95.5    95.5    95.5\nIlyas et al. (2018)'s  20.5    46.37  53.33   53.33   53.33  53.33  53.33  53.33 \n\n\n== Q3: you don't perform clipping ==\nWe did perform clipping in steps 1--4 of the paper, where we generate adversarial examples from a Gaussian distribution. In contrast, Ilyas et al. (2018)\u2019s performs the clipping of gradients due its employment of PGD attack. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "Bkx94ECt2m", "original": null, "number": 5, "cdate": 1541166130053, "ddate": null, "tcdate": 1541166130053, "tmdate": 1541166130053, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "BkxNNaW_hX", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Please specify the exact differences", "comment": "I appreciate the additional experiments, thanks! Could you specify exactly what the difference between NES and your attack is? If I understand you correctly, then the difference is (1) you are using a smaller standard deviation for sampling and (2) you don't perform clipping. However, the standard deviation is merely a hyperparameter of NES and should be tuned for optimal attack efficiency. Second, the clipping is necessary in any real world scenario where you don't have full access to the model but can only query it with images, right?"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "BkxNNaW_hX", "original": null, "number": 4, "cdate": 1541049644313, "ddate": null, "tcdate": 1541049644313, "tmdate": 1541049644313, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "HJg9cjD6o7", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Experimental comparison with \"Black-box Adversarial Attacks with Limited Queries and Information\" ", "comment": "With the open-source code released by Ilyas et al. (2018), we have evaluated their method on attacking three defense methods: SAP and Therm for CIFAR10 and Randomization for ImageNet. The results (success rate vs. number of optimization iterations) are shown in the tables below. We have also tested larger sample size and higher number of iterations for NES, and yet the results remain about the same. \n\nIlyas A, Engstrom L, Athalye A, Lin J. Black-box Adversarial Attacks with Limited Queries and Information. arXiv preprint arXiv:1804.08598. 2018 Apr 23.\n\nThe inferior attacking results of (Ilyas et al., 2018) verify our conjecture above, i.e., due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients of PGD. As a result, NES is not able to approach PGD\u2019s strong attack  performance. \n\nTable 1: Success rate on attacking SAP (CIFAR10)\n# of iterations           30       90       150      210       270     300      360      400\nOurs                          45.13  96.21  99.00   99.54   99.81   100      100      100\nIlyas et al. (2018)'s  33.36  34.51  36.03   37.36   37.36   37.36   37.36   37.36\n\nTable 2: Success rate on attacking Therm (CIFAR10)\n# of iterations            30       90       150      210       270     300       360     400\nOurs                          67.38   96.38  98.92   99.53   99.74   99.89   100     100\nIlyas et al. (2018)'s  59.22   83.32  83.82   84.32   85.33   85.33   85.33   85.33\n\nTable 3: Success rate on attacking Randomization (ImageNet)\n# of iterations          30       90       150      210       270    300    360    400\nOurs                         21.54  78.58  90.02   95.41   95.5    95.5   95.5   95.5\nIlyas et al. (2018)'s  3.33    4.56    6.77     8.5       8.5      8.5     8.5     8.5\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "HJg9cjD6o7", "original": null, "number": 3, "cdate": 1540352913905, "ddate": null, "tcdate": 1540352913905, "tmdate": 1540352913905, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "HkxngDtniX", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Differences from \"Black-box Adversarial Attacks with Limited Queries and Information\"", "comment": "That\u2019s a great catch. Thank you very much! We should have read the paper before\u2026 It is intriguing (and yet disappointing for us) to see that a similar approach has been proposed (Ilyas et al. 2018) by also resorting to the natural evolution strategy (NES), but it is not surprising. After all, derivative-free methods, such as NES, REINFORCE, and the zero-th order algorithms, are a natural choice for the blackbox attack. \n\nWhile we mainly attack up to 10 recently published defense methods by the proposed approach, Ilyas et al. (2018) focus on attacking a vanilla neural network under the constraints of limited queries and information (e.g., top k entries as opposed to the full output vector). \n\nOn the algorithmic aspect, both ours and Ilyas et al. (2018)\u2019s employ NES as the optimization algorithm. However, we arrive at it via different routes and for different purposes. We assume a probabilistic generation process of the adversarial examples (Steps 1\u20134, Section 2), which finds an adversarial example by a one-step addition to the input. In contrast, Ilyas et al. (2018)\u2019s modeling assumption is that an adversarial example can be found by PGD, which iteratively updates the original input with a small learning rate until it becomes adversarial. To this end, we use NES to estimate the parameters of the distribution, while Ilyas et al. (2018) use NES to replace the true (stochastic) gradients in PGD. We contend that, due to the non-differentiable clip and projection operations and the fairly large Gaussian covariance, NES is *not* an efficient (and possibly a biased) estimator of the true gradients \u2014 we are running experiments to empirically verify if this is true or not.  \n\nIt is a conceptual change from the traditional attack methods (e.g., PGD) to the way of modeling the adversarial examples by a distribution. This change may enable some exciting future works. For instance, we can draw samples from the distribution to characterize the adversarial boundaries, efficiently do adversarial training, etc.\n\nAnother notable difference from (Ilyas et al. 2018)\u2019s is that we train a regression neural network to find a good initialization for NES. Experiments verify the benefit of this regression network. \n\nOn the experimental aspect, we attack the recently proposed defense methods following the protocols set up in the original papers. As a result, we experiment with both CIFAR10 and ImageNet, both the $\\ell_2$ and $\\ell_infty$ distances, and different types of defenses (e.g., input randomization and discretization, ensembeling, denoising, etc.). In contrast, Ilyas et al. (2018) experiment with ImageNet with an $\\ell_\\infty$ distance. In addition, we examine the adversarial examples\u2019 transferabilities across different defense methods. Unlike the findings about the transferability across vanilla neural networks, our results indicate several unique characteristics of the transferability of our adversarial examples for the defended neural networks (cf. Section 3.3). Finally, we plot the curves of the attack success rates versus the iteration numbers, a new evaluation scheme which is complementary to the final attack success rates. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "HkxngDtniX", "original": null, "number": 2, "cdate": 1540294388145, "ddate": null, "tcdate": 1540294388145, "tmdate": 1540294388145, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Comparison with state-of-the-art", "comment": "Could the authors elaborate as to how this attack differs from [1]? As far as I can see this work uses the same gradient estimate with Gaussian bases.\n\n[1] \"Black-box Adversarial Attacks with Limited Queries and Information\" (https://arxiv.org/abs/1804.08598)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "B1e2hLFnoX", "original": null, "number": 1, "cdate": 1540294323880, "ddate": null, "tcdate": 1540294323880, "tmdate": 1540294323880, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "content": {"title": "Difference to state of the art", "comment": "Could the authors elaborate as to how this attack differs from [1]? As far as I can see this work uses the same gradient estimate with Gaussian bases.\n\n[1] \"Black-box Adversarial Attacks with Limited Queries and Information\" (https://arxiv.org/abs/1804.08598)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1106/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605477, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryeoxnRqKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1106/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1106/Authors|ICLR.cc/2019/Conference/Paper1106/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605477}}}, {"id": "rklrRiqY5Q", "original": null, "number": 1, "cdate": 1539054541123, "ddate": null, "tcdate": 1539054541123, "tmdate": 1539054541123, "tddate": null, "forum": "ryeoxnRqKQ", "replyto": "ryeoxnRqKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "content": {"comment": "Code released: https://github.com/gaussian-attack/Nattack", "title": "Code released: https://github.com/gaussian-attack/Nattack"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1106/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NATTACK: A STRONG AND UNIVERSAL GAUSSIAN BLACK-BOX ADVERSARIAL ATTACK", "abstract": "Recent works find that DNNs are  vulnerable to adversarial examples, whose changes from the benign ones are imperceptible and yet lead DNNs to make wrong predictions. One can find various adversarial examples for the same input to a DNN using different attack methods. In other words, there is a population of adversarial examples, instead of only one, for any input to a DNN. By explicitly modeling this adversarial population with a Gaussian distribution, we propose a new black-box attack called NATTACK. The adversarial attack is hence formalized as an optimization problem, which searches the mean of the Gaussian under the guidance of increasing the target DNN's prediction error. NATTACK achieves 100%  attack success rate  on six out of eleven recently published defense methods (and greater than 90% for four), all using the same algorithm. Such results are on par with or better than  powerful state-of-the-art white-box attacks. While the white-box attacks are often model-specific or defense-specific, the proposed black-box NATTACK is universally applicable to different defenses. ", "keywords": ["adversarial attack", "black-box", "evolutional strategy", "policy gradient"], "authorids": ["lyndon.leeseu@outlook.com", "lilijun1990@buaa.edu.cn", "lwang@cs.ucf.edu", "bradymzhang@tencent.com", "boqinggo@outlook.com"], "authors": ["Yandong Li", "Lijun Li", "Liqiang Wang", "Tong Zhang", "Boqing Gong"], "pdf": "/pdf/6510e82cd870e51ee93428137d9f03a08f786426.pdf", "paperhash": "li|nattack_a_strong_and_universal_gaussian_blackbox_adversarial_attack", "_bibtex": "@misc{\nli2019nattack,\ntitle={{NATTACK}: A {STRONG} {AND} {UNIVERSAL} {GAUSSIAN} {BLACK}-{BOX} {ADVERSARIAL} {ATTACK}},\nauthor={Yandong Li and Lijun Li and Liqiang Wang and Tong Zhang and Boqing Gong},\nyear={2019},\nurl={https://openreview.net/forum?id=ryeoxnRqKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1106/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311677268, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryeoxnRqKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1106/Authors", "ICLR.cc/2019/Conference/Paper1106/Reviewers", "ICLR.cc/2019/Conference/Paper1106/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311677268}}}], "count": 34}