{"notes": [{"id": "ByeVWkBYPH", "original": "HJeaJqjuvr", "number": 1539, "cdate": 1569439483777, "ddate": null, "tcdate": 1569439483777, "tmdate": 1577168265720, "tddate": null, "forum": "ByeVWkBYPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "HnAMBCWpHq", "original": null, "number": 1, "cdate": 1576798725961, "ddate": null, "tcdate": 1576798725961, "tmdate": 1576800910536, "tddate": null, "forum": "ByeVWkBYPH", "replyto": "ByeVWkBYPH", "invitation": "ICLR.cc/2020/Conference/Paper1539/-/Decision", "content": {"decision": "Reject", "comment": "Quoting from R3: \"This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices.\"  With two weak acceptance recommendations and a recommendation for rejection, this paper is borderline in terms of its scores.\n\nThe approach and idea are interesting.  The main shortcoming of the paper, as highlighted by the reviewers, is that the approach and theoretical analysis are not properly motivated to solve an actual problem faced in real-world data.  The approach does not provide a better algorithm for recovering the eigenvectors of the data, nor is it proposed as part of a learning framework to solve a real-world problem.  Experiments are shown on synthetic data and MNIST.  As a stand-alone theoretical result, it leaves open questions as to the proposed utility.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ByeVWkBYPH", "replyto": "ByeVWkBYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718934, "tmdate": 1576800269494, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1539/-/Decision"}}}, {"id": "S1gCqzfYsH", "original": null, "number": 5, "cdate": 1573622421951, "ddate": null, "tcdate": 1573622421951, "tmdate": 1573624104527, "tddate": null, "forum": "ByeVWkBYPH", "replyto": "Byl_DpEy9S", "invitation": "ICLR.cc/2020/Conference/Paper1539/-/Official_Comment", "content": {"title": "Response to Reviewer #3- Part 1/2", "comment": "This review has been extremely useful---responding to it has broadened our understanding of our submission and enabled us to identify several connections that were heretofore less clear to us. \n\nWe would love to hear your feedback on the following discussions inspired by your review, and we will be more than happy to incorporate them into the revised paper if you are in favor of so.\n\nBefore addressing the reviewer's objections, we consider two key points worthy of emphasizing, that we will use throughout this response:\n\n(i) Corollary 1 and Remark (5) which follows it: Based on the corollary any critical point of our loss $L$ is a critical point of the original MSE loss but not vice versa. In light of Theorem 2 this means that $L$ eliminates those undesirable global minima of the original loss (i.e., exactly those which suffer from the invariance).\n\nAbove describes advantage owing to the difference from the original loss, but there is also further profit gained from their similarities:\n\n(ii) Consider the side by side comparison of our loss and MSE loss, along with their respective gradients, provided on pages 4 and 5 of the paper. Any gradient written for the original loss can be turned simply into the gradient for $L$ by just two component-wise matrix products with constant matrices. Moreover, by Lemma 1 the complexity of evaluating $L$ itself is of the same order as MSE loss too. Given the many repeated terms in the formulas, a careful implementation will eliminate much of the added complexity.\n\nBuilt on the above two points, the clarification for the two claims mentioned in the review are then as follows.\n\n(1) Theoretical contribution: \nWe believe point (i) alone is a substantial and important theoretical contribution: Analyzing the loss surface for various architectures of linear/non-linear neural networks is a highly active and prominent area of research. Many of these works (e.g. [2, 3, 4, 5]) start by citing the seminal results of [1] for shallow LAEs before extending it to more complex networks. However, most work retains the original MSE loss, and they prove the same critical point characterization of [1] for their specific architecture of interest. Most notably [2] extends the results of [1] to deep linear networks and shallow RELU networks. First, the submission is unique in going after a loss with better loss surface properties. In addition, secondly, given that the set of critical points of $L$  is a subset of critical points of MSE loss, many of the mentioned results likely extend. In light of the removal of undesirable global minima through $L$, examining more complex networks is certainly a very promising direction."}, "signatures": ["ICLR.cc/2020/Conference/Paper1539/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeVWkBYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1539/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1539/Authors|ICLR.cc/2020/Conference/Paper1539/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154519, "tmdate": 1576860540808, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1539/-/Official_Comment"}}}, {"id": "HkeAmzftor", "original": null, "number": 4, "cdate": 1573622310416, "ddate": null, "tcdate": 1573622310416, "tmdate": 1573623996960, "tddate": null, "forum": "ByeVWkBYPH", "replyto": "Byl_DpEy9S", "invitation": "ICLR.cc/2020/Conference/Paper1539/-/Official_Comment", "content": {"title": "Response to Reviewer #3- Part 2/2  ", "comment": "(2) Practical implications: \nThe question of whether randomized SVD outperforms SGB-based methods, or visa versa, remains an open one, and is a likely data-dependent question, as factors such as size and sparsity are important.  There have been several developments by others who themselves have outlined the benefits of SGD-based PCA/SVD (for instance, [6] and also cf. their quote from [3]). Chief among the compellingly reasons is that, in recent years, we have seen unprecedented gains in the performance of very large SGD optimizations, with autoencoders in particular successfully handling larger numbers of high-dimensional training data (e.g., images). The loss function we offer is attractive in terms of parallelizability and distributability, and does not prescribe any single specific algorithm or implementation, so stands to continue to benefit from the arms race between SGD and its competitors.\n\nFinally, the submission's focus has been on rigorously establishing theoretical properties. It is not our current focus of interest, for instance, to conduct a size analysis, as this is better deferred to some a treatment with a clear and specific characterization of problem instances of particular interest. In contrast, our own research directions involve us seeking to generalize the theory to tensors and tensor rank decomposition.\n\nNext, we offer a response to the suggestion that one can always recover the eigenvectors/eigenvalues by a final decomposition step and the SVD would be dominated by MSE LAE costs anyway. We clarify our position in two parts:\n\n(1) The exact cost of a post hoc processing step to perform the SVD will depend on the density and size of the data. It isn't hard to imagine circumstances (e.g., with large, dense inputs) in which even on the reduced output, this cubic step dominates and is prohibitive.\n\n(2) More importantly, this single loss function (without an additional post hoc processing step) fits seamlessly into optimization pipelines (where SGD is but one instance). The result is that the loss allows for PCA/SVD computation\nas a single optimization layer, akin to an instance of a fully differentiable building block in a NN pipeline [7], potentially as part of a much larger network.  \n\nIn light of the importance of (2), we intend to make this benefit much more explicit in the paper's introduction.\n\n[1] Baldi, Pierre, and Kurt Hornik. \"Neural networks and principal component analysis: Learning from examples without local minima.\" Neural networks 2.1 (1989): 53-58.\n\n[2] Zhou, Y., and Y. Liang. \"Critical points of linear neural networks: Analytical forms and landscape properties.\" Proc. Sixth International Conference on Learning Representations (ICLR). 2018.\n\n[3] Kunin, Daniel, et al. \"Loss Landscapes of Regularized Linear Autoencoders.\" International Conference on Machine Learning. 2019.\n\n[4] Pretorius, Arnu, Steve Kroon, and Herman Kamper. \"Learning Dynamics of Linear Denoising Autoencoders.\" International Conference on Machine Learning. 2018.\n\n[5] Frye, Charles G., et al. \"Numerically Recovering the Critical Points of a Deep Linear Autoencoder.\" arXiv preprint arXiv:1901.10603 (2019).\n\n[6] Plaut, Elad. \"From principal subspaces to principal components with linear autoencoders.\" arXiv preprint arXiv:1804.10253 (2018)\n\n[7] Amos, Brandon, and J. Zico Kolter. \"Optnet: Differentiable optimization as a layer in neural networks.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1539/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeVWkBYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1539/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1539/Authors|ICLR.cc/2020/Conference/Paper1539/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154519, "tmdate": 1576860540808, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1539/-/Official_Comment"}}}, {"id": "r1g7nlftiS", "original": null, "number": 3, "cdate": 1573621931160, "ddate": null, "tcdate": 1573621931160, "tmdate": 1573622352666, "tddate": null, "forum": "ByeVWkBYPH", "replyto": "SylQ9IDCtB", "invitation": "ICLR.cc/2020/Conference/Paper1539/-/Official_Comment", "content": {"title": "Response to Reviewer #1  ", "comment": "This reviewer's comments were especially valuable in helping to establish where some assumptions were in fact not needed.\n  \n1. This is a very good point. After careful examination, in the case that input and output have different dimensions, say ${Y}\\in \\mathbb{R}^{n\\times m}$ and ${X}\\in \\mathbb{R}^{n'\\times m}$, all the claims actually still hold and the given loss can be used as a linear least square regressor. In the writing we assumed the same dimension since the focus was on low rank decomposition where ${Y}={X}$. We have added a remark (6) in the paper to be explicit about this fact. The reason for the validity of the claims for the case $n'\\neq n$ as explained in the remark is as follows: \n\nThe given loss function structurally is very similar to MSE loss and can be represented as a sum of Frobenius norms on the space of $n\\times m$ matrices. In this case the covariance matrix $ {\\Sigma}={\\Sigma}_{yx} {\\Sigma}_{xx}^{-1} {\\Sigma}_{xy}$ is still $n\\times n$. Clearly, for under-constrained systems with $n<n'$ the full rank assumption of $ {\\Sigma}$ holds. For the overdetermined case, where $n'>n$ the second and third assumptions in Assumption 1 can be relaxed: we only require ${\\Sigma}_{xx}$ to be full rank since this is the only matrix that is inverted in the theorems. Note that if $p>\\min(n',n)$ then ${\\Lambda}_{\\mathbb{I}_p}$: the $p\\times p$ diagonal matrix of eigenvalues of ${\\Sigma}$ for a $p$-index-set $\\mathbb{I}_p$ bounds to have some zeros and will be say rank $r<p$, which in turn, results in an encoder with rank $r$. However, the Theorem 1 is proved for encoder of any rank $r\\leq p$. Finally, following Theorem 2 then the first $r$ columns of the encoder ${A}$ converges to ordered eigenvectors of ${\\Sigma}$ while the $p-r$ remaining columns span the kernel (sub)space of ${\\Sigma}$.\n\n2 and 3- we have updated the introduction.\n\n4-Fixed\n\n5- Dealing with large datasets is a leading edge of our algorithm when the whole data is too large to fit in memory. We don't expect the performance to be different if we switch to a larger dataset since our algorithm allows processing the data in batches, in which case the algorithm will yield the result that converges to the desired real ordered eigenvectors as well. \n \n6- We conducted extra experiments on MNIST dataset with compressed dimension p being 1,5,10,20,50 and 100. The settings of the other parameters is the same as the ones shown in our paper. The results are as follows: reconstruction error is 2.857e6, 2.113e6, 1.619e6, 1.127e6, 5.546e5, 2.700e5, respectively, and the total running time on average (with one GeForce GTX 1080 Ti Graphics Card) is 0.253 seconds, 7.062 seconds, 26.855 seconds, 4 minutes 18.408 seconds, 17 minutes 10.213 seconds, 35 minutes 31.986 seconds, respectively."}, "signatures": ["ICLR.cc/2020/Conference/Paper1539/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper1539/Authors", "everyone", "ICLR.cc/2020/Conference/Paper1539/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeVWkBYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1539/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1539/Authors|ICLR.cc/2020/Conference/Paper1539/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154519, "tmdate": 1576860540808, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1539/-/Official_Comment"}}}, {"id": "HJebJxzKiH", "original": null, "number": 1, "cdate": 1573621720921, "ddate": null, "tcdate": 1573621720921, "tmdate": 1573621720921, "tddate": null, "forum": "ByeVWkBYPH", "replyto": "HkxnO6hitS", "invitation": "ICLR.cc/2020/Conference/Paper1539/-/Official_Comment", "content": {"title": "Response to Reviewer #2 ", "comment": "Thanks. We have added one paragraph at the start of section 4 that provides an overview of the arguments of the proofs. For the other point, please refer to remarks 2, 3 (new), 4, and 5 which hopefully clarifies the significance of the theorems."}, "signatures": ["ICLR.cc/2020/Conference/Paper1539/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1539/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ByeVWkBYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1539/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1539/Authors|ICLR.cc/2020/Conference/Paper1539/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154519, "tmdate": 1576860540808, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1539/Authors", "ICLR.cc/2020/Conference/Paper1539/Reviewers", "ICLR.cc/2020/Conference/Paper1539/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1539/-/Official_Comment"}}}, {"id": "HkxnO6hitS", "original": null, "number": 1, "cdate": 1571700083995, "ddate": null, "tcdate": 1571700083995, "tmdate": 1572972455323, "tddate": null, "forum": "ByeVWkBYPH", "replyto": "ByeVWkBYPH", "invitation": "ICLR.cc/2020/Conference/Paper1539/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a new loss function to compute the exact ordered eigenvectors of a dataset. The loss is motivated from the idea of computing the eigenvectors sequentially. However doing so would be computationally expensive, and the authors show that the loss function they propose (sum of sequential losses) has the same order (constant less than 7) of computational complexity as using the squared loss. A proof of the correctness of the algorithm is given, along with experiments to verify its performance.\n\nThe loss function proposed in the paper is useful, and the decomposition in Lemma 1 shows that it is not computationally expensive. While the writing of the proofs of the theorems is clear, I find it hard to understand the flow of the paper. It would help if the authors could summarize the argument of the proof at the start of Sec 4. Along a similar vein, it would also help if the authors could describe in words the significance / claim of every theorem.\n\nThe repetition in stating the theorems can be avoided. The main result (Theorem 2) is stated twice."}, "signatures": ["ICLR.cc/2020/Conference/Paper1539/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1539/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeVWkBYPH", "replyto": "ByeVWkBYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575897887111, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1539/Reviewers"], "noninvitees": [], "tcdate": 1570237735909, "tmdate": 1575897887124, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1539/-/Official_Review"}}}, {"id": "SylQ9IDCtB", "original": null, "number": 2, "cdate": 1571874443343, "ddate": null, "tcdate": 1571874443343, "tmdate": 1572972455289, "tddate": null, "forum": "ByeVWkBYPH", "replyto": "ByeVWkBYPH", "invitation": "ICLR.cc/2020/Conference/Paper1539/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). With this new loss function, the decoder weights of LAEs can eventually converge to the exact ordered unnormalized eigenvectors of the sample covariance matrix. The main contribution is to add the identifiability of principal components in PCA using LAEs and. Two empirical experiments were done to show the effectiveness of proposed loss function on one synthetic dataset and the MNIST dataset. \nOverall, this paper provides a nontrivial contribution for performing principal component analysis (PCA) using linear autoencoders (LAEs), with this new novel loss function. This paper is well presented.\nThere are some issues to be addressed:\n1. The output matrix is constrained to be the same size of the input, which is scarcely seen in practical applications.\n2. Literature on (denoising) auto-encoder can be reviewed more thoroughly.\n3. Comparison with state-of-the-art auto-encoder can be provided to demonstrate the effectiveness of the proposed algorithm.\n4. It is better to explain the meaning of each variable when it first appears, e.g., , the projection matrices A and B, and Variable A* in theorem 2.\n5. In the experiment part, in both the Synthetic Data or MNIST, the size of each data set is relatively small. It's better to add experimental results on big data sets with larger dimension.\n6. In order to better show the effectiveness of the new loss function, you can add some comparative test for different choice of compressed dimension p.\n7. There are some typos, such as \u2018faila\u2019 in the second line of the second paragraph in the INTRODUCTION. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1539/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1539/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeVWkBYPH", "replyto": "ByeVWkBYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575897887111, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1539/Reviewers"], "noninvitees": [], "tcdate": 1570237735909, "tmdate": 1575897887124, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1539/-/Official_Review"}}}, {"id": "Byl_DpEy9S", "original": null, "number": 3, "cdate": 1571929440430, "ddate": null, "tcdate": 1571929440430, "tmdate": 1572972455245, "tddate": null, "forum": "ByeVWkBYPH", "replyto": "ByeVWkBYPH", "invitation": "ICLR.cc/2020/Conference/Paper1539/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices. My intuition is that the weights that touch every subproblem are the most motivated to find the largest principal component, the weights that touch all but one find the next largest, and so forth; I found this idea clever and elegant.\n\nThat said, I lean towards rejection, because the paper does not do a very good job of demonstrating the practical or theoretical utility of this approach. As I see it, there are two main claims that one could make to motivate this work:\n1. This is a practical algorithm for doing PCA.\n2. This is a step towards better understanding (and perhaps improving) nonlinear autoencoders, which do things that PCA can't.\nClaim (2) might be compelling, but the authors do not make it, and it isn't self evident.\n\nI do not find claim (1) convincing on the basis of the evidence presented. PCA is an extremely well studied problem, with lots of good solutions such as randomized SVD (Halko et al., 2009). A possible advantage of using LAEs to address the PCA problem is that they play nicely with SGD, but again, the claim that the SGD-LAE approach is superior to, say, randomized SVD on a data subsample requires evidence. Also, even if one buys the claim that LAEs are a good way to solve PCA, one can always recover the eigenvectors/eigenvalues by a final decomposition step; the authors claim that an advantage of their approach is that it does not require such \"bells and whistles\", but this seems like a pretty minor consideration; implementing the proposed loss function seems at least as complicated as making a call to an SVD solver, and it's hard for me to imagine a situation where the cost of that final SVD isn't dominated by the cost of solving the MSE LAE problem.\n\nIn summary, I think this paper proposes a clever and elegant solution to a problem that doesn't seem to be very important. I can't recommend acceptance unless the authors can come up with a stronger argument for why it's not just interesting but also useful."}, "signatures": ["ICLR.cc/2020/Conference/Paper1539/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1539/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors ", "authors": ["Reza Oftadeh", "Jiayi Shen", "Zhangyang Wang", "Dylan Shell"], "authorids": ["oftadeh.reza@gmail.com", "asjyjya-617@tamu.edu", "atlaswang@tamu.edu", "dshell@tamu.edu"], "keywords": ["Principal Component Analysis", "Autoencoder", "Neural Network"], "TL;DR": "A new loss function for PCA with linear autoencoders that provably yields ordered exact eigenvectors ", "abstract": "In this paper, we propose a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). Optimizing the standard L2 loss results in a decoder matrix that spans the principal subspace of the sample covariance of the data, but fails to identify the exact eigenvectors. This downside originates from an invariance that cancels out in the global map. Here, we prove that our loss function eliminates this issue, i.e. the decoder converges to the exact ordered unnormalized eigenvectors of the sample covariance matrix. For this new loss, we establish that all local minima are global optima and also show that computing the new loss (and also its gradients) has the same order of complexity as the classical loss. We report numerical results on both synthetic simulations, and a real-data PCA experiment on MNIST (i.e., a 60,000 x784 matrix), demonstrating our approach to be practically applicable and rectify previous LAEs' downsides.", "pdf": "/pdf/c6410990833f1eb928d6837ed71a5049ef91e000.pdf", "paperhash": "oftadeh|neural_networks_for_principal_component_analysis_a_new_loss_function_provably_yields_ordered_exact_eigenvectors", "original_pdf": "/attachment/4a606f1c2cdd2dd7cd35a163bbe38c583775facf.pdf", "_bibtex": "@misc{\noftadeh2020neural,\ntitle={Neural Networks for Principal Component Analysis: A New Loss Function Provably Yields Ordered Exact Eigenvectors },\nauthor={Reza Oftadeh and Jiayi Shen and Zhangyang Wang and Dylan Shell},\nyear={2020},\nurl={https://openreview.net/forum?id=ByeVWkBYPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ByeVWkBYPH", "replyto": "ByeVWkBYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1539/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575897887111, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1539/Reviewers"], "noninvitees": [], "tcdate": 1570237735909, "tmdate": 1575897887124, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1539/-/Official_Review"}}}], "count": 9}