{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487276127765, "tcdate": 1478270035818, "number": 184, "id": "SkhU2fcll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkhU2fcll", "signatures": ["~Yongxin_Yang1"], "readers": ["everyone"], "content": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396418379, "tcdate": 1486396418379, "number": 1, "id": "Bk9ZnMUux", "invitation": "ICLR.cc/2017/conference/-/paper184/acceptance", "forum": "SkhU2fcll", "replyto": "SkhU2fcll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviews for this paper were quite mixed, with one strong accept and a marginal reject. A fourth reviewer with strong expertise in multi-task learning and deep learning was brought in to read the latest manuscript. Due to time constraints, this fourth review was not entered in the system but communicated through personal communication. \n \n Pros:\n - Reviewers in general found the paper clear and well written.\n - Multi-task learning in deep models is of interest to the community\n - The approach is sensible and the experiments show that it seems to work\n \n Cons:\n - Factorization methods have been used extensively in deep learning, so the reviewers may have found the approach incremental\n - One reviewer was not convinced that the proposed method would work better than existing multi-task learning methods\n - Not all reviewers were convinced by the experiments\n - The fourth reviewer found the approach very sensible but was not excited enough to champion the paper\n \n The paper was highly regarded by at least one reviewer and two thought it should be accepted. The PCs also agree that this paper deserves to appear at the conference.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396418880, "id": "ICLR.cc/2017/conference/-/paper184/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkhU2fcll", "replyto": "SkhU2fcll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396418880}}}, {"tddate": null, "tmdate": 1485304248033, "tcdate": 1485304248033, "number": 7, "id": "B1lTWuSPg", "invitation": "ICLR.cc/2017/conference/-/paper184/public/comment", "forum": "SkhU2fcll", "replyto": "Bk7WO8XSl", "signatures": ["~Yongxin_Yang1"], "readers": ["everyone"], "writers": ["~Yongxin_Yang1"], "content": {"title": "Comparison with classic methods", "comment": "Dear R1, hopefully we have answered your question satisfactorily. Please let us know if we can provide any further clarification."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287696099, "id": "ICLR.cc/2017/conference/-/paper184/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkhU2fcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper184/reviewers", "ICLR.cc/2017/conference/paper184/areachairs"], "cdate": 1485287696099}}}, {"tddate": null, "tmdate": 1483739264915, "tcdate": 1483739264915, "number": 5, "id": "HyFFxcTrg", "invitation": "ICLR.cc/2017/conference/-/paper184/public/comment", "forum": "SkhU2fcll", "replyto": "SkhU2fcll", "signatures": ["~Yongxin_Yang1"], "readers": ["everyone"], "writers": ["~Yongxin_Yang1"], "content": {"title": "Revision", "comment": "\nDear readers and reviewers,\n\nWe would like to thank all the reviewers that helped us to improve this submission. Based on their comments, we have revised the submission.\n\nThe main changed parts are:\n\n1. We discuss some related work in multi-task learning from speech recognition community. (Sec 2, Comment from Reviewer 3).\n\n2. We add a new section in related work to discuss the literature in the line of constructing a MTL-like problem via clustering training data. (Sec 2, Pre-review question from Reviewer2)\n\n3. We give more details about initialisation, particularly how to make the random initialisation condition work in practice. (Sec 4, Comment from Reviewer 2)\n\nFinally, we add two more experiments. Due to page limits, they are placed in the appendix.\n\nA) We compare our methods with some classic (shallow, matrix-based) MTL methods using both classic and CNN features. (Appendix A, Comment from Reviewer 1)\n\nB) We add details about model size in terms of number of parameters, and demonstrate what happens if we make an almost equal sized UD-MTL competitor. (Appendix B, Pre-review question from Reviewer2)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287696099, "id": "ICLR.cc/2017/conference/-/paper184/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkhU2fcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper184/reviewers", "ICLR.cc/2017/conference/paper184/areachairs"], "cdate": 1485287696099}}}, {"tddate": null, "tmdate": 1483482838936, "tcdate": 1483475596626, "number": 4, "id": "HJS95tYHg", "invitation": "ICLR.cc/2017/conference/-/paper184/public/comment", "forum": "SkhU2fcll", "replyto": "Bk7WO8XSl", "signatures": ["~Yongxin_Yang1"], "readers": ["everyone"], "writers": ["~Yongxin_Yang1"], "content": {"title": "To AnonReviewer1", "comment": "\n--- Comparison with classic (shallow) MTL methods\n\nAs the reviewer asked, we provide a comparison with classic MTL methods for the first experiment (MNIST, binary one-vs-rest classification, 1% training data, mean of error rates for 10-fold). PCA feature is obtained by flattening the image then dimension reduction by PCA. CNN feature is obtained by the penultimate layer of the CNN network used in our experiment (a variant of LeNet).\n\n+--------------------------+-------------+-------------+\n|                          | PCA Feature | CNN Feature |\n+--------------------------+-------------+-------------+\n| Single Task Learning     | 16.89       | 11.52       |\n+--------------------------+-------------+-------------+\n| Evgeniou & Pontil, 2004  | 15.27       | 10.32       |\n+--------------------------+-------------+-------------+\n| Argyriou et al., 2008    | 15.64       | 9.56        |\n+--------------------------+-------------+-------------+\n| Kumar & Daume III, 2012  | 14.08       | 9.41        |\n+--------------------------+-------------+-------------+\n| DMTRL-LAF                | N.A.        | 8.25        |\n+--------------------------+-------------+-------------+\n| DMTRL-Tucker             | N.A.        | 9.24        |\n+--------------------------+-------------+-------------+\n| DMTRL-TT                 | N.A.        | 7.31        |\n+--------------------------+-------------+-------------+\n| UD-MTL                   | N.A.        | 9.34        |\n+--------------------------+-------------+-------------+\n\nWe have previously tried classic MTL methods in our experiments, and DMTRL decisively outperforms them throughout. Despite these positive results, this comparison was deliberately excluded in the paper, and we briefly explained why in Section 4.1. Here we detail the reasons:\n\n1) As seen above, PCA feature is not competitive with deep learning. This also answers the reviewer's question about if deep learning is necessary here.\n\n2) From an experimental design view, it is not a very scientific comparison to make. If our DMTRL outperforms classic MTL methods with PCA feature: it is not clear if the difference is due to better MTL strategy (ours v.s. others) or more discriminative feature (CNN v.s. PCA). Alternatively, we can replace the PCA feature by CNN feature (second column of above table). Then classic MTL methods have two steps: feature extraction via CNN and classifier training by the MTL method. Again we can not conclude that our method is better because the difference could be due to either the MTL method itself, or the optimisation procedure: ours is end-to-end training, and classic MTL + CNN feature approaches have two independent steps. Therefore, when we designed the experiments, we made sure that all competitors are (i) trained in an end-to-end fashion (ii) all based on the same CNN architecture. Then we can confidently say that any performance difference is due to different MTL methods, rather than other factors. \n\n3) Classic MTL methods can be a valid baseline for the first experiment only because they are binary classification problems. In the first experiment we cast a multi-class classification problem into multiple binary classification problems, so each class is a task. Though this is a well established setting in MTL benchmarking, for many practical problems, esp. those needing deep models, each task itself is a multi-class classification problem. We denote this setting as heterogeneous MTL (i.e., our second and third experiment). This setting is usually addressed by some user-defined neural network architecture, and a typical design of which is to keep the bottom layers' parameters tied, and each task has its own top layer(s) parameters. One of the key motivations of this work is to eliminate the need for manually designing such a topology; instead finding it in a data-driven way. Classic MTL methods, in general, can not apply to this setting of multiple tasks where each task has multiple classes.\n\nDespite these caveats, we can easily add comparisons to the classic MTL methods in the final version.\n\n--- Positioning of this submission\n\nWe agree if a problem is well solved by shallow methods, it is not necessary to get deep learning involved. The reviewer may be aware that some very classic MTL problems (datasets) are not evaluated in this submission, for example, School Dataset, Computer Survey Dataset. The reason is exactly what the reviewer mentioned: you do not need deep learning for these datasets. The experiments in this submission are object recognition problems from computer vision community, and for those problems, deep learning (CNN) has a clear advantage. Then it is desirable to find a systematic way to add MTL upon deep learning. In summary, the need of deep learning comes first.\n\n--- Extending nuclear norm to deep learning\n\nWe appreciate the reviewer mentioned this point, and coincidentally it is our on-going project. Even though it is totally out of scope of reviewing this submission, we can share some thoughts on this direction.\n\nThe nuclear (trace) norm is defined for matrix, and has a close connection to matrix rank. Thus, some work based on the assumption of low-rank matrix, e.g., multi-task learning and matrix completion, use it as a regularisation term.\n\nThe parameters in deep learning or deep MTL are usually in a form of higher-order (>=3) tensors. But it is not straightforward to extend nuclear norm from matrices to tensors, because tensor rank is non-uniquely defined. We have at least three: CP-, Tucker-, Tensor_Train-rank. For these different definitions of rank, there are correspondingly different ways to extend the nuclear norm. Some of those are easy, e.g., Tucker-rank and Tensor_Train-rank, for which we can define the tensor nuclear norm as a sum of nuclear norms of unfolded matrices from the tensor. CP-rank is tricky. We have not figured out an analytical form of nuclear norm corresponding to it, unless we make some (very strong) assumptions, e.g., assuming the tensor is orthogonally decomposable. \n\nAnother challenge is optimisation. It is true that we can derive the (sub-)gradient of nuclear norm, thus technically we can apply a gradient descent method for it. However, optimisation experts advised us not to do so because the nuclear norm is essentially non-differentiable. A better alternative is semidefinite programming. However, deep neural networks are usually trained by gradient descent, and it is challenging to marry these two different optimisation strategies.\n\nNevertheless, we tried some easy solutions and obtained some interesting results. If the reviewer is interested in this topic, the code and technical report can be found in https://github.com/wOOL/TNRDMTL.\n\nWe are keen to hear your views on this direction, as you mention \"nuclear norm regularization can also be extended to deep learning as gradient descent are popular\"."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287696099, "id": "ICLR.cc/2017/conference/-/paper184/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkhU2fcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper184/reviewers", "ICLR.cc/2017/conference/paper184/areachairs"], "cdate": 1485287696099}}}, {"tddate": null, "tmdate": 1483069434556, "tcdate": 1483069434556, "number": 3, "id": "Bk7WO8XSl", "invitation": "ICLR.cc/2017/conference/-/paper184/official/review", "forum": "SkhU2fcll", "replyto": "SkhU2fcll", "signatures": ["ICLR.cc/2017/conference/paper184/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper184/AnonReviewer1"], "content": {"title": "Comparison with other standard MTL methods is missing", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposed a nice framework leveraging Tucker and Tensor train low-rank tensor factorization to induce parameter sharing for multi-task learning.\n\nThe framework is nice and appealing. \n\nHowever, MTL is a very well studied problem and the paper considers simple task for different classification, and it is not clear if we really need ``Deep Learning\" for these simple datasets. A comparison with existing shallow MTL is necessary to show the benefits of the proposed methods (and in particular being deep) on the dataset. The authors ignore them on the basis of speculation and it is not clear if the proposed framework is really superior to simple regularizations like the nuclear norm. The idea of nuclear norm regularization can also be extended to deep learning as gradient descent are popular in all methods. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483069435180, "id": "ICLR.cc/2017/conference/-/paper184/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper184/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper184/AnonReviewer3", "ICLR.cc/2017/conference/paper184/AnonReviewer2", "ICLR.cc/2017/conference/paper184/AnonReviewer1"], "reply": {"forum": "SkhU2fcll", "replyto": "SkhU2fcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper184/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper184/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483069435180}}}, {"tddate": null, "tmdate": 1482873471046, "tcdate": 1482873403204, "number": 2, "id": "r1QB98xBe", "invitation": "ICLR.cc/2017/conference/-/paper184/public/comment", "forum": "SkhU2fcll", "replyto": "S1DiTxMNe", "signatures": ["~Yongxin_Yang1"], "readers": ["everyone"], "writers": ["~Yongxin_Yang1"], "content": {"title": "To AnonReviewer2", "comment": "\n--- Random initialisation\n\nIn our experiments, we found that the performance is not sensitive to STL-based vs Random initialisation, thus both can be used with our framework. The important aspect of making random initialisation work well is as follows:\n\nRecall that what our model learned is *NOT* the tensor itself, but the factors of tensor, so we are always talking about initialising factors, from which the tensor is constructed by multiplying them out. Randomly initialising those factors is not a good idea, because it is not straightforward to control the distribution of resulting tensor. E.g., you initialise two weight matrices from a specific Gaussian distribution, but the dot-product of them is clearly not the same distribution. It would have some extremely large/small values, which is a very bad starting point for the first round of forward-backward pass. \n\nTherefore, if random initialisation is desired, a solution is to initialise the whole tensor to a suitable distribution of weights first, then do decomposition, and use the decomposed values for initialising the factors. In this way, the resulting re-composed tensor after multiplication has approximately the intended distribution of weight values. \n\nWhile STL and the above random-based initialisation strategies perform similarly, our STL-based strategy has the advantage that it makes the choice of rank easier and more intuitive. If random initialisation is used, every tensor's low-rank choice(s) must be specified by the user. (Because there is no low-rank structure in a random tensor that can be found automatically). However, if you stack the STL models, the formed tensor already has a low-rank structure. E.g., the first convolutional layers are likely to look similar for different tasks/domains (like Gabor filters). Thus, analysing the rank of the stacked STL tensor gives you a good heuristic to set the ranks of the entire network. There is then only one hyper parameter of the entire process, the reconstruction error threshold used to choose the ranks, which has the straightforward interpretation of \"prior on sharing strength\".\n\n--- Unbalanced tasks/classes\n\nTasks: The implementation of our loss function is the average of mean error of each task. For each task, we get the mean of its instances' errors, and then we take the average of all those mean errors. This protects the tasks with few instances, because if the loss function was designed to be the mean error of all instances in all tasks, tasks with very small data would be down-weighted.\n\nBy default, all our tasks are equally weighted. However, the user is free to specify a non-uniform per-task weight, so they can make something like \"one main task, several less important side tasks\". It is also good practice to normalise the errors from different tasks so they are on a similar scale. Especially when your tasks are very different (e.g., one task is classification, another is regression).\n\nClasses: For unbalanced classification problem, there are various classic solutions like re-sampling, class-wise re-weighting, optimisation on AUC instead of accuracy, etc. As our method is about modelling the weights of a neural network, it can work with any of these specially designed losses.\n\nAll of these are good-practice engineering points, however, they are not key to our methodology.\n\n--- Revised version\n\nWe will add more discussions that address the questions you mentioned before in the revised version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287696099, "id": "ICLR.cc/2017/conference/-/paper184/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkhU2fcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper184/reviewers", "ICLR.cc/2017/conference/paper184/areachairs"], "cdate": 1485287696099}}}, {"tddate": null, "tmdate": 1482873455185, "tcdate": 1482873434716, "number": 3, "id": "BkXPqUxHx", "invitation": "ICLR.cc/2017/conference/-/paper184/public/comment", "forum": "SkhU2fcll", "replyto": "B17a61-Vg", "signatures": ["~Yongxin_Yang1"], "readers": ["everyone"], "writers": ["~Yongxin_Yang1"], "content": {"title": "To AnonReviewer3", "comment": "\n--- SVD in the conventional MTL\n\nIt is possible to reduce the number of parameters in the conventional MTL via SVD or other factorisation techniques, while it is for neural network compression purpose, not for knowledge sharing among tasks.\n\n--- Earlier work\n\nThanks for mentioning the earlier work in neural network MTL from speech recognition community, we will add the reference in the revised version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287696099, "id": "ICLR.cc/2017/conference/-/paper184/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkhU2fcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper184/reviewers", "ICLR.cc/2017/conference/paper184/areachairs"], "cdate": 1485287696099}}}, {"tddate": null, "tmdate": 1481932191106, "tcdate": 1481932191106, "number": 2, "id": "S1DiTxMNe", "invitation": "ICLR.cc/2017/conference/-/paper184/official/review", "forum": "SkhU2fcll", "replyto": "SkhU2fcll", "signatures": ["ICLR.cc/2017/conference/paper184/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper184/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "The paper proposed a tensor factorization approach for MTL to learn cross task structures for better generalization. The presentation is clean and clear and experimental justification is convincing. \n\nAs mentioned, including discussions on the effect of model size vs. performance would be useful in the final version and also work in other fields related to this. \n\nOne question on Sec. 3.3, to build the DMTRL, one DNN per-task is trained with the same architecture. How important is this pretraining? Would random initialization also work here? If the data is unbalanced, namely, some classes have very few examples, how would that affect the model?\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483069435180, "id": "ICLR.cc/2017/conference/-/paper184/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper184/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper184/AnonReviewer3", "ICLR.cc/2017/conference/paper184/AnonReviewer2", "ICLR.cc/2017/conference/paper184/AnonReviewer1"], "reply": {"forum": "SkhU2fcll", "replyto": "SkhU2fcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper184/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper184/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483069435180}}}, {"tddate": null, "tmdate": 1481862586571, "tcdate": 1481862586571, "number": 1, "id": "B17a61-Vg", "invitation": "ICLR.cc/2017/conference/-/paper184/official/review", "forum": "SkhU2fcll", "replyto": "SkhU2fcll", "signatures": ["ICLR.cc/2017/conference/paper184/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper184/AnonReviewer3"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper proposed a deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network with tensor factorization and end-to-end knowledge sharing. This approach removed the requirement of a user-de\ufb01ned multi-task sharing strategy in conventional approach. Their experimental results indicate that their approach can achieve higher accuracy with fewer design choices.\n\nAlthough factorization ideas have been exploited in the past for other tasks I think applying it to MTL is interesting. The only thing I want to point out is that the saving of parameter is from the low-rank factorization. In the conventional MTL each layer's weight size can also be reduced if SVD is used. \n\nBTW, recent neural network MTL was explored first (earlier than 2014, 2015 work cited) in speech recognition community. see, e.g., \n\nHuang, J.T., Li, J., Yu, D., Deng, L. and Gong, Y., 2013, May. Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 7304-7308). IEEE.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483069435180, "id": "ICLR.cc/2017/conference/-/paper184/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper184/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper184/AnonReviewer3", "ICLR.cc/2017/conference/paper184/AnonReviewer2", "ICLR.cc/2017/conference/paper184/AnonReviewer1"], "reply": {"forum": "SkhU2fcll", "replyto": "SkhU2fcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper184/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper184/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483069435180}}}, {"tddate": null, "tmdate": 1481584209144, "tcdate": 1481584048817, "number": 1, "id": "S1t36s3Xg", "invitation": "ICLR.cc/2017/conference/-/paper184/public/comment", "forum": "SkhU2fcll", "replyto": "r1eT6NJ7g", "signatures": ["~Yongxin_Yang1"], "readers": ["everyone"], "writers": ["~Yongxin_Yang1"], "content": {"title": "To AnonReviewer2", "comment": "\nQuestion 1: Model capacity and performance.\n\nWe list the number of parameters for each model in the first experiment (MNIST, binary one-vs-rest classification, 1% training data) and the performance (mean of error rates for 10-fold).\n\n+--------------+------------+--------------+-------+\n| Model        | Error Rate | # of param   | Ratio |\n+--------------+------------+--------------+-------+\n| STL          | 11.52      | 4351K        | 1.0   |\n+--------------+------------+--------------+-------+\n| DMTRL-LAF    | 8.25       | 1632K        | 0.38  |\n+--------------+------------+--------------+-------+\n| DMTRL-Tucker | 9.24       | 1740K        | 0.40  |\n+--------------+------------+--------------+-------+\n| DMTRL-TT     | 7.31       | 2187K        | 0.50  |\n+--------------+------------+--------------+-------+\n| UD-MTL       | 9.34       | 436K         | 0.10  |\n+--------------+------------+--------------+-------+\n| UD-MTL-Large | 9.39       | 1644K        | 0.38  |\n+--------------+------------+--------------+-------+\n\nThe conventional hard-sharing method (UD-MTL) design is to share all layers except the top layer. Its number of parameter is roughly the 10% of single task learning method (STL), as most parameters are shared across the 10 tasks corresponding to 10 digits. Similarly, all our soft-sharing methods significantly reduce the number of parameters compared to STL.\n\nTo directly compare our method to UD-MTL, while controlling for network capacity, we expanded UD-MDL by adding more hidden neurons so its number of parameter is close to our methods (Noted as UD-MTL-Large). However UD-MDL performance does not increase. This is evidence that our model's good performance is not simply due to greater capacity than UD-MTL.\n\nFor most contemporary deep learning methods, the number of parameters per-se is not the key factor determining performance, but whether your method can effectively train all those parameters without overfitting, etc. Therefore another way to interpret this result is that DMTRL provides a more effective approach to training many parameters than UD-MTL.\n\nNevertheless we emphasise that our focus is on comparison to STL, compared to which our model is much smaller. Recall that our UD-MTL architecture is chosen by brute force search over many possible architectures. Thus our perspective is that surpassing STL - and even just matching UD-MTL performance - is an interesting achievement for DMTRL, since the sharing is found automatically rather than by brute force search, which is not scalable for larger problems.\n\n-------\n\nQuestion 2: Re: \"Cluster Adaptive Training for Deep Neural Network Based Acoustic Model\".\n\nFormalisation: Their formulation is similar to our simplest model, DMTRL-LAF. Compare their model in Page 4, Eq.19 with our Eq.2 in Page 3. However, we go further by modelling the structure of higher-order tensors, rather than simply flattening the tensor and treating it as a vector. Thus we proposed DMTRL-Tucker and DMTRL-TT which usually outperform DMTRL-LAF.\n\nMotivation + Application: Our factorisation approach can indeed be used beyond MTL. For example, in the mentioned paper, they demonstrated that, for a new task, it is possible to only train the coefficients of base models (\\lambda in their Eq.19, or \"S\" in our Eq.2. In fact we showed something similar but even more interesting in our previous ICLR'15 paper on (shallow) MTL, \"A Unified Perspective on Multi-Domain and Multi-Task Learning\". By encoding tasks as semantically descriptive vectors, we can generate a model for unseen problem on-the-fly (i.e., without training) by building a parameterization network that maps the task description vector to \"S\" (Eq.1 in page 3 of https://arxiv.org/pdf/1412.7489v3.pdf). Thus the deep models developed in this submission are compatible with both these ideas of generating models with minimal or no effort. We didn't write this into our submission because of the volume limit and to keep the paper focused."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287696099, "id": "ICLR.cc/2017/conference/-/paper184/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkhU2fcll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper184/reviewers", "ICLR.cc/2017/conference/paper184/areachairs"], "cdate": 1485287696099}}}, {"tddate": null, "tmdate": 1480703416192, "tcdate": 1480703416186, "number": 1, "id": "r1eT6NJ7g", "invitation": "ICLR.cc/2017/conference/-/paper184/pre-review/question", "forum": "SkhU2fcll", "replyto": "SkhU2fcll", "signatures": ["ICLR.cc/2017/conference/paper184/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper184/AnonReviewer2"], "content": {"title": "model size and cluster adaptive training ", "question": "1) As the proposed soft-sharing utilizes task dependent weights, it would be great to discuss the increase of model size compared to the conventional hard-sharing. \n\nFurthermore, experiments that shown the gains of the proposed method are not coming from the extra params would make the paper more convincing. \n\n\n2) The paper focus on MTL, but the factorization approach can be used more generically, not restricted to MTL tasks. A simple way is to cluster the training data and each cluster can be your \"task\". In speech community, there's some work on cluster adaptive training of DNN, which does similar factorization for each DNN layer, such as \"Cluster Adaptive Training for Deep Neural Network Based Acoustic Model\" (http://ieeexplore.ieee.org/document/7364207/)\nDiscussions on the similarities or differences between them would be interesting.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Multi-task Representation Learning: A Tensor Factorisation Approach", "abstract": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "pdf": "/pdf/ec8f6654ac93c61aee2c445af25d3b49c6db086c.pdf", "TL;DR": "A multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network.", "paperhash": "yang|deep_multitask_representation_learning_a_tensor_factorisation_approach", "conflicts": ["qmul.ac.uk"], "authorids": ["yongxin.yang@qmul.ac.uk", "t.hospedales@qmul.ac.uk"], "keywords": [], "authors": ["Yongxin Yang", "Timothy M. Hospedales"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959419451, "id": "ICLR.cc/2017/conference/-/paper184/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper184/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper184/AnonReviewer2"], "reply": {"forum": "SkhU2fcll", "replyto": "SkhU2fcll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper184/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper184/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959419451}}}], "count": 12}