{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488274705238, "tcdate": 1478271195459, "number": 189, "id": "HyQJ-mclg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HyQJ-mclg", "signatures": ["~Aojun_Zhou2"], "readers": ["everyone"], "content": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396421631, "tcdate": 1486396421631, "number": 1, "id": "rJRbnfLdg", "invitation": "ICLR.cc/2017/conference/-/paper189/acceptance", "forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The paper presents a method for iterative quantization of neural networks weights to powers of 2. The technique is simple, but novel and effective, with thorough evaluation on a variety of ImageNet classification models.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396422147, "id": "ICLR.cc/2017/conference/-/paper189/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396422147}}}, {"tddate": null, "tmdate": 1484607699750, "tcdate": 1484399531499, "number": 7, "id": "HJQ3XoPIe", "invitation": "ICLR.cc/2017/conference/-/paper189/public/comment", "forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "signatures": ["~Aojun_Zhou2"], "readers": ["everyone"], "writers": ["~Aojun_Zhou2"], "content": {"title": "Paper update: all required result comparisons have been added!", "comment": "Thanks to all the reviewers for constructive suggestions and comments. We are really excited that the novelty of our paper has been well recognized.\n\nIn this updated version, we carefully considered all reviewers\u2019 suggestions to improve the paper. Generally, we performed four aspects of works: (1) the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1] was incorporated into the paper (please see Section 3.4 for details); (2) the result comparison of weight quantization between our method and vector quantization [2] was also incorporated into the paper (please see Section 3.4 for details); (3) we tried our best to improve the clarifications of our encoding method for weight quantization, definition of bit-width, detailed experimental settings and so on, and several rounds of proof-reading and revising were also conducted; (4) more experimental results (including the statistical analyses on the distribution of weights after quantization and our latest progress on developing INQ for deep CNNs with low-precision weights and low-precision activations) that reviewers may be interested were added to the paper as the supplementary materials.\n \nMoreover, to make our work fully reproducible, the code (along with an instruction manual) will be released to public as we promised in the paper submission.\n\n(1) To reviewer 1:\n\nQuestion1: \u201cAlso, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.\u201d\n\nFollowing your suggestion, we added detailed parameter settings (such as splitting ratio and etc.) to the respective sets of experiments described in Section 3 accordingly.\n\nQuestion 2: \u201cThe paper could use another second pass for writing style and grammar.\u201d\n\nFollowing your suggestion, we tried our best to do a much better work on revising and proof-reading, with the helps from the native colleagues in USA.\n\n(2) To reviewer 2:\n\nThanks for your recognition of the novelty of our method. We believe that our responses posted on Dec. 16, 2016 should well address your concern on the result comparison of pruning + quantization between our method and Han et al.\u2019s method [1]. Furthermore, detailed result comparisons can be found in Section 3.4 of the paper. It can be clearly seen that our method outperforms Han et al.\u2019s method [1] with significant margins.\n\n(3) To reviewer 3:\n\nQuestion 1: \u201c1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\u201d\n\nFollowing your suggestion, we incorporated related results into the paper (please see Section 3.4 for details).\n\nQuestion 2: \u201cIt would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2).\u201d\n\nFollowing your suggestion, we revised related parts, especially the clarification of our encoding method based on our previous responses to your questions accordingly (please see Section 2.1 for details).\n\nQuestion 3: \"The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where: - 0 is represented with 1 bit, e.g. 0 - other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\u201d\n\nFollowing your suggestion, we made a clear clarification on the definition of bit-width accordingly.\n\nReferences:\nSong Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115v1, 2014."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287693265, "id": "ICLR.cc/2017/conference/-/paper189/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyQJ-mclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper189/reviewers", "ICLR.cc/2017/conference/paper189/areachairs"], "cdate": 1485287693265}}}, {"tddate": null, "tmdate": 1484535461186, "tcdate": 1481867791214, "number": 6, "id": "B1PMMZ-4l", "invitation": "ICLR.cc/2017/conference/-/paper189/public/comment", "forum": "HyQJ-mclg", "replyto": "Hk1EThxNl", "signatures": ["~Aojun_Zhou2"], "readers": ["everyone"], "writers": ["~Aojun_Zhou2"], "content": {"title": "RE: Questions of\u00a0AnonReviewer2", "comment": "Thanks for your recognition of the novelty of our idea. To your concerns, we could immediately provide some combined result comparison that you are interested in, please see our following responses for details.\n\nQuestion1:  \u201cNice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare\u201d\n\nOur responses are in five folds: \n\n(1) Actually, we did some related experiments that you are interested using a combination of pruning and quantization. However, since this paper mainly addresses the problem of how to efficiently convert any floating-point CNN model into a lossless low-precision version whose connection weights are constrained to be either powers of two or zero, we did not put this part of result comparison into the original paper. To the best of our knowledge, with the proposed INQ, we provided comprehensive quantization results on ImageNet using almost all existing deep CNN models including AlexNet, VGG-16, GoogleNet and ResNets. Another reason why we did not compare our INQ with deep compression [Han et al. 2016] is that network pruning and scalar quantization (i.e., the 1st and 2nd modules in deep compression) do not work well on Conv. layer (details will be described later), however recent GoogleNet and ResNet architectures are fully convolutional (excluding last classification layer);\n\n(2) Since it is not fair to directly compare our INQ with Han et al. 2016, we compared them in a more fair way just as you suggested. Note that Han et al. 2016 is a hybrid compression solution combining three different techniques, namely network pruning [Han et al. 2015], scalar quantization [Gong et al. 2014] and Huffman encoding. First, network pruning of Han et al. 2015 can achieve 9X compression (as can be seen in the Table 4 of Han et al. 2016) for CNN model like AlexNet, however the compression is mainly obtained from the FC layers. Actually its performance on Conv. layers is <3X (as can be seen in the Table 4 of Han et al. 2016). Besides, Han et al.\u2019s pruning method is run by separately performing pruning and re-training in an iterative way, which is very time-consuming, it will cost at least several weeks for processing AlexNet. We solved this problem in our recent NIPS work [Guo et al., 2016], obtaining ~7X speed-up in training. Besides, in our NIPS work, we also improved compression performance of network pruning from 9X to 18X. Second, in Han et al. 2016, scalar quantization [Gong et al. 2014] further improves the compression from 9X to 27X. Finally, Huffman encoding brings additional gain (from 27X to 35X) in compression. Taking AlexNet as an instance for comparison, when combing the proposed INQ with our network pruning method [Guo et al., 2016], we achieved much better compression results compared with Han et al. 2016. Specifically, with 5-bit quantization, we can achieve 53X compression with slightly improved top-5 and top-1 accuracy (i.e., no accuracy loss), yielding 51.43%/96.30% improvement in compression performance compared with Han et al.\u2019s full solution/fair solution (i.e., combination of pruning and scalar quantization only); \n\n(3) We also did related experiments on AlexNet to compare the performance of our INQ and scalar quantization [Gong et al. 2014]. For scalar quantization (for fair comparison, re-training was also used to enhance its performance), we set the number of cluster centers for each of 5 Conv. layers and 3 FC layers to 32. In the experiment, scalar quantization incurred >3% accuracy loss. When we changed the number of cluster centers for Conv. layers from 32 to 128, scalar quantization got an accuracy loss of 0.98%. This is consistent with Gong et al.\u2019s paper that scalar quantization is mainly proposed to compress the parameters in the densely connected layers (i.e., FC layers) of a CNN model. Compared with our INQ which addresses all layers of a CNN model, and achieved improved accuracy with 5-bit and 4-bit quantization, we believe that our method is much better than scalar quantization;\n\n4) The final weights obtained from scalar quantization [Gong et al. 2014], network pruning [Han et al. 2015] and combined solution [Han et al. 2016] are still floating-point values, but in our INQ, the final weights are in the form of either powers of two or zero. Comparatively, the direct advantage of our INQ is that the original floating-point multiplication operations can be replaced by binary bit shift operations on dedicated hardware like FPGA; \n\n(5) As we described in the conclusion section of our original paper, besides low-precision weights, we also plan to make the outputs (i.e., activations) of all layers of a CNN model into low-precision versions (i.e., either powers of two or zero). In this way, all complex floating-point multiplication operations can be replaced by binary bit shift operations both with software and hardware. Recently, we have already made great progress in this task. Taking VGG-16 as an instance, with 5-bit weights and 4-bit activations, our INQ achieved improved top-5 and top-1 recognition rates (1.64%/1.16 gain in top-1/top-5 accuracy) compared with original VGG-16 reference with floating-point weights and floating-point activations. To the best of our knowledge, this should be the best results so far reported in the field.\n\nDefinitively, we will update our paper with more related comparisons if they are necessary.\n\nHope our above responses are helpful to address your concerns.\n\nReferences:\nSong Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.\nSong Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks. NIPS, 2015.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.\nYiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. NIPS, 2016."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287693265, "id": "ICLR.cc/2017/conference/-/paper189/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyQJ-mclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper189/reviewers", "ICLR.cc/2017/conference/paper189/areachairs"], "cdate": 1485287693265}}}, {"tddate": null, "tmdate": 1484468951601, "tcdate": 1481808884990, "number": 4, "id": "HJal2GlNx", "invitation": "ICLR.cc/2017/conference/-/paper189/public/comment", "forum": "HyQJ-mclg", "replyto": "HJeiK0kQx", "signatures": ["~Aojun_Zhou2"], "readers": ["everyone"], "writers": ["~Aojun_Zhou2"], "content": {"title": "RE: Questions of\u00a0AnonReviewer1", "comment": "Thanks for your valuable comments and suggestions. In the past week, we performed comprehensive analyses and experiments accordingly.\n\nQuestion1:  \u201cCould you comment on the distribution of weights after quantization? For instance, for 5 bit quantization, how often did the weights assume the most extreme values?\u201d\n\nWe made statistical analyses on the distribution of quantized weights accordingly. Taking AlexNet as an instance, for 5-bit quantization: (1) in the 1st Conv. layer, the quantized weights fall into the set of {-2^-8(5.04%, this is the accumulated portion), -2^-7(6.56%), -2^-6(9.22%), -2^-5(10.52%), -2^-4(9.75%), -2^-3(4.61%), -2^-2(0.67%), 0(5.51%), 2^-8(5.20%), 2^-7(6.79%), 2^-6(9.99%), 2^-5(11.15%), 2^-4(10.14%), 2^-3(4.26%), 2^-2(0.60%)}, that is, the values of {-2^-6, -2^-5, -2^-4, 2^-6, 2^-5, 2^-4} occupy over 60% of all quantized weights; (2) in the 2nd Conv. layer, the quantized weights fall into the set of {-2^-8(10.55%), -2^-7(12.09%), -2^-6(13.08%), -2^-5(8.73%), -2^-4(2.70%), -2^-3(0.39%), -2^-2(0.01%), 0(11.30%), 2^-8(9.70%), 2^-7(11.01%), 2^-6(11.05%), 2^-5(6.57%), 2^-4(2.26%), 2^-3(0.53%), 2^-2(0.05%), 2^-1(0.0003%)}, that is, the values of {-2^-8, -2^-7, -2^-6, -2^-5, 0, 2^-8, 2^-7, 2^-6, 2^-5} occupy over 94% of all quantized weights; (3) the distributions of the quantized weights in the 3rd, 4th and 5th Conv. layers are similar to that of the 2nd Conv. layer. Besides, more weights are quantized into zero in the 2nd, 3rd, 4th and 5th Conv. layers compared with the 1st Conv. layer; (4) in the 1st FC layer, the quantized weights fall into the set of {-2^-10(8.95%), -2^-9(12.29%), -2^-8(16.48%), -2^-7(10.84%), -2^-6(0.79%), -2^-5(0.002%), 0(8.86%), 2^-10(8.30%), 2^-9(10.51%), 2^-8(12.91%), 2^-7(8.95%), 2^-6(1.12%), 2^-5(0.01%), 2^-5(0.00001%)}, that is, the values of {-2^-10, -2^-9, -2^-8, -2^-7, 0, 2^-10, 2^-9, 2^-8, 2^-7} occupy about 98% of all quantized weights; (5) the distributions of the quantized weights in the 2nd and 3rd FC layers are similar to that of the 1st FC layer. Generally, for AlexNet, the distributions of the quantized weights in the Conv. layers are usually more scattered compared with the FC. layers, this may be partially the reason why it is much easier to get good parameter compression results on FC layers in comparison to Conv. layers when using methods such as SVD, hashing [Chen et al. 2015] and scalar quantization [Gong et al. 2014].\n\nAnother thing we want to emphasize here is that, in 5-bit quantization, we use 1 bit to represent zero value only (since we cannot write zero into a power of 2), and another 4 bits to represent the quantized values of powers of two. Here, the number of the quantized values of powers of two and zero is at most 17 (i.e., 16+1), but for the most of layers of a CNN model, it is usually less than 17. Taking AlexNet as an instance, the number of the quantized values of powers of two and zero is 15, 16, 16, 15, 14, 14, 14 and 14 for layers from 1 to 8, respectively.\n\nDetailed analyses on the distribution of weights after quantization will be added to the paper as the part of supplementary material.\n\n\nQuestion2: \u201cDid you compare quantization to powers of two vs. scalar quantization (uniform spacing in some interval) at the same bit-rate?\u201d\n\nOur responses are in two aspects: (1) in the paper, we cited scalar quantization [Gong et al. 2014], but we did not compare our proposed INQ with it. The reasons are in two folds. First, scalar quantization is mainly proposed to compress the parameters in the densely connected layers (i.e., FC layers) of a CNN model. As reported in the paper of Gong et al. 2014, at the best performance, scalar quantization will introduce within 1% accuracy loss when setting 16 centers in clustering. However, our INQ addresses all layers of a CNN model, and it can achieve lossless compression when using 5-bit, 4-bit and 3-bit for quantization, and it can also achieve competitive accuracy when using 2-bit for ternary quantization (we updated our results in the paper). Second, the final weights obtained from scalar quantization are still floating-point clustered centers, but in our INQ, the final weights are in the form of either powers of two or zero. Comparatively, the direct advantage of our INQ is that the original floating-point multiplication operations can be replaced by binary bit shift operations on dedicated hardware like FPGA; (2) following your suggestion, we also did related experiments on AlexNet to compare the performance of our INQ and scalar quantization. For scalar quantization (for fair comparison, re-training was also used to enhance its performance), we set the number of cluster centers for each of 5 Conv. layers and 3 FC layers to 32. In the experiment, scalar quantization incurred >3% accuracy loss. When we changed the number of cluster centers for Conv. layers from 32 to 128, scalar quantization got an accuracy loss of 0.98%. This is consistent with Gong et al.\u2019s paper that scalar quantization is mainly proposed to compress the parameters in the densely connected layers of a CNN model. Compared with our INQ which achieved improved accuracy with 5-bit (also 4-bit and 3-bit) quantization, we believe that our method is better than scalar quantization.\n\nDetailed result comparisons will be added to the paper as the part of the supplementary materials.\n\nReferences:\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.\nWenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing neural networks with the hashing trick. ICML, 2015."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287693265, "id": "ICLR.cc/2017/conference/-/paper189/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyQJ-mclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper189/reviewers", "ICLR.cc/2017/conference/paper189/areachairs"], "cdate": 1485287693265}}}, {"tddate": null, "tmdate": 1484466523449, "tcdate": 1481809041826, "number": 5, "id": "S1qqnzgNx", "invitation": "ICLR.cc/2017/conference/-/paper189/public/comment", "forum": "HyQJ-mclg", "replyto": "rkPvvgyme", "signatures": ["~Aojun_Zhou2"], "readers": ["everyone"], "writers": ["~Aojun_Zhou2"], "content": {"title": "RE: Questions of\u00a0AnonReviewer3", "comment": "\nWe really appreciate your insightful comments and suggestions. In the past 1.5 weeks, we performed comprehensive analyses and experiments accordingly.\n\n\nQuestion1: \u201cCan you compare to more existing methods than just to BWN and TWN in Table 4? For example, a very relevant one is Han et al. 2016. Looking at your Table 1, AlexNet ref 32-bit versus AlexNet 5-bit, the relative size of the networks is 5/32=16%. Comparing this to their figure 6, it seems that at 16% they get 0% accuracy loss. So, is this method really worth it?\u201d\n\nOur responses are in four folds: (1) following your suggestion, we shall add more existing methods for comparison. However, we argue that it is not fair to directly compare our INQ with Han et al. 2016, so we compared them in a more fair way as described below. Note that Han et al. 2016 is a hybrid compression solution combining three different techniques, namely network pruning [Han et al. 2015], scalar quantization [Gong et al. 2014] and Huffman encoding. First, network pruning of Han et al. 2015 can achieve about 9X compression for CNN model like AlexNet, however the compression is mainly obtained from the FC layers, actually its performance on Conv. layers is <3X (as can be seen in the Table 5 of Han et al. 2015). Besides, Han et al.\u2019s pruning method is run by separately performing pruning and re-training in an iterative way, which is very time-consuming, usually in several weeks or even months. We solved this problem in our recent NIPS work [Guo et al., 2016], obtaining ~7X speed-up in training. Besides, in our NIPS work, we also improved compression performance of network pruning from 9X to 18X. Second, in Han et al. 2016, scalar quantization [Gong et al. 2014] further improves the compression from 9X to 27X (as can be seen in the Figure 1 of Han et al. 2016). However, scalar quantization is mainly proposed to compress parameters in the densely connected layers (i.e., FC layers) of a CNN model, and it will introduce within 1% accuracy loss when using a decent number of clustering centers. When using it to compress the parameters in the Conv. layers with a relevant compression ratio (e.g., 4X), it will introduce >3% accuracy loss (details can be seen in our responses to following Question 2). Finally, Huffman encoding only brings little gain (from 27X to 35X) in compression compared with the other two techniques; (2) in this work, our proposed INQ method mainly address the problem of how to efficiently (the whole procedure of our INQ usually costs <1 day for processing AlexNet, VGG-16, GoogleNet and ResNets) convert floating-point weights of all layers of an arbitrary CNN model into either powers of two or zero without accuracy loss, and its performance has been extensively evaluated on almost all existing deep CNN models including recent residual networks which have no FC layer (excluding classification layer), using 5-bit, 4-bit and 3-bit quantization. Another advantage is the original complex floating-point multiplication operations can be replaced by binary bit shift operations on dedicated hardware like FPGA; (3) taking AlexNet as an instance, when we combing our INQ with our network pruning method [Guo et al., 2016], we achieved much better compression results compared with Han et al. 2016. Specifically, we can achieve 53X to 89X compression with slightly improved or very similar top-5 and top-1 accuracy, yielding significant margins compared with Han et al.\u2019s work; (4) besides, with our INQ, we also made great progress on developing lossless CNNs with low-precision weights and low-precision activations (details as can be seen in our responses to following Question 2). \n\nTherefore, it is clear that our proposed INQ is definitively worthwhile.\n\nMore comparative results will be added to the paper.\n\n\nQuestion2: \u201cWhy did you stick to weights as powers of 2, why not simply do k-means?\u201d \n\nThe reasons are in three folds: (1) with k-means as did in scalar quantization [Gong et al. 2014], the final weight values are still in the floating-point. The advantage of making the weights as either powers of two or zero by our INQ is that the original complex floating-point multiplication operations can be replaced by binary bit shift operations on dedicated hardware like FPGA. That is, promising speed up can be achieved; (2) k-means is more suitable for compressing parameters in the FC layers of a CNN model, and it will introduce ~1% accuracy loss when using a decent number of clustering centers, such as 16 in the paper of [Gong et al. 2014]. What\u2019s more, we also did related experiments on AlexNet to compare the performance of our INQ and scalar quantization. For scalar quantization (for fair comparison, re-training was also used to enhance its performance), we set the number of cluster centers for each of 5 Conv. layers and 3 FC layers to 32. In the experiment, scalar quantization incurred >3% accuracy loss. When we changed the number of cluster centers for Conv. layer from 32 to 128, scalar quantization got an accuracy loss of 0.98%. Compared with our INQ which achieved improved accuracy with 5-bit, 4-bit and 3-bit quantization, we believe that our method is better than scalar quantization; (3) as we described in the conclusion section of our paper, besides low-precision weights, we also plan to make the outputs (i.e., activations) of all layers of a CNN model into low-precision versions (i.e., either powers of two or zero). In this way, all complex floating-point multiplication operations can be replaced by binary bit shift operations both with software and hardware. Fortunately, we have already made great progress in this task. Taking VGG-16 as an instance, with 5-bit weights and 4-bit activations, our INQ achieved improved top-5 and top-1 recognition rates (>1% accuracy gain) compared with original VGG-16 reference with floating-point weights and floating-point activations. To the best of our knowledge, this should be the best results reported so far.\n\nDetailed result comparisons will be added to the paper as the part of the supplementary materials.\n\n\nQuestion3: \u201cRegarding weight partitioning - you tried only random partitioning and partitioning based on the absolute value. Doesn't it make sense to also partition based on quantization error? You can just find the set of weights that are most similar to values they would be quantized to, and tune the rest.\u201d\n \nThanks for your constructive suggestion, our responses are in two folds: (1) the reason why we tried only random partitioning and partitioning based the absolute value of the weight is mainly inspired by our recent network pruning work published in NIPS [Guo et al. 2016]. In our NIPS work, we tried a lot of different solutions on identifying weight importance, and finally found the two solutions we used in this work are the best choices compared with the others. Thus in this paper, we directly tried random portioning and partitioning based on the absolute value of the weight; (2) we think your suggestion for partitioning with quantization error is insightful, so we made some experiments on AlexNet to explore its potential. However, until so far we have not obtained good results as our INQ can do. We shall continue to tune it.\n\n\nQuestion4: \u201cI'm not completely clear about the formulation that is repeated a few times \"5-bit (4 bits for powers of two plus 1 bit for zero)\" ? Does it mean that out of the possible 2^5 values that can be expressed with 5 bits, one is a zero, others are +- powers of 2, and one is unused? Or are you talking about some variable length encoding (though in this case the representation wouldn't be 5 bit)?\u201d\n\nOur responses are in two folds: (1) in 5-bit quantization, we use 1 bit to represent zero value only (because we cannot write zero into a power of 2), and another 4 bits to represent the quantized values of powers of two (including sign). Here, the number of the quantized values of powers of two and zero is at most 17 (i.e., 16+1); (2) with 5-bit quantization, taking AlexNet as an instance, the final number of the quantized values of powers of two and zero is 15, 16, 16, 15, 14, 14, 14 and 14 for layers from 1 to 8, respectively. Specifically, taking 1st and 2nd Conv. layers of Alexnet as examples, the quantized values fall into the sets of {-2^-8(5.04%, this is the accumulated portion), -2^-7(6.56%), -2^-6(9.22%), -2^-5(10.52%), -2^-4(9.75%), -2^-3(4.61%), -2^-2(0.67%), 0(5.51%), 2^-8(5.20%), 2^-7(6.79%), 2^-6(9.99%), 2^-5(11.15%), 2^-4(10.14%), 2^-3(4.26%), 2^-2(0.60%)} and {-2^-8(10.55%), -2^-7(12.09%), -2^-6(13.08%), -2^-5(8.73%), -2^-4(2.70%), -2^-3(0.39%), -2^-2(0.01%), 0(11.30%), 2^-8(9.70%), 2^-7(11.01%), 2^-6(11.05%), 2^-5(6.57%), 2^-4(2.26%), 2^-3(0.53%), 2^-2(0.05%), 2^-1(0.0003%)}, respectively. It is clear that the bit length for every layer of AlexNet is actually less than 5 even though each layer has at most 17 candidate values in 5-bit quantization. Therefore, for different layers of a CNN model, there will be variable length encoding if such kind of representation is necessary.\n\n\nQuestion5:  \u201cI think the region between equation 1 the end of section 2.1 needs some clarifications, though I think I understand it, it required a bit of thinking to understand what is going on. Firt, \"bounding the elements in P_l\" below equation 1 is a bit confusing, as the elements are bounded just by [-2^n1, +2^n1] (so no dependence on n2). Second, where does equation 2 come from, it's not explained apart from saying that it is \"tricky\". I would guess n1=round(log2(s)), so where does 4/3 come from? From later on (eq 4) 3/2 is also feasible, but 4/3 just seems random. Then, first paragraph of page 4, shouldn't the last term of the equation for n2 be (2^b - 1)/2, i.e. -1 should be out of the exponent, especially since 2^(b-1)/2 is just 2^(b-2)? Again this is not explained, but my thinking is that: 2^b bits are available for representing these many values: 1 for 0, 2*(n1-n2+1) for both signs of the n1-n2+1 whole numbers that are in the range [n2, n1] So: 2^b = 2*(n1-n2+1)+1, thus n2=n1+1-(2^b-1)/2 Below equation 4, it might be worth saying that 2*beta is the following number in the sequence alpha, beta, 2 beta (apart from when beta=0), in order to understand equation 4 easier. Also, doesn't it make sense to perform this quantization in the log scale instead of in the linear scale? The elements in P_l are distributed uniformly in the log scale (apart from 0) so it seems more natural to quantize that way i.e. something like w_l = 2^b sign(w_l) when .. for alpha and beta being the consecutive exponents.\u201d\n\nWe would like to thank your valuable suggestions first. Here, we address your comments one by one as the follows: (1) hyper-parameters n1 and n2 help to bound P_l in the sense that its non-zero elements are constrained to be in the range of either [-2^n1, -2^n2] or [2^n2, 2^n1]. That is, network weights/connections with absolute values smaller than 2^n2 will be pruned away (i.e., setting to zero) in the final low-precision model; (2) factor 4/3 in Equation (2) is set to make sure that all the elements in P_l (including -2^n1 and 2^n1) correspond with the quantization rule defined in Equation (4). In other words, factor 4/3 in Equation (2) is related to factor 3/2 in Equation (4). If n1 is defined as round(log2(s)), then 2^n1 may be larger than s and at least half of its rounding space will be wasted. Besides, according to experimental results, we found round(log2(s)) will introduce larger quantization error compared with our proposed method. For instance, if s=0.354, it will be quantized into 2^-1 by round(log2(s)) and 2^-2 by our Equation (2). Comparatively, the respective quantization error is 0.146 vs 0.104; (3) for the relationship between n1 and n2 described in the first paragraph of page 4, your understanding is correct. Definitively, we shall add more explanatory sentences for a more clear clarification; (4) we fully agree that making quantization in the log scale is worth exploring, but our concern is that it should be a little bit more difficult to implement it and may cause some extra computational overhead. Thus, we may consider it as part of our future work and report related results after the experimental analyses.\n\nWe updated results in the paper, and more related modifications and results will also be added to the paper.\n\nReferences:\nSong Han, Jeff Pool, John Tran, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. ICLR, 2016.\nSong Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks. NIPS, 2015.\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.\nYiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. NIPS, 2016."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287693265, "id": "ICLR.cc/2017/conference/-/paper189/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyQJ-mclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper189/reviewers", "ICLR.cc/2017/conference/paper189/areachairs"], "cdate": 1485287693265}}}, {"tddate": null, "tmdate": 1484409591139, "tcdate": 1481850150656, "number": 1, "id": "Hk1EThxNl", "invitation": "ICLR.cc/2017/conference/-/paper189/official/review", "forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "signatures": ["ICLR.cc/2017/conference/paper189/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper189/AnonReviewer2"], "content": {"title": "Quantize a fully trained network with an iterative 3 step process of partition/hard quantize/retrain, repeated on the retrained partition until fully quantized. Achieves nice results on ImageNet tasks down to 4 bits, but is missing pruning steps which is needed for large competitive compression.", "rating": "7: Good paper, accept", "review": "Nice idea but not complete, model size is not reduced by the large factors found in one of your references (Song 2016), where they go to 5 bits, but this is ontop of pruning which gives overall 49X reduction in model size of VGG (without loss of accuracy). You may achieve similar reductions with inclusion of pruning (or better since you go to 4 bits with no loss) but we should see this in the paper, so at the moment it is difficult to compare", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512670609, "id": "ICLR.cc/2017/conference/-/paper189/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper189/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper189/AnonReviewer2", "ICLR.cc/2017/conference/paper189/AnonReviewer3", "ICLR.cc/2017/conference/paper189/AnonReviewer1"], "reply": {"forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512670609}}}, {"tddate": null, "tmdate": 1482191649581, "tcdate": 1482191649581, "number": 3, "id": "H1cm7x8Nl", "invitation": "ICLR.cc/2017/conference/-/paper189/official/review", "forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "signatures": ["ICLR.cc/2017/conference/paper189/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper189/AnonReviewer1"], "content": {"title": "Great idea, very impressive results.", "rating": "8: Top 50% of accepted papers, clear accept", "review": "There is a great deal of ongoing interest in compressing neural network models. One line of work has focused on using low-precision representations of the model weights, even down to 1 or 2 bits. However, so far these approaches have been accompanied by a significant impact on accuracy. The paper proposes an iterative quantization scheme, in which the network weights are quantized in stages---the largest weights (in absolute value) are quantized and fixed, while unquantized weights can adapt to compensate for any resulting error. The experimental results show this is extremely effective, yielding models with 4 bit or 3 bit weights with essentially no reduction in accuracy. While at 2 bits the accuracy decreases slightly, the results are substantially better than those achieved with other quantization approaches.\n\nOverall this paper is clear, the technique is as far as I am aware novel, the experiments are thorough and the results are very compelling, so I recommend acceptance. The paper could use another second pass for writing style and grammar. Also, the description of the pruning-inspired partitioning strategy could be clarified somewhat... e.g., the chosen splitting ratio of 50% only seems to be referenced in a figure caption and not the main text.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512670609, "id": "ICLR.cc/2017/conference/-/paper189/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper189/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper189/AnonReviewer2", "ICLR.cc/2017/conference/paper189/AnonReviewer3", "ICLR.cc/2017/conference/paper189/AnonReviewer1"], "reply": {"forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512670609}}}, {"tddate": null, "tmdate": 1481904925197, "tcdate": 1481904925197, "number": 2, "id": "rkHQ7cZNl", "invitation": "ICLR.cc/2017/conference/-/paper189/official/review", "forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "signatures": ["ICLR.cc/2017/conference/paper189/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper189/AnonReviewer3"], "content": {"title": "Reasonable idea", "rating": "7: Good paper, accept", "review": "The idea of this paper is reasonable - gradually go from original weights to compressed weights by compressing a part of them and fine-tuning the rest. Everything seems fine, results look good, and my questions have been addressed.\n\nTo improve the paper:\n\n1) It would be good to incorporate some of the answers into the paper, mainly the results with pruning + this method as that can be compared fairly to Han et al. and outperforms it.\n\n2) It would be good to better explain the encoding method (my question 4) as it is not that clear from the paper (e.g. made me make a mistake in question 5 for the computation of n2). The \"5 bits\" is misleading as in fact what is used is variable length encoding (which is on average close to 5 bits) where:\n- 0 is represented with 1 bit, e.g. 0\n- other values are represented with 5 bits, where the first bit is needed to distinguish from 0, and the remaining 4 bits represent the 16 different values for the powers of 2.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512670609, "id": "ICLR.cc/2017/conference/-/paper189/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper189/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper189/AnonReviewer2", "ICLR.cc/2017/conference/paper189/AnonReviewer3", "ICLR.cc/2017/conference/paper189/AnonReviewer1"], "reply": {"forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512670609}}}, {"tddate": null, "tmdate": 1480743320460, "tcdate": 1480743320456, "number": 2, "id": "HJeiK0kQx", "invitation": "ICLR.cc/2017/conference/-/paper189/pre-review/question", "forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "signatures": ["ICLR.cc/2017/conference/paper189/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper189/AnonReviewer1"], "content": {"title": "Questions", "question": "1. Could you comment on the distribution of weights after quantization? For instance, for 5 bit quantization, how often did the weights assume the most extreme values?\n2. Did you compare quantization to powers of two vs. scalar quantization (uniform spacing in some interval) at the same bit-rate?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959416406, "id": "ICLR.cc/2017/conference/-/paper189/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper189/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper189/AnonReviewer3", "ICLR.cc/2017/conference/paper189/AnonReviewer1"], "reply": {"forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959416406}}}, {"tddate": null, "tmdate": 1480685407226, "tcdate": 1480685407221, "number": 1, "id": "rkPvvgyme", "invitation": "ICLR.cc/2017/conference/-/paper189/pre-review/question", "forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "signatures": ["ICLR.cc/2017/conference/paper189/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper189/AnonReviewer3"], "content": {"title": "Various", "question": "1) Can you compare to more existing methods than just to BWN and TWN in Table 4? For example, a very relevant one is Han et al. 2016. Looking at your Table 1, AlexNet ref 32-bit versus AlexNet 5-bit, the relative size of the networks is 5/32=16%. Comparing this to their figure 6, it seems that at 16% they get 0% accuracy loss. So, is this method really worth it?\n\n2) Why did you stick to weights as powers of 2, why not simply do k-means?\n\n3) Regarding weight partitioning - you tried only random partitioning and partitioning based on the absolute value. Doesn't it make sense to also partition based on quantization error? You can just find the set of weights that are most similar to values they would be quantized to, and tune the rest.\n\n4) I'm not completely clear about the formulation that is repeated a few times \"5-bit (4 bits for powers of two plus 1 bit for zero)\" ? Does it mean that out of the possible 2^5 values that can be expressed with 5 bits, one is a zero, others are +- powers of 2, and one is unused? Or are you talking about some variable length encoding (though in this case the representation wouldn't be 5 bit)?\n\n5) I think the region between equation 1 the end of section 2.1 needs some clarifications, though I think I understand it, it required a bit of thinking to understand what is going on. Firt, \"bounding the elements in P_l\" below equation 1 is a bit confusing, as the elements are bounded just by [-2^n1, +2^n1] (so no dependence on n2). Second, where does equation 2 come from, it's not explained apart from saying that it is \"tricky\". I would guess n1=round(log2(s)), so where does 4/3 come from? From later on (eq 4) 3/2 is also feasible, but 4/3 just seems random. Then, first paragraph of page 4, shouldn't the last term of the equation for n2 be (2^b - 1)/2, i.e. -1 should be out of the exponent, especially since 2^(b-1)/2 is just 2^(b-2)? Again this is not explained, but my thinking is that:\n2^b bits are available for representing these many values: 1 for 0, 2*(n1-n2+1) for both signs of the n1-n2+1 whole numbers that are in the range [n2, n1]\nSo: 2^b = 2*(n1-n2+1)+1, thus n2=n1+1-(2^b-1)/2\nBelow equation 4, it might be worth saying that 2*beta is the following number in the sequence alpha, beta, 2 beta (apart from when beta=0), in order to understand equation 4 easier.\nAlso, doesn't it make sense to perform this quantization in the log scale instead of in the linear scale? The elements in P_l are distributed uniformly in the log scale (apart from 0) so it seems more natural to quantize that way i.e. something like w_l = 2^b sign(w_l) when .. for alpha and beta being the consecutive exponents.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Network Quantization: Towards Lossless CNNs with Low-precision Weights", "abstract": "This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization (a variable-length encoding: 1 bit for representing zero value, and the remaining 4 bits represent at most 16 different values for the powers of two), our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. We believe that our method sheds new insights on how to make deep CNNs to be applicable on mobile or embedded devices. The code will be made publicly available.", "pdf": "/pdf/518904b552874f0bd9b9781d203e13cc1031b9af.pdf", "TL;DR": "This paper presents INQ, targeting to efficiently transform any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose connection weights are constrained to be either powers of two or zero.", "paperhash": "zhou|incremental_network_quantization_towards_lossless_cnns_with_lowprecision_weights", "authors": ["Aojun Zhou", "Anbang Yao", "Yiwen Guo", "Lin Xu", "Yurong Chen"], "keywords": ["Deep learning", "Optimization"], "conflicts": ["intel.com", "tsinghua.edu.cn", "ia.ac.cn"], "authorids": ["aojun.zhou@intel.com", "anbang.yao@intel.com", "yiwen.guo@intel.com", "lin.x.xu@intel.com", "yurong.chen@intel.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959416406, "id": "ICLR.cc/2017/conference/-/paper189/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper189/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper189/AnonReviewer3", "ICLR.cc/2017/conference/paper189/AnonReviewer1"], "reply": {"forum": "HyQJ-mclg", "replyto": "HyQJ-mclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959416406}}}], "count": 11}