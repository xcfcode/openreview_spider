{"notes": [{"id": "BJxWx0NYPr", "original": "BylBHGXdDH", "number": 918, "cdate": 1569439208816, "ddate": null, "tcdate": 1569439208816, "tmdate": 1583912047873, "tddate": null, "forum": "BJxWx0NYPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "BketPXOxiB", "original": null, "number": 3, "cdate": 1573057376691, "ddate": null, "tcdate": 1573057376691, "tmdate": 1576939047562, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "HklUNY5Htr", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment", "content": {"title": "We thank the reviewer for the valuable comments and we have addressed your concerns.", "comment": "First we highly appreciate the valuable comment from the reviewer and we are glad to have received positive comments. The reviewer gives a great summarization of the main idea of our work, and also an interesting suggestion, which allows us to further improve the quality of our work. Our response is as follows.  \n\n\u201cI wonder what happens if one uses only the structural similarity for the attention (without the feature similarity). Are there datasets where this would be sufficient? Even a toy task which is constructed such that the structure is more informative than the features could be a nice way to further demonstrate the idea of the paper.\u201d\n\n---This is a very interesting idea to try, namely, only consider graph connections but without node content. It is related to graph partitioning or community detection, where the grouping is determined only using graph structure and topology (i.e., edge connections between nodes).  Examples include power grid, internet, and some biological networks, just to name a few. Indeed, we anticipate that our method will work very well on such data too. This is because our structural fingerprint for each node serves as a highly robust and informative structural identify descriptor, which can improve the evaluation of the similarities between two nodes. With better node-pair similarities, good clustering result is surely expected in graph partitioning. \n\nBesides, we can also tune the fingerprint size to achieve multi-scale clustering on graphs, which is an interesting future direction. We will start finding related graph data sets and experiment as the reviewer suggested, and to show the power of structural clues/information in graph clustering problems. We have included these discussions in the future work sectionn of our paper. We will also be pursuing these new directions and add results to the future arxiv versions of this paper. \n\nFinally, we would like to thank the reviewer again for your valuable time and comments. "}, "signatures": ["ICLR.cc/2020/Conference/Paper918/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxWx0NYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper918/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper918/Authors|ICLR.cc/2020/Conference/Paper918/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164179, "tmdate": 1576860539726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment"}}}, {"id": "rPClWhIXrf", "original": null, "number": 1, "cdate": 1576798709698, "ddate": null, "tcdate": 1576798709698, "tmdate": 1576800926621, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "BJxWx0NYPr", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper is consistently supported by all three reviewers during initial review and discussions. Thus an accept is recommended.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJxWx0NYPr", "replyto": "BJxWx0NYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705108, "tmdate": 1576800252812, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper918/-/Decision"}}}, {"id": "S1xPXdp2jS", "original": null, "number": 8, "cdate": 1573865503312, "ddate": null, "tcdate": 1573865503312, "tmdate": 1573868483815, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "BJlXMvwgjr", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment", "content": {"title": "Updated review score", "comment": "I thank the authors for addressing my comments. I particularly appreciate the details and experiments added to the paper.  In light of these changes, I have updated my score. "}, "signatures": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxWx0NYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper918/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper918/Authors|ICLR.cc/2020/Conference/Paper918/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164179, "tmdate": 1576860539726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment"}}}, {"id": "HkxhtAD6KS", "original": null, "number": 3, "cdate": 1571810947894, "ddate": null, "tcdate": 1571810947894, "tmdate": 1573865308305, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "BJxWx0NYPr", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "---- Problem setting and contribution summary ----\nThe paper considers the problem of graph node classification in a semi-supervised learning setting. When classifying a node, the decision is based on the node\u2019s features, as well as by a weighted combination of its neighbors (where the weights are computed using a learnt attention mechanism). The authors extend upon the recent Graph Attention Networks (GAT) paper by proposing a different way of computing the attention over the neighboring nodes. This new attention mechanism takes into account not just their feature similarity, but also extra structure information, which also enables their method to attend not only over direct neighbors, but also up to k-hop neighbors.\n\n---- Overall opinion ----\nWhile I believe the general idea indeed has merit and empirically shows great promise, I believe the paper in its current state is not ready for publication. However, I believe that a more thorough revision can lead to an a publication with potential impact on the applications side.\n\n---- Pros ----\n1. The paper is easy to read.\n2. I really appreciated the good visualizations (Figures 2,3,4) that indeed help in understanding the method.\n3. Really good empirical results on the 3 datasets that were presented.\n\n---- Major issues ----\n1. Motivation:  \nI believe the paper is not well motivated from an applications perspective. In section 2.1., the authors did a good job explaining the limitations of current approaches on a generic graph structure, but this is only under the main assumption that a node should attend more to neighbors in its denser community than other neighbors that not connected so strongly (Fig 1). The issue that I have with this is that:\n      a) Why should we take this assumption for granted? What are some concrete practical node classification problems where it is indeed better if a node attends to its neighbors in this way (as opposed to the GAT approach)?\n      b) Even if the above is proven true, suppose node A in Fig. 1(a) attends with equal weights to all its 4 neighbors. That means node C (which is outside its densest community) gets 1/4, while the nodes inside the dense community get a total of \u00be. That means node A puts most of its attention to the dense community anyway. In what conditions is it necessary to bias this attention even further?\n\n2. Experiments: \nWhile the reported accuracies for the three datasets look good and also the authors have provided a link to their code (great to see that!), I believe the experimental section is missing an important amount of details for reproducibility purposes and also for explaining how certain parameters have been chosen:\n    a) There are no details on the model size and training procedure (hidden units, optimizer, learning rate schedule). \n    b) Maybe I am missing this, but I don\u2019t see any reference on what alpha and beta from equation (6) were used in the experiments. \n    c) What is the value of k in Table 2? How did you choose it? I believe Figure 5 shows test accuracies for different k, but I hope the authors did not choose k based on the test set performance. \n    d) How did you select the structural fingerprint to be 3?\n     e) \u201c...optimizing c through the learning process also gives very similar choice\u201d \u2192 how did you optimize c exactly?\n     f) Fig 5 a) Why does increasing the number of hops to 3 or 4 decrease the performance so much? Shouldn\u2019t the attention weights learn to ignore the further neighbors, if they are not useful?  \n\n\tAnother important question regarding experiments: since the ablation study shows that the optimal neighbor range is actually 2, a natural baseline to compare with would be something similar to GAT, where an attention weight is applied to all neighbors within two hops (basically skip the fingerprint step, and assume s_{ij} is 1 for all neighbors within 2 hops, and 0 otherwise).\n\n\tAlso regarding experiments, these 3 datasets, although common across many recent graph node classification papers, they are known to be quite limited (small in size and not very representative of real world). Since GAT is your main competitor, why not also show experiments on the PPI dataset they also test?\n\n3. Writing quality: \nWhile the language is clear and easy to follow, there are many grammatical mistakes throughout the paper (e.g. \u201cbenefitial\u201d, subject-verb agreement).\n\n---- More minor issues ----\n    a) Section 2.1: One could argue that GAT also contains longer range node dependencies through the node embeddings it learns. Since the node embeddings is trained through gradient descent, and at each iteration the embedding of a node changes according to its neighbors, you could say that information does get propagated from the neighbor\u2019s neighbors.\n     b) Why do you need a LeakyRelu in Equation (5) ? Also, aren\u2019t e_{ij} non-negative anyway (in which case LeakyRelu doesn\u2019t change anything)?\n     c) Why would a Sigmoid be a good choice for alpha and beta in Eq. (6)?\n     d) Section 3: A bag of words is typically represented as a binary vector, which is also categorical.\n     e) Please use \\citet when specifically referring to the authors of a paper as part of your sentence (e.g. \u201cFollowing Velickovic et. al., 2017 we\u2026.\u201d as opposed to \u201cFollowing (Velickovic et al., 2017), we....\u201d).\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxWx0NYPr", "replyto": "BJxWx0NYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087528125, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper918/Reviewers"], "noninvitees": [], "tcdate": 1570237745073, "tmdate": 1575087528142, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Review"}}}, {"id": "ryxd7o9hsS", "original": null, "number": 7, "cdate": 1573853983692, "ddate": null, "tcdate": 1573853983692, "tmdate": 1573853983692, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "r1l6ctjijH", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment", "content": {"title": "We highly appreciate the reviewer's decision", "comment": "We would like to thank the reviewer again for the valuable time and comments.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper918/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxWx0NYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper918/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper918/Authors|ICLR.cc/2020/Conference/Paper918/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164179, "tmdate": 1576860539726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment"}}}, {"id": "BJlXMvwgjr", "original": null, "number": 2, "cdate": 1573054218752, "ddate": null, "tcdate": 1573054218752, "tmdate": 1573837862953, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "HkxhtAD6KS", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment", "content": {"title": "We thank the reviewer for the valuable comments and we have addressed your concerns.", "comment": "We feel the reviewer has read our paper in great depth, which is already a reward to our work. We're greatly inspired by the valuable comments, which allow us to better elaborate our idea and obtain interesting new experimental findings.\n\n1.Motivation: \n\n\u201cI believe the paper is not well motivated from an applications perspective. .. under the main assumption that a node should attend more to neighbors in its denser community than other neighbors that not connected so strongly\u201d\n\n---First we want to clarify a subtle difference between our approach and reviewer\u2019s comment. What reviewer summarizes \u201ca node should attend more\u2026\u201d is not on computing the attention yet, but rather on the initial step of building fingerprint (receptive domain) of each node. So, allow us to rephrase to \u201cthe structural fingerprint of a node should emphasize more on densely inter-connected neighbors within predefined vicinity (shown in Fig3).\n\n---*Validity*: similar assumption is prevalent in many learning paradigms: min-cut/normalized-cut minimizes cross connections between two clusters/subgraphs [Shi and Malik, PAMI 00]; low-density-separation assumes cluster boundaries pass through low-density regions, leading to successful semi-supervised clustering [Chapelle and Zien AI&STAT 05]; mean-shift algorithm explicitly finds local density peaks as clusters [Comaniciu and Meer, PAMI 02]. These works are cited tens of thousands of times, reflecting wide acceptance of the assumption that \u201cstructural\u201d clues such as dense-subgraphs/high-density-regions are valuable for clustering.   \n\n---*Consequence*: due to this assumption, fingerprint (receptive field) of each node can be constructed more accurately. This is because neighbor nodes in higher-density regions, which are more likely from the same cluster (by references above), will have higher weights in our fingerprint. So, structural relation (attention) between two nodes can be estimated more accurately, leading to better accuracy.\n\n---Finally, we also proposed Gaussian-decay to build fingerprint. It makes no assumption but ''closer nodes are more important''. It effectively improves GAT too. Clearly, key to both success is the use of structural information to improve depiction of a node. \n\nWe added these in Sec2.1 (last 2 paragraphs); Sec2.3 (last paragraph)\n \n\n\u201ca) Why should we take this assumption for granted\u2026\u201d\n\n---The assumption has been used widely in clustering/semi-supervised learning. Besides, we did NOT directly enforce it in attention and require \"one node has to attend high-density neighbors more\", instead, we used it to build more accurate fingerprints, which then improves accuracy of subsequent attention computation (which is a bit subtle).\n\n\"b) ...suppose node A in Fig. 1(a)...?\"\n\n---This pic was meant to show node distance may not reflect true closeness good enough. If one wants to link it to attention, one can put it this way: fingerprint of A and B emphasize more on left subgraph; that for C will emphasize right subgraph. So, structure similarity between A and C is small, while for A and B is large.\n\n\n\n2. Experiments\n\n\u201ca) There are no details on the model size\u201d \n\n---Our setting is the same as GAT; we added detailes now in sec4.1.  \n\n\u201c b) \u2026reference on alpha and beta from eq (6)\u201d. \n\n---we used scalar alpha/beta. \n\n\u201c c) What is the value of k in Table2?\u201d\n\n--- K = 3. It is not chosen by looking into Fig5a (there, best K is 2) \n\n\u201cd) How did you select the structural fingerprint to be 3?\u201d \n\n---Number of K-hop neighbors increases quickly with K. For efficiency, we restrict K to 3;  larger K gives more information but more costly.  \n\n\u201ce) how did you optimize c\u201d? \n\n---Back-propagation\n\n\u201cf) Fig 5 a) Why does increasing number of hops to 3 or 4 decrease \u2026Shouldn\u2019t attention weights learn to ignore further neighbors\u2026\u201d\n\n---Our method surely learns when to ignore far-away neighbors. But in this ablation study, we varied K while Fixing c=0.5, the parameter controlling effective size of fingerprint, as stated in original submission (last line, page 7). In final experiments (Table 2), we optimized c anyway. \n\n\u201ca natural baseline...would be GAT applied to all neighbors within two\u2026\u201d \n\n---Great suggestion! We have tried GAT with 2/3-hop neighbors; its performance drops.  Namely, GAT prefers small neighbors (1-hop neighbors). This reslt shows that our improvement is not just due to large neighbors, but rather the useful structural clues extracted in our method. We added these in the supplementary section.\n\n\u201cwhy not also show experiments on PPI dataset\u201d \n\n---Preliminary one round result is promising (>97% accuracy). Yet, due to large data size and limited computing resource, we cannot finish 10 repeated runs on time (to compute average performance), so we cannot include this incomplete result. We will add them in future arxiv versions.\n\n\n\n3. Minor comments \n\n---we have revised accordingly, and  proof-read the paper and corrected typos. We highly appreciate the reviewer for the detailed checking."}, "signatures": ["ICLR.cc/2020/Conference/Paper918/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxWx0NYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper918/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper918/Authors|ICLR.cc/2020/Conference/Paper918/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164179, "tmdate": 1576860539726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment"}}}, {"id": "r1l_HDiojB", "original": null, "number": 5, "cdate": 1573791552508, "ddate": null, "tcdate": 1573791552508, "tmdate": 1573792330382, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "BJlXMvwgjr", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment", "content": {"title": "Brief summary of our revisions", "comment": "For reviewer's convenience, we asummarize our revisions as follows\n\n1. Adding more references supporting the assumption that ''dense-connections/high-density regisions'' are valuable structural clues for clustering (normalized cut, low-density separation, and mean shift algorithm); related discussions are added to the last 2 paragraphs of Sec 2.1, and last paragraph of Sec 2.3. \n\n2. We have added a new paragraph in sec4.1 (2nd paragraph) describing the detailed model setting, which is basically the same as in the GAT framework, as well as the choice of the neighborhood size\n\n3. We have added a section in the supplementary material, studying the performance of the GAT method when larger neighborhood is used (i.e., 2-hop neighbors), as suggested by the reviewer.\n\n4. We have clarified the discussion of the ablation study in Sec 4.2, in particular for Fig6(a), namely, the results here are obtained when fixing c, the restart probability, to be 0.5, in order to better study the impact of k (neighbor-degree).\n\n5. We have thoroughly proofread the paper and corrected typos and format of references\n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper918/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxWx0NYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper918/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper918/Authors|ICLR.cc/2020/Conference/Paper918/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164179, "tmdate": 1576860539726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment"}}}, {"id": "H1eSNZHrKr", "original": null, "number": 1, "cdate": 1571275053239, "ddate": null, "tcdate": 1571275053239, "tmdate": 1573792182329, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "BJxWx0NYPr", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This work suggests a graph structure based methodology to augment the attention mechanism of graph neural networks.\n\nThe main idea is to explore the interaction  between different types of nodes of the local neighborhood of a root node. One component of this \"fingerprinting\" is a node weighting (closeness) that is computed using various methods proposed in the paper. The fingerprint is the vector of weights of all nodes in the local neighborhood. \n\nFor computing a closeness metric, the paper suggests random walks with restart, which has generally been used for graph clustering. Once the relative closeness to a node in the neighborhood is measured, the symmetric \"structural interaction\" between the fingerprint of two nodes is given by the Jaccard similarity of their structural fingerprints (or a smooth alternative thereof).  This structural similarity will be then considered in a (multi-head) attention mechanism using learned transfer functions.\n\nThe efficacy of the proposed method is tested on the Cora, Citeseer and Pubmed node classification benchmarks and compares favorably to non-augmented graph neural networks, beating all baselines on those datasets. Also the results in the paper beat those of the GResNet on Cora, but not on PubMed and , which is a recent paper not cited by this work.\n\nIn general this work goes into the direction of adding hand-engineered features to DeepLearning approaches. I am not a big fan of these methods, especially without significant theoratical justification. The approach is well motivated but very heuristical. Still the results presents a significant improvement of SOTA on those benchmark and the paper presents ideas  that seem to be generally useful for processing large-scale, structure-rich graph date. Hence I am in the favor of acceptance of this paper.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxWx0NYPr", "replyto": "BJxWx0NYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087528125, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper918/Reviewers"], "noninvitees": [], "tcdate": 1570237745073, "tmdate": 1575087528142, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Review"}}}, {"id": "r1l6ctjijH", "original": null, "number": 6, "cdate": 1573792148634, "ddate": null, "tcdate": 1573792148634, "tmdate": 1573792148634, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "Sygd55Olir", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment", "content": {"title": "Borderline", "comment": "Thanks a lot for the updates. Still, my opinion stands. \n\nI find the paper noteworthy, but somewhat incremental, therefore my original recommendation stands: slightly favoring acceptance, but would not argue for its merits."}, "signatures": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxWx0NYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper918/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper918/Authors|ICLR.cc/2020/Conference/Paper918/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164179, "tmdate": 1576860539726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment"}}}, {"id": "Sygd55Olir", "original": null, "number": 4, "cdate": 1573059215840, "ddate": null, "tcdate": 1573059215840, "tmdate": 1573790282952, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "H1eSNZHrKr", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment", "content": {"title": "We thank the reviewer for the valuable comments and we have addressed your concerns.", "comment": "First we highly appreciate the valuable comment from the reviewer and we are glad to have received positive comments, that our work \u201cseem to be generally useful for processing large-scale, structure-rich graph data.\u201d. The reviewer has given a great summary of main idea, and also valuable suggestions so that we can further improve the quality of our work. Our responses to the reviewer's comment is as follows.\n\n\n\u201cThe efficacy of the proposed method is tested on the Cora, Citeseer and Pubmed node classification benchmarks and compares favorably to non-augmented graph neural networks, beating all baselines on those datasets. Also the results in the paper beat those of the GResNet on Cora, but not on PubMed and , which is a recent paper not cited by this work.\u201d\n\n--- We thank the reviewer for pointing out a related work on Residual Graph Attention Network, which also presents promising results on the data sets we have used. We have added this paper as a reference in our updated version. Using residual connection is an important way to improve the performance of graph attention networks in general.\n\n\u201cIn general this work goes into the direction of adding hand-engineered features to DeepLearning approaches.\u201d\n\n---We believe that a thorough theoretic analysis will add to the depth and quality of the paper. Along this direction, we are planning to borrow theoretic works in semi-supervised kernel learning, to see if the behavior of the proposed method can be analyzed more strictly. This may further shed light on how to improve the construction of the fingerprint from learning theoretic perspectives. We will add these discussion in the future work section considering that it involves a tremendous amount of effort and new contributions.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper918/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJxWx0NYPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper918/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper918/Authors|ICLR.cc/2020/Conference/Paper918/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164179, "tmdate": 1576860539726, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper918/Authors", "ICLR.cc/2020/Conference/Paper918/Reviewers", "ICLR.cc/2020/Conference/Paper918/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Comment"}}}, {"id": "HklUNY5Htr", "original": null, "number": 2, "cdate": 1571297582486, "ddate": null, "tcdate": 1571297582486, "tmdate": 1572972535736, "tddate": null, "forum": "BJxWx0NYPr", "replyto": "BJxWx0NYPr", "invitation": "ICLR.cc/2020/Conference/Paper918/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper extends the idea of self-attention in graph NNs, which is typically based on feature similarity between nodes, to include also structural similarity. This is done by computing for each node a value for each other node within its receptive field, calculated based on some distance metric from the node (either a Gaussian decay profile, or a learned weighting of number of hops distance, of based on fixed point of a random walk with restart). When evaluating the attention between two nodes, a function that compares the structural values between the two nodes (based on Jacard similarity) gives a score to the two nodes, which is further used for calculating the attention weight between them.\n\nI found the idea to be elegant and well-explained, and overall the paper is well-written. \nUsing the structural similarity makes a lot of sense, and the proposed method is both easy to implement, and flexible \u2014 the structural similarity profile can be learned, which seems important for getting this idea to work in practice.\n\nI wonder what happens if one uses only the structural similarity for the attention (without the feature similarity). Are there datasets where this would be sufficient? Even a toy task which is constructed such that the structure is more informative than the features could be a nice way to further demonstrate the idea of the paper.\n\nThe experiments show a clear yet small advantage to the proposed method over the conventional attention method (GAT).\n\nOverall, this seems to be a solid contribution (even if the empirical results are a bit incremental) and I recommend acceptance."}, "signatures": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper918/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kzhang980@gmail.com", "52184501026@stu.ecnu.edu.cn", "wongjun@gmail.com", "jzhang080@gmail.com"], "title": "Adaptive Structural Fingerprints for Graph Attention Networks", "authors": ["Kai Zhang", "Yaokang Zhu", "Jun Wang", "Jie Zhang"], "pdf": "/pdf/d628a352c692f2480ce51c29e1b67b3609a3ffe6.pdf", "TL;DR": "Exploiting rich strucural details in graph-structued data via adaptive \"strucutral fingerprints''", "abstract": "Graph attention network (GAT) is a promising framework to perform convolution and massage passing on graphs. Yet, how to fully exploit rich structural information in the attention mechanism remains a challenge. In the current version, GAT calculates attention scores mainly using node features and among one-hop neighbors, while increasing the attention range to higher-order neighbors can negatively affect its performance, reflecting the over-smoothing risk of GAT (or graph neural networks in general), and the ineffectiveness in exploiting graph structural details. In this paper, we propose an ``\"adaptive structural fingerprint\" (ADSF) model to fully exploit graph topological details in graph attention network. The key idea is to contextualize each node with a weighted, learnable receptive field  encoding rich and diverse local graph structures. By doing this, structural interactions between the nodes can  be inferred accurately, thus significantly improving subsequent attention layer as well as the convergence of learning. Furthermore, our model provides a useful platform  for different subspaces of node features and various scales of graph structures to ``cross-talk'' with each other through the learning of multi-head attention, being particularly useful in handling complex real-world data. Empirical results demonstrate the power of our approach in exploiting rich structural information in GAT and in alleviating  the intrinsic oversmoothing problem in graph neural networks.", "code": "http://github.com/AvigdorZ", "keywords": ["Graph attention networks", "graph neural networks", "node classification"], "paperhash": "zhang|adaptive_structural_fingerprints_for_graph_attention_networks", "_bibtex": "@inproceedings{\nZhang2020Adaptive,\ntitle={Adaptive Structural Fingerprints for Graph Attention Networks},\nauthor={Kai Zhang and Yaokang Zhu and Jun Wang and Jie Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJxWx0NYPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/2288f8ecb6afc5e1d254ca4ca63674e351071be1.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJxWx0NYPr", "replyto": "BJxWx0NYPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper918/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575087528125, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper918/Reviewers"], "noninvitees": [], "tcdate": 1570237745073, "tmdate": 1575087528142, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper918/-/Official_Review"}}}], "count": 12}