{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396697152, "tcdate": 1486396697152, "number": 1, "id": "ByWmpfLug", "invitation": "ICLR.cc/2017/conference/-/paper596/acceptance", "forum": "rkuDV6iex", "replyto": "rkuDV6iex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper proposes an empirical investigation of the energy landscape of deep neural networks using several stochastic optimization algorithms.\n \n The extensive experiments conducted by the authors are interesting and inspiring. However, several reviewers expressed major concerns pointing out the limitations of a experimental investigation on real-world datasets. The paper would benefit from additional experiments on simulated datasets. This would allow to complement the experimental analysis. Furthermore, theoretical analysis and analytical derivations, consistent with the simulated datasets, could shed light on the experimental results and allow to make more precise claims. \n \n A revised version, following the reviewers' suggestions, will result in a stronger submission to a future venue."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396697756, "id": "ICLR.cc/2017/conference/-/paper596/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkuDV6iex", "replyto": "rkuDV6iex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396697756}}}, {"tddate": null, "tmdate": 1485442434837, "tcdate": 1481813975138, "number": 1, "id": "HJyJl4eEe", "invitation": "ICLR.cc/2017/conference/-/paper596/official/review", "forum": "rkuDV6iex", "replyto": "rkuDV6iex", "signatures": ["ICLR.cc/2017/conference/paper596/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper596/AnonReviewer2"], "content": {"title": "A good evaluation of optimization methods", "rating": "6: Marginally above acceptance threshold", "review": "This paper provides an extensive analysis of the error loss function for different optimization methods. The presentation is well done and informative. The experimental procedure is clarified sufficiently well. Theoretical evaluations like this are crucial for a wide range of applications and help to better understand and improve the convergence behavior for a given system.\n\nPros:\n\n- Important analysis\n- Good visualizations\n\nCons:\n\n- The paper describes mostly the observation that the optima vary for different methods, however doesn't attempt to explain why it happens and how to solve it (aside from batch-norm)\n- Some fonts are very small (e.g. Fig. 5)\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512529043, "id": "ICLR.cc/2017/conference/-/paper596/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper596/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper596/AnonReviewer2", "ICLR.cc/2017/conference/paper596/AnonReviewer3", "ICLR.cc/2017/conference/paper596/AnonReviewer1"], "reply": {"forum": "rkuDV6iex", "replyto": "rkuDV6iex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512529043}}}, {"tddate": null, "tmdate": 1485194424047, "tcdate": 1485193757913, "number": 1, "id": "BkLmfpXvx", "invitation": "ICLR.cc/2017/conference/-/paper596/public/review", "forum": "rkuDV6iex", "replyto": "rkuDV6iex", "signatures": ["~George_Philipp1"], "readers": ["everyone"], "writers": ["~George_Philipp1"], "content": {"title": "From an interested reader: many issues to be ironed out", "rating": "4: Ok but not good enough - rejection", "review": "First of all, I would like to thank the authors for putting this much work into a necessary but somewhat tedious topic. While I think the paper is somewhat below the standard of a conference paper (see detailed comments below), I would definitely love to see a version of this paper published with some of the issues ironed out. I also agree with many of the points raised by other reviewers and will not repeat them here.\n\nMajor points:\n\n-- \"As we saw in the previous section, the minima of deep network loss functions are for the most part decent.\"\n\nAll you said in the previous section was that theory shows that there are no bad minima under \"strong assumptions\". There is no practical proof that minima do not vary in quality.\n\n-- \"This implies that we probably do not need to take many precautions to avoid bad minima in practice. If all minima are decent, then the task of finding a \"decent minima quickly\" is reduced to the task of finding any minima quickly.\"\n\nFirst of all, as one of the reviewers pointed out, we are never guaranteed in practice to actually reach a local minimum. We could always hit a region of the objective function where the algorithm makes essentially no further progress. The final error level, in practice, actually does depend significantly on many factors such as (i) optimization algorithm (ii) learning rate schedule (iii) initialization of weights (iv) presence of unsupervised pretraining (v) whether neurons are added or eliminated during training etc. etc. Therefore, the task of optimizing neural networks is far from being \"reduced to finding any minima quickly\".\n\n-- Figure 1\n \nI don't like Figure 1, because it suggests to me that you diagnosed exactly where the transition between the two phases happened, which I don't think you did.\n \nAlso, the concept of having a fast-decaying error followed by a slow-decaying error is simple enough for readers to understand without a dedicated graph. Minor point on presentation: The red brace is positioned lower in the figure than the blue brace and the braces don't join up horizontally. Please be more careful.\n\n-- Misuse of the transient phase / minimization phase concept\n\nIn section 4.3, you talk about the transient and minimization phase of optimization. However, you have no way of diagnosing when or if your algorithm reaches the minimization phase. You seem to think that the minimization phase is simply the part of the optimization process where the error decreases slowly. AFAIK, this is not the case. The minimization phase is where the optimization algorithm enters the vicinity of the local minimum that can be approximated by the second-order Taylor expansion. For this to even occur, one would have to verify, for example, that the learning rate is small enough. You change the algorithm after 25%, 50% and 75% of training, but these points seem arbitrary. What is the minimization phase was reached at 99%, or 10 epochs after you decided to stop training?\n\n-- Only 1 dataset\n \nYou run most experiments on only 1 dataset (CIFAR). Please replicate with at least one more dataset.\n \n-- Many figures are unclear\n \nFor each figure, the following information are relevant: network used; dataset used; learning rate used; batch norm yes / no; whether figure shows train, test, or validation error. It should be easy for the reader to ascertain this information for all figures, not just for some.\n \n-- You say at the beginning of section 4.1 that each algorithm finds a different minimum as if this is a significant finding. However, this is obvious because the updates taken by these algorithms vary wildly. Keep in mind that there is an exponentially large number of minima. The probability of different algorithms choosing the same minimum is essentially zero because of their sheer number. The same would be true if you even shift the learning rate slightly or use a different random seed for minibatch generation etc. etc.\n  \n-- Lack of confidence intervals\n\nThe value of Figures 1, 2, 3 and 6 is limited is because it is unclear how these plots would change if the random seed were changed. We only get information for a single weight initialization and a single minibatch sequence. While figures 5 and 7 can be used to try and infer what confidence intervals around plots in figures 1, 2, 3 and 6 might look like, I think those confidence intervals should still be shown for at least a subset of the configurations presented.\n\n-- Lack of information regarding learning rate\n\nThere is big question mark left open regarding how all your results would change if different learning rates were used. You don't even tell us how you chose the learning rates from the intervals you gave in section 3.4.\n\n-- Lack of information regarding the absolute distance of interpolated points\n\nIn most figures, you interpolate between two or three trained weight configuration. However, you do not say how far the interpolated points are apart. This is highly significant, because if points are close together and there is a big \"hump\" between them, it means that those points are more \"brittle\" than if they are far apart and there is a big \"hump\" between them.\n \nMinor points:\n \n-- LSTM is not a fixed network architecture like NiN or VGG, but a layer type. LSTM would be equivalent to CNN. Also, the VGG paper has multiple versions of VGG. You should specify which one you used.\n  \n-- The font size for the legends in the upper triangle of Table 1 is too small. You can't just write \"best viewed in zoom\" in the table caption and pretend that somehow fixes the problem. Personally, I prefer no legend over an unreadable legend.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485193758487, "id": "ICLR.cc/2017/conference/-/paper596/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkuDV6iex", "replyto": "rkuDV6iex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org", "ICLR.cc/2017/conference/paper596/reviewers", "ICLR.cc/2017/conference/paper596/areachairs", "~George_Philipp2"], "cdate": 1485193758487}}}, {"tddate": null, "tmdate": 1481932028495, "tcdate": 1481931997786, "number": 3, "id": "BJI1peGEe", "invitation": "ICLR.cc/2017/conference/-/paper596/official/review", "forum": "rkuDV6iex", "replyto": "rkuDV6iex", "signatures": ["ICLR.cc/2017/conference/paper596/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper596/AnonReviewer1"], "content": {"title": "Interesting paper, but the message is not clear", "rating": "4: Ok but not good enough - rejection", "review": "The paper is dedicated to better understanding the optimization landscape in deep learning, in particular when explored with different optimization algorithms, and thus it also characterizes the behavior of these algorithms. It heavily re-uses the approach of Goodfellow et al. (2015). I find it hard to understand the contributions of the paper, for example: is it surprising that different algorithms reach different solutions when starting from the same initialization? It would be useful if the authors build such basic intuition in the paper. I also did not receive a clear answer to the question I posed to reviewers regarding clarifying how does the findings of the paper can contribute to future works on optimization in deep learning. And this is what I find fundamentally missing. So for example, there are probably plenty of ways to modify approach of Goodfellow et al. (2015), and similar works, and come up with interesting visualization methods for deep learning - but the question is: how is this helpful in terms of designing better algorithms, gaining more intuition how the optimization surface looks like in general, etc.? This is an interesting paper, though I am fairly confident it is a better fit for the journal than this conference. \n\nIt would be interesting and instructive, even for sanity check, to plot the eigenspectra of the solutions recovered by the algorithms to see the order of critical points recovered.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512529043, "id": "ICLR.cc/2017/conference/-/paper596/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper596/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper596/AnonReviewer2", "ICLR.cc/2017/conference/paper596/AnonReviewer3", "ICLR.cc/2017/conference/paper596/AnonReviewer1"], "reply": {"forum": "rkuDV6iex", "replyto": "rkuDV6iex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512529043}}}, {"tddate": null, "tmdate": 1481892201368, "tcdate": 1481892134740, "number": 2, "id": "BJJNbw-Ne", "invitation": "ICLR.cc/2017/conference/-/paper596/official/review", "forum": "rkuDV6iex", "replyto": "rkuDV6iex", "signatures": ["ICLR.cc/2017/conference/paper596/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper596/AnonReviewer3"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "I appreciate the work but I do not think the paper is clear enough. \nMoreover, the authors say \"local minimia\" ~70 times but do not show (except for Figure 11?) that the solutions found are not necessarily local minima. \nThe authors do not talk about that fact that slices of a non-convex problem can look like the ones they show. \nIt is well-known that the first-order methods may just fail to deal with certain non-convex ill-conditioned problems even in low-dimensional noiseless cases, the place/solution where they fail to make progress is not necessarily a local minimum. \nSome sentences like the one given below suggest that the study is too superficial:  \n\"One of the interesting empirical observation is that we often observe is that the incremental improvement\nof optimization methods decreases rapidly even in non-convex problems.\"", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512529043, "id": "ICLR.cc/2017/conference/-/paper596/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper596/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper596/AnonReviewer2", "ICLR.cc/2017/conference/paper596/AnonReviewer3", "ICLR.cc/2017/conference/paper596/AnonReviewer1"], "reply": {"forum": "rkuDV6iex", "replyto": "rkuDV6iex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512529043}}}, {"tddate": null, "tmdate": 1481238298131, "tcdate": 1481238298127, "number": 3, "id": "B1fXDwwQe", "invitation": "ICLR.cc/2017/conference/-/paper596/public/comment", "forum": "rkuDV6iex", "replyto": "BJcBxFJ7e", "signatures": ["~Daniel_Jiwoong_Im6"], "readers": ["everyone"], "writers": ["~Daniel_Jiwoong_Im6"], "content": {"title": "Clarifications ", "comment": "When we referred to \u201cminima\u201d and \u201clocal minima\u201d, we were being loose with our language, and should have used the term \u201ccritical points\u201d, as the weight vectors compared were the convergence points of the stochastic gradient descent algorithms. We do not believe that these points are global minima, but that many of them are local minima as opposed to saddle points. We have changed our terminology to clarify this. In addition, we have added evidence that the following evidence that the convergence points are local minima (see Figure 11)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506954, "id": "ICLR.cc/2017/conference/-/paper596/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkuDV6iex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper596/reviewers", "ICLR.cc/2017/conference/paper596/areachairs"], "cdate": 1485287506954}}}, {"tddate": null, "tmdate": 1481238207290, "tcdate": 1481238207286, "number": 2, "id": "SyvTIDvXl", "invitation": "ICLR.cc/2017/conference/-/paper596/public/comment", "forum": "rkuDV6iex", "replyto": "SklPsulXl", "signatures": ["~Daniel_Jiwoong_Im6"], "readers": ["everyone"], "writers": ["~Daniel_Jiwoong_Im6"], "content": {"title": "Updates on conclusions ! ", "comment": "We have rewritten the Discussion section of the paper to clarify our conclusions. \nMinor typos are fixed and all occurrences of \u201cAdam\u201d has been changed to \u201cADAM\u201d."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506954, "id": "ICLR.cc/2017/conference/-/paper596/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkuDV6iex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper596/reviewers", "ICLR.cc/2017/conference/paper596/areachairs"], "cdate": 1485287506954}}}, {"tddate": null, "tmdate": 1481238137920, "tcdate": 1481238137915, "number": 1, "id": "rkzFLDwQl", "invitation": "ICLR.cc/2017/conference/-/paper596/public/comment", "forum": "rkuDV6iex", "replyto": "rJMQjUqMx", "signatures": ["~Daniel_Jiwoong_Im6"], "readers": ["everyone"], "writers": ["~Daniel_Jiwoong_Im6"], "content": {"title": "Some clarifications ", "comment": "(1) We used batch-normalization and dropout to regularize our networks. This information has been added to Section 3.4 of the revised paper. \n\n\n(2) Our paper provides an empirical analysis of deep learning, and the main contributions are the experiments we performed, using a method similar to Goodfellow et al., and their conclusions. Goodfellow et al. used only one optimization algorithm, and their main experiment involved interpolating between the initial and final weights. In contrast, our experiments compared different optimization algorithms. We performed several novel experiments, and reached the following novel conclusions:\n\n- Comparison of local minima found by different optimization algorithm (Sections 4.1 and 4.2). Based on these experiments, we concluded that different optimization algorithms reach different local minima from the same initialization, and that the shape of the loss function around these local minima are characteristic for each algorithm. \n\n- Comparison of the local minima found when switching optimization algorithm during the late \u201cminimization\u201d phase of learning (Section 4.3). We observed that different local minima are found by different algorithms even when we switch algorithms very late in the optimization -- when the training accuracy is improving only very slowly. It had previously been assumed that, at this point in training,  the local minimum had been selected, and that it was just being localized within a neighborhood. We conclude that this does not appear to be the case. \n\n- Comparison of loss function shape around local minima found with and without batch normalization (Section 4.4). Based on these experiments, we found that the shape of the loss function was extremely stereotyped when using batch normalization, but not without it (thus, there was a much smaller effect of the initialization with batch normalization). \n\n\n(3,4) Thank you for pointing out the typographic errors. We have fixed them in the latest version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287506954, "id": "ICLR.cc/2017/conference/-/paper596/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkuDV6iex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper596/reviewers", "ICLR.cc/2017/conference/paper596/areachairs"], "cdate": 1485287506954}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481237986833, "tcdate": 1478378592017, "number": 596, "id": "rkuDV6iex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkuDV6iex", "signatures": ["~Daniel_Jiwoong_Im2"], "readers": ["everyone"], "content": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480784728046, "tcdate": 1480784728040, "number": 3, "id": "SklPsulXl", "invitation": "ICLR.cc/2017/conference/-/paper596/pre-review/question", "forum": "rkuDV6iex", "replyto": "rkuDV6iex", "signatures": ["ICLR.cc/2017/conference/paper596/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper596/AnonReviewer2"], "content": {"title": "conclusion", "question": "This is a comprehensive and useful evaluation. Some remarks:\n\n- conclusions are somewhat missing and are not entirely clear\n- there are some minor typos, e.g. page 8 on bottom (swithced) and inconsistencies in the capitalization of Adam"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959195956, "id": "ICLR.cc/2017/conference/-/paper596/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper596/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper596/AnonReviewer1", "ICLR.cc/2017/conference/paper596/AnonReviewer3", "ICLR.cc/2017/conference/paper596/AnonReviewer2"], "reply": {"forum": "rkuDV6iex", "replyto": "rkuDV6iex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959195956}}}, {"tddate": null, "tmdate": 1480720450212, "tcdate": 1480720450208, "number": 2, "id": "BJcBxFJ7e", "invitation": "ICLR.cc/2017/conference/-/paper596/pre-review/question", "forum": "rkuDV6iex", "replyto": "rkuDV6iex", "signatures": ["ICLR.cc/2017/conference/paper596/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper596/AnonReviewer3"], "content": {"title": "local minima", "question": "This paper refers to \"local minima\" and \"minima\" without demonstrating a support for their presence. \nPlease describe the conditions of the latter and demonstrate whether they are satisfied. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959195956, "id": "ICLR.cc/2017/conference/-/paper596/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper596/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper596/AnonReviewer1", "ICLR.cc/2017/conference/paper596/AnonReviewer3", "ICLR.cc/2017/conference/paper596/AnonReviewer2"], "reply": {"forum": "rkuDV6iex", "replyto": "rkuDV6iex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959195956}}}, {"tddate": null, "tmdate": 1480451888903, "tcdate": 1480383257845, "number": 1, "id": "rJMQjUqMx", "invitation": "ICLR.cc/2017/conference/-/paper596/pre-review/question", "forum": "rkuDV6iex", "replyto": "rkuDV6iex", "signatures": ["ICLR.cc/2017/conference/paper596/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper596/AnonReviewer1"], "content": {"title": "Clarifications", "question": "Dear Authors,\n\n1) What network regularizations were used in the experiments?\n2) The paper seems incremental over Goodfellow et al. 2015 (it extends this work though). Could you clarify more the significant novelty of this paper and the message it conveys? What future research directions it gives rise to? Could you discuss what are the algorithmic/other consequences of the findings in the paper?\n3) The sentence starting at \"For every pair of optimization algorithms, we...\" on page 4 seems to have a missing word.\n4) Similarly, sentence on page 4: \"This suggests to use that...\" seems to have a missing part.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Empirical Analysis of Deep Network Loss Surfaces", "abstract": "The training of deep neural networks is a high-dimension optimization problem with respect to the loss function of a model. Unfortunately, these functions are of high dimension and non-convex and hence difficult to characterize. In this paper, we empirically investigate the geometry of the loss functions for state-of-the-art networks with multiple stochastic optimization methods. We do this through several experiments that are visualized on polygons to understand how and when these stochastic optimization methods find minima.", "pdf": "/pdf/9dd579e275c938e412d15a96693ee6d73c6a5ce0.pdf", "TL;DR": "Analyzing the loss surface of deep neural network trained with different optimization methods", "paperhash": "im|an_empirical_analysis_of_deep_network_loss_surfaces", "keywords": ["Deep learning"], "conflicts": ["toronto.edu", "uoguelph.ca", "umontreal.ca", "janelia.hhmi.org"], "authors": ["Daniel Jiwoong Im", "Michael Tao", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "mtao@dgp.toronto.edu", "bransonk@janelia.hhmi.org"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959195956, "id": "ICLR.cc/2017/conference/-/paper596/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper596/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper596/AnonReviewer1", "ICLR.cc/2017/conference/paper596/AnonReviewer3", "ICLR.cc/2017/conference/paper596/AnonReviewer2"], "reply": {"forum": "rkuDV6iex", "replyto": "rkuDV6iex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper596/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959195956}}}], "count": 12}