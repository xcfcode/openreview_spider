{"notes": [{"id": "Bkga90VKDB", "original": "Bylph0O_PB", "number": 1299, "cdate": 1569439381101, "ddate": null, "tcdate": 1569439381101, "tmdate": 1577168225132, "tddate": null, "forum": "Bkga90VKDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "nZ-cM6aGZ3", "original": null, "number": 1, "cdate": 1576798719722, "ddate": null, "tcdate": 1576798719722, "tmdate": 1576800916806, "tddate": null, "forum": "Bkga90VKDB", "replyto": "Bkga90VKDB", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to further distill token embeddings via what is effectively a simple autoencoder with a ReLU activation. All reviewers expressed concerns with the degree of technical contribution of this paper. As Reviewer 3 identifies, there are simple variants (e.g. end-to-end training with the factorized model) and there is no clear intuition for why the proposed method should outperform its variants as well as the other baselines (as noted by Reviewer 1). Reviewer 2 further expresses concerns about the merits of the propose approach over existing approaches, given the apparently small effect size of the improvement (let alone the possibility that the improvement may not in fact be statistically significant).\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bkga90VKDB", "replyto": "Bkga90VKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717169, "tmdate": 1576800267415, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Decision"}}}, {"id": "BJeih5VjsH", "original": null, "number": 5, "cdate": 1573763763331, "ddate": null, "tcdate": 1573763763331, "tmdate": 1573853898504, "tddate": null, "forum": "Bkga90VKDB", "replyto": "HJl-Mn5kcS", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for your review and critique. \n\n1) The main strengths of our model are simplicity, a single parameter to control compression rate (bottleneck size), no reliance on frequency information which is seldom available for pre-trained models and a faster running time which we demonstrate in Section 2 b). Although in hindsight the approach looks simple, which we think is a strength, we considered many factors and ran many experiments to come up with this approach. The embedding reconstruction loss was an innovation that we found consistently performed better and we did not come across any embedding reduction/compression paper which tried it. However, in other works we agree that weight reconstruction loss is common.\n\n2) \n\nb) We are running an analysis on that and will update our response with it.\n\nc) We ran an experiment on language modelling using the transformer-xL on wiki-text 103. We replicated the setup on the Github repository [1] and were able to achieve a similar test perplexity. We then tried SVD and Distilled Embedding (proposal) with a bottleneck of 32 to compress the model 12.79x times. We also present the result on Distilled Embedding at 6.32x compression with a bottleneck size of 64.  \n\nWikitext-103:\n\n| Model                                   | Compression  | Val PPl | Test PLL |\n|                                               |                           |              |                 |\n| Transformer-XL standard |           1x           |  23.23  |    24.16    |\n| with SVD (32)                      |        12.79x       |  36.49  |    37.86    |\n| with Distilled Emb (32)      |        12.79x       |  34.35  |    35.51    |\n| with Distilled Emb (64)      |         6.32x        |  26.81  |    27.63    |\n\nWe pay a steeper price (in perplexity lower is better) for compressing the embedding layer of the transformer-xl language model however we have a lower perplexity than SVD. We will complete the comparison against other techniques on the same baseline.\n\n\nd) We ran a sensitivity analysis on the Pt-En translation. Based on the result, the experiments are not very sensitive to the value of alpha. We did not tune the alpha for our different experiments but chose the one which gave us good validation results on En-De translation. These results suggest that we can gain a little performance if we tune alpha for every dataset. \n\nPt-En:\n\n| Alpha | BLEU |\n|     ---    |    ---    |\n| 0          | 42.50 |\n| 0.01    | 42.62 |\n| 0.1      | 42.65 |\n| 0.3      | 42.66 |\n| 0.5      | 42.72 |\n| 0.7      | 42.57 |\n| 0.9      | 42.03 |\n\n\n[1]: https://github.com/kimiyoung/transformer-xl"}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkga90VKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1299/Authors|ICLR.cc/2020/Conference/Paper1299/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158134, "tmdate": 1576860557187, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment"}}}, {"id": "ByeEBVrhjS", "original": null, "number": 6, "cdate": 1573831740502, "ddate": null, "tcdate": 1573831740502, "tmdate": 1573831740502, "tddate": null, "forum": "Bkga90VKDB", "replyto": "HJl-Mn5kcS", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment", "content": {"title": "Running Time and justification of approach", "comment": "1) We specifically chose RELU so that the model can learn to regularize certain embedding dimensions, which is useful when dealing with a high dimensional embedding space, further, since this will lead to a reduction in reconstruction performance, we introduce the reconstruction loss and hyper-parameter \u2018alpha\u2019, to balance out regularization and reconstruction. These were mentioned in the paper but you are right that they were not highlighted well.\n2) The reason we did not run experimental results for measuring the inference time is that the only accurate method to do it is either on edge device or in a simulated environment. Secondly, more than inference speed, running memory reduction is also important that is where the SVD based techniques (including ours) are superior, as there is no need to reconstruct the entire embedding matrix. We ran the experiment on inference speed and the results are shown below,\nExperimental Setup: We used 1 P100 GPU (12GB), and measured the time for the forward graph on the validation dataset (size 7590), with a batch size of 1024. We averaged this time for 30 runs and summarize are results below.\n\n|              Model                        | Inference Time (Sec) |\n| Distilled Embedding (ours) |           29.23                  |\n| SVD                                         |           29.63                   |\n| Structured Embedding        |           31.18                  |\n| Base Model                           |           27.92                   |\nWe did not perform experiments on Group Reduce and Tensor Train, but they are likely to perform comparably to SVD and Our Method, or even slower.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkga90VKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1299/Authors|ICLR.cc/2020/Conference/Paper1299/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158134, "tmdate": 1576860557187, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment"}}}, {"id": "HyexmOEisH", "original": null, "number": 4, "cdate": 1573763095760, "ddate": null, "tcdate": 1573763095760, "tmdate": 1573763095760, "tddate": null, "forum": "Bkga90VKDB", "replyto": "HJlNIMM9jH", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment", "content": {"title": "Response to concerns raised by AnonReviewer3 ", "comment": "Thanks for the comment. For SVD and our approach we first train a machine translation model with full embedding matrix to convergence. Then we compress the embedding off-line and plug it back and fine-tune. So essentially we have 2 training rounds.\n\nFor the end-to-end approach, we train to convergence with random initialization once. It is possible that if we let it train much longer it may slowly converge to the first solution. However, if our goal is to compress a pre-trained model it would be faster to compress the embedding and fine-tune rather than training end-to-end longer.\n\nAnother interesting observation is in Table 4 in our paper. If we retain the pretrained model weights and initialize the embedding randomly we do worse than initializing everything randomly:\n\n\n                Random Init.     Model Init. + Random emb.       Proposal\n\nEn - Fr           37.23                              37.04                               37.78\nEn - De         26.14                               26.07                               26.97\nPt - En          42.27                               42.29                               42.62\n\n\nSo fully random is slightly better than initializing the model with the pre-trained weights and the embedding randomly. Initializing both, our proposal and proposal of Shi & Yu (2018) and Chen et. al (2018), is the best. We agree that further experimentation would help us to generalize our findings better. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkga90VKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1299/Authors|ICLR.cc/2020/Conference/Paper1299/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158134, "tmdate": 1576860557187, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment"}}}, {"id": "HJlNIMM9jH", "original": null, "number": 3, "cdate": 1573687884080, "ddate": null, "tcdate": 1573687884080, "tmdate": 1573687884080, "tddate": null, "forum": "Bkga90VKDB", "replyto": "HyesYBaYjr", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment", "content": {"title": "Explanations", "comment": "Thanks for making the experiments, which indeed shows improvements over the end-to-end approach. I'm not sure I have the intuition why the end-2-end approach is better than the SVD. This seems to contradict most of the recent results of deep learning literature, which show that learning end-2-end is beneficial provided the right optimisation method and regularisation scheme. I think this specific point would need to be addressed with more scrutiny."}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkga90VKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1299/Authors|ICLR.cc/2020/Conference/Paper1299/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158134, "tmdate": 1576860557187, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment"}}}, {"id": "HyesYBaYjr", "original": null, "number": 1, "cdate": 1573668226610, "ddate": null, "tcdate": 1573668226610, "tmdate": 1573686792767, "tddate": null, "forum": "Bkga90VKDB", "replyto": "BJgi5BU8qS", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We sincerely thank the reviewer for their comments and suggestions to improve the paper.\n\nWe present the BLEU scores for the End-to-End 2 Layer NN (E2E-NN) approach and compare with SVD and our proposed solution below:\n\n| Dataset | E2E-NN |  SVD  | Proposal |\n|       ---     |      ---      |    ---   |       ---       |\n|    En-Fr  |   37.23   | 37.44 |    37.78    |\n|    En-De |   26.14   | 26.32 |    26.97    |\n|    Pt-En  |   42.27   | 42.37 |    42.62    | \n\nThe end-to-end scheme does a little worse than SVD. We think the performance improvement for SVD and our proposal both is due to a better initialization during offline training. The difference between the End-to-End 2 layer NN and our proposal is that we initialize our method off-line and use the distillation term to regularize the embedding network."}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkga90VKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1299/Authors|ICLR.cc/2020/Conference/Paper1299/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158134, "tmdate": 1576860557187, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment"}}}, {"id": "BkgMMQW9oB", "original": null, "number": 2, "cdate": 1573683977576, "ddate": null, "tcdate": 1573683977576, "tmdate": 1573684194601, "tddate": null, "forum": "Bkga90VKDB", "replyto": "B1lOWHeJ9H", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment", "content": {"title": "Response to Review#2", "comment": "1) We were careful not to use the word 'significantly' since we did not include any statistical significance tests. We agree that it gives a better account of the improvement in performance but for these models, the computational cost for these analyses would be prohibitive. We propose another way to compare the results of the proposed method against competing methods. \n\nProposed vs Shi & Yu (2018):\n\nEn-Fr       37.78 (proposed) <=> 37.78       \nEn-De      26.97 (proposed) <=> 26.34 \nPt-En       42.62 (proposed) <=> 41.27 \n\n\nProposed vs Chen et al. (2018):\n\nEn-Fr       37.78 (proposed) <=> 37.63\nEn-De      26.97 (proposed) <=> 26.75\nPt-En       42.62 (proposed) <=> 42.13\n\n\nProposed vs SVD rank 64:\n\nEn-Fr       37.78 (proposed) <=> 37.44       \nEn-De      26.97 (proposed) <=> 26.32\nPt-En       42.62 (proposed) <=> 42.37\n\nBased on this we conclude that we are consistently better and .49 BLEU better on at least one dataset. Our experimental philosophy was to use widely reported translation datasets, standard architectures and to re-train the models to convergence. This meant that the performance of all competing methods was closer than previously anticipated and our proposed method scored consistently higher BLEU scores compared to the rest.  \n\n2) We think that any commercial edge deployment of NLP models will combine a range of solutions including but not limited to weight quantization (depending on hardware), embedding compression, network weight reduction, parameter sharing and knowledge distillation. Embedding matrices, in the experiments we presented, constitute  27% (En-Fr) to 74.45% (Pt-En) of the network parameters. So depending on the rest of the model, embedding compression may help us shave 23% to 63% of the model (with our solution) without much loss of performance. Any solution, other than quantization, which aims to reduce model size would need to compress or reduce the embedding size. Quantization does not reduce the number of parameters but reduces the storage size. However, it is hardware dependant and not always a viable option. \n\nMoreover, embedding matrices are present in all NLP applications and constitute a majority of parameters for smaller models.\n\nRegarding Table 6 we thank you for noticing. We updated the formatting to make it clear. We have revised the submission but briefly:\n\nModel                                                         BLEU\nProposal                                                     42.60\n    - embedding dist.                                 42.44\n    - non-linearity                                       42.34\nProposal (Freeze non-emb weights)     33.34\nProposal (Freeze emb. weights)            20.49\n\nWe compare the proposal against removing embedding distillation and removing embedding distillation and non-linearity. We also show the effect of freezing the embedding weights during fine-tuning and freezing the non-embedding weights. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bkga90VKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1299/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1299/Authors|ICLR.cc/2020/Conference/Paper1299/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158134, "tmdate": 1576860557187, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Authors", "ICLR.cc/2020/Conference/Paper1299/Reviewers", "ICLR.cc/2020/Conference/Paper1299/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Comment"}}}, {"id": "HJl-Mn5kcS", "original": null, "number": 2, "cdate": 1571953672709, "ddate": null, "tcdate": 1571953672709, "tmdate": 1572972486886, "tddate": null, "forum": "Bkga90VKDB", "replyto": "Bkga90VKDB", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes to use low-rank matrix decomposition for embedding compression, with relu in the reconstruction layer to gain non-linearity. Experiments on machine translation task shows improvement compared with state-of-the-art methods with different compression rates.\n\nDetailed comments:\n1)\tThe technical contribution seems to be a bit limited. Using relu in the reconstruction function looks straightforward and adding reconstruction loss in objective function is also common practice. Also, not much insight is provided on why such approach works better than other baselines. \n\n2)\tExperiments:\na.\tIt is good to see such simple approach outperforms several more sophisticated baseline methods. Also, ablation study is also performed to show the effect of different components.\n\nb.\tHow does the time complexity and running time of the proposed method compared to the baselines?\n\nc.\tThe paper only evaluates distilled embedding on one task (i.e., machine translation). The experiments would be more convincing if evaluated on more tasks as well.\n\nd.\tIt could be helpful to include some sensitivity analysis on the hyperparameters such as \\alpha which controls the weight of reconstruction loss. \n\nIn conclusion, this paper seems to be below the bar and I would recommend a \u2018weak reject\u2019 for the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkga90VKDB", "replyto": "Bkga90VKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576446521929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Reviewers"], "noninvitees": [], "tcdate": 1570237739395, "tmdate": 1576446521951, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Review"}}}, {"id": "B1lOWHeJ9H", "original": null, "number": 1, "cdate": 1571910912500, "ddate": null, "tcdate": 1571910912500, "tmdate": 1572972486840, "tddate": null, "forum": "Bkga90VKDB", "replyto": "Bkga90VKDB", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "\n\nThis paper proposes a method for compressing embedding matrices of both encoder/decoder embeddings.\nThe basic idea of the proposed method is to reconstruct the embedding matrix by what they called the \u201cfunneling decomposition\u201d method, whose parameter shape is identical to the SVD (low-rank matrix) decomposition with additional non-linear function.\nTherefore, the idea itself is not so novel and innovative.\nMoreover, their method requires the embedding matrix as the teacher signal for calculating the reconstruction loss.\nWe need to note that the memory requirement of the proposed method during training will increase.\n \nOne of the notable advantages of the proposed method is that their proposed method seems to successfully reduce the embedding matrix even if it shares the parameters with the output layer, which is a de-facto standard model architecture for NMT.\nAs pointed out by the authors, this seems to be the first success of reducing the embedding matrix with a tied embedding setting.\n\n\n1,\nThe authors claim that \u201cWe demonstrate that at the same compression rate our method outperforms existing state-of-the-art methods.\u201d at the end of the Introduction section.\nHowever, according to Tables 1, 2, and 3, it seems that the performance gain is marginal compared with similar methods.\nFor example, \n37.78 (proposed) <=> 37.78 (Shi & Yu (2018)) \n26.97 (proposed) <=> 26.75 (Chen et al. (2018)\nand\n 42.62 (proposed) <=> 42.37 (SVD with rank 64),\nwhich are the at most 0.25 BLEU gain.\nI believe that most of MT researchers hardly say that BLEU 0.25 difference is a significant improvement. Besides, the authors should perform a statistically significant test if they say \u201cour method outperforms existing state-of-the-art methods.\u201d\n \n \n2\nI am a bit confused about the following inconsistency;\nThe authors say that \u201cCompressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment\u201d in the abstract.\nHowever, according to Table 1, the number of parameters for embeddings is 16.3M, which is only 27% of the total number of parameters in Transformer base.\nBy this fact, compressing embedding matrices seems not essential for successful commercial edge deployment.\n \nIn Table 6, it is explicitly unclear what is the difference between \n\u201cFunneling with Emb. Distillation\u201d, \u201cFunneling (with non-linearity),\u201d and \u201cFunneling (with retraining all weights).\u201d\nPlease give us a more precise explanation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkga90VKDB", "replyto": "Bkga90VKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576446521929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Reviewers"], "noninvitees": [], "tcdate": 1570237739395, "tmdate": 1576446521951, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Review"}}}, {"id": "BJgi5BU8qS", "original": null, "number": 3, "cdate": 1572394387189, "ddate": null, "tcdate": 1572394387189, "tmdate": 1572972486793, "tddate": null, "forum": "Bkga90VKDB", "replyto": "Bkga90VKDB", "invitation": "ICLR.cc/2020/Conference/Paper1299/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "There are many ways to reduce the memory footprint and increase speed of a neural network: weight quantisation, compression, coarse-to-fine, knowledge distillation, etc. The method proposed in this work is a specific case of knowledge distillation that focuses on the discrete-input-to-first-layer and output-layer-to-discrete-output transformations, which represent a large portion of the parameters.\n\nThe authors propose to use a variant of SVD (which can be viewed as 2 linear transformation, with a middle dimension that represents an embedding), where the first transformation is linear with a ReLu, and the second is linear. By approximating the learned matrices of the model, the experiments show that using the proposed variant of SVD gives similar predictive performance compared to the original model, with a fraction of the parameters.\n\nHowever, it seems that the authors could have simply replaced the input by a 2-layer NN (first a linear+ReLu, then a Linear) to obtain the same parametrisation, but they could have learned the parameters in a end-to-end fashion. It is not clear to me why using a surrogate L2 loss within the model should give better predictive performance than a fully end-to-end trained neural network. Without this comparison, I do not think the proposed experiments are conclusive enough.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1299/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1299/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["vasileios.lioutas@carleton.ca", "ahmad.rashid@huawei.com", "krtin.kumar@huawei.com", "md.akmal.haidar@huawei.com", "mehdi.rezagholizadeh@huawei.com"], "title": "Distilled embedding: non-linear embedding factorization using knowledge distillation", "authors": ["Vasileios Lioutas", "Ahmad Rashid", "Krtin Kumar", "Md Akmal Haidar", "Mehdi Rezagholizadeh"], "pdf": "/pdf/da6abb4122fef835800a2e21f70fceaae43c0133.pdf", "TL;DR": "We present an embedding decomposition and distillation technique for NLP model compression which is state-of-the-art in machine translation and simpler than existing methods", "abstract": "Word-embeddings are a vital component of Natural Language Processing (NLP) systems and have been extensively researched. Better representations of words have come at the cost of huge memory footprints, which has made deploying NLP models on edge-devices challenging due to memory limitations. Compressing embedding matrices without sacrificing model performance is essential for successful commercial edge deployment. In this paper, we propose Distilled Embedding, an (input/output) embedding compression method based on low-rank matrix decomposition with an added non-linearity. First, we initialize the weights of our decomposition by learning to reconstruct the full word-embedding and then fine-tune on the downstream task employing knowledge distillation on the factorized embedding. We conduct extensive experimentation with various compression rates on machine translation, using different data-sets with a shared word-embedding matrix for both embedding and vocabulary projection matrices. We show that the proposed technique outperforms conventional low-rank matrix factorization, and other recently proposed word-embedding matrix compression methods. \n", "keywords": ["Model Compression", "Embedding Compression", "Low Rank Approximation", "Machine Translation", "Natural Language Processing", "Deep Learning"], "paperhash": "lioutas|distilled_embedding_nonlinear_embedding_factorization_using_knowledge_distillation", "original_pdf": "/attachment/b693d1f8361e598020ce57e78818a7218cad92eb.pdf", "_bibtex": "@misc{\nlioutas2020distilled,\ntitle={Distilled embedding: non-linear embedding factorization using knowledge distillation},\nauthor={Vasileios Lioutas and Ahmad Rashid and Krtin Kumar and Md Akmal Haidar and Mehdi Rezagholizadeh},\nyear={2020},\nurl={https://openreview.net/forum?id=Bkga90VKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bkga90VKDB", "replyto": "Bkga90VKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1299/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576446521929, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1299/Reviewers"], "noninvitees": [], "tcdate": 1570237739395, "tmdate": 1576446521951, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1299/-/Official_Review"}}}], "count": 11}