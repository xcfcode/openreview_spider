{"notes": [{"id": "K9e1ZJNn3Kk", "original": null, "number": 1, "cdate": 1599252107409, "ddate": null, "tcdate": 1599252107409, "tmdate": 1599252107409, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Public_Comment", "content": {"title": "Architecture Selection", "comment": "First of all, I would like to thank you for your grateful work. My question is about architecture selection. If we want to use network criticality measure for an architecture selection, should I train the architectures that I try to make a selection in on the data and after that make a selection or can I use the network criticality measures you found in your paper for any dataset? \n\nIf I summarize my question, are the network criticality measures you found in your paper for architectures like ResNet 18, VGG 16 etc. generic and can be usable for any dataset?\n\nThank you."}, "signatures": ["~Cemal_Gurpnar1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Cemal_Gurpnar1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1e4jkSKvB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504188022, "tmdate": 1576860586883, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Public_Comment"}}}, {"id": "S1e4jkSKvB", "original": "H1xUC6AdvH", "number": 1911, "cdate": 1569439644068, "ddate": null, "tcdate": 1569439644068, "tmdate": 1583912036990, "tddate": null, "forum": "S1e4jkSKvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "bqlxhX1Y2E", "original": null, "number": 1, "cdate": 1576798735708, "ddate": null, "tcdate": 1576798735708, "tmdate": 1576800900669, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The paper analyses the importance of different DNN modules for generalization performance, explaining why certain architectures may be much better performing than others. All reviewers agree that this is an interesting paper with a novel and important contribution. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713948, "tmdate": 1576800263681, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Decision"}}}, {"id": "SJlrtEmAKS", "original": null, "number": 2, "cdate": 1571857532682, "ddate": null, "tcdate": 1571857532682, "tmdate": 1574465993189, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper introduces a new way to reason about neural network generalization using a module criticality measure. The measure is tangible and intuitive. It leads to some formal bounds on the generalization of deep networks, and is able to better rank trained image classification architectures than previous measures.\n\nI am leaning to accept, as I expect this to be a significant theoretical contribution with several potential practical applications. With a few additional details, this could be a very strong submission:\n\n(1)\tChoice of module decomposition. Having each module be a single convolutional or fully-connected layer makes intuitive sense, but is there some theoretical motivation for this choice? If the only requirement for a module is that it includes some linear transformation, in the extreme, a module could consist of a single weight, or the entire network. Would those choices change the generalization bounds or relative criticality across different architectures?\n(2)\tScope of experimental results. The ranking results would be much more compelling if they included a broader range of architectures, including more recent models with more branching, e.g., DenseNet. Is there some reason ResNet101 has higher generalization error than 18 and 34? Net. Criticality for ResNets is inversely correlated with the number of layers; is there an explanation for this? Is this true for other very deep models?\n(3)\tPractical use. To compute the criticality measure, we must train the model; but, if we train the model, we can compute generalization directly. So, what is the practical application of the measure? Is there some way it could be used to save computation? Could it help in the case of a small validation dataset, which we do not want to look at many times during model selection?\n\nMinor typos:\n-\tSection 2.2: \u201cAn stable phenomena\u201d\n-\tSection 2.3: \u201c\u2026an the\u2026\u201d\n-\tIn appendix: \u201cResNet101: ResNet34 architectures\u2026\u201d\n\n----------------------------\n\nAfter rebuttal:\n\nThe authors have addressed my concerns, and I've increased my rating. There are still a few points I'd like to see addressed in the final version:\n\n1. The fact that the approach cannot yet be applied to batch normalization is a big practical drawback. Some discussion of approaches you tried, why they didn't work, and possible future directions for overcoming this would be appreciated.\n\n2. Clarify in the paper that the \"PAC Bayes\" approach used for comparison (Table 1) is a your method, i.e., an ablated version of criticality. As is, someone reading the paper quickly may think all you've done is add an alpha parameter to an existing \"PAC Bayes\" approach, which does fairly well on its own.\n\n3. Visualizing the experimental tables as scatterplots could make them easier for a reader to interpret.\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1911/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1911/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576556781109, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1911/Reviewers"], "noninvitees": [], "tcdate": 1570237730510, "tmdate": 1576556781119, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Official_Review"}}}, {"id": "ByxzqndmsB", "original": null, "number": 3, "cdate": 1573256329902, "ddate": null, "tcdate": 1573256329902, "tmdate": 1574107390598, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "The paper introduces concept of \"module criticality\" to understand the role played by several modules in a model and how this affects the generalization of the models. This is quite an important problem to study as this helps to develop better understanding of the current architectures and potentially reduce their size without suffering the accuracy drop. This is a great theoretical contribution.\n\nThe paper studies this per module compared to previous works where the entire architecture is rewounded. The paper also studies this for ResNet models which are more widely/practically used than just the fully connected layers alone. This helps better understand the model as a whole. \n\nThe authors do a good robust experimental study for different network initialization, various CNN models like ResNet18, 34, 101, VGG16 and also FCN. The results demonstrate the module criticality to be a good metric for generalization of models.\n\nOverall, a good paper.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1911/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1911/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576556781109, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1911/Reviewers"], "noninvitees": [], "tcdate": 1570237730510, "tmdate": 1576556781119, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Official_Review"}}}, {"id": "ByxYbrVioB", "original": null, "number": 6, "cdate": 1573762304634, "ddate": null, "tcdate": 1573762304634, "tmdate": 1573762304634, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "B1gk2JoDoH", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment", "content": {"title": "Added Densenet experiments.", "comment": "Apart from the changes listed above we have also updated our submission to include DenseNets in our experiments."}, "signatures": ["ICLR.cc/2020/Conference/Paper1911/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1e4jkSKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1911/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1911/Authors|ICLR.cc/2020/Conference/Paper1911/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149090, "tmdate": 1576860553716, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment"}}}, {"id": "Skl10NNsor", "original": null, "number": 5, "cdate": 1573762247030, "ddate": null, "tcdate": 1573762247030, "tmdate": 1573762247030, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "ByeWYJswsH", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment", "content": {"title": "Added Densenet experiments.", "comment": "Apart from the changes listed above we have also updated our submission to include DenseNets in our experiments as you suggested."}, "signatures": ["ICLR.cc/2020/Conference/Paper1911/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1e4jkSKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1911/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1911/Authors|ICLR.cc/2020/Conference/Paper1911/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149090, "tmdate": 1576860553716, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment"}}}, {"id": "B1gk2JoDoH", "original": null, "number": 4, "cdate": 1573527462900, "ddate": null, "tcdate": 1573527462900, "tmdate": 1573527462900, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "SJeMgKD5tS", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your positive comments on the paper. We have updated the paper to address both of your suggestions adequately and we hope you consider increasing your score if you find these changes satisfactory.\n\n- Intuitively moving closer to the initialization values would indicate that the effective function class is smaller and hence the network should generalize better. For example, in the extreme case where none of the weights change from their initialization value the function class would be a single function (the initial function) and the generalization error would be very low (~0%) as both the train and test error would be very high (but equal).\n\n- Thank you for your suggestion. We have added 3 networks: ResNet50, VGG11 and another fully connected network to our experimental section. We have also repeated our experiments on the CIFAR100 dataset. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1911/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1e4jkSKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1911/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1911/Authors|ICLR.cc/2020/Conference/Paper1911/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149090, "tmdate": 1576860553716, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment"}}}, {"id": "ByeWYJswsH", "original": null, "number": 3, "cdate": 1573527416934, "ddate": null, "tcdate": 1573527416934, "tmdate": 1573527416934, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "SJlrtEmAKS", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your very encouraging remarks and useful feedback. We have added the discussions and experiments you suggested (and even more) to the paper. In light of this and your strong review, we hope you would consider increasing your score to \u201caccept\u201d. \n\n\nWe next address your questions in detail:\n(1) Thanks for this insightful question! As you correctly pointed out, the choice of module decomposition is somewhat arbitrary. One could choose a module to comprise of a single scalar weight or the entire network and this would lead to different generalization scores. To answer your question more directly, the theoretical results hold for any decomposition. Why did we choose the modules this way? Since we are looking at a linear combination of parameters with their initialization, it makes sense to choose modules to be transformations that are linear in the parameters. Otherwise, the output would be very sensitive to a linear combination with initialization. Among all such decompositions, we chose the one with the minimum number of modules. In a sense, modules are chosen to be the largest well-behaved units in the network (and that happens to be the most natural choice). It is certainly possible that a smarter choice for module exists for each architecture which could lead to a lower network criticality measure. We are encouraged by your question and will add more discussion around this issue to the paper.\n\n(2) Thank you for your suggestions. We have added 3 networks, ResNet50, VGG11 and another fully connected network, to our experimental section. We have also repeated all our experiments on the CIFAR100 dataset. Moreover, we have  added another complexity metric to our table of comparison. Therefore, our current empirical results are much stronger than the submitted version. \n\n\nThe network criticality for ResNets is inversely correlated on the CIFAR10 dataset, but this is not true in the CIFAR100 dataset (which we just added). Here ResNet101 has the lowest generalization error and that is reflected by the network criticality score. We believe that the reason ResNet101 has higher generalization error for CIFAR10 in our experiments is that we do not use batch normalization to train. We make this choice since it is not clear how to accurately rewind batch normalization layers (we have tried several natural choices and they did not work). \n\n(3) Since criticality measure correlates with generalization, we would ideally want to encourage this measure to be low. This can happen in two different ways: 1) We can design regularizers based on the criticality measure and add them to the objective function. This would ensure that the trained network has low criticality. 2) We can explicitly design architecture with low criticality measure. This for example can happen by adding an explicit change in the modules (such as rewinding them to their initialization) to make sure the the learned network has low criticality. Another potential application is architecture search. As you mentioned, the model selection is usually done using a validation set but the search is over a large number of models, the result would overfit to the validation set. Therefore, measures such as the criticality score can be used in these scenarios.\n\nWe have corrected the typos that you identified in our updated draft. Thanks for pointing them out!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1911/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1e4jkSKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1911/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1911/Authors|ICLR.cc/2020/Conference/Paper1911/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149090, "tmdate": 1576860553716, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment"}}}, {"id": "BJxOkkjvsH", "original": null, "number": 2, "cdate": 1573527263542, "ddate": null, "tcdate": 1573527263542, "tmdate": 1573527263542, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "ByxzqndmsB", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for your positive remarks about both our theoretical contributions and our experimental study. We are a bit puzzled that your positive remarks are not reflected in the final score (weak reject). We hope that \u201cweak reject\u201d is chosen by mistake; otherwise, we would be happy to answer any concerns/questions you may have about our submission.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1911/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1e4jkSKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1911/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1911/Authors|ICLR.cc/2020/Conference/Paper1911/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149090, "tmdate": 1576860553716, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1911/Authors", "ICLR.cc/2020/Conference/Paper1911/Reviewers", "ICLR.cc/2020/Conference/Paper1911/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Official_Comment"}}}, {"id": "SJeMgKD5tS", "original": null, "number": 1, "cdate": 1571612905600, "ddate": null, "tcdate": 1571612905600, "tmdate": 1572972407706, "tddate": null, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "invitation": "ICLR.cc/2020/Conference/Paper1911/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "The paper builds upon the \"module criticality\" phenomenon and proposes a quantitative approach to measure this at the module and the network level.  A module's criticality is low if when it is switched to its initialization value, the error does not change drastically. \n\nThe paper uses a convex combination of the initial weights and the final weights of a layer/module to define an optimization path to traverse. The authors quantitatively define the module criticality such that it depends on how much closer the weights can get to the initial weights on this path while still being robust to random permutations. The network critically is defined as the sum of the module criticality measure of all the layers. \n\nEmpirical results on CIFAR10 show that the network's criticality is reflective of the generalization performance. For example, increasing resnet depth leads to improved generalization and low criticality. Though intuitively, it is not clear why moving closer to the initial values and thus lower average criticality indicated better generalization. It will be useful ot have a discussion on this issue. Results on other datasets will also be useful.\n\nOverall, the network criticality measure appears a useful tool to predict the generalization performance compared to other measures such as distance from initialization, weight spectrum, and others. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1911/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1911/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["niladri.chatterji@berkeley.edu", "neyshabur@google.com", "hsedghi@google.com"], "title": "The intriguing role of module criticality in the generalization of deep networks", "authors": ["Niladri Chatterji", "Behnam Neyshabur", "Hanie Sedghi"], "pdf": "/pdf/33967785f99d434685df6d4f8d33364d59b8d62c.pdf", "TL;DR": "We study the phenomenon that some modules of DNNs are more critical than others. Our analysis leads us to propose a complexity measure, that is able to explain the superior generalization performance of some architectures over others.", "abstract": "We study the phenomenon that some modules of deep neural networks (DNNs) are more critical than others. Meaning that rewinding their parameter values back to initialization, while keeping other modules fixed at the trained parameters, results in a large drop in the network's performance. Our analysis reveals interesting properties of the loss landscape which leads us to propose a complexity measure, called module criticality, based on the shape of the valleys that connect the initial and final values of the module parameters. We formulate how generalization relates to the module criticality, and show that this measure is able to explain the superior generalization performance of some architectures over others, whereas, earlier measures fail to do so.", "keywords": ["Module Criticality Phenomenon", "Complexity Measure", "Deep Learning"], "paperhash": "chatterji|the_intriguing_role_of_module_criticality_in_the_generalization_of_deep_networks", "_bibtex": "@inproceedings{\nChatterji2020The,\ntitle={The intriguing role of module criticality in the generalization of deep networks},\nauthor={Niladri Chatterji and Behnam Neyshabur and Hanie Sedghi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=S1e4jkSKvB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cf253bb58e1678c162f1a142c19878dc915fd8.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1e4jkSKvB", "replyto": "S1e4jkSKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1911/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576556781109, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1911/Reviewers"], "noninvitees": [], "tcdate": 1570237730510, "tmdate": 1576556781119, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1911/-/Official_Review"}}}], "count": 11}