{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392782340000, "tcdate": 1392782340000, "number": 9, "id": "guIXuVCQXMuQh", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["Marc'Aurelio Ranzato"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We thank the reviewers for their comments and suggestions.\r\n\r\nIn this abstract, we limit the investigation to:\r\n+ the use of multiple GPUs all residing in the same server\r\n+ the architecture and the task as defined in Krizhevsky et al. NIPS 2012\r\n+ the use of regular synchronous stochastic gradient descent.\r\nWe clarified this in the revised version of the paper.\r\n\r\nWe have also added references to prior work as recommended. However, notice the following major differences:\r\n\u2014 Krizhevsky et al. and Coates et al. only considered model parallelism\r\n\u2014 Chen et al, Dean et al. and Zhang et al. used different variants of asynchronous SGD\r\n\r\nThe objective of this study is to determine the speed up of a popular model using the most straightforward parallelization techniques without changing the optimization method. This should serve as a baseline comparison for any more advanced parallelization method. \r\n\r\nIt will be avenue of future work the study of asynchronous approaches using multiple servers.\r\n\r\nWe have updated the draft accordingly (the new version should appear shortly).\r\n\r\nThank you very much."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391647980000, "tcdate": 1391647980000, "number": 7, "id": "A50KACiEo4AE6", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["Liangliang Cao"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This is a light but interesting paper. I guess we are seeing a 'baby' version of Facebook's deep learning infostructure. \r\n\r\nFirst an easy-to-fix point: I didn't find explicitly how many layers are there in the deep NN, and which dataset is used. I guess the answers are 7 layers and ImageNet'12?\r\n\r\nCurrently the results are very reasonable: 2-GPU version is 1.6 times faster than 1-GPU. But I guess the audience is more interesting in the performance with more GPUs. Could 20 GPU be 16 times faster than 1GPU? What if 50, or 100GPUs?\r\n\r\nScalability may also bring interesting insights in the model design.  By the use of model parallelism, I wonder whether we can build an larger CNN with more neurals. It may have more convolutional filters in each layer, and could process larger image like 1024 * 1024 * 3. I wonder whether ensemble learning as well as sparse models will be useful in such a big neural network. \r\n\r\nHope to see more updates following the current submission."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391638500000, "tcdate": 1391638500000, "number": 6, "id": "FgxqTOu1qBF1I", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["anonymous reviewer 95e3"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Multi-GPU Training of ConvNets", "review": "Problem is clearly important, but paper is light on details, data sets, which gpu's, etc. All such things matter when judging the speed-up. For example, if you used an older gpu, it's easier to get a speed-up because the trade-off between the gain of multiple gpu's vs. the communication overhead is clearly different.\r\n\r\nCNN's for audio processing was done in 2012 by Abdel-Hamid. I would recommend to include this reference:\r\nAbdel-Hamid, Ossama, et al. 'Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition.' Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012.\r\n\r\nMulti-GPU architectures for non-convolutional networks were discussed in:\r\nXie Chen, Adam Eversole, Gang Li, Dong Yu, and Frank Seide, Pipelined Back-Propagation for Context-Dependent Deep Neural Networks, in Interspeech, ISCA, September 2012\r\n\r\nI don't really see things that are new. Model and Data parallelization was tried in Chen'2012, and the extension for CNN's are obvious. Also, which layer to parallelize depends really on the network structure. For example, if you have a very large output layer with 128k nodes, you might be better off parallelizing the output layer."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391030520000, "tcdate": 1391030520000, "number": 8, "id": "PPkvPCYirqPUb", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["Marc'Aurelio Ranzato"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thank you, Tom.\r\n\r\nThe main difference between this work and yours is that our data parallelism framework is synchronous (i.e., we use SGD not A-SGD).\r\nAlso, all our experiments refer to a set up where all the GPU boards reside in the same server. \r\n\r\nIn the future, we will extend this work to multiple servers and A-SGD."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390287360000, "tcdate": 1390287360000, "number": 5, "id": "oV4tZMH-QOols", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hello everyone,\r\nWe would like to bring your attention to a similar paper my colleagues and I submitted to this ICLR workshop track:\r\n\r\nTitle: GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training\r\nLink: http://openreview.net/document/a4a87af0-ce63-450d-9d4b-41cfb0390667#a4a87af0-ce63-450d-9d4b-41cfb0390667\r\n\r\nBoth papers explore using many GPUs for training convnets in using an ASGD framework. \r\n\r\nIn this paper, they use 2 GPUs on one machine for model parallelization (similar to Alex Krizhevsky's NIPS 2012 paper), as well as 2 and 4 nodes for data parallelization (ASGD).\r\n\r\nIn ours, a single GPU is used for model parallelization, but many nodes are used for data parallelization (ASGD). The ASGD methods are similar and our method is compatible with the model parallelization they use.\r\n\r\nOurs work has additional experiments that explore how to tune ASGD to get the best performance with GPUs, and how this scales to as many as 32 GPUs.\r\n\r\nBest,\r\nTom"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390287240000, "tcdate": 1390287240000, "number": 4, "id": "YlXrYpUzr3h7n", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hello authors,\r\nI would like to bring your attention to a similar paper my colleagues and I submitted to this ICLR workshop track:\r\n\r\nTitle: GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training\r\nLink: http://openreview.net/document/a4a87af0-ce63-450d-9d4b-41cfb0390667#a4a87af0-ce63-450d-9d4b-41cfb0390667\r\n\r\nBoth papers explore using many GPUs for training convnets in using an ASGD framework. \r\n\r\nIn your paper, you use 2 GPUs on one machine for model parallelization (similar to Alex Krizhevsky's NIPS 2012 paper), as well as 2 and 4 nodes for data parallelization (ASGD).\r\n\r\nIn ours, a single GPU is used for model parallelization, but many nodes are used for data parallelization (ASGD). The ASGD methods are similar and our method is compatible with the model parallelization you use.\r\n\r\nOurs work has additional experiments that explore how to tune ASGD to get the best performance with GPUs, and how this scales to as many as 32 GPUs.\r\n\r\nBest,\r\nTom"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390287120000, "tcdate": 1390287120000, "number": 3, "id": "3fpu-K60iD30c", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hello reviewers,\r\nWe would like to bring your attention to a similar paper my colleagues and I submitted to this ICLR workshop track:\r\n\r\nTitle: GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training\r\nLink: http://openreview.net/document/a4a87af0-ce63-450d-9d4b-41cfb0390667#a4a87af0-ce63-450d-9d4b-41cfb0390667\r\n\r\nBoth papers explore using many GPUs for training convnets in using an ASGD framework. \r\n\r\nIn this paper, they try using 2 GPUs on one machine for model parallelization (similar to Alex Krizhevsky's NIPS 2012 paper), as well as 2 and 4 nodes for data parallelization (ASGD).\r\n\r\nIn ours, a single GPU is used for model parallelization, but many nodes are used for data parallelization (ASGD). The ASGD methods are similar and our method is compatible with the model parallelization they use.\r\n\r\nOurs work has additional experiments that explore how to tune ASGD to get the best performance with GPUs, and how this scales to as many as 32 GPUs.\r\n\r\nWe bring this up because a reviewer has recommend their paper for the Conference track, though they submitted to the workshop track. Since the papers have a lot of overlap we think it would be best to compare them on the same footing.\r\n\r\nBest,\r\nTom"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390287120000, "tcdate": 1390287120000, "number": 1, "id": "n_6j_UpOmw_Od", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["Thomas Paine"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hello reviewers,\r\nWe would like to bring your attention to a similar paper my colleagues and I submitted to this ICLR workshop track:\r\n\r\nTitle: GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training\r\nLink: http://openreview.net/document/a4a87af0-ce63-450d-9d4b-41cfb0390667#a4a87af0-ce63-450d-9d4b-41cfb0390667\r\n\r\nBoth papers explore using many GPUs for training convnets in using an ASGD framework. \r\n\r\nIn this paper, they try using 2 GPUs on one machine for model parallelization (similar to Alex Krizhevsky's NIPS 2012 paper), as well as 2 and 4 nodes for data parallelization (ASGD).\r\n\r\nIn ours, a single GPU is used for model parallelization, but many nodes are used for data parallelization (ASGD). The ASGD methods are similar and our method is compatible with the model parallelization they use.\r\n\r\nOurs work has additional experiments that explore how to tune ASGD to get the best performance with GPUs, and how this scales to as many as 32 GPUs.\r\n\r\nWe bring this up because a reviewer has recommend their paper for the Conference track, though they submitted to the workshop track. Since the papers have a lot of overlap we think it would be best to compare them on the same footing.\r\n\r\nBest,\r\nTom"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389837600000, "tcdate": 1389837600000, "number": 2, "id": "22eX5RjNOqpg1", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "zze5zJIRq7lRt", "replyto": "zze5zJIRq7lRt", "signatures": ["anonymous reviewer 3960"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Multi-GPU Training of ConvNets", "review": "The paper is about various ways of training convolutional neural networks (CNNs)\r\nusing multiple GPUs attached to the same machine.\r\n\r\nI think it is sufficiently interesting for the conference track.  The authors\r\nmay not be aware of all relevant prior work, but they can fix this easily.  I\r\nthink the paper should definitely be accepted because Facebook is growing in\r\nthis area right now, and conference-goers will be wanting to talk to the\r\npresenters about what's going on there and what opportunities there are.\r\n\r\nThere are a couple of papers I think the authors should\tbe aware of; the titles\tare\r\n\r\n'Asynchronous stochastic gradient descent for DNN training'\r\n'Pipelined Back-Propagation for Context-Dependent Deep Neural Networks'\r\n\r\nAlso I know that Andrew Ng's group was doing some work on model parallelism for CNNs.  Andrew Maas (Andrew Maas <amaas@cs.stanford.edu>) would be able to tell you who it was and forward any relevant presentations."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1388006580000, "tcdate": 1388006580000, "number": 19, "id": "zze5zJIRq7lRt", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "zze5zJIRq7lRt", "signatures": ["omry@fb.com"], "readers": ["everyone"], "content": {"title": "Multi-GPU Training of ConvNets", "decision": "submitted, no decision", "abstract": "In this work we evaluate different approaches to parallelize computation of convolutional neural networks across several GPUs.", "pdf": "https://arxiv.org/abs/1312.5853", "paperhash": "yadan|multigpu_training_of_convnets", "keywords": [], "conflicts": [], "authors": ["Omry Yadan", "Keith Adams", "Yaniv Taigman", "Marc'Aurelio Ranzato"], "authorids": ["omry@fb.com", "kma@fb.com", "yaniv@fb.com", "ranzato@google.com"]}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 10}