{"notes": [{"id": "HkeWSnR5Y7", "original": "H1erLXRqt7", "number": 1515, "cdate": 1538087992888, "ddate": null, "tcdate": 1538087992888, "tmdate": 1545355425520, "tddate": null, "forum": "HkeWSnR5Y7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results", "abstract": "One intriguing property of neural networks is their inherent vulnerability to adversarial inputs, which are maliciously crafted samples to trigger target networks to misbehave. The state-of-the-art attacks generate adversarial inputs using either pixel perturbation or spatial transformation. Thus far, several provable defenses have been proposed against pixel perturbation-based attacks; yet, little is known about whether such solutions exist for spatial transformation-based attacks. This paper bridges this striking gap by conducting the first systematic study on provable defenses against spatially transformed adversarial inputs. Our findings convey mixed messages. On the impossibility side, we show that such defenses may not exist in practice: for any given networks, it is possible to find legitimate inputs and imperceptible transformations to generate adversarial inputs that force arbitrarily large errors. On the possibility side, we show that it is still feasible to construct adversarial training methods to significantly improve the resilience of networks against adversarial inputs over empirical datasets. We believe our findings provide insights for designing more effective defenses against spatially transformed adversarial inputs.", "keywords": [], "authorids": ["xizc15@lehigh.edu", "yih319@lehigh.edu", "cpn217@lehigh.edu", "sji@zju.edu.cn", "inbox.ting@gmail.com"], "authors": ["Xinyang Zhang", "Yifan Huang", "Chanh Nguyen", "Shouling Ji", "Ting Wang"], "pdf": "/pdf/59c062c4bca5c470745a50f836ba3f48b3c9d7f4.pdf", "paperhash": "zhang|provable_defenses_against_spatially_transformed_adversarial_inputs_impossibility_and_possibility_results", "_bibtex": "@misc{\nzhang2019provable,\ntitle={Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results},\nauthor={Xinyang Zhang and Yifan Huang and Chanh Nguyen and Shouling Ji and Ting Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeWSnR5Y7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1g_ZvFXgE", "original": null, "number": 1, "cdate": 1544947456106, "ddate": null, "tcdate": 1544947456106, "tmdate": 1545354490384, "tddate": null, "forum": "HkeWSnR5Y7", "replyto": "HkeWSnR5Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1515/Meta_Review", "content": {"metareview": "This paper conducts a study on provable defenses to spatially transformed adversarial examples. In general, the paper pursues an interesting direction, but reviewers had many concerns regarding the clarity of the presentation and the depth of the experimental results, which the authors did not address in a rebuttal. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "The clarity of presentation and evaluation is a concern"}, "signatures": ["ICLR.cc/2019/Conference/Paper1515/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1515/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results", "abstract": "One intriguing property of neural networks is their inherent vulnerability to adversarial inputs, which are maliciously crafted samples to trigger target networks to misbehave. The state-of-the-art attacks generate adversarial inputs using either pixel perturbation or spatial transformation. Thus far, several provable defenses have been proposed against pixel perturbation-based attacks; yet, little is known about whether such solutions exist for spatial transformation-based attacks. This paper bridges this striking gap by conducting the first systematic study on provable defenses against spatially transformed adversarial inputs. Our findings convey mixed messages. On the impossibility side, we show that such defenses may not exist in practice: for any given networks, it is possible to find legitimate inputs and imperceptible transformations to generate adversarial inputs that force arbitrarily large errors. On the possibility side, we show that it is still feasible to construct adversarial training methods to significantly improve the resilience of networks against adversarial inputs over empirical datasets. We believe our findings provide insights for designing more effective defenses against spatially transformed adversarial inputs.", "keywords": [], "authorids": ["xizc15@lehigh.edu", "yih319@lehigh.edu", "cpn217@lehigh.edu", "sji@zju.edu.cn", "inbox.ting@gmail.com"], "authors": ["Xinyang Zhang", "Yifan Huang", "Chanh Nguyen", "Shouling Ji", "Ting Wang"], "pdf": "/pdf/59c062c4bca5c470745a50f836ba3f48b3c9d7f4.pdf", "paperhash": "zhang|provable_defenses_against_spatially_transformed_adversarial_inputs_impossibility_and_possibility_results", "_bibtex": "@misc{\nzhang2019provable,\ntitle={Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results},\nauthor={Xinyang Zhang and Yifan Huang and Chanh Nguyen and Shouling Ji and Ting Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeWSnR5Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1515/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352810297, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkeWSnR5Y7", "replyto": "HkeWSnR5Y7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1515/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1515/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1515/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352810297}}}, {"id": "S1lrfwa327", "original": null, "number": 3, "cdate": 1541359373467, "ddate": null, "tcdate": 1541359373467, "tmdate": 1541533071671, "tddate": null, "forum": "HkeWSnR5Y7", "replyto": "HkeWSnR5Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1515/Official_Review", "content": {"title": "Review comments", "review": "This paper proposed a defense against spatially transformed adversarial inputs and give the two main results on possibility (still possible to construct adversarial training methods to improve robustness) and impossibility (always exist spatially-transformed adversarial examples for any given networks and thus no certified defense) \n\nThe topic of studying certified defenses on adversarial examples is important, and I think the direction of dealing with spatially-transformed adversarial examples is interesting. However, this paper only analyze a simple one hidden layer neural network and the technique (e.g. sec 4, possibility result) does not seem to easily scale to deeper networks and networks with other types of layers (e.g pooling layers). Also, \n\nI also feel the clarity of the paper should be improved.  \n\nHere are some questions:\n1. Are there other metrics to measure spatial transformation? For the current setting as introduced in sec 2.1, it looks like there is no a uniform spatial transformation on the full image but rather different transformation applied on different local areas. Does it make more sense to say rotate the full image by some angle or shift it by some distance?\n \n2. What is the pi_infty and pi_2 in Theorem 1? Why is it called Lower bound attack in sec 3.1? \n\n3. What is the difference between f_fro, f_spe and f_sdp? \n\n4. In Figure 6 (b), is the classification accuracy the nominal test accuracy of a classifier? If so, then the accuracy is too low (<90% for mnist) and thus considering the corresponding attack rates (Fig 6(a)) on these models are not meaningful. Please explain.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1515/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results", "abstract": "One intriguing property of neural networks is their inherent vulnerability to adversarial inputs, which are maliciously crafted samples to trigger target networks to misbehave. The state-of-the-art attacks generate adversarial inputs using either pixel perturbation or spatial transformation. Thus far, several provable defenses have been proposed against pixel perturbation-based attacks; yet, little is known about whether such solutions exist for spatial transformation-based attacks. This paper bridges this striking gap by conducting the first systematic study on provable defenses against spatially transformed adversarial inputs. Our findings convey mixed messages. On the impossibility side, we show that such defenses may not exist in practice: for any given networks, it is possible to find legitimate inputs and imperceptible transformations to generate adversarial inputs that force arbitrarily large errors. On the possibility side, we show that it is still feasible to construct adversarial training methods to significantly improve the resilience of networks against adversarial inputs over empirical datasets. We believe our findings provide insights for designing more effective defenses against spatially transformed adversarial inputs.", "keywords": [], "authorids": ["xizc15@lehigh.edu", "yih319@lehigh.edu", "cpn217@lehigh.edu", "sji@zju.edu.cn", "inbox.ting@gmail.com"], "authors": ["Xinyang Zhang", "Yifan Huang", "Chanh Nguyen", "Shouling Ji", "Ting Wang"], "pdf": "/pdf/59c062c4bca5c470745a50f836ba3f48b3c9d7f4.pdf", "paperhash": "zhang|provable_defenses_against_spatially_transformed_adversarial_inputs_impossibility_and_possibility_results", "_bibtex": "@misc{\nzhang2019provable,\ntitle={Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results},\nauthor={Xinyang Zhang and Yifan Huang and Chanh Nguyen and Shouling Ji and Ting Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeWSnR5Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1515/Official_Review", "cdate": 1542234213321, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeWSnR5Y7", "replyto": "HkeWSnR5Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1515/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335962921, "tmdate": 1552335962921, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1515/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xG3oOtnX", "original": null, "number": 2, "cdate": 1541143465862, "ddate": null, "tcdate": 1541143465862, "tmdate": 1541533071469, "tddate": null, "forum": "HkeWSnR5Y7", "replyto": "HkeWSnR5Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1515/Official_Review", "content": {"title": "Limited novelty compared to earlier work; poor presentation of results and conceptual differences between the proposed spatial transformation attack model vs. existing lp norm attack model", "review": "Summary: The paper studies a new attack model based on spatial transformations. The authors first formalize an attack model based on spatial transformation and then study attacks and defenses for this model. \n\nClarity: While the paper studies an important problem -- it's important to move out of the norm ball based attack models and consider different attacks like spatial transformations, in the current version, the presentation lacks clarity in both the formulation of the attack model, attacks, defenses and explanation of the results. For example, the impossibility result isn't clear: the claim is that any classifier has adversarial spatial transformations that are successful in causing misclassifcation for some threshold on the size of transformation. There is no explanation of how large this threshold is in practice. Is it small enough to be called an \"impossibility result\"? What does this threshold intuitively depend on?\n\nOriginality: The key contribution seems to be the formalization of some notion of spatial transformation. However, the final expression (Proposition 1) basically looks just like an l_p norm but after transforming it by some \"fixed\" matrix M. The expressions for this new attack model where || M r|| < \\eps for some perturbation \\eps look pretty similar to the case previously considered (where M was essentially identity). For example, Raghunathan et al. 2018 and Hein & Andriushchenko 2017. The paper is also missing discussion on the structure of this matrix M, and how it changes the attacks and defenses in practice\u00a0\n\nSignificance: I think the problem of spatial transformation based adversarial examples is important and the authors have the right goals. However, the current presentation makes it hard to understand the main results provided and hence I would rate that the contribution is not very significant. \n\nOverall: I highly recommend the authors to revise the presentation and clarify a) the main conceptual differences of the new attack model (matrix M of proposition 1) b) Formalize the impossibility and possibility results carefully with concrete theoretical/empirical results to back the claims", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1515/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results", "abstract": "One intriguing property of neural networks is their inherent vulnerability to adversarial inputs, which are maliciously crafted samples to trigger target networks to misbehave. The state-of-the-art attacks generate adversarial inputs using either pixel perturbation or spatial transformation. Thus far, several provable defenses have been proposed against pixel perturbation-based attacks; yet, little is known about whether such solutions exist for spatial transformation-based attacks. This paper bridges this striking gap by conducting the first systematic study on provable defenses against spatially transformed adversarial inputs. Our findings convey mixed messages. On the impossibility side, we show that such defenses may not exist in practice: for any given networks, it is possible to find legitimate inputs and imperceptible transformations to generate adversarial inputs that force arbitrarily large errors. On the possibility side, we show that it is still feasible to construct adversarial training methods to significantly improve the resilience of networks against adversarial inputs over empirical datasets. We believe our findings provide insights for designing more effective defenses against spatially transformed adversarial inputs.", "keywords": [], "authorids": ["xizc15@lehigh.edu", "yih319@lehigh.edu", "cpn217@lehigh.edu", "sji@zju.edu.cn", "inbox.ting@gmail.com"], "authors": ["Xinyang Zhang", "Yifan Huang", "Chanh Nguyen", "Shouling Ji", "Ting Wang"], "pdf": "/pdf/59c062c4bca5c470745a50f836ba3f48b3c9d7f4.pdf", "paperhash": "zhang|provable_defenses_against_spatially_transformed_adversarial_inputs_impossibility_and_possibility_results", "_bibtex": "@misc{\nzhang2019provable,\ntitle={Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results},\nauthor={Xinyang Zhang and Yifan Huang and Chanh Nguyen and Shouling Ji and Ting Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeWSnR5Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1515/Official_Review", "cdate": 1542234213321, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeWSnR5Y7", "replyto": "HkeWSnR5Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1515/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335962921, "tmdate": 1552335962921, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1515/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1xiO7ruhX", "original": null, "number": 1, "cdate": 1541063538837, "ddate": null, "tcdate": 1541063538837, "tmdate": 1541533071271, "tddate": null, "forum": "HkeWSnR5Y7", "replyto": "HkeWSnR5Y7", "invitation": "ICLR.cc/2019/Conference/-/Paper1515/Official_Review", "content": {"title": "Can the proposed defense is secure against pixel-based AEs?", "review": "The presented analysis well characterizes the behavior of the spatially transformed adversarial inputs and the proposed defense is empirically confirmed to achieve more accurate and robust classification under attacks.\n\nOne concern is that the defender cannot learn whether the adversary employs spatially transformed AEs or pixel-based AEs (or some others). What happens if the classifier trained with the proposed defense accept pixel-based AEs? I recommend the authors to associate spatially transformed AEs with pixel-based AEs to learn whether the proposed defense performs more robustly compared to existing defenses. If the proposed defense method performs well for spatially transformed AEs but is vulnerable to pixel-based AEs, it is useless.\n\nIt should be better to discuss more on computational efficiency of the proposed defense since it contains SDP solving. Is the proposed deense works with larger datasets such as CIFAR100 or ImageNet?\n\n \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1515/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results", "abstract": "One intriguing property of neural networks is their inherent vulnerability to adversarial inputs, which are maliciously crafted samples to trigger target networks to misbehave. The state-of-the-art attacks generate adversarial inputs using either pixel perturbation or spatial transformation. Thus far, several provable defenses have been proposed against pixel perturbation-based attacks; yet, little is known about whether such solutions exist for spatial transformation-based attacks. This paper bridges this striking gap by conducting the first systematic study on provable defenses against spatially transformed adversarial inputs. Our findings convey mixed messages. On the impossibility side, we show that such defenses may not exist in practice: for any given networks, it is possible to find legitimate inputs and imperceptible transformations to generate adversarial inputs that force arbitrarily large errors. On the possibility side, we show that it is still feasible to construct adversarial training methods to significantly improve the resilience of networks against adversarial inputs over empirical datasets. We believe our findings provide insights for designing more effective defenses against spatially transformed adversarial inputs.", "keywords": [], "authorids": ["xizc15@lehigh.edu", "yih319@lehigh.edu", "cpn217@lehigh.edu", "sji@zju.edu.cn", "inbox.ting@gmail.com"], "authors": ["Xinyang Zhang", "Yifan Huang", "Chanh Nguyen", "Shouling Ji", "Ting Wang"], "pdf": "/pdf/59c062c4bca5c470745a50f836ba3f48b3c9d7f4.pdf", "paperhash": "zhang|provable_defenses_against_spatially_transformed_adversarial_inputs_impossibility_and_possibility_results", "_bibtex": "@misc{\nzhang2019provable,\ntitle={Provable Defenses against Spatially Transformed Adversarial Inputs: Impossibility and Possibility Results},\nauthor={Xinyang Zhang and Yifan Huang and Chanh Nguyen and Shouling Ji and Ting Wang},\nyear={2019},\nurl={https://openreview.net/forum?id=HkeWSnR5Y7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1515/Official_Review", "cdate": 1542234213321, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkeWSnR5Y7", "replyto": "HkeWSnR5Y7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1515/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335962921, "tmdate": 1552335962921, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1515/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}