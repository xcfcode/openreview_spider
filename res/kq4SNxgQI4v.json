{"notes": [{"id": "kq4SNxgQI4v", "original": "PvNc_G8uXb9", "number": 1688, "cdate": 1601308186823, "ddate": null, "tcdate": 1601308186823, "tmdate": 1614985692082, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "NVXk-mKhgb", "original": null, "number": 1, "cdate": 1610040457001, "ddate": null, "tcdate": 1610040457001, "tmdate": 1610474059759, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "invitation": "ICLR.cc/2021/Conference/Paper1688/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Two reviewers suggested to reject and the other reviewer also thought it below the threshold."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "tags": [], "invitation": {"reply": {"forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040456988, "tmdate": 1610474059744, "id": "ICLR.cc/2021/Conference/Paper1688/-/Decision"}}}, {"id": "hNjyPWos7Bl", "original": null, "number": 1, "cdate": 1603944171961, "ddate": null, "tcdate": 1603944171961, "tmdate": 1606774368463, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "invitation": "ICLR.cc/2021/Conference/Paper1688/-/Official_Review", "content": {"title": "Interesting problem but the modeling is confusing", "review": "This paper proposes to integrate word alignment obtained from SMT into an NMT system. This is an exciting topic not only because it can help interpretability, but also because the same mechanism could be used e.g. for imposing a specific terminology in translations, something that was relatively easy to do with SMT but is much harder to achieve with NMT. The proposed method involves computing word-word alignment using existing SMT models (GIZA and FastAlign in the experiments) and integrating that information in the decoder of a transformer-based NMT model. Experiments on English-Romanian and English Korean show small improvement over a standard baseline.\n\nModeling is described at a high level and would require significant guesswork to re-implement. The core of the method is in the word substitution model (Sec 3.2) and how it is integrated in the decoder (Sec 3.3). Unfortunately neither is described with much detail. For example, there are a dozen operators involved in describing the model in Eqs. 1-17, some of them defined, some of them relatively easily guessable, other entirely unclear.\nIt would greatly help to clearly define the input and outputs of the model (e.g. vectors of context, matrices of probabilities or activation, appropriate dimensions, etc.)\n\nRegarding evaluation, it is a positive to have results on two language pairs, in both directions. In the English-Romanian pair, GIZA seems to produce slightly better results, although the difference is unlikely to be significant. Still, it is odd that only  FastAlign was used for Korean. Although gains seem fairly consistent, they are also very limited and unlikely to be significant (no significance test seems to have been performed). Also the variability of performance (e.g. due to sampling) is not assessed. Finally, the only comparison is to a straight transformer baseline. None of the methods mentioned in the prior art have been tested.\n\nThe analysis of the impact of the alignment on the attention weights in Sec 5.2 is interesting. What is the motivation for using different sentences and epoch #, rather than show how alignment is modeled as training progresses (as claimed in the text) on the same sentence?\n\nMisc:\n[Sec 5.1]: why is the sampling rate suddenly called the \u00ab\u00a0compression ratio\u00a0\u00bb?\n\n** UPDATE**\nRead the author answers, thanks. There is a significant amount of new material added in response to the reviews, including some interesting new findings (e.g. Tab. 3, 4). Unfortunately that did not help the clarity issue. I did not see a new example in Section 3.2.\nBasically, the stronger points of the paper (the results) are stronger, but the weaknesses (clarity, motivation) are not really addressed. The new version uploaded by the authors is an improvement but I still lean towards rejection.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1688/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1688/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113028, "tmdate": 1606915788638, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1688/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1688/-/Official_Review"}}}, {"id": "9gX-2zg1iX", "original": null, "number": 2, "cdate": 1603952404430, "ddate": null, "tcdate": 1603952404430, "tmdate": 1606545983757, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "invitation": "ICLR.cc/2021/Conference/Paper1688/-/Official_Review", "content": {"title": "Interesting idea of incorporating word alignment in a model", "review": "This work proposes to incorporate word alignment information as a word substitution model. Basic idea is to jointly train a separate encoder using a cross entropy loss which predicts a source input sequence with words substituted by aligned target words. The learned representation is combined with a Transformer either by simple summation, gating or joint attention mechanism. Experimental results on Romanian/English and Korean/English tasks show very marginal gains over the baseline Transformer.\n\n# Pros\n\n* It is a very interesting work on combining word alignment information in a model by predicting aligned target words by following alignment links.\n\n* Small gains are observed on Romanian/English and Korean/English.\n\n* The attention weight visualization especially for word substituted encoder presents sharper distribution, which might indicate that useful information might be learned in the attention mechanism.\n\n# Cons\n\n* The gains are very small and thus, I suspect that the difference might be negligible when running statistical significance tests.\n\n* Analysis is a bit weak in that it does not compare  the quality of alignment against the translation qualities.\n\n# Details\n\nThis work presents an interesting idea, though, I have a couple of concerns to this submission. Thus, I have some hesitation for full acceptance.\n\n* I'd like to see any tradeoffs of word alignment qualities and translation qualities by randomly distorting word alignment links. This work simply tweak sampling rate to vary the ratio of using word alignment links, not distorting links, and thus, it is not a direct measure to verify the quality tradeoff.\n\n  * Thank you very much for adding the comparison of GIZA and fastalign. However, simply swapping a model for alilgnment does not given us details about the tradeoff of alignment quality and the end-to-end results, since alignment models have different characteristic to capture the correspondence in two langauges, e.g., assuming linearlity and/or fertility. I'd rather like to see a much simpler approach of distroting alignment to avoid influence of the models employed for word alignment.\n\n* Given that non-aligned words are also predicted in the word substituted model in Equation 8, I suspect the model is simply learning to copy from input to output for non-aligned words. I think the loss in Equation 8 should be applied only to those words substituted by alignment links.\n  \n  * Thanks for the explanation. I'd like to see ablation studies regarding the loss.\n\n* Please verify the experimental results by statistical significance tests.\n\n  * Thank you for adding the tests. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1688/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1688/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113028, "tmdate": 1606915788638, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1688/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1688/-/Official_Review"}}}, {"id": "izbNmMi8VCr", "original": null, "number": 5, "cdate": 1605946649424, "ddate": null, "tcdate": 1605946649424, "tmdate": 1606229504782, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "invitation": "ICLR.cc/2021/Conference/Paper1688/-/Official_Comment", "content": {"title": "Paper Revision", "comment": "Dear Reviewers and AC, Thanks so much for your time and the constructive reviews. We have updated our revised paper. The detailed revision includes:\n\n1. revise Section 1 to make our motivation clearer. \\\n2. update Section 3.2 to explain the model in more detail. \\\n3. provide alignment error rate (AER) w.r.t FastAlign and GIZA++ alignment on Romanian-English and English-to-German datasets . (see Table 1)\\\n4. conduct statistical significance tests on all translation tasks. (see Table 2) \\\n5. For a more detailed evaluation, we add experimental results on the English-to-German dataset in Table2b. Besides, we reported the new baseline systems, including Constraint decoding [1], Training-by-replacing [2], and Training-by-appending [2] on the English-to-German translation task. (see Section 4.2)\\\n6. add an evaluation of alignment usage on low-resource cases (see Section 5.3 and Table 3c) and the analysis on translation examples. (see Section 5.4 and Table 4)\n\n\n[1]Matt Post and David Vilar. Fast lexically constrained decoding with dynamic beam allocation for neural machine translation. NAACL, 2018. \\\n[2]Georgiana Dinu, Prashant Mathur, Marcello Federico, and Yaser Al-Onaizan.  Training neural ma-chine translation to apply terminology constraints. ACL, 2019."}, "signatures": ["ICLR.cc/2021/Conference/Paper1688/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kq4SNxgQI4v", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1688/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1688/Authors|ICLR.cc/2021/Conference/Paper1688/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856889, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1688/-/Official_Comment"}}}, {"id": "pwITMXvXP0N", "original": null, "number": 8, "cdate": 1605946754870, "ddate": null, "tcdate": 1605946754870, "tmdate": 1606229344984, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": "hNjyPWos7Bl", "invitation": "ICLR.cc/2021/Conference/Paper1688/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your comprehensive review and suggestions to improve the paper. Below are our responses to your comments:\n\n**Need GIZA++ case for Korean-English translation tasks.**\n\nWe added the experiment results, which use alignment from GIZA++ for Korean-English translation tasks, in the new pdf version. Unfortunately, we could not find the gold alignment dataset for Korean-English, so we could not check the dataset's alignment error rate (AER) result.\n\n**Describe the model with much detail.**\n\nTo explain the model in more detail, We added an example of the input and output of the model to section 3.2. \n\n**What is the motivation for using different sentences and epoch # in Sec 5.2?**\n\nWe thought that it would be better to show multiple sentences because the additional attention head is already well learned at the beginning of learning. However, we removed Section 5.2. Instead, we added an analysis of the translation examples of our model to explain in more detail how our model contributes to the translation quality."}, "signatures": ["ICLR.cc/2021/Conference/Paper1688/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kq4SNxgQI4v", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1688/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1688/Authors|ICLR.cc/2021/Conference/Paper1688/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856889, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1688/-/Official_Comment"}}}, {"id": "E7sRtuQvNBL", "original": null, "number": 7, "cdate": 1605946728108, "ddate": null, "tcdate": 1605946728108, "tmdate": 1605946728108, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": "9gX-2zg1iX", "invitation": "ICLR.cc/2021/Conference/Paper1688/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your comments and suggestions to improve the paper. Below are our responses to your comments:\n\n**Verify the experimental results by statistical significance tests.**\n\nWe conducted statistical significance tests on all translation tasks and updated Table 2.\n\n**The loss in Equation 8 should be applied only to those words substituted by alignment links.**\n\nWe also thought it would be best to apply the loss function only to those words substituted by alignment links. However, we observed that the method significantly lower the translation quality in a preliminary experiment. As a result of analyzing the alignment-based word substitution (ABWS) model's output, we found that the method leads to many invalid word replacement. Even unaligned words were all replaced with improper target terms. The reason is that the loss for unaligned words is not applied, and the replacement process proceeds without considering the context. Therefore, we applied the loss for unaligned words.\n\n**Show tradeoffs of word alignment qualities and translation qualities by randomly distorting word alignment links.**\n\nWe can see this tradeoff through the difference in alignment error rate (AER) w.r.t alignment information extracted from FastAlign and GIZA++. We added the AER results for those two auto aligners to Table 1. The AER gap (4% on average) between the two different prior word alignments in Romanian-English translation tasks is small. However, the gap (10%) in English-to-German translation tasks is relatively large. This aspect is the same in translation quality in Table 2. For example, for Romanian-English, the difference in BLEU between the two aligners is 0.075 on average. However, for English-to-German, the difference is 0.26 on average. This means that alignment quality and translation quality are in a proportional relationship. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1688/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kq4SNxgQI4v", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1688/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1688/Authors|ICLR.cc/2021/Conference/Paper1688/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856889, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1688/-/Official_Comment"}}}, {"id": "X795aLMb-c", "original": null, "number": 6, "cdate": 1605946697895, "ddate": null, "tcdate": 1605946697895, "tmdate": 1605946697895, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": "5R_j-mLAX4x", "invitation": "ICLR.cc/2021/Conference/Paper1688/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "First, thank you very much for your valuable comments. \n\n**why the prior alignments are crucial information that is necessary to be given a-priori to the Transformer?**\n\nSeveral previous studies [1,2,3] inject alignment signal directly into an attention head of Transformer for constraint decoding or better alignment extraction, but they do not lower or change the translation performance.\nHowever, Our purpose is to provide the model hints or guidelines for the target sentence at running time. This approach has proven in many previous papers that it can lead to improved quality of translation. In particular, unlike previous studies that must require a pre-defined dictionary or change the beam search, our model achieves the goal only with prior word alignment. As shown in Table 2 (b), our methods outperform the previous methods.\n\n**Need usage of the idea in previous work and SMT literature.**\n\nWe added the new baseline models on English-to-German translation tasks to compare our proposed model with them in Table 2. And we cited SMT literature using the alignment model in our revised version.\n\n**Most languages do not have a one-to-one mapping in words.**\n\nYes, there are one-to-null (unaligned word) and one-to-many (multi-aligned word) in addition to one-to-one for the bilingual word alignment extracted from automatic word aligner. In addition, when we na\u00efvely extracted the dictionary from the alignment, we observed that there are very few one-to-one matches. Therefore, we propose the ABWS model to proceed with the replacement process that considers the context for translation. We added an analysis of the corresponding replacement process in Section 5.4 and Table 4. Specifically, We let the model is simply learning to copy from input to output for non-aligned words and present solutions for one-to-many matches in Section 3.2. \n\n**About extremely low-resource cases**\n\nWe added evaluation of alignment usage on low-resource cases in revised version. We found that our proposed model achieved even more significant improvement on small English-to-German dataset (10M). Please see the corresponding analysis in Section 5.3 and Table 3c.\n\n**Have the authors analyzed what is exactly being improved in the translations?**\n\nWe added the analysis on translation examples in revised version. Please see the corresponding analysis in Section 5.4. and Table 4.\n\n[1]Tamer Alkhouli, et al. \u201cOn the alignment problem in multi-head attention-based neural machine translation\u201d, ACL, 2018.\\\n[2]Sarthak Garg, et al. \"Jointly learning to align and translate with transformer models\", EMNLP, 2018.\\\n[3]Kai Song, et al. \"Alignment-enhanced transformer for constraining nmt with pre-specified translations\", AAAI, 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1688/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "kq4SNxgQI4v", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1688/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1688/Authors|ICLR.cc/2021/Conference/Paper1688/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856889, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1688/-/Official_Comment"}}}, {"id": "5R_j-mLAX4x", "original": null, "number": 3, "cdate": 1603966708282, "ddate": null, "tcdate": 1603966708282, "tmdate": 1605024382616, "tddate": null, "forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "invitation": "ICLR.cc/2021/Conference/Paper1688/-/Official_Review", "content": {"title": "An interesting idea but needs more progress", "review": "The paper presents a strategy to integrate prior word alignments into NMT models. It is not clear the motivation for this in the NMT context, especially why the prior alignments are crucial information that is necessary to be given a-priori to the Transformer. Besides this, the description of the method and the discussion of related work is given, SMT methods are briefly mentioned but the usage of the idea in previous work, also SMT literature is necessary.\nThe applicability of the method is discussed very generally for NMT, however most languages do not have a one-to-one mapping in words and even in extremely low-resource cases it is difficult to see how the alignment information can be useful except for translating for instance foreign words and named entities (in languages where transliteration is not necessary).\nThe description of the method seems sound and the experiments are performed for two languages with improvements in BLEU scores. Have the authors analyzed what is exactly being improved in the translations?\nThe paper is mostly clear but has too many grammatical errors and can benefit from revisions and more editing.\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1688/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1688/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Neural Machine Translation with Prior Word Alignment", "authorids": ["~Jeonghyeok_Park2", "~hai_zhao1"], "authors": ["Jeonghyeok Park", "hai zhao"], "keywords": ["Word Alignment", "Neural Machine Translation"], "abstract": "Prior word alignment has been shown indeed helpful for a better translation if such prior is good enough and can be acquired in a convenient way at the same time. Traditionally, word alignment can be learned through statistical machine translation (SMT) models. In this paper, we propose a novel method that infuses prior word alignment information into neural machine translation (NMT) to provide hints or guidelines for the target sentence at running time. To this end, previous works of similar approaches should build dictionaries for specific domains, or constraint the decoding process, or both. While being effective to some extent, these methods may greatly affect decoding speed and hurt translation flexibility and efficiency.  Instead, this paper introduces an enhancement learning model, which can learn how to directly replace specific source words with their target counterparts according to prior alignment information. The proposed model is then inserted into a neural MT model and augments MT input with the additional target information from the learning model in an effective and more efficient way. Our novel method achieves BLEU improvements (up to 1.1) over a strong baseline model on English-Korean and English-Romanian translation tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "park|efficient_neural_machine_translation_with_prior_word_alignment", "pdf": "/pdf/0cf13e458b68c2b7ca01cc7e930c40a06a4dfb6d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=RfP-uGF2td", "_bibtex": "@misc{\npark2021efficient,\ntitle={Efficient Neural Machine Translation with Prior Word Alignment},\nauthor={Jeonghyeok Park and hai zhao},\nyear={2021},\nurl={https://openreview.net/forum?id=kq4SNxgQI4v}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "kq4SNxgQI4v", "replyto": "kq4SNxgQI4v", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1688/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538113028, "tmdate": 1606915788638, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1688/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1688/-/Official_Review"}}}], "count": 9}