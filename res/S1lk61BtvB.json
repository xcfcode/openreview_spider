{"notes": [{"id": "S1lk61BtvB", "original": "BJeDEQJYvS", "number": 1974, "cdate": 1569439671195, "ddate": null, "tcdate": 1569439671195, "tmdate": 1577168272957, "tddate": null, "forum": "S1lk61BtvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["abhattac@mpi-inf.mpg.de", "fritz@cispa.saarland", "schiele@mpi-inf.mpg.de"], "title": "``\"Best-of-Many-Samples\" Distribution Matching", "authors": ["Apratim Bhattacharyya", "Mario Fritz", "Bernt Schiele"], "pdf": "/pdf/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "TL;DR": "We propose a new objective for training hybrid VAE-GANs which lead to significant improvement in mode coverage and quality.", "abstract": "Generative Adversarial Networks (GANs) can achieve state-of-the-art sample quality in generative modelling tasks but suffer from the mode collapse problem. Variational Autoencoders (VAE) on the other hand explicitly maximize a reconstruction-based data log-likelihood forcing it to cover all modes, but suffer from poorer sample quality. Recent works have proposed hybrid VAE-GAN frameworks which integrate a GAN-based synthetic likelihood to the VAE objective to address both the mode collapse and sample quality issues, with limited success. This is because the VAE objective forces a trade-off between the data log-likelihood and divergence to the latent prior. The synthetic likelihood ratio term also shows instability during training. We propose a novel objective with a ``\"Best-of-Many-Samples\" reconstruction cost and a stable direct estimate of the synthetic likelihood. This enables our hybrid VAE-GAN framework to achieve high data log-likelihood and low divergence to the latent prior at the same time and shows significant improvement over both hybrid VAE-GANS and plain GANs in mode coverage and quality.", "code": "https://drive.google.com/drive/folders/10RCFaA8kOgkRHXIJpXIWAC-uUyLiEhlY?usp=sharing", "keywords": ["Distribution Matching", "Generative Adversarial Networks", "Variational Autoencoders"], "paperhash": "bhattacharyya|``bestofmanysamples_distribution_matching", "original_pdf": "/attachment/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "_bibtex": "@misc{\nbhattacharyya2020bestofmanysamples,\ntitle={``''Best-of-Many-Samples'' Distribution Matching},\nauthor={Apratim Bhattacharyya and Mario Fritz and Bernt Schiele},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lk61BtvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "B-wu7JbbUM", "original": null, "number": 1, "cdate": 1576798737266, "ddate": null, "tcdate": 1576798737266, "tmdate": 1576800899110, "tddate": null, "forum": "S1lk61BtvB", "replyto": "S1lk61BtvB", "invitation": "ICLR.cc/2020/Conference/Paper1974/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposed an improvement on VAE-GAN which draws multiple samples from the reparameterized latent distribution for each inferred q(z|x), and only backpropagates reconstruction error for the resulting G(z) which has the lowest reconstruction.  While the idea is interesting, the novelty is not high compared with existing similar works, and the improvement is not significant.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abhattac@mpi-inf.mpg.de", "fritz@cispa.saarland", "schiele@mpi-inf.mpg.de"], "title": "``\"Best-of-Many-Samples\" Distribution Matching", "authors": ["Apratim Bhattacharyya", "Mario Fritz", "Bernt Schiele"], "pdf": "/pdf/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "TL;DR": "We propose a new objective for training hybrid VAE-GANs which lead to significant improvement in mode coverage and quality.", "abstract": "Generative Adversarial Networks (GANs) can achieve state-of-the-art sample quality in generative modelling tasks but suffer from the mode collapse problem. Variational Autoencoders (VAE) on the other hand explicitly maximize a reconstruction-based data log-likelihood forcing it to cover all modes, but suffer from poorer sample quality. Recent works have proposed hybrid VAE-GAN frameworks which integrate a GAN-based synthetic likelihood to the VAE objective to address both the mode collapse and sample quality issues, with limited success. This is because the VAE objective forces a trade-off between the data log-likelihood and divergence to the latent prior. The synthetic likelihood ratio term also shows instability during training. We propose a novel objective with a ``\"Best-of-Many-Samples\" reconstruction cost and a stable direct estimate of the synthetic likelihood. This enables our hybrid VAE-GAN framework to achieve high data log-likelihood and low divergence to the latent prior at the same time and shows significant improvement over both hybrid VAE-GANS and plain GANs in mode coverage and quality.", "code": "https://drive.google.com/drive/folders/10RCFaA8kOgkRHXIJpXIWAC-uUyLiEhlY?usp=sharing", "keywords": ["Distribution Matching", "Generative Adversarial Networks", "Variational Autoencoders"], "paperhash": "bhattacharyya|``bestofmanysamples_distribution_matching", "original_pdf": "/attachment/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "_bibtex": "@misc{\nbhattacharyya2020bestofmanysamples,\ntitle={``''Best-of-Many-Samples'' Distribution Matching},\nauthor={Apratim Bhattacharyya and Mario Fritz and Bernt Schiele},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lk61BtvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1lk61BtvB", "replyto": "S1lk61BtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725872, "tmdate": 1576800277868, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1974/-/Decision"}}}, {"id": "SkxNbzojYB", "original": null, "number": 1, "cdate": 1571693052226, "ddate": null, "tcdate": 1571693052226, "tmdate": 1572972399528, "tddate": null, "forum": "S1lk61BtvB", "replyto": "S1lk61BtvB", "invitation": "ICLR.cc/2020/Conference/Paper1974/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\u201cBest of Many Samples\u201d Distribution matching\n\nSummary:\n\nThis paper proposes to a novel VAE-GAN hybrid which, during training, draws multiple samples from the reparameterized latent distribution for each inferred q(z|x), and only backpropagates reconstruction error for the resulting G(z) which has the lowest reconstruction. The authors appear to use the AAE method instead of analytically matching KL(p||q) for enforcing that the latents q(z|x) match the prior. The authors present results on MoG toy datasets, CIFAR-10, and CelebA, and compare  against several other models.\n\nMy take:\n\nThe idea in this paper is moderately interesting, well-founded, has plenty of precedent in the literature (while still being reasonably novel),  but the results present only a minimal improvement (a 5% relative improvement in FID over the baseline model from Rosca et al on CIFAR, especially when including SN [which is not a contribution of this paper]) and come at a substantial compute cost, requiring up to 30 extra samples per batch in order to attain this minimal increase. While I think the idea is interesting, the change in results over Rosca et. al does not seem to justify its increased computational expense (which is also not characterized in sufficient thoroughness). I am pretty borderline on this paper ( about a 5/10) but under the 1-3-6-8 scoring constraint I tend to lean reject because while I like the idea, I do not think the results are significant enough to support its adoption; I think the relative compute and implementation cost limit this method\u2019s potential impact. I am keen to discuss this paper with the other reviewers.\n\nNotes:\n\n-The results on the 2D MoG toy datasets are good but are also suspect\u2014the authors state that they use a 32-dimensional latent space, but the original code provided for VEEGAN uses a 2-dimensional latent space. The authors should re-run the experiment for BMS-VAE-GAN  using a 2D latent space (this should be very easy and take less than an hour on a GPU to get several runs in).\n\n-\u201cagain outperforming by a significant margin (21.8 vs 22.9 FID)\u201d This is not a significant margin; this is less than a 5% margin and, at those FID scores, represents an imperceptible change in sample quality.\n\n-The authors seem to suggest that applying spectral norm to the GAN of Rosca et. al. is somehow a contribution (e.g. having \u201cours\u201d next to this model in the tables); I would advise against even appearing to suggest this as it is clearly not a contribution.\n\n-Characterize the increase in compute cost. \u201c. We use as many samples during training as would fit in GPU memory so that we make the same number of forward/backward passes as other approaches and minimize the computational overhead of sampling multiple samples\u201d is a qualitative description; I would like to see this quantitatively described. How do the runtimes differ between your baseline and the T=10 and T=30 runs? If they don\u2019t differ, why? Are the authors e.g. reducing the batch size by a factor of 10 or 30 to make this computationally tractable?\n\n-The latent space discriminator D_L should be referred to in section 3; its formal introduction is deferred to later in the paper, hampering the presentation and flow. \n\n-CelebA is not multimodal; it is in fact, highly constrained, and primarily only has textural variation (virtually no pose variation).\n\n-ALI and BiGAN are listed under Hybrid VAE-GANs. These models are not VAE-GAN hybrids. Additionally, this section states that BiGAN builds upon ALI. This is not true, these papers are in fact proposing the same thing and were released at nearly the exact same time.  Do not mischaracterize or incorrectly summarize papers. Please re-read both papers and refer to them correctly.\n\n-Mode collapse (when many points in z map to an unexpectedly small region in G(z)) is a different phenomenon from mode dropping (when many points in x are not represented in G(z), i.e. no point in z maps to a cluster of x\u2019s, as is the case if e.g. a celebA model generates frowning and neutral faces but no smiling faces). While these phenomena often co-occur (especially during complete training collapse), they are not the same thing, and this paper conflates them in several places.\n\nMinor:\n\nSection 3, paragraph 2: \u201cThe GAN (G\u03b8,DI\u2026\u201d There\u2019s a close parenthesis missing here. \n\nSection 3.3: \u201cThe network is traiend\u2026\u201d \n\nPlease thoroughly proofread your paper for typos and grammatical mistakes.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1974/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1974/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abhattac@mpi-inf.mpg.de", "fritz@cispa.saarland", "schiele@mpi-inf.mpg.de"], "title": "``\"Best-of-Many-Samples\" Distribution Matching", "authors": ["Apratim Bhattacharyya", "Mario Fritz", "Bernt Schiele"], "pdf": "/pdf/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "TL;DR": "We propose a new objective for training hybrid VAE-GANs which lead to significant improvement in mode coverage and quality.", "abstract": "Generative Adversarial Networks (GANs) can achieve state-of-the-art sample quality in generative modelling tasks but suffer from the mode collapse problem. Variational Autoencoders (VAE) on the other hand explicitly maximize a reconstruction-based data log-likelihood forcing it to cover all modes, but suffer from poorer sample quality. Recent works have proposed hybrid VAE-GAN frameworks which integrate a GAN-based synthetic likelihood to the VAE objective to address both the mode collapse and sample quality issues, with limited success. This is because the VAE objective forces a trade-off between the data log-likelihood and divergence to the latent prior. The synthetic likelihood ratio term also shows instability during training. We propose a novel objective with a ``\"Best-of-Many-Samples\" reconstruction cost and a stable direct estimate of the synthetic likelihood. This enables our hybrid VAE-GAN framework to achieve high data log-likelihood and low divergence to the latent prior at the same time and shows significant improvement over both hybrid VAE-GANS and plain GANs in mode coverage and quality.", "code": "https://drive.google.com/drive/folders/10RCFaA8kOgkRHXIJpXIWAC-uUyLiEhlY?usp=sharing", "keywords": ["Distribution Matching", "Generative Adversarial Networks", "Variational Autoencoders"], "paperhash": "bhattacharyya|``bestofmanysamples_distribution_matching", "original_pdf": "/attachment/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "_bibtex": "@misc{\nbhattacharyya2020bestofmanysamples,\ntitle={``''Best-of-Many-Samples'' Distribution Matching},\nauthor={Apratim Bhattacharyya and Mario Fritz and Bernt Schiele},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lk61BtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lk61BtvB", "replyto": "S1lk61BtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1974/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1974/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575847305384, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1974/Reviewers"], "noninvitees": [], "tcdate": 1570237729595, "tmdate": 1575847305400, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1974/-/Official_Review"}}}, {"id": "B1lD7rTsFB", "original": null, "number": 2, "cdate": 1571702046791, "ddate": null, "tcdate": 1571702046791, "tmdate": 1572972399493, "tddate": null, "forum": "S1lk61BtvB", "replyto": "S1lk61BtvB", "invitation": "ICLR.cc/2020/Conference/Paper1974/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "1.  Using Discriminator to estimate the likelihood ratio is a commonly used approach, which was first proposed in [1]. This is also generalized as a reversed KL based GAN in [2] [3]. The authors failed to discuss this with these previous works in Section 3.3 and in Related works.\n\n2. How is the best of many comparing with importance sampling method? I think using importance sampling is the most intuitive baseline.\n\n3. this paper is not well written. L_1/L_2 has never explained throughout this paper, also has typos such as \"taiend\".\n\n\n[1] Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks\n[2] Variational Annealing of GANs: A Langevin Perspective\n[3] Symmetric variational autoencoder and connections to adversarial learning"}, "signatures": ["ICLR.cc/2020/Conference/Paper1974/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1974/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abhattac@mpi-inf.mpg.de", "fritz@cispa.saarland", "schiele@mpi-inf.mpg.de"], "title": "``\"Best-of-Many-Samples\" Distribution Matching", "authors": ["Apratim Bhattacharyya", "Mario Fritz", "Bernt Schiele"], "pdf": "/pdf/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "TL;DR": "We propose a new objective for training hybrid VAE-GANs which lead to significant improvement in mode coverage and quality.", "abstract": "Generative Adversarial Networks (GANs) can achieve state-of-the-art sample quality in generative modelling tasks but suffer from the mode collapse problem. Variational Autoencoders (VAE) on the other hand explicitly maximize a reconstruction-based data log-likelihood forcing it to cover all modes, but suffer from poorer sample quality. Recent works have proposed hybrid VAE-GAN frameworks which integrate a GAN-based synthetic likelihood to the VAE objective to address both the mode collapse and sample quality issues, with limited success. This is because the VAE objective forces a trade-off between the data log-likelihood and divergence to the latent prior. The synthetic likelihood ratio term also shows instability during training. We propose a novel objective with a ``\"Best-of-Many-Samples\" reconstruction cost and a stable direct estimate of the synthetic likelihood. This enables our hybrid VAE-GAN framework to achieve high data log-likelihood and low divergence to the latent prior at the same time and shows significant improvement over both hybrid VAE-GANS and plain GANs in mode coverage and quality.", "code": "https://drive.google.com/drive/folders/10RCFaA8kOgkRHXIJpXIWAC-uUyLiEhlY?usp=sharing", "keywords": ["Distribution Matching", "Generative Adversarial Networks", "Variational Autoencoders"], "paperhash": "bhattacharyya|``bestofmanysamples_distribution_matching", "original_pdf": "/attachment/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "_bibtex": "@misc{\nbhattacharyya2020bestofmanysamples,\ntitle={``''Best-of-Many-Samples'' Distribution Matching},\nauthor={Apratim Bhattacharyya and Mario Fritz and Bernt Schiele},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lk61BtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lk61BtvB", "replyto": "S1lk61BtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1974/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1974/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575847305384, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1974/Reviewers"], "noninvitees": [], "tcdate": 1570237729595, "tmdate": 1575847305400, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1974/-/Official_Review"}}}, {"id": "r1ejzBAX5r", "original": null, "number": 3, "cdate": 1572230418904, "ddate": null, "tcdate": 1572230418904, "tmdate": 1572972399457, "tddate": null, "forum": "S1lk61BtvB", "replyto": "S1lk61BtvB", "invitation": "ICLR.cc/2020/Conference/Paper1974/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper presents a new objective function for hybrid VAE-GANs. To overcome a number of known issues with VAE-GANs, this work uses multiple samples from the generator network to achieve a high data log-likelihood and low divergence to the latent prior.\nIn the experimental section, the ``\"Best-of-Many-Samples\" approach is shown to outperform other state-of-the-art methods on CIFAR-10 and a synthetic dataset.\n\nThanks for submitting code with your submission!\n\nCaveat: I'm not an expert in this domain and did my best to review this paper.\n\nQuestions:\n- Considering the smaller gap between \u03b1-GAN+SN and BMS-VAE-GAN, I was wondering how much of the improvement is due to spectral normalization vs using multiple samples. Did you do an ablation study of BMS-VAE-GAN without SN?\n- I noticed some minor typos in the text. Please fix (3.2 \"constrains\" -> \"constraints\", 3.3 \"traiend\", 3.3 \"unsure\" -> \"ensure\")."}, "signatures": ["ICLR.cc/2020/Conference/Paper1974/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1974/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["abhattac@mpi-inf.mpg.de", "fritz@cispa.saarland", "schiele@mpi-inf.mpg.de"], "title": "``\"Best-of-Many-Samples\" Distribution Matching", "authors": ["Apratim Bhattacharyya", "Mario Fritz", "Bernt Schiele"], "pdf": "/pdf/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "TL;DR": "We propose a new objective for training hybrid VAE-GANs which lead to significant improvement in mode coverage and quality.", "abstract": "Generative Adversarial Networks (GANs) can achieve state-of-the-art sample quality in generative modelling tasks but suffer from the mode collapse problem. Variational Autoencoders (VAE) on the other hand explicitly maximize a reconstruction-based data log-likelihood forcing it to cover all modes, but suffer from poorer sample quality. Recent works have proposed hybrid VAE-GAN frameworks which integrate a GAN-based synthetic likelihood to the VAE objective to address both the mode collapse and sample quality issues, with limited success. This is because the VAE objective forces a trade-off between the data log-likelihood and divergence to the latent prior. The synthetic likelihood ratio term also shows instability during training. We propose a novel objective with a ``\"Best-of-Many-Samples\" reconstruction cost and a stable direct estimate of the synthetic likelihood. This enables our hybrid VAE-GAN framework to achieve high data log-likelihood and low divergence to the latent prior at the same time and shows significant improvement over both hybrid VAE-GANS and plain GANs in mode coverage and quality.", "code": "https://drive.google.com/drive/folders/10RCFaA8kOgkRHXIJpXIWAC-uUyLiEhlY?usp=sharing", "keywords": ["Distribution Matching", "Generative Adversarial Networks", "Variational Autoencoders"], "paperhash": "bhattacharyya|``bestofmanysamples_distribution_matching", "original_pdf": "/attachment/4bc3077bb44c606fda33ddd4f51c5f233ff7a032.pdf", "_bibtex": "@misc{\nbhattacharyya2020bestofmanysamples,\ntitle={``''Best-of-Many-Samples'' Distribution Matching},\nauthor={Apratim Bhattacharyya and Mario Fritz and Bernt Schiele},\nyear={2020},\nurl={https://openreview.net/forum?id=S1lk61BtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1lk61BtvB", "replyto": "S1lk61BtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1974/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1974/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575847305384, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1974/Reviewers"], "noninvitees": [], "tcdate": 1570237729595, "tmdate": 1575847305400, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1974/-/Official_Review"}}}], "count": 5}