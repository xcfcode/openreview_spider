{"notes": [{"id": "HJxf53EtDr", "original": "ryeiCXI0Ir", "number": 107, "cdate": 1569438857939, "ddate": null, "tcdate": 1569438857939, "tmdate": 1577168283875, "tddate": null, "forum": "HJxf53EtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BZIvm0T2T", "original": null, "number": 1, "cdate": 1576798687571, "ddate": null, "tcdate": 1576798687571, "tmdate": 1576800947535, "tddate": null, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Decision", "content": {"decision": "Reject", "comment": "The paper makes an interesting attempt at connecting graph convolutional neural networks (GCN) with matrix factorization (MF) and then develops a MF solution that achieves similar prediction performance as GCN. \n\nWhile the work is a good attempt, the work suffers from two major issues: (1)  the connection between GCN and other related models have been examined recently. The paper did not provide additional insights; (2) some parts of the derivations could be problematic. \n\nThe paper could be a good publication in the future if the motivation of the work can be repositioned. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730435, "tmdate": 1576800283228, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper107/-/Decision"}}}, {"id": "BylXmt2nYH", "original": null, "number": 2, "cdate": 1571764506728, "ddate": null, "tcdate": 1571764506728, "tmdate": 1574421055636, "tddate": null, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "\nThe work poses an interesting question: Are GCNs (and GNNs) just special types of matrix factorization methods? Unfortunately, the short answer is **no**, which goes against what the authors say. \n\nUntil recently I thought like the authors, but the concurrent work [1] (On the Equivalence between Node Embeddings and Structural Graph Representations) https://openreview.net/forum?id=SJxzFySKwH changed my mind. \nThe work of (Li et al., 2018) shows that nearby nodes tend to get similar representations. There is mounting experimental evidence of that being the case in real-world graphs (e.g., https://arxiv.org/abs/1908.08572). But [1] shows that GCNs and GNNs are fundamentally different from matrix factorization methods, regardless of the loss function used to learn the embeddings. Consider Figure 1 in [1], and it is easy to see that matrix factorization will give different embeddings to the Lynx and Orca nodes, while GCNs and GNNs must give the same embedding. Even if we connect the graphs through the Spruce and the Zooplankton nodes, their conclusion would not change. Matrix factorization (as broadly understood) will give embeddings that can even be used to cluster nodes. The eigenvectors of the symmetric Laplacian encode the diffusion of a type of random walk and nodes that are far away in the graph must have different embeddings (because through the diffusion operator, they are far away).\n\nIn GCNs, the convergence of the embeddings is better explained by the mixing of a random walk (Theorem 1 of (Xu et al., 2018)), which, in the special case of a GCN, converges to 1/sqrt(degree of node), as shown by (Li et al., 2018) in their Theorem 1 for the symmetric Laplacian. This is unrelated to what we get in matrix factorization as explained earlier. \n\nWhat is wrong with the math: Equation (11) is equated with matrix factorization, but note that it does not account for nonedges, while matrix factorization accounts for nonedges. This issue is more clear in Equation (14). The problem happens when the paper jumps from Equation (14), which is correct but not MF, to Equation (15) which is MF but unrelated to Equation (14). The argument is that \u201cnegative edges sampling is used, for better convergence\u201d\u2026 sorry, not for better convergence, it completely changes the optimization objective. Hence, GCNs are not matrix factorization methods.\n\nI think the paper is a valiant effort, but unfortunately the core premise is incorrect. The jump from Equation (14) to equation (15) cannot be justified, and I believe showcases a fundamental flaw the argument. I do not see a way to fix the paper. I vote to reject it. \n\n[1] On the Equivalence between Node Embeddings and Structural Graph Representations, https://openreview.net/forum?id=SJxzFySKwH \nXu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.I. and Jegelka, S., 2018. Representation learning on graphs with jumping knowledge networks. ICML 2018.\nLi, Qimai, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. AAAI, 2018.\n\n\n--------------\n\nRead rebuttal. Will keep my original assessment.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575587105957, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper107/Reviewers"], "noninvitees": [], "tcdate": 1570237756968, "tmdate": 1575587105973, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Review"}}}, {"id": "B1e9X3qDjr", "original": null, "number": 7, "cdate": 1573526562011, "ddate": null, "tcdate": 1573526562011, "tmdate": 1573526562011, "tddate": null, "forum": "HJxf53EtDr", "replyto": "rkxH0TrLsB", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment", "content": {"title": "Thank you for the response.", "comment": "Thank you for the response. However, the arguments about the connection to MF is not very strong. I will remain my initial evaluation."}, "signatures": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper107/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper107/Authors|ICLR.cc/2020/Conference/Paper107/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176300, "tmdate": 1576860533910, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment"}}}, {"id": "rJl7sDDviH", "original": null, "number": 6, "cdate": 1573513115430, "ddate": null, "tcdate": 1573513115430, "tmdate": 1573513115430, "tddate": null, "forum": "HJxf53EtDr", "replyto": "B1luJxPUjr", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment", "content": {"title": "Thank you for the response.", "comment": "Thank you for the response. While I appreciate the empirical results, the arguments however are not strong enough to override the original assessment. Hope the work can be improved addressing the four critical points mentioned in the review."}, "signatures": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper107/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper107/Authors|ICLR.cc/2020/Conference/Paper107/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176300, "tmdate": 1576860533910, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment"}}}, {"id": "B1luJxPUjr", "original": null, "number": 5, "cdate": 1573445599648, "ddate": null, "tcdate": 1573445599648, "tmdate": 1573445599648, "tddate": null, "forum": "HJxf53EtDr", "replyto": "HJxxpshhYB", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment", "content": {"title": "Thanks for your comments.", "comment": "Thanks for your comments.\n\n[1-2] We are sorry that our expression might be misleading. And I agree that the conclusion in our work is not an extreme equivelant form of GCN. The aim of our work is to give a MF-based form to GCN for flexible modeling in real-world large-scale applications. Finding an extreme equivelant alternative model of GCN is not our real purpose. And according to our observation, this purpose is satisfied. According to our experimental results, MF is good enough for node classification in all kinds of settings, and we don't need to use GCN in real-world applications. I think this is a valuable conclusion of our work.\n\n(3) This is incorporated following previos work [1].\n\n(4) We ran the models on three workers. The speedup is not our focus in this work. We only want to show that batched or distributed GCN suffers from performance loss.\n\n[1] Weston J, Ratle F, Mobahi H, et al. Deep learning via semi-supervised embedding[M]//Neural Networks: Tricks of the Trade. Springer, Berlin, Heidelberg, 2012: 639-655."}, "signatures": ["ICLR.cc/2020/Conference/Paper107/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper107/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper107/Authors|ICLR.cc/2020/Conference/Paper107/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176300, "tmdate": 1576860533910, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment"}}}, {"id": "rkgCKBUUjB", "original": null, "number": 4, "cdate": 1573442949608, "ddate": null, "tcdate": 1573442949608, "tmdate": 1573442949608, "tddate": null, "forum": "HJxf53EtDr", "replyto": "BylXmt2nYH", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment", "content": {"title": "Thanks for your comments.", "comment": "Thanks for your comments.\n\nI agree that the conclusion in our work is not an extreme equivelant form of GCN, but I don't think it is \"incorrect\". The aim of our work is to give a MF-based form to GCN for flexible modeling in real-world large-scale applications. Finding an extreme equivelant alternative model of GCN is not our real purpose. And according to our observation, this purpose is satisfied. According to our experimental results, MF is good enough for node classification in all kinds of settings, and we don't need to use GCN in real-world applications. I think this is a valuable conclusion of our work."}, "signatures": ["ICLR.cc/2020/Conference/Paper107/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper107/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper107/Authors|ICLR.cc/2020/Conference/Paper107/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176300, "tmdate": 1576860533910, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment"}}}, {"id": "rkxH0TrLsB", "original": null, "number": 3, "cdate": 1573440973264, "ddate": null, "tcdate": 1573440973264, "tmdate": 1573440973264, "tddate": null, "forum": "HJxf53EtDr", "replyto": "Skx8x7nTuS", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment", "content": {"title": "Thanks for your comments.", "comment": "Thanks for your comments. Here are our replies:\n\n(1) Yes, the connection is not quiet direct. The aim of our work is to give a MF-based form to GCN for flexible modeling in real-world large-scale applications. Finding an extreme equivelant alternative model of GCN is not our real purpose. And node embedding methods can be unified as MF in previous works [1], so GCN is related to node embedding as well as MF.\n\n(2) Yes, this will be futher investigated in our future works.\n\n(3) Actually, the two methods achieve similar accuracy. And the method with alternative batches between structure and classification loss can converge easier. The difference is not significant.\n\n(4) As only accuracy is reported in previous works, so we followed this setting in our work.\n\n\n[1] Qiu J, Dong Y, Ma H, et al. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec[C]//Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 2018: 459-467."}, "signatures": ["ICLR.cc/2020/Conference/Paper107/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper107/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper107/Authors|ICLR.cc/2020/Conference/Paper107/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176300, "tmdate": 1576860533910, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment"}}}, {"id": "Skx8x7nTuS", "original": null, "number": 1, "cdate": 1570779885650, "ddate": null, "tcdate": 1570779885650, "tmdate": 1572972637852, "tddate": null, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In this paper, the authors propose a new method for semi-supervised node classification by drawing connection between GCN and MF. The authors borrow the idea of convergence of GCN as Laplacian Smoothing. With this observation, the authors propose a joint loss with two components: classification loss and structure loss for the similarity between embedding of neighboring nodes. The authors train the parameters via optimizing the two losses alternatively. Experiments are carried out on seven networks with comparison to baselines.\n\nStrength:\n1. It is an interesting and innovative idea to draw connection between GCN and MF.\n2. The propose method is more suitable for distributed setting. With negative sampling for structure loss, both structure batch and classification batch can be constructed locally with only one-hop information.\n3. The authors carry out experiments on seven real-world networks with ablation study for components in the model. Moreover, the authors carry out comparison to baselines in distributed setting.\n\nWeakness:\n1. The connection of GCN to MF is very indirect. It holds only when the GCN converges to the Laplacian smoothing. It is not clear whether this holds empirically. Moreover, there are too much intermediate steps and approximation between the Laplacian smoothing to the matrix factorization. As far as I am concerned, the connection is closer to node embedding versus matrix factorization.\n2. Given that GCN serves as Laplacian smoothing, it would be great if the authors can simply add additional regularization on dis(h_i, h_i) for (v_i, v_j)\\in E. Moreover, there is no reference and description to the Planetoid* algorithm.\n3. The authors use alternative batches between structure and classification loss. It would be interesting to see if joint training the two loss in mini-batch among a node and its neighbors can leads to any difference.\n3. The authors report only accuracy as evaluation metrics. It would be better If the authors could report recall@K and F1 score as well.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575587105957, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper107/Reviewers"], "noninvitees": [], "tcdate": 1570237756968, "tmdate": 1575587105973, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Review"}}}, {"id": "HJxxpshhYB", "original": null, "number": 3, "cdate": 1571765176399, "ddate": null, "tcdate": 1571765176399, "tmdate": 1572972637774, "tddate": null, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper derives a matrix-factorization approach for node classification. The approach is closely related to GCN. The authors show that the proposed approach outperforms GCN and Planetoid empirically.\n\nThough empirically appealing, this paper has a few pitfalls that need be addressed.\n\n1. The wording \"unifying\" is a misnomer. The title \"unifying graph convolutional networks\" hallucinates a framework that unifies several neural network architectures, which is not precise. In reality, the authors propose a learning objective that consists of two loss terms, the classification loss and the structure loss. The classification loss is nothing but the usual GCN. The structure loss is the contribution of the paper. The derivation of this term starts from GCN and a Laplacian smoothing argument, and arrives at a matrix factorization form through a series of modeling modifications. By and large, the title is misleading.\n\n2. The wording \"correctness of our theoretical analysis\" is dubious. The paper does not present a theoretical analysis. The derivation of the matrix factorization is only a modeling process. In no mathematical sense the factorization is equivalent to GCN.\n\n3. The alternating training is questionable. The authors propose alternately optimizing the structure loss and the classification loss. Since taking the gradient of the whole loss function is straightforward in all graph neural network approaches, it is unclear why the authors prefer the alternating optimization approach. Supplementing a convergence plot and comparing the two approaches may help, if the alternating approach is indeed better.\n\n4. The \"distributed computing\" component needs more substantiation. It is unclear whether this phrase actually means the concept familiar by the parallel computing community. Therein, computation is done by using several machines communicated through networked protocols. Machine setting, parallel implementation details, and speedup are the primary interests in distributed computing. All information should be reported.\n\nQuestions:\n\n1. First sentence of section 5. What does \"all-round\" mean?\n\n2. Stability Analysis. What is b? The reader does not find a definition elsewhere. A probably related concept is alpha (see eqn (8)).\n\n3. Figure 3(b) shows that larger b leads to poorer performance. The authors state that a larger b means a stronger emphasis on the structure loss. Consequently, it appears that putting more emphasis on the structure term leads to poorer performance. Then, does it mean that the structure term is a useless contribution?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575587105957, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper107/Reviewers"], "noninvitees": [], "tcdate": 1570237756968, "tmdate": 1575587105973, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Review"}}}, {"id": "rJeyquvktS", "original": null, "number": 2, "cdate": 1570891911479, "ddate": null, "tcdate": 1570891911479, "tmdate": 1571197577627, "tddate": null, "forum": "HJxf53EtDr", "replyto": "SJlmqC8JYr", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment", "content": {"comment": "Yes, according to our theoretical analysis, and analysis between graph embedding and MF, this is of great possibility.", "title": "Yes"}, "signatures": ["ICLR.cc/2020/Conference/Paper107/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper107/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper107/Authors|ICLR.cc/2020/Conference/Paper107/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176300, "tmdate": 1576860533910, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment"}}}, {"id": "SJlmqC8JYr", "original": null, "number": 2, "cdate": 1570889355336, "ddate": null, "tcdate": 1570889355336, "tmdate": 1570889355336, "tddate": null, "forum": "HJxf53EtDr", "replyto": "BJgt6Q0CdH", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Public_Comment", "content": {"comment": "Thanks.\n\nSo, with cotraining and unitizing, other graph embedding methods, like LINE, DeepWalk and Node2Vec, can also perform well on semi-supervised learning?", "title": "More discussion"}, "signatures": ["~Xiao_Qi2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xiao_Qi2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504213850, "tmdate": 1576860567551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Public_Comment"}}}, {"id": "BJgt6Q0CdH", "original": null, "number": 1, "cdate": 1570853824798, "ddate": null, "tcdate": 1570853824798, "tmdate": 1570853857720, "tddate": null, "forum": "HJxf53EtDr", "replyto": "HkefsuGA_H", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment", "content": {"comment": "Thanks for your reply.\n\nHere are our responses:\n\n(1)\tDuring the unification, the difference between the two forms of GCN only lies in different $\\beta_{i,j}$ in Eq. (14). And the $\\beta_{i,j}$ can be eliminate in the following deduction, and we can achieve the same unified matrix as in Eq. (24). Thus, the two forms of GCN does not affect our unification.\n\n(2)\tWe have tried the Euclidean distance. And the Euclidean distance is totally not consistent with the convergence of GCN.\n\n(3)\tYes, other graph embedding methods can do this.\nThe reason of \u201cwhy there are no previous work using these graph embedding methods to beat GCNs\u201d is, there lacks co-training and unitizing process in existing graph embedding methods. According to our theoretical analysis, we add co-training and unitization in MF, and obtain our CUMF model. According to the results in table 2, CUMF without co-training or unitization (i.e., CUMF-C or CUMF-U) is totally not competitive comparing with CUMF. Thus, co-training or unitization are extremely important for graph embedding methods to achieve good performances in semi-supervised learning.\nMoreover, Planetoid can be somehow viewed as a co-training DeepWalk, without the process of unitization. It is the most competitive baseline for GCN, and even beats GCN on the BlogCatalog dataset and the Flickr dataset according to the results in table 3.\n", "title": "Thanks for your reply."}, "signatures": ["ICLR.cc/2020/Conference/Paper107/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper107/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper107/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper107/Authors|ICLR.cc/2020/Conference/Paper107/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176300, "tmdate": 1576860533910, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Official_Comment"}}}, {"id": "HkefsuGA_H", "original": null, "number": 1, "cdate": 1570805913691, "ddate": null, "tcdate": 1570805913691, "tmdate": 1570806251698, "tddate": null, "forum": "HJxf53EtDr", "replyto": "HJxf53EtDr", "invitation": "ICLR.cc/2020/Conference/Paper107/-/Public_Comment", "content": {"comment": "Great work. If the claims in this paper can work, the most exciting thing is, we can easily perform semi-supervised learning on large graphs, instead of using GCNs. Here, I have several questions:\n\nGCNs usually have two different forms:\n$\\mathop {\\rm{H}}\\nolimits^{l + 1}  = \\sigma \\left( {\\mathop {\\tilde D}\\nolimits^{ - 1/2} \\tilde A\\mathop {\\tilde D}\\nolimits^{ - 1/2} \\mathop H\\nolimits^l \\mathop W\\nolimits^l } \\right)$\nand\n$\\mathop {\\rm{H}}\\nolimits^{l + 1}  = \\sigma \\left( {\\mathop {\\tilde D}\\nolimits^{ - 1} \\tilde A\\mathop H\\nolimits^l \\mathop W\\nolimits^l } \\right)$\nWill the two forms affect the unification?\n\nCan Euclidean distance have similar performance as in Fig. 1?\n\nConsidering GCNs can be unified as MF, and this achieves better performances, can other graph embedding methods (like LINE and DeepWalk) do this? And why there are no previous work using these graph embedding methods to beat GCNs?\n", "title": "Interesting, but I have something to discuss."}, "signatures": ["~Xiao_Qi2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Xiao_Qi2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zhaocheng.liu@realai.ai", "qiang.liu@realai.ai", "haoli.zhang@realai.ai", "dcszj@mail.tsinghua.edu.cn"], "title": "Unifying Graph Convolutional Networks as Matrix Factorization", "authors": ["Zhaocheng Liu", "Qiang Liu", "Haoli Zhang", "Jun Zhu"], "pdf": "/pdf/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "TL;DR": "We unify graph convolutional networks as co-training and unitized matrix factorization.", "abstract": "In recent years, substantial progress has been made on graph convolutional networks (GCN). In this paper, for the first time, we theoretically analyze the connections between GCN and matrix factorization (MF), and unify GCN as matrix factorization with co-training and unitization. Moreover, under the guidance of this theoretical analysis, we propose an alternative model to GCN named Co-training and Unitized Matrix Factorization (CUMF). The correctness of our analysis is verified by thorough experiments. The experimental results show that CUMF achieves similar or superior performances compared to GCN. In addition, CUMF inherits the benefits of MF-based methods to naturally support constructing mini-batches, and is more friendly to distributed computing comparing with GCN. The distributed CUMF on semi-supervised node classification significantly outperforms distributed GCN methods. Thus, CUMF greatly benefits large scale and complex real-world applications.", "code": "https://github.com/code20190923/CUMF", "keywords": ["graph convolutional networks", "matrix factorization", "unification"], "paperhash": "liu|unifying_graph_convolutional_networks_as_matrix_factorization", "original_pdf": "/attachment/e70584d8427e71ec7fad9665991d9402118c0e89.pdf", "_bibtex": "@misc{\nliu2020unifying,\ntitle={Unifying Graph Convolutional Networks as Matrix Factorization},\nauthor={Zhaocheng Liu and Qiang Liu and Haoli Zhang and Jun Zhu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxf53EtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxf53EtDr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504213850, "tmdate": 1576860567551, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper107/Authors", "ICLR.cc/2020/Conference/Paper107/Reviewers", "ICLR.cc/2020/Conference/Paper107/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper107/-/Public_Comment"}}}], "count": 14}