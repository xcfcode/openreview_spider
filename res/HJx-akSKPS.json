{"notes": [{"id": "HJx-akSKPS", "original": "r1e5a7JYwB", "number": 1979, "cdate": 1569439673310, "ddate": null, "tcdate": 1569439673310, "tmdate": 1577168282198, "tddate": null, "forum": "HJx-akSKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "0HpAtHFe03", "original": null, "number": 1, "cdate": 1576798737419, "ddate": null, "tcdate": 1576798737419, "tmdate": 1576800898955, "tddate": null, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a method called Dynamic Intermedium Attention Memory Network (DIAMNet) to learn the subgraph isomorphism counting for a given pattern graph P and target graph G. However, the reviewers think the experimental comparisons are insufficient. Furthermore,  the evaluation is only for synthetic dataset for which generating process is designed by the authors. If possible, evaluation on benchmark graph datasets would be convincing though creating the ground truth might be difficult for larger graphs.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717959, "tmdate": 1576800268356, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Decision"}}}, {"id": "SJgrUVjFjS", "original": null, "number": 3, "cdate": 1573659724737, "ddate": null, "tcdate": 1573659724737, "tmdate": 1573820375148, "tddate": null, "forum": "HJx-akSKPS", "replyto": "SylbC532YB", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thanks for your questions.\n\nQ.1 \nQ.1.1. Besides what we explained in the \u201cgeneral response of why counting\u201d, we would like to emphasize that simply using a binary classifier for subgraph isomorphism and graph isomorphism would be less useful than counting in knowledge discovery and \"how many\" based KBQA, although the same representation and ways of representation learning could be applied. \n\nQ.1.2. \u201cNote that there is some existing research on GNNs targeted 'graph matching' and 'graph similarity'.\u201d\nYes, we have cited some of these existing works in the related work section. However, for subgraph isomorphism counting, we need different types of graph encoding, which is shown in sections 4.1.1 and 4.2.1.\n\nQ.1.3. \u201cthe used datasets intentionally restrict the possible values for the number of subgraph isomorphisms, but the counts would be exponentially large if we consider practical (dense) graphs.\u201d\nIt is true for homogeneous patterns querying dense graphs, but in practice, heterogeneous patterns with node and edge types are more useful, e.g., to query knowledge graphs with node and edge types. \n\n2) The objectives based on (R)MSE or (R)MAE correspond to a regression loss. Although the labels seem to be skewed, given the power of deep representation learning, it will be able to map the graph and pattern pair in a semantic space that can better perform regression.  Log errors are not good because final models cannot handle complex cases (whose counts are large). Errors between log predictions and log counts are also not suitable because predictions at the early training steps can be negative. If we simply use ReLU (in prediction) when computing losses, models are easy to get stuck in a local optimum to predict zero all the time. We have tried all the above options but training processes did not even converge. \n\nWe constrain counts for the computational time of traditional algorithms and the interpretability of errors. Traditional algorithms will spend much more time on complex graphs. Errors are easy to be disturbed by those cases. In our datasets, we limit the count <= 1024 when |E| <= 256 and the count <= 4096 when |E| <= 2048. \n\n3) We use Zero as one of our baselines because it is a local optimum. We have added the average count of training data as our baseline in the latest submission as well as follows. The average count is worse than Zero prediction.\n\nSmall\n        |  RMSE  | MAE     | F1_0   | F1_nonzero\nZero | 67.195 | 13.716 | 0.761 | 0.0\nAvg  | 65.780 | 21.986  | 0.0     | 0.557\n\nLarge\n         |  RMSE    | MAE    | F1_0   | F1_nonzero\nZero | 237.904 | 35.445 | 0.769 | 0.0\nAvg  | 235.253 | 60.260 | 0.0      | 0.545\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJx-akSKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1979/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1979/Authors|ICLR.cc/2020/Conference/Paper1979/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148088, "tmdate": 1576860534586, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment"}}}, {"id": "r1xZ5ViKiB", "original": null, "number": 4, "cdate": 1573659784769, "ddate": null, "tcdate": 1573659784769, "tmdate": 1573661609059, "tddate": null, "forum": "HJx-akSKPS", "replyto": "r1x1gwAcYH", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thanks for your questions.\n\n1) Thanks for correcting the Zero baseline. Corrected precision, recall, and F1 scores have been updated.\n\n2) The constant prediction, e.g., the average count of training data, is also added in the latest version as well as follows. \n\nSmall\n        |  RMSE  | MAE     | F1_0   | F1_nonzero\nZero | 67.195 | 13.716 | 0.761 | 0.0\nAvg  | 65.780 | 21.986  | 0.0     | 0.557\n\nLarge\n         |  RMSE    | MAE    | F1_0   | F1_nonzero\nZero | 237.904 | 35.445 | 0.769 | 0.0\nAvg  | 235.253 | 60.260 | 0.0      | 0.545\n\nThe average of training data is very close to that of test data. But using the average of training data is fairer than using the average of test data. This baseline (Avg) is worse than Zero. \n\n3) \u201c75% zero counting graphs\u201d. This setting is designed in purpose to evaluate neural models. As traditional algorithms do not have problems when there is no subgraph isomorphism detected in a data graph, neural models would fit the training data. However, in practice, there will be a lot of applications that zero counting exists for most of the cases. Therefore, we also add a lot of zero counting data. In addition, we can also get some sense of the performance when evaluating the F1_0 and F1_nonzero to compare different models, although we are not building a binary classifier. It may be possible to set different percentages of zero counting data. However, the results will be similar in terms of relative performance. We also agree that current MSE/MAE values might be underestimated compared with when all the test points have non-zero countings, and this also means that we are challenging ourselves and the community with a more difficult problem.\n\n4) Because this problem is NP-complete, there is no suitable dataset for both traditional algorithms and neural algorithms. Traditional algorithms are hard to scale to large graphs while neural models require plenty of data. But we agree with you that benchmark datasets are more convincing. We will release our generation code and learning code as one benchmark for future research.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJx-akSKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1979/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1979/Authors|ICLR.cc/2020/Conference/Paper1979/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148088, "tmdate": 1576860534586, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment"}}}, {"id": "Syec_BjYsr", "original": null, "number": 5, "cdate": 1573660018409, "ddate": null, "tcdate": 1573660018409, "tmdate": 1573661187098, "tddate": null, "forum": "HJx-akSKPS", "replyto": "Hkeg_NnxoB", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment", "content": {"title": "Response to Review #4", "comment": "Thanks for your questions.\n\n1) Please refer to the general response of \u201cwhy counting\u201d.\n\n2) The reason we didn't compare with TurboISO and VF3 is that our graphs are generated by Algorithm 2, where the idea comes from TurboISO and VF3. When generating a graph, we do not need to run traditional algorithms to get the count but to add pattern isomorphisms. Random edges are added following the rule that breaks necessary conditions (Line 20 in Alg 2). These necessary conditions are used in TurboISO and VF3 to find candidate subregions. If we use TurboISO and VF3 as baseline algorithms, we believe the two methods will terminate in a short time. VF2 is considered one of the most representative algorithms and we use it to demonstrate the time of the magnitude of traditional methods.\n\nAs for other approximation methods, [Q1] is designed for graph isomorphism rather than subgraph isomorphism; [Q2] is still not suitable due to the exponential space requirement (19 vertices requires 1.2 Mbyte of disk space, shown Page 23). [Q2] only compares their method with Ullman\u2019s algorithm on graphs with 19 vertices and achieves 16 times speedup. However, VF2 is 1,000 times faster than Ullman\u2019s algorithm when |V| > 200. We do not think [Q2] can be applied to our two datasets. Isomorphism and subgraph isomorphism problem cannot be solved by sampling as [Q3], and that\u2019s the reason why RGCN is worse than RGCN-SUM. \n\n[Q1]  A Neural Graph Isomorphism Algorithm Based on Local Invariants, ESANN'2003\n[Q2] Subgraph Isomorphism in Polynomial Time\n[Q3] FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\n\n3) The output of DIAMNet is the memory itself, where it has M blocks and each block is a d-dimensional vector shown in Section 4.3. \n\n4) Graphlets are small connected non-isomorphic induced subgraphs (usually 3-5 nodes) of a large network. We want to use neural models to approximately solve a general pattern counting problem. The pattern can be sparse or dense, homogeneous or heterogeneous. As Table 4 shows, we have many diverse structures of both patterns and graphs. This generalization requires a much more powerful ability of inference.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJx-akSKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1979/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1979/Authors|ICLR.cc/2020/Conference/Paper1979/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148088, "tmdate": 1576860534586, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment"}}}, {"id": "HJe_WNjFiS", "original": null, "number": 2, "cdate": 1573659648007, "ddate": null, "tcdate": 1573659648007, "tmdate": 1573660871563, "tddate": null, "forum": "HJx-akSKPS", "replyto": "B1lIjMJPjr", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thanks for your suggestions.\n\n1) The hardware information and software information has been added to the latest version. Training and evaluating were finished on one single NVIDIA GTX 1080 Ti GPU under the PyTorch framework.\n\n2) When the edge size of a graph increases to 256, it is already hard for neural models to do self-attention for graphs. Transformer-XL [1] is proposed to solve the computational cost problem. Generally, a 6-layer Transformer-XL should be better than a 3-layer GRU, but results in Table 2 and Table 3 show that Transformer is worse instead. Subgraph isomorphism counting requires the whole pattern information and the whole graph information. \n\nWe can try to implement a model with self-attention and source attention, but it can be only trained in rather small batch sizes and applied to toy data. We think this model cannot be helpful to solve the subgraph isomorphism counting problem.\n\n[1] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL, pp. 2978\u20132988, 2019.\n\n3) We have provided 24 more figures and further discussions in Appendix F. Those figures can provide more information about the behaviors of different models with different data.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJx-akSKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1979/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1979/Authors|ICLR.cc/2020/Conference/Paper1979/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148088, "tmdate": 1576860534586, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment"}}}, {"id": "HJg6JmstoH", "original": null, "number": 1, "cdate": 1573659365157, "ddate": null, "tcdate": 1573659365157, "tmdate": 1573660363073, "tddate": null, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment", "content": {"title": "General response of \u201cwhy counting\u201d", "comment": "Although solving subgraph matching/enumeration can solve subgraph counting but not the other way round, counting itself is still very useful. Counting the number of isomorphic copies has been proven to be useful for bioinformatics [1], [2], chemoinformatics [3], and online social network analysis [4]. Especially, when counting a new structure that a professional may query (e.g., a gene structure, a protein structure, or a social network structure), a first step may be a rough estimation instead of exact finding. Then a fast algorithm to estimate may save a lot of time for such kind of knowledge discovery. Our counting task is also related to graphlet counting in database and data mining fields. However, our framework can count patterns with much more heterogeneous nodes and edges rather than 3-5 nodes in graphlets.\n\nCounting is also an important task especially for knowledge-based question answering (KBQA). More importantly, nowadays, most modern knowledge graphs are stored in RDF graph databases. The schema of such databases are more complex and counting based on the graph is preferred, for example:\nEx (used in our introduction): \u201chow many languages are there in Africa speaking by people living near the banks of the Nile River?\u201d\nAfter semantic parsing, such questions should be mapped to be a subgraph counting problem, where the subgraphs should follow some types of nodes and relations. Therefore, automatically counting can solve a particular KBQA problem in the future. However, to our best knowledge, we haven\u2019t found any existing large-scale KBQA dataset that is specifically developed for the subgraph counting problem. This is why we developed our training and test datasets, which can serve as a pre-training step for higher-order \u201chow many\u201d KBQA problems.\n\n[1] R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon, \u201cNetwork motifs: Simple building blocks of complex networks,\u201d Science, vol. 298, no. 5594, pp. 824\u2013827, 2002.\n[2] N. Alon, P. Dao, I. Hajirasouliha, F. Hormozdiari, and S. C. Sahinalp, Biomolecular network motif counting and discovery by color coding, Bioinformatics, vol. 24, no. 13, pp. i241\u2013i249, 2008.\n[3] J. Huan, W. Wang, and J. Prins, Efficient mining of frequent subgraphs in the presence of isomorphism, ICDM, 2003, p. 549.\n[4] M. Kuramochi and G. Karypis, Frequent subgraph discovery, ICDM, 2001, pp. 313\u2013320.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJx-akSKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1979/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1979/Authors|ICLR.cc/2020/Conference/Paper1979/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148088, "tmdate": 1576860534586, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Authors", "ICLR.cc/2020/Conference/Paper1979/Reviewers", "ICLR.cc/2020/Conference/Paper1979/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Comment"}}}, {"id": "B1lIjMJPjr", "original": null, "number": 4, "cdate": 1573479070256, "ddate": null, "tcdate": 1573479070256, "tmdate": 1573479070256, "tddate": null, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "This paper proposes a dynamic inter-medium attention memory network and model the sub-graph isomorphism counting problem as a learning problem with both polynomial training and prediction time complexities.\nSince the testing time is reported in this paper, and the time complexity is one of the main contribution of this paper. The hardware and software used to run the algorithm should be reported in the main article.\n\nThe author argues that if we use neural networks to learn distributed representations for V_G and V_p or \\xi_G and \\xi_P without self-attention, the computational cost will acceptable for large graphs, but the missing of self-attention will hurt the performance. It\u2019s encouraged to do corresponding experiments to compare it with the proposed method and better support the algorithm.\n\nOne of the main advantages of this paper is that the proposed method can efficiently deal with large graph tasks, so the model behaviors of different models in large dataset similar to Figure 5 is encouraged to be given.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575880553747, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Reviewers"], "noninvitees": [], "tcdate": 1570237729523, "tmdate": 1575880553762, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Review"}}}, {"id": "Hkeg_NnxoB", "original": null, "number": 3, "cdate": 1573074023767, "ddate": null, "tcdate": 1573074023767, "tmdate": 1573074023767, "tddate": null, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper studied how to leverage the power of graph neural networks for counting subgraph isomorphism. The motivation is that the current subgraph isomorphism detection is NP-complete problem and a proposed approach based on GNN could approximately solve the counting problem in polynomial time. Then they relaxed original subgraph isomorphism (which is equivalent to the exact subgraph matching problem)  and proposed the problem of doing subgraph isomorphism counting task. The GNN and sequence modeling methods are discussed for solving this problem. The experimental results confirmed the effectiveness of these methods. \n\nAlthough I found the subgraph isomorphism counting problem is an interesting problem, I did not know how much practical usefulness of this task. More practical use case would be search for the matched subgraphs given the sub-graph query using subgraph isomorphism detection. \n\nAlso, although authors mentioned some approximation systems/methods in graph database community such as TurboISO (Han et al., 2013), VF3 (Carletti et al., 2018), and other approximation techniques [1][2], authors did not consider them as baselines to compare. These methods may also have limitations to deal with real-large graph but for the graph size that this paper studied I think they are fine to deal with. A parallel issue is that GNN also has scalability issues as well when dealing with large graphs [3]. Without comparing these existing fast (approximation) methods, it is really unfair to compare with only non-DL baseline VF2, which seems served as ground-truth as well. \n\n[1]  A Neural Graph Isomorphism Algorithm Based on Local Invariants, ESANN'2003\n[2] Subgraph Isomorphism in Polynomial Time\n[3] FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling\n\nIn terms of technical contributions, they leverage some existing sequence models (CNN, RNN and so on) and graph models (RGNN) and the whole framework is similar to doing a graph matching networks (without considering node alignment) for a regression task. The DYNAMIC INTERMEDIUM ATTENTION MEMORY NETWORK is interesting yet simple. I am not entirely clear what's the output  of this interactional module. The figure 4 shows the overall architecture of subgraph isomorphism counting model, which needs better descriptions to understand exact input and output for each module. In general, the novelty of this part is incremental. \n\nFinally, this subgraph isomorphism counting problem is closely related to graphlet counting problem. In the paper, the subgraph pattern considered seems like almost identical to graphlets the previous research extensively studied. I did not see any discussion about the connection of these two tasks either. \n\nMinor comments:\n\n|V_G| is is the number of pattern nodes -> |V_p| is is the number of pattern nodes", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575880553747, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Reviewers"], "noninvitees": [], "tcdate": 1570237729523, "tmdate": 1575880553762, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Review"}}}, {"id": "r1x1gwAcYH", "original": null, "number": 1, "cdate": 1571641062579, "ddate": null, "tcdate": 1571641062579, "tmdate": 1572972398867, "tddate": null, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposed NN based subgraph counting. By using synthetically generated graphs, NN learns the number of occurences of a given queried graph called 'pattern'. The author proposes a specific architecture for learning the count based on the multi-head attention method. The authors empirically evaluated the performance on the synthetic dataset.\n\nThe problem setting would be interesting. Applying NN to counting a subgraph is novel as far as I know. My current concerns are mainly on the appropriateness of the experimental evaluation. \n\nIn the tables, the trivial baseline 'Zero' is shown as F1_zero = 0, but is this correct? I think this should be non-zero. If zero is 'positive' in F1_zero, recall is 1 and precision is 0.75 (because the author set 75% of data as zero). F-score is harmonic mean of them, which is 0.86.\n\nRMSE and MAE of the Zero prediction is shown, but the more standard baseline of the error would be a constant prediction (e.g., the average of test points is often used, which can evaluate how much variance can be explained by the model).\n\nWhy were 75 percent of countings set as 0 in the evaluation dataset? This rate is seemingly a bit large for the evaluation purpose. I guess that when this percentage is much more smaller, MSE would increase. In other words, current MSE/MAE values might be underestiamted compared with when all the test points have non-zero countings.\n\nThe evaluation is only for synthetic dataset for which generating process is designed by the authors. If possible, evaluation on benchmark graph datasets would be convincing though creating the ground truth might be difficult for larger graphs.\n\nMinor comment:\nAt the third line of Sec 3.2: '|V_G| is the number of pattern nodes' should be |V_P|.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575880553747, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Reviewers"], "noninvitees": [], "tcdate": 1570237729523, "tmdate": 1575880553762, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Review"}}}, {"id": "SylbC532YB", "original": null, "number": 2, "cdate": 1571764937340, "ddate": null, "tcdate": 1571764937340, "tmdate": 1572972398826, "tddate": null, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "invitation": "ICLR.cc/2020/Conference/Paper1979/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a method called Dynamic Intermedium Attention Memory Network (DIAMNet) to learn the subgraph isomorphism counting for a given pattern graph P and target graph G. This requires global information unlike usual GNN cases such as node classification, link prediction, community detection. First, input graphs P and G are converted embedding vectors through sequence models (CNN, RNN, Transformer-XL) or graph models (RGCN), and fed into their DIAMNet that uses an external memory as an intermedium to attend both the pattern and the graph. The external memory is updated based on multi-head attention as in Transformer. The output of DIAMNet is passed to FC that outputs 'count' directly. The training is based on minimizing MSE loss as a regression problem. Extensive experimental evaluations report that DIAMNet showed superior performance over competing methods and baselines.\n\nThis paper targets subgraph isomorphism counting as a learning problem for the first time I guess, and the proposed method combined with both graph- and sequence-based encoding is technically interesting. However, there are still two major issues of 1) why counting? 2) the RMSE loss for regression on counts 3) baseline of 'Zero'.\n\n1) the most unclear point is 'why counting?'. If I understand it, this method can be applied to subgraph isomorphism (NP-hard) or graph isomorphism (unknown complexity) as binary classification, and experimental evaluations can use the datasets used in evaluating VF2 or Naughty. It would be better to start this fundamental problem that would have many clear applications. Compared to subgraph isomorphism or graph isomorphism, the need for knowing accurate 'counts' of subgraph isomorphisms is unconvincing (given that we cannot explicitly obtains all subgraph matchings). Note that there is some existing research on GNNs targeted 'graph matching' and 'graph similarity'. \n\nAlso, the used datasets intentionally restrict the possible values for the number of subgraph isomorphisms, but the counts would be exponentially large if we consider practical (dense) graphs. \n\n2) the method fits the model using (R)MSE loss, but minimizing log errors ((R)MSLE) would be better considering distributions of response values (counts) of the used datasets in Figure 6. Fitting the MSE loss is not good for such highly skewed cases, and for example, might focus only on the few instances having very large count values. Or, if such instances are very small, training ignores all such extreme instances. Either way would be questionable when we consider learning 'subgraph isomorphism counting' in general. \n\nAlso, the error of counts by MSE or MAE would be less informative and it would be unclear how much errors are tolerant in practical use cases of this method. \n\n3) To interpret the RMSE and MAE values, Table 2 has the value for 'Zero'. This is for a constant predictor always returning zeros for any inputs. However, given that the loss is MSE, constant prediction values should be the average counts in the training data, not zero. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1979/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1979/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xliucr@cse.ust.hk", "hpanad@cse.ust.hk", "mhear@cse.ust.hk", "yqsong@cse.ust.hk", "jiang.xin@huawei.com"], "title": "Neural Subgraph Isomorphism Counting", "authors": ["Xin Liu", "Haojie Pan", "Mutian He", "Yangqiu Song", "Xin Jiang"], "pdf": "/pdf/b64b481c229b19bb46400fa38a77c18e69118ab7.pdf", "TL;DR": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms.", "abstract": "In this paper, we study a new graph learning problem: learning to count subgraph isomorphisms. Although the learning based approach is inexact, we are able to generalize to count large patterns and data graphs in polynomial time compared to the exponential time of the original NP-complete problem. Different from other traditional graph learning problems such as node classification and link prediction, subgraph isomorphism counting requires more global inference to oversee the whole graph. To tackle this problem, we propose a dynamic intermedium attention memory network (DIAMNet) which augments different representation learning architectures and iteratively attends pattern and target data graphs to memorize different subgraph isomorphisms for the global counting. We develop both small graphs (<= 1,024 subgraph isomorphisms in each) and large graphs (<= 4,096 subgraph isomorphisms in each) sets to evaluate different models. Experimental results show that learning based subgraph isomorphism counting can help reduce the time complexity with acceptable accuracy. Our DIAMNet can further improve existing representation learning models for this more global problem.", "keywords": ["subgraph isomorphism", "graph neural networks"], "paperhash": "liu|neural_subgraph_isomorphism_counting", "original_pdf": "/attachment/786a3a5a9b6e9354846a660eacad9bc02933f58b.pdf", "_bibtex": "@misc{\nliu2020neural,\ntitle={Neural Subgraph Isomorphism Counting},\nauthor={Xin Liu and Haojie Pan and Mutian He and Yangqiu Song and Xin Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=HJx-akSKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJx-akSKPS", "replyto": "HJx-akSKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1979/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575880553747, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1979/Reviewers"], "noninvitees": [], "tcdate": 1570237729523, "tmdate": 1575880553762, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1979/-/Official_Review"}}}], "count": 11}