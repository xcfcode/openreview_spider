{"notes": [{"id": "BJgAf6Etwr", "original": "HyeyMraUPH", "number": 432, "cdate": 1569438998146, "ddate": null, "tcdate": 1569438998146, "tmdate": 1577168291233, "tddate": null, "forum": "BJgAf6Etwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering", "authors": ["Jasdeep Singh", "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher"], "authorids": ["jasdeep@cs.stanford.edu", "bmccann@salesforce.com", "nkeskar@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "keywords": ["cross-lingual", "transfer learning", "BERT"], "TL;DR": "Translating portions of the input during training can improve cross-lingual performance.", "abstract": "While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages. \nWe introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language. XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark. With improvements of up to 4.8, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language. On the SQuAD question answering task, we see that XLDA provides a 1.0 performance increase on the English evaluation set. Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.", "pdf": "/pdf/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "paperhash": "singh|xlda_crosslingual_data_augmentation_for_natural_language_inference_and_question_answering", "original_pdf": "/attachment/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "_bibtex": "@misc{\nsingh2020xlda,\ntitle={{\\{}XLDA{\\}}: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering},\nauthor={Jasdeep Singh and Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgAf6Etwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6BLZXeFTk", "original": null, "number": 1, "cdate": 1576798696243, "ddate": null, "tcdate": 1576798696243, "tmdate": 1576800939406, "tddate": null, "forum": "BJgAf6Etwr", "replyto": "BJgAf6Etwr", "invitation": "ICLR.cc/2020/Conference/Paper432/-/Decision", "content": {"decision": "Reject", "comment": "The authors provide an analysis of a cross-lingual data augmentation technique which they call XLDA. This consists of replacing a segment of an input text with its translation in another language. They show that when fine-tuning, it is more beneficial to train on the cross-lingual hypotheses than on the in-language pairs, especially for low resource languages such as Greek, Turkish and Urdu. The paper explores an interesting idea however they lack comparison with other techniques such as backtranslation and XLM models, and would benefit from a wider range of tasks. I feel like this paper is more suitable for an NLP-focussed venue. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering", "authors": ["Jasdeep Singh", "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher"], "authorids": ["jasdeep@cs.stanford.edu", "bmccann@salesforce.com", "nkeskar@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "keywords": ["cross-lingual", "transfer learning", "BERT"], "TL;DR": "Translating portions of the input during training can improve cross-lingual performance.", "abstract": "While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages. \nWe introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language. XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark. With improvements of up to 4.8, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language. On the SQuAD question answering task, we see that XLDA provides a 1.0 performance increase on the English evaluation set. Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.", "pdf": "/pdf/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "paperhash": "singh|xlda_crosslingual_data_augmentation_for_natural_language_inference_and_question_answering", "original_pdf": "/attachment/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "_bibtex": "@misc{\nsingh2020xlda,\ntitle={{\\{}XLDA{\\}}: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering},\nauthor={Jasdeep Singh and Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgAf6Etwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJgAf6Etwr", "replyto": "BJgAf6Etwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729554, "tmdate": 1576800282164, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper432/-/Decision"}}}, {"id": "rkgxzAJTFB", "original": null, "number": 1, "cdate": 1571778056463, "ddate": null, "tcdate": 1571778056463, "tmdate": 1572972596014, "tddate": null, "forum": "BJgAf6Etwr", "replyto": "BJgAf6Etwr", "invitation": "ICLR.cc/2020/Conference/Paper432/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a cross-lingual data augmentation method to improve the language inference and question answering tasks. The core idea is to replace a port of the input text (such as one of the sentence in a sentence pair in the language inference tasks) with its translation in another language. The authors empirically show that deploying the XLDA data augment improves the baseline methods for both the XNLI language inference data sets and the SQuAD task. \n\nThe paper is generally easy to follow and the experiments show promising results. Nevertheless, I have a concern with the comparison baseline used in the paper, as follows.\n\nThe authors indicate that their work is close to back-translation at the end of the related work section. Also, the authors provide their hypotheses about the possible disadvantages of using back-translation as data augmentation, compared to the proposed work here. However, the authors did not provide experimental studies to support that. To me, the proposed data augmentation strategy is very similar to the back-translation, so I would expect the authors to use that as a comparison baseline in their experimental studies. Without those results, it is difficult to judge the contributions of the proposed approach. \n\nAlso, the paper could be significantly improved if the authors could provide some analysis/observations on why replacing one of the sentences in the sentence pair for the inference tasks helps? In this sense, using some other data augmentation methods as comparison baselines would be very helpful. For example, simply drop some words in the sentence pair. \n\nI found the observations in the paper interesting, but the lack of comparison baselines makes me think the paper in its current form is not ready yet.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper432/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper432/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering", "authors": ["Jasdeep Singh", "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher"], "authorids": ["jasdeep@cs.stanford.edu", "bmccann@salesforce.com", "nkeskar@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "keywords": ["cross-lingual", "transfer learning", "BERT"], "TL;DR": "Translating portions of the input during training can improve cross-lingual performance.", "abstract": "While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages. \nWe introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language. XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark. With improvements of up to 4.8, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language. On the SQuAD question answering task, we see that XLDA provides a 1.0 performance increase on the English evaluation set. Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.", "pdf": "/pdf/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "paperhash": "singh|xlda_crosslingual_data_augmentation_for_natural_language_inference_and_question_answering", "original_pdf": "/attachment/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "_bibtex": "@misc{\nsingh2020xlda,\ntitle={{\\{}XLDA{\\}}: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering},\nauthor={Jasdeep Singh and Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgAf6Etwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgAf6Etwr", "replyto": "BJgAf6Etwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575301515987, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper432/Reviewers"], "noninvitees": [], "tcdate": 1570237752220, "tmdate": 1575301515999, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper432/-/Official_Review"}}}, {"id": "ryl6iV7AtB", "original": null, "number": 2, "cdate": 1571857572748, "ddate": null, "tcdate": 1571857572748, "tmdate": 1572972595970, "tddate": null, "forum": "BJgAf6Etwr", "replyto": "BJgAf6Etwr", "invitation": "ICLR.cc/2020/Conference/Paper432/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "Summary: This paper proposes to augment crosslingual data with heuristic swaps using aligned translations, somewhat like what bilingual humans do in code-switching. I think this paper investigates a neat extension of the XNLI dataset, which is in fact the sort of thing it was created to enable! It also looks at SQuaD translations (but, I'd have preferred a bit more depth on one of these datasets over having both, but I understand why you made this rhetorical choice). \n\nYour augmentation extension to XNLI also uncovers a bunch of surprising results, like that code-switched utterances help models do better than monolingual ones! My main issue, if i had to find one, is that the paper doesn't try to offer (even possible) explanations for the unexpected results; maybe try to find space for more of these in a discussion section? Finally, this paper is really fun and well written, thanks for the effort! I'm going to leave a bunch of questions: it would be cool to see some in the final, but if they don't fit, you can consider them for a follow up.\n\nQuestions: \n-Are all \"portions\" full sentences? Did performance change based on which \"portions\" you swapped?  In the human code-switching literature, there are syntactic generalizations about what gets switched. If you analyze the swapping, you could figure out which parts of the sentence (say, verb phrases v. prepositional phrases, beginning v. middle v. end, etc.) mattered more for NLI performance. I'd love to know the answer to that question!\n-you say this: \"The BLEU score of the translation system has little effect on a language\u2019s performance as a cross-lingual augmentor. \" Any ideas on why?\n-you also say this: \"for every language a XLDA approach exists that improves over the standard approach\", what a tantalizing statement! Why did that happen?! \n-Are there any generalizations over whether typologically similar languages are better augmentors for each other than they are for really different ones? I feel like if you could redo your XLR method (fig. 4) by adding augmentors in order from most similar to least (or vice versa), and you might find the answer to this.\n-for XNLI, I'd love to see if you have differences by label (maybe in an appendix?)\n\nSmall Notes:\n-the text in fig1 should be bigger.\n-too many Ms and Ls, you had me chuckling at all the acronym puns!\n-define \"augmentor\" somewhere"}, "signatures": ["ICLR.cc/2020/Conference/Paper432/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper432/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering", "authors": ["Jasdeep Singh", "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher"], "authorids": ["jasdeep@cs.stanford.edu", "bmccann@salesforce.com", "nkeskar@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "keywords": ["cross-lingual", "transfer learning", "BERT"], "TL;DR": "Translating portions of the input during training can improve cross-lingual performance.", "abstract": "While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages. \nWe introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language. XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark. With improvements of up to 4.8, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language. On the SQuAD question answering task, we see that XLDA provides a 1.0 performance increase on the English evaluation set. Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.", "pdf": "/pdf/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "paperhash": "singh|xlda_crosslingual_data_augmentation_for_natural_language_inference_and_question_answering", "original_pdf": "/attachment/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "_bibtex": "@misc{\nsingh2020xlda,\ntitle={{\\{}XLDA{\\}}: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering},\nauthor={Jasdeep Singh and Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgAf6Etwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgAf6Etwr", "replyto": "BJgAf6Etwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575301515987, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper432/Reviewers"], "noninvitees": [], "tcdate": 1570237752220, "tmdate": 1575301515999, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper432/-/Official_Review"}}}, {"id": "ryxJD5u89r", "original": null, "number": 3, "cdate": 1572403798949, "ddate": null, "tcdate": 1572403798949, "tmdate": 1572972595924, "tddate": null, "forum": "BJgAf6Etwr", "replyto": "BJgAf6Etwr", "invitation": "ICLR.cc/2020/Conference/Paper432/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper provides an analysis of a cross-lingual data augmentation technique dubbed XLDA, which consists of replacing parts of an input text with its translation in another language. Building on the mBERT approach, the authors show that at fine-tuning time it is beneficial to augment the training set of XNLI with cross-lingual hypotheses and premises instead of in-language pairs. For each language in XNLI, they show results by augmenting with each of the 14 other languages in the dataset, and show significant improvements over per-language performance.\n\nThe paper explores an interesting idea and shows that cross-lingual data augmentation works well. However, their analysis is limited to the XNLI and the Squad dataset, which do not cover a suitable range of tasks to fully conclude on the importance of XLDA for generally improving NLU tasks. It would have been interesting to show the effect of cross-lingual data augmentation for other GLUE tasks by augmenting the datasets with machine translation for instance. And also compare this model on these tasks to the monolingual back-translation approach, similar to https://arxiv.org/abs/1904.12848 . Applying XLDA with the latest open-source XLM models from the cross-lingual language model pretraining paper which obtain higher performance than the multilingual BERT would also make the results more convincing. While I share the excitement of using cross-lingual models to improve monolingual performance, I also feel like this paper lacks novelty and further evaluation to be accepted at ICLR, and would be more suited in a more NLP-focused venue."}, "signatures": ["ICLR.cc/2020/Conference/Paper432/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper432/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering", "authors": ["Jasdeep Singh", "Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher"], "authorids": ["jasdeep@cs.stanford.edu", "bmccann@salesforce.com", "nkeskar@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "keywords": ["cross-lingual", "transfer learning", "BERT"], "TL;DR": "Translating portions of the input during training can improve cross-lingual performance.", "abstract": "While natural language processing systems often focus on a single language, multilingual transfer learning has the potential to improve performance, especially for low-resource languages. \nWe introduce XLDA, cross-lingual data augmentation, a method that replaces a segment of the input text with its translation in another language. XLDA enhances performance of all 14 tested languages of the cross-lingual natural language inference (XNLI) benchmark. With improvements of up to 4.8, training with XLDA achieves state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast to, and performs markedly better than, a more naive approach that aggregates examples in various languages in a way that each example is solely in one language. On the SQuAD question answering task, we see that XLDA provides a 1.0 performance increase on the English evaluation set. Comprehensive experiments suggest that most languages are effective as cross-lingual augmentors, that XLDA is robust to a wide range of translation quality, and that XLDA is even more effective for randomly initialized models than for pretrained models.", "pdf": "/pdf/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "paperhash": "singh|xlda_crosslingual_data_augmentation_for_natural_language_inference_and_question_answering", "original_pdf": "/attachment/ea36a6d49738d244781fdda6276a9281382ce1e4.pdf", "_bibtex": "@misc{\nsingh2020xlda,\ntitle={{\\{}XLDA{\\}}: Cross-Lingual Data Augmentation for Natural Language Inference and Question Answering},\nauthor={Jasdeep Singh and Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgAf6Etwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgAf6Etwr", "replyto": "BJgAf6Etwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575301515987, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper432/Reviewers"], "noninvitees": [], "tcdate": 1570237752220, "tmdate": 1575301515999, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper432/-/Official_Review"}}}], "count": 5}