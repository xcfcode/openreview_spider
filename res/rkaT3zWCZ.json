{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730159765, "tcdate": 1509137616787, "number": 976, "cdate": 1518730159754, "id": "rkaT3zWCZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rkaT3zWCZ", "original": "H1u53f-C-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": ["r1bO6vyDG"], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260089626, "tcdate": 1517249696570, "number": 427, "cdate": 1517249696555, "id": "HyKsVJTrM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "The authors present an environment for semantic navigation that is based on an existing dataset, SUNCG. Datasets/environments are important for deep RL research, and the contribution of this paper is welcome. However, this paper does not offer enough novelty in terms of approach/method and its claims are somewhat misleading, so it would probably be a better fit to publish it at a workshop. ", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642537194, "tcdate": 1511300301431, "number": 1, "cdate": 1511300301431, "id": "BySAhfGgM", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Review", "forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "signatures": ["ICLR.cc/2018/Conference/Paper976/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Not much novelty; overselling the framework and the task", "rating": "4: Ok but not good enough - rejection", "review": "Paper Summary: The paper proposes a simulator for the SUNCG dataset to perform rendering and collision detection. The paper also extends A3C and DDPG (reinforcement learning methods) by augmenting them with gated attention. These methods are applied for the task of navigation.\n\nPaper Strengths:\n- It is interesting that the paper shows generalization to unseen scenes unlike many other navigation methods.\n- The renderer/simulator for SUNCG is useful.\n\nPaper Weaknesses:\nThe paper has the following issues: (1) It oversells the task/framework. The proposed task/framework is not different from what others have done. (2) There is not much novelty in the paper. The SUNCG dataset already exists. Adding a renderer to that is not a big deal. There is not much novelty in the method either. The paper proposes to use gated attention, which is not novel and it does not help much according to Figures 3b and 4b. (3) Other frameworks have more functionalities than the proposed framework. For example, other frameworks have physics or object interaction while this framework is only useful for navigation. (4) The paper keeps mentioning \"Instructions\". This implies that the method/framework handles natural language, while this is not the case. This is over-selling as well.\n\nQuestions and comments:\n\n- Statements like \"On the contrary, we focus on building a flexible platform that intersects with multiple research directions in an efficient manner allowing users to customize the rules and level of complexity to their needs.\" are just over-selling. This environment is not very different from existing platforms.\n\n- What is referred to as \u201cphysics\u201d is basically collision detection. It is again over-selling the environment. Other environments model real physics.\n\n- It is not clear what customizable mean in Table 1. I searched through the paper, but did not find any definition for that. All of the mentioned frameworks are customizable.\n\n- \"we compute the approximate shortest distance from the target room to each location in the house\" --> This assumption is somewhat unrealistic since agents in the real world do not have access to such information.\n\n- Instruction is an overloaded word for \"go to a RoomType\". The paper tries to present the tasks/framework as general tasks/framework while they are not.\n\n- In GATED-LSTM, h_t is a function of I. Why is I concatenated again with h_t?\n\n- Success rate is not enough for evaluation. The number of steps should be reported as well.\n\n- The paper should include citations to SceneNet and SceneNet RGBD.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642537102, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper976/AnonReviewer1", "ICLR.cc/2018/Conference/Paper976/AnonReviewer3", "ICLR.cc/2018/Conference/Paper976/AnonReviewer2"], "reply": {"forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642537102}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642537156, "tcdate": 1511743969524, "number": 2, "cdate": 1511743969524, "id": "ByK1GkteM", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Review", "forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "signatures": ["ICLR.cc/2018/Conference/Paper976/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "The authors propose a virtual environment built from SUNCG with interactive indoor scenes where agents can perform a variety of human-like tasks. They later focus on the task of navigation, where agents are asked to navigate to a destination without colliding with objects given a high level natural language instruction.", "rating": "5: Marginally below acceptance threshold", "review": "Building rich 3D environments where to run simulations is a very interesting area of research. \n\nStrengths:\n1.\tThe authors propose a virtual environment of indoor scenes having a much larger scale compared to similar interactive environments and access to multiple visual modalities. They also show how the number of available scenes greatly impacts generalization in navigation based tasks. \n2.\tThe authors provide a thorough analysis on the contribution of different feature types (Mask, Depth, RGB) towards the success rate of the goal task. The improvements and generalization brought by the segmentation and depth masks give interesting insights towards building new navigation paradigms for real-world robotics. \n\nWeaknesses:\n1.\tThe authors claim that the proposed environment allows for multiple applications and interactions, however from the description in section 3, the capacities of the simulator beyond navigation are unclear.\nThe dataset proposed, Home3D, adds a number of functionalities over the SUNCG dataset. The SUNCG dataset provides a large number of 3D scanned houses.  The most important contributions with respect to SUNCG are:\n- An efficient renderer: an important aspect.\n- Introducing physics: this is very interesting, unfortunately the contribution here is very small. Although I am sure the authors are planing to move beyond the current state of their implementation, the only physical constraint currently implemented is an occupancy rule and collision detection. This is not technically challenging. \nTherefore, the added novelty with respect to SUNCG is very limited.\n2.\tThe paper presents the proposed task as navigation from high level task description, but given that the instructions are fixed for a given target, there are only 5 possible instructions which are encoded as one-hot vectors. Given this setting, it is unclear the need for a gated attention mechanism. While this limited setting allows for a clear generalization analysis, it would have been good to study a setting with more complex instructions, allowing to evaluate instructions not seen during training.\n3.\tWhile the authors make a good point showing generalization towards unseen scenes, it would have been good to also show generalization towards real scenarios, demonstrating the realistic nature of House3D and the advantages of using non-RGB features.\n4.\tIt would have been good to report an analysis on the number of steps performed by the agent before reaching its goal on the success cases. It seems to me that the continuous policy would be justified in this setting. \nComments\n-\tIt is unclear to me how the reward shaping addition helps generalize to unseen houses at test time, as suggested by the authors.\n-\tI miss a reference to (https://arxiv.org/pdf/1609.05143.pdf) beyond the AI-THOR environment, given that they also approach target driven navigation using an actor-critic approach.\n\n\nThe paper proposes a new realistic indoor virtual environment, having a much larger number of scenes than similar environments. From the experiments shown, it seems that the scale increase, together with the availability of features such as Segmentation and Depth improve generalization in navigation tasks, which makes it a promising framework for future work on this direction. However, the task proposed seems too simple considering the power of this environment, and the models used to solve the task don\u2019t seem to bring  relevant novelties from previous approaches. (https://arxiv.org/pdf/1706.07230.pdf)\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642537102, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper976/AnonReviewer1", "ICLR.cc/2018/Conference/Paper976/AnonReviewer3", "ICLR.cc/2018/Conference/Paper976/AnonReviewer2"], "reply": {"forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642537102}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642537118, "tcdate": 1511809480113, "number": 3, "cdate": 1511809480113, "id": "BJlAby9gM", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Review", "forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "signatures": ["ICLR.cc/2018/Conference/Paper976/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "The paper in general is well written, and the environment will be a useful addition to the community", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper introduces House3D, a virtual 3D environment consisting of in-door scenes with a diverse set of scene types, layouts and objects. This was originally adapted from the SUNCG dataset, enhanced with the addition of a physics model and an API for interacting with the environment.  They then focus on a single high-level instruction following task where an agent is randomly assigned at a location in the house and is asked to navigate to a destination described by a high-level concept (\u201ckitchen\u201d) without colliding with objects.  They propose two models with gated-attention architecture for solving this task, a gated-CNN and a gated-LSTM. Whilst the novelty of the two models is questionable (they are adaptations of existing models to the task),  they are a useful addition to enable a benchmark on the task.  The paper in general is well written, and the environment will be a useful addition to the community.\n\nGeneral Comments\n- In the related work section the first part talks about several existing environments.  Whilst the table is useful, for the \u201clarge-scale\u201d and \u201cfast-speed\u201dcolumns, it would be better if there were some numbers attached - e.g.  are these orders of magnitude differences?  Are these amenable to Bayesian optimisation?\n- I  didn\u2019t  see  any  mention  of  a  pre-specified  validation  set  or  pre-defined cross-validation sets.  This would surely be essential for hyperparameter tuning\n- For the discrete action space state what the 12 actions are.\n- The  reward  function  should  be  described  in  more  detail  (can  be  in  appendix).  How is the shortest distance calculated?  As a general comment it seems that this is a very strong (and unrealistic) reward signal, particularly for generalisation.\n- There are a number of hyperparameters (\u03b1DDPG, \u03b1A3C,  entropy bonus terms, learning rates etc).  Some discussion of how these were chosen and the sensitivity to these parameters were helpful\n- Figures 3 and 4 are hard to compare, as they are separated by a page, and the y-axes are not shared.\n- The additional 3D scenes datasets mentioned by Ankur Handa should be cited.\n\nTypographical Issues\n- Page 1:  intelligence\u2192intelligent \n- Page  4:  On  average,  there  is\u2192On  average,  there  are;  we  write\u2192we wrote\n- Page 7:  softmax-gumbel trick\u2192softmax-Gumbel trick; gumbel-softmax\u2192Gumbel-softmax\n- References.  The references should have capitalisation where appropriate.For example,  openai\u2192OpenAI, gumbel\u2192Gumbel,  malmo\u2192Malmo", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642537102, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper976/AnonReviewer1", "ICLR.cc/2018/Conference/Paper976/AnonReviewer3", "ICLR.cc/2018/Conference/Paper976/AnonReviewer2"], "reply": {"forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642537102}}}, {"tddate": null, "ddate": null, "tmdate": 1515052598034, "tcdate": 1515052598034, "number": 7, "cdate": 1515052598034, "id": "ByREAIo7z", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "forum": "rkaT3zWCZ", "replyto": "BySAhfGgM", "signatures": ["ICLR.cc/2018/Conference/Paper976/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper976/Authors"], "content": {"title": "Answer to your concerns", "comment": "Dear Reviewer,\n \nWe thank you for your feedback. Below we address your comments and point to the changes made to accommodate them.\n \nSee general comments (the top official thread) for the novelty of the framework and the definition of the task and instructions.\n \nUnrealistic shortest path:\n1.     The shortest path is computed only during training to provide the agent with intermediate reward signals. In evaluation, there is no need to compute shortest path (or other statistics involving ground truth) and thus the trained agent can be evaluated in the real world.\n2.     Even if an agent needs to be trained in the real environment, we can also find surrogate shortest path, e.g., by building a map first. Note that building a map is not a laborious task, since it can be reused many times during training. Again in evaluation, these quantities are not needed.\n \nPhysics:\nCurrently we only have collision detection and in the future we will add more realistic physics engine (e.g., bullet).\n \nTo better reflect the nature of interactions used for RoomNav, we describe the interaction rules in House3D (see Environment Section). However, note that House3D is able to support more complicated engines. For the purpose of our work, we adopt the most lightweight interaction rules for fast experimental cycles. We have updated the changes regarding to physics in the new version of the paper.\n \nReward shaping\nSee general comments.\n \nAnalysis of number of steps:\nWe have added the analysis for the number of steps. See Appendix B.4. All reported success rates are computed from a fixed number of 100 steps.\n \nGated LSTM:\nWe concatenate the concept I with h_t so that the LSTM module can have direct access to the target when computing its states. In practice, this affected performance by a small amount but it made training more stable.\n \nMissing citations:\nWe have cited the relevant environments to our work. See Related Work Section.\n \nTo better reflect the motivation and contributions of our work, we have rephrased the parts of the text that seem to cause confusions with the hope that they would clarify hopefully all of your questions.\n \nWe sincerely hope that you could read the modified version.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725065, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper976/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper976/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper976/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725065}}}, {"tddate": null, "ddate": null, "tmdate": 1515052526986, "tcdate": 1515052526986, "number": 6, "cdate": 1515052526986, "id": "SywxCIj7f", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "forum": "rkaT3zWCZ", "replyto": "ByK1GkteM", "signatures": ["ICLR.cc/2018/Conference/Paper976/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper976/Authors"], "content": {"title": "Answer to your concerns", "comment": "Dear Reviewer,\n \nWe thank you for your feedback. Below we address your comments and point to the changes made to accommodate them.\n \nPlease check general comments (the very first official thread on the top) for Point 1-2.\n \n3.  Transfer learning from House3D to real environments (e.g., within an actual building) is an important direction to pursue. Our results also show that using semantic segmentation, depth and RGB images as input, it is possible to have a navigation agent that can be transferred to unseen scenarios. This suggests that using semantic segmentation given by state-of-the-art vision approaches should achieve strong performance without training from raw pixels. As a future work, this could be a promising direction for transfer learning.\n \n4. We have added the analysis for the number of steps in the Appendix Section B.4. In general, we notice that continuous policies involve fewer steps, as expected.\n \n5. Stated in the general comments.\n \n6. There are a few key differences between our task and the task proposed by Al-Thor:\n \n(1)\tIn AI-Thor, the learned agent is evaluated on the same environments as training, while ours is evaluated on unseen environments. We emphasize that this is a huge difference.\n(2)\tIn AI-Thor, navigation is restricted within a single room, while our work shows navigation results in houses with multiple rooms and indoor/outdoor situations.  \n(3)\tAI-Thor designs different networks for different room types, and the target is provided with an actual observation of the object. In contrast, ours use a shared network for 200 houses, and the target is provided with a word (concept). Therefore, the agent needs to associate the concept with the observations.\n \nIn summary, our navigation setting poses a major improvement over the setting in AI-Thor, and requires sophisticated actions to achieve the goal. As the first task proposed in House3D dataset, it is not simple at all.\n \nIn terms of writing, we have updated the paper and the terminology to clarify the motivation and contributions of our work. Our changes should better reflect the impact of our proposed environment and task.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725065, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper976/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper976/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper976/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725065}}}, {"tddate": null, "ddate": null, "tmdate": 1515052423155, "tcdate": 1515052423155, "number": 5, "cdate": 1515052423155, "id": "SyJ9TUiXM", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "forum": "rkaT3zWCZ", "replyto": "BJlAby9gM", "signatures": ["ICLR.cc/2018/Conference/Paper976/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper976/Authors"], "content": {"title": "We sincerely thank you for your comments", "comment": "Dear Reviewer,\n \nWe thank you for your valuable comments. Note that we have improved the text to better reflect our contributions (see Introduction). We sincerely hope that our proposed House3D environment and RoomNav task will be adopted as a benchmark in RL. We strongly agree that our proposed methods to tackle RoomNav are useful and necessary in order to encourage further research in House3D and concretely describe their extensions from existing RL techniques (see Introduction & Method Section)\n \nWe address your comments:\n \n1. Regarding to simulation speed, our environment is around 1.8K FPS on 120x90 image using only a single M40 GPUs. This makes our environment suitable for RL approaches that typically require both fast simulation and realistic scenarios. To our knowledge, very few environments strike such a balance (e.g., atari games can achieve 6K FPS but its observation is simple, while DeepMind lab and Malmo render more complex images but with a few hundred frames per second, slower than House3D). \n \n2.     In the first part of our experiments, following standard practice in RL, we trained on 200 house environments and report the success rate on these environments accordingly. Unlike traditional supervised learning, there is no pre-specified validation set or pre-defined cross-validation sets, since every image perceived from the agent in the same house environment can be different.\n \nIn the second part of our experiments, we tested our trained agent on 50 unseen house environments, a practice known as transfer learning in RL. We show that the trained agent is able to navigate around unseen environments, much better than other baselines (e.g., random exploration).\n \n3.      We test both continuous and discrete action space. For discrete action space, there are 8 movement actions and 4 rotation actions with different scales Please see Appendix A for more details.\n \n4. Sparse rewards pose difficulty in learning, as noted by several RL works in environments that are even simpler than House3D (Mirowski et al., 2016, Jaderberg et al., 2016). As a standard practice in RL, in all our experiments, to guide the agent towards finishing the task, intermediate reward is provided when the agent moves closer to the target, computed by shortest path. Note that this technique, known as reward shaping, is used during training but never during evaluation, in which an agent needs to navigate to the destination alone. Therefore, it does not affect the generalization capability, no matter how strong such a signal is. More details are provided in the Appendix.\n \n5. The hyper-parameters in our models are tuned using a very rough grid search without extensive tuning.\n \n6. We have fixed the typos, cited the existing 3D environments mentioned by other commenters and have fixed the figure layout for better comprehension.  \n \nWe sincerely appreciate your valuable suggestions!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725065, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper976/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper976/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper976/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725065}}}, {"tddate": null, "ddate": null, "tmdate": 1515052322780, "tcdate": 1515052322780, "number": 4, "cdate": 1515052322780, "id": "r1jXpIi7z", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "signatures": ["ICLR.cc/2018/Conference/Paper976/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper976/Authors"], "content": {"title": "General Clarifications to Common Misunderstandings", "comment": "Dear Reviewers and AC\nWe note some common misunderstandings in the reviews, so we highlight and clarify our main contributions in this thread. We also improved our introduction section with all these points clarified. \n\n**Novelty and Contributions**\n In addition to novel algorithms, we believe that it is also important to acknowledge the contributions regarding to environments and implementations. These contributions also help tremendously to research community, not by solving technically challenging problems, but by pointing to the right directions to pursue. For example, ImageNet fuels the usage of DL models in computer vision research because of its scale; AlphaGo achieves super-human Go AI by combining many old ideas in RL (e.g., ConvNet, MCTS, Selfplay) and massive computational resource together. An algorithm-centric criterion of novelty would reject both of the great works, and might lock researchers in the circle of \u201csmart algorithms\u201dthat might look mathematically interesting but never generalize well in the case of large scale and complicated situations.\n\n A main contribution of our work is that we are the first ones to explore \u201csemantic-level\u201d generalization in RL. Semantic-level generalization studies an agent\u2019s ability to extract conceptual abstractions (e.g., kitchen) from observations from a diverse set of scenes, and apply the same abstraction in unseen environments. Note that this is contrary to popular definitions of generalization in the RL literature such as pixel-level perturbations (e.g., object colors) or levels of difficulties (e.g., maze configurations). \n\n To explore semantic-level generalization, we develop a suitable large-scale environment, House3D. We focus on its scale (45k human-labored houses from SUNCG), its flexibility and efficiency. House3D can render images of 120x90 resolution on a single M40 GPU with 1600fps. As a testament to its flexible design, House3D has already been used for tasks beyond navigation, such as embodied QA (Das et al., 2017a). Moreover, our platform is not restricted to SUNCG, but can also use other data sources (e.g., Matterport, SceneNet).\n\n As an attempt to study semantic-level generalization in House3D, we define a \u201cconcept-driven navigation\u201d task, RoomNav. Here the goal is conveyed by a concept rather than a natural language instruction. We propose RoomNav in order to evaluate whether an agent can understand \u201csemantic concepts\u201d (e.g., room types) and can generalize to unseen scenarios. We hope RoomNav can serve as a benchmark task for semantic generalization.\n\n To tackle RoomNav, we propose to use gated-attention networks, which are shown to be effective for this task and can potentially serve as strong baselines for further benchmarking and fair comparisons.\n\n**Concept/Instructions**\nIn the submitted version of this paper, we refer\u201cinstruction\u201das the concept (e.g., roomtypes) that the agent needs to associate with its observations during exploration. We emphasize that in the paper, we did not suggest any connections between instruction and natural language. In fact, natural language instructions are beyond the scope of this submission. We have updated Method Section (Sec. 4) for a clearer narration to avoid possible confusion (as R2/R3\u2019s comments suggest). \n\n**Reward Shaping**\nReward shaping, as we state in the paper, provides a supervisory signal that helps the agent to learn faster. It is not provided to the agent during test time. Note that the RoomNav task is difficult and a sparse reward hinders training. We actually tried several reward-shaping approaches but didn\u2019t see any impact on generalization in experiments. So we only report the most effective reward shaping in the paper.\n\nWe have updated the abstract, introduction and related work of the paper to clarify these points. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725065, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper976/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper976/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper976/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725065}}}, {"tddate": null, "ddate": null, "tmdate": 1511645982115, "tcdate": 1511645953078, "number": 4, "cdate": 1511645953078, "id": "SJKZ7DPxM", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Public_Comment", "forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "signatures": ["~Ankesh_Anand1"], "readers": ["everyone"], "writers": ["~Ankesh_Anand1"], "content": {"title": "Questions regarding the environment", "comment": "Hi, \nI had a few questions regarding the environment:\n\n- Interaction: What agent-object interactions does House3D implement? Is there interaction aside from collision detection?\n-Physics Engine:  Is there support for rigid body dynamics, and can objects react to external forces and gravity?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791675247, "id": "ICLR.cc/2018/Conference/-/Paper976/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Area_Chair"], "cdate": 1512791675247}}}, {"tddate": null, "ddate": null, "tmdate": 1511414859344, "tcdate": 1511414859344, "number": 3, "cdate": 1511414859344, "id": "ryfUh0Xlf", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Public_Comment", "forum": "rkaT3zWCZ", "replyto": "HJWt2VcAZ", "signatures": ["~Saurabh_Gupta1"], "readers": ["everyone"], "writers": ["~Saurabh_Gupta1"], "content": {"title": "Alternative Environments", "comment": "Hi Ankesh,\n\nIn the meantime, you could try out our realtime 3D environment simulation and rendering code (https://github.com/tensorflow/models/tree/master/research/cognitive_mapping_and_planning).\n\nFor our CVPR 17 paper (https://sites.google.com/view/cognitive-mapping-and-planning/), we used it for visual navigation tasks using Matterport Scans from the Stanford Building Parser Dataset (http://buildingparser.stanford.edu/index.html), but it should be easy to plug in other sources of mesh data.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791675247, "id": "ICLR.cc/2018/Conference/-/Paper976/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Area_Chair"], "cdate": 1512791675247}}}, {"ddate": null, "tddate": 1510505678722, "tmdate": 1510505684794, "tcdate": 1509735545349, "number": 2, "cdate": 1509735545349, "id": "HJWt2VcAZ", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Public_Comment", "forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "signatures": ["~Ankesh_Anand1"], "readers": ["everyone"], "writers": ["~Ankesh_Anand1"], "content": {"title": "Open-sourcing the environment?", "comment": "Very neat work. We would be very interested in working with your environment. Would you be able to publish the source code / dataset for the work anonymously using a service like http://anonymous.4open.science/ ? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791675247, "id": "ICLR.cc/2018/Conference/-/Paper976/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Area_Chair"], "cdate": 1512791675247}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1510092430397, "tcdate": 1509846550444, "number": 1, "cdate": 1509846550444, "id": "HJAzR1nR-", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "forum": "rkaT3zWCZ", "replyto": "HJWt2VcAZ", "signatures": ["ICLR.cc/2018/Conference/Paper976/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper976/Authors"], "content": {"title": "We will open-source it ASAP", "comment": "We will provide flexible APIs as soon as possible, once the codebase of the environment is cleaned up."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725065, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper976/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper976/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper976/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725065}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1510092428605, "tcdate": 1509846611771, "number": 2, "cdate": 1509846611771, "id": "Hk38Cy2Cb", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "forum": "rkaT3zWCZ", "replyto": "Hkv6GtMRW", "signatures": ["ICLR.cc/2018/Conference/Paper976/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper976/Authors"], "content": {"title": "Really appreciate your suggestion", "comment": "Thank you very much for bringing these datasets to our attention. They are indeed relevant and very valuable to our work. We promise to cite all of them in the next version.\n\nActually, the design of our environment is flexible and can utilize 3D models from different sources. We have done some initial work to incorporate Matterport3D to our environment and our goal is to release a general API that can be used to incorporate a variety of 3D model sources, including SceneNet.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825725065, "id": "ICLR.cc/2018/Conference/-/Paper976/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper976/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper976/Authors|ICLR.cc/2018/Conference/Paper976/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper976/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper976/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper976/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825725065}}}, {"tddate": null, "ddate": null, "tmdate": 1509392066023, "tcdate": 1509229247137, "number": 1, "cdate": 1509229247137, "id": "Hkv6GtMRW", "invitation": "ICLR.cc/2018/Conference/-/Paper976/Public_Comment", "forum": "rkaT3zWCZ", "replyto": "rkaT3zWCZ", "signatures": ["~Ankur_Handa1"], "readers": ["everyone"], "writers": ["~Ankur_Handa1"], "content": {"title": "3D Scenes", "comment": "You might also be interested in the following work\n\n- SceneNet https://robotvault.bitbucket.io/\n- SceneNet RGB-D https://robotvault.bitbucket.io/scenenet-rgbd.html\n\nThey are photorealistic, 3D, customisable and potentially large scale. The source code is here https://bitbucket.org/dysonroboticslab/scenenetrgb-d/src. I think these papers are worth citing too. \n\nMatterport3D is a recent paper which has real 3D scenes https://niessner.github.io/Matterport/. This is also worth citing. \n\nYou could talk about the limitations/benefits of these datasets in your table. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "abstract": "Teaching an agent to navigate in an unseen 3D environment is a challenging task, even in the event of simulated environments. To generalize to unseen environments, an agent needs to be robust to low-level variations (e.g. color, texture, object changes), and also high-level variations (e.g. layout changes of the environment). To improve overall generalization, all types of variations in the environment have to be taken under consideration via different level of data augmentation steps. To this end, we propose House3D, a rich, extensible and efficient environment that contains 45,622 human-designed 3D scenes of visually realistic houses, ranging from single-room studios to multi-storied houses, equipped with a diverse set of fully labeled 3D objects, textures and scene layouts, based on the SUNCG dataset (Song et al., 2017). The diversity in House3D opens the door towards scene-level augmentation, while the label-rich nature of House3D enables us to inject pixel- & task-level augmentations such as domain randomization (Tobin et al., 2017) and multi-task training. Using a subset of houses in House3D, we show that reinforcement learning agents trained with an enhancement of different levels of augmentations perform much better in unseen environments than our baselines with raw RGB input by over 8% in terms of navigation success rate. House3D is publicly available at http://github.com/facebookresearch/House3D.", "pdf": "/pdf/708a189aabce4c44e2e80af691ff497126929126.pdf", "paperhash": "wu|building_generalizable_agents_with_a_realistic_and_rich_3d_environment", "_bibtex": "@misc{\nwu2018building,\ntitle={Building Generalizable Agents with a Realistic and Rich 3D Environment},\nauthor={Yi Wu and Yuxin Wu and Georgia Gkioxari and Yuandong Tian},\nyear={2018},\nurl={https://openreview.net/forum?id=rkaT3zWCZ},\n}", "authors": ["Yi Wu", "Yuxin Wu", "Georgia Gkioxari", "Yuandong Tian"], "keywords": ["reinforcement learning", "generalization", "navigation", "3D scenes"], "authorids": ["jxwuyi@gmail.com", "ppwwyyxxc@gmail.com", "georgia.gkioxari@gmail.com", "yuandong.tian@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791675247, "id": "ICLR.cc/2018/Conference/-/Paper976/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "rkaT3zWCZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper976/Authors", "ICLR.cc/2018/Conference/Paper976/Reviewers", "ICLR.cc/2018/Conference/Paper976/Area_Chair"], "cdate": 1512791675247}}}], "count": 15}