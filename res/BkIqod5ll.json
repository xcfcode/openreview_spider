{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396598084, "tcdate": 1486396598084, "number": 1, "id": "BkChnGLug", "invitation": "ICLR.cc/2017/conference/-/paper463/acceptance", "forum": "BkIqod5ll", "replyto": "BkIqod5ll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This work studies the problem of generalizing a convolutional neural network to data lacking grid-structure. \n \n The authors consider the Random Walk Normalized Laplacian and its finite powers to define a convolutional layer in a general graph. Experiments in Merck molecular discovery and mnist are reported. \n \n The reviewers all agreed that this paper, while presenting an interesting and important problem, lacks novelty relative to existing approaches. In particular, the AC would like to point out that important references seem to be missing from the current version. \n \n The proposed approach is closely related to 'Convolutional neural networks on graphs with fast localized spectral filtering', Defferrand et al. NIPS'16 , which considers Chevyshev polynomials of the Laplacian and learns the polynomial coefficients in an efficient manner. Since the Laplacian and the Random Walk Normalized Laplacian are similar operators (have same eigenvectors), the resulting model is essentially equivalent. Another related model that precedes all the cited works and is deeply related to the current submission is the Graph Neural Network from Scarselli et al.; see 'Geometric Deep Learning: going beyond Euclidean Data', Bronstein et al, https://arxiv.org/pdf/1611.08097v1.pdf' and references therein for more detailed comparisons between the models."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396598673, "id": "ICLR.cc/2017/conference/-/paper463/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkIqod5ll", "replyto": "BkIqod5ll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396598673}}}, {"tddate": null, "tmdate": 1485194439346, "tcdate": 1481948984835, "number": 2, "id": "S1bH1BMNg", "invitation": "ICLR.cc/2017/conference/-/paper463/official/review", "forum": "BkIqod5ll", "replyto": "BkIqod5ll", "signatures": ["ICLR.cc/2017/conference/paper463/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper463/AnonReviewer1"], "content": {"title": "Final review.", "rating": "6: Marginally above acceptance threshold", "review": "Update: I thank the authors for their comments! After reading them, I decided to increase the rating.\n\nThis paper proposes a variant of the convolution operation suitable for a broad class of graph structures. For each node in the graph, a set of neighbours is devised by means of random walk (the neighbours are ordered by the expected number of visits). As a result, the graph is transformed into a feature matrix resembling MATLAB\u2019s/Caffe\u2019s im2col output. The convolution itself becomes a matrix multiplication. \n\nAlthough the proposed convolution variant seems reasonable, I\u2019m not convinced by the empirical evaluation. The MNIST experiment looks especially suspicious. I don\u2019t think that this dataset is appropriate for the demonstration purposes in this case. In order to make their method applicable to the data, the authors remove important structural information (relative locations of pixels) thus artificially increasing the difficulty of the task. At the same time, they are comparing their approach with regular CNNs and conclude that the former performs poorly (and does not even reach an acceptable accuracy for the particular dataset).\n\nI guess, to justify the presence of MNIST (or similar datasets) in the experimental section, the authors should modify their method to incorporate additional graph structure (e.g. relative locations of nodes) in cases when the relation between nodes cannot be fully described by a similarity matrix.\n\nI believe, in its current form, the paper is not yet ready for publication but may be later resubmitted to a workshop or another conference after the concern above is addressed.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512577133, "id": "ICLR.cc/2017/conference/-/paper463/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper463/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper463/AnonReviewer2", "ICLR.cc/2017/conference/paper463/AnonReviewer1", "ICLR.cc/2017/conference/paper463/AnonReviewer3"], "reply": {"forum": "BkIqod5ll", "replyto": "BkIqod5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper463/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper463/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512577133}}}, {"tddate": null, "tmdate": 1482538427508, "tcdate": 1482538427508, "number": 5, "id": "HJQTaEsNe", "invitation": "ICLR.cc/2017/conference/-/paper463/public/comment", "forum": "BkIqod5ll", "replyto": "Hky8MaWVx", "signatures": ["~Yotam_Hechtlinger1"], "readers": ["everyone"], "writers": ["~Yotam_Hechtlinger1"], "content": {"title": "This is a naturel extension of CNN", "comment": "We wish to thank you for reviewing our paper.\n\nThis paper\u2019s main contribution is a novel way to apply convolutions on data which lacks a grid structure. This is being done in a similar way to what you've described in (2), only we keep the weights shared across all the variables according to their order (this is (3) - the decision to fix the order is important). In addition (1) explains how to do it for a general graph which lacks the similarity matrix. We combine (1), (2) and (3) together, then explain how to implement this in an efficient way, and demonstrate through empirical experiments that this works.\n\nWith regards to the opinion that the ideas in the the paper are obvious: we disagree. We instead believe they are natural given the nature of the problem and also easy to understand, both of which are arguably good qualities. We also think this is a straightforward, immediate generalization of CNN. \n\nWe would like to challenge the \"Clear rejection\" conclusion the reviewer draws from the fact that our solutions seem obvious to her/him. In particular, we would love to hear about any reference in which a methodology similar to the one we propose is used for similar purpose, in a similar manner and with comparable performance, rendering our contribution not novel and possibly redundant.\n\nRegarding the clarity - If you would elaborate on what troubled you with our writing style we will be happy to address it and revise the paper.\n\nRegarding Specific Comments - Thank you for the remark. Lusci et. al. do Recursive Neural Networks on the graph, and Duvenaud et. al. offer a specific solution for molecules. But we agree this can be written better. We were mostly referring to methods using the graph Laplacian. We will be more accurate in the next revision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287566469, "id": "ICLR.cc/2017/conference/-/paper463/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkIqod5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper463/reviewers", "ICLR.cc/2017/conference/paper463/areachairs"], "cdate": 1485287566469}}}, {"tddate": null, "tmdate": 1482538104993, "tcdate": 1482538104993, "number": 4, "id": "HJZF2NsEg", "invitation": "ICLR.cc/2017/conference/-/paper463/public/comment", "forum": "BkIqod5ll", "replyto": "S1bH1BMNg", "signatures": ["~Yotam_Hechtlinger1"], "readers": ["everyone"], "writers": ["~Yotam_Hechtlinger1"], "content": {"title": "Merck is the main experiment, but MNIST provide interesting intuition", "comment": "We thank you for your review.\n\nThe MNIST experiment was done to show how the graph convolution can generalize regular convolution when specific graph structure is used, and to show that this method works also when the spatial structure is not present and CNNs are not applicable.\n\nWe think that CNN is a great tool, which should be used whenever possible. We do not try to compete with CNNs on images or other data sets with grid structure, but rather offer an alternative to the other methods when the grid structure is not present and CNNs are not applicable. For that reason we do not expect this method to break the 0.75% of CNN on MNIST (even with more innovative graph structures).\n\nWith that said, following your review we have redone the experiment, increasing the epochs from 40->100, and the number of convolutions from 20->40 and 50->80. This resulted with 0.88% error rate. Still not CNNs, but better. Our intuition on this result is that there is finite number of ways to break ties, and the larger the number of convolutions, the closer we get to the regular CNNs.\n\nWe've also done a sainty check to see where we stand in terms of the required publication benchmark when using the MNIST dataset, by checking  NIPS 2016 published papers with MNIST in the abstract (https://nips.cc/Conferences/2016/Schedule?q=mnist). There are 13 different papers total. 3 papers use CNN and report excellent results. 5 papers address different type of problems and doesn't report accuracy. The remaining 5 use MNIST to demonstrate other methods than CNN. The papers (and their best MNIST result) are:\n\n-Supervised Learning Tensor Networks (0.97% error)\n-Binarized Neural Networks (0.96% error)\n-Direct Feedback Alignment Provides Learning in Deep Neural Networks (1.01% error)\n-Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering (0.86% error)\n-Dense Associative Memory for Pattern Recognition (around 1.4% - not directly reported)\n\nSo we think this is an acceptable result, especially given the fact that grid structure data is not the main purpose of the method.\n\nIt should be noted that in spite of the impression we might have made - we consider the Merck experiment to be the main experiment, in which we tackle a non trivial regression problem that has been tackled quite thoroughly in Kaggle, and achieve almost state of the art with a shallow application of the method. The MNIST usage is mostly due to the intuition it provides, and if you still think it is the paper major problem, we will be happy to explore other alternatives.\n\nThank you again for taking the time to review our work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287566469, "id": "ICLR.cc/2017/conference/-/paper463/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkIqod5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper463/reviewers", "ICLR.cc/2017/conference/paper463/areachairs"], "cdate": 1485287566469}}}, {"tddate": null, "tmdate": 1482538006009, "tcdate": 1482538006009, "number": 3, "id": "SkAzhNjEl", "invitation": "ICLR.cc/2017/conference/-/paper463/public/comment", "forum": "BkIqod5ll", "replyto": "Sk0nICB4l", "signatures": ["~Yotam_Hechtlinger1"], "readers": ["everyone"], "writers": ["~Yotam_Hechtlinger1"], "content": {"title": "The two papers address different neural networks architectures and have different goals", "comment": "Please notice that the paper by Coates & Ng (2011) construct locally connected receptive fields in a feed forward neural network style. They connect a given feature with it's 200 most correlated neighbors to create a single output unit. We share the weights and convolve the same weights on all the variables according to the order, generalizing convolutional neural networks. \n\nThe two papers address different neural networks architectures and have different goals - regardless of the hyper-parameter k. \n    \nRegarding the hyper-parameter - in 3.3 we explain why lower values of k are preferred over larger values of k. Indeed we have used k=1 when it makes sense. There are situations when this is infeasible, particularly when the graph is sparse. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287566469, "id": "ICLR.cc/2017/conference/-/paper463/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkIqod5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper463/reviewers", "ICLR.cc/2017/conference/paper463/areachairs"], "cdate": 1485287566469}}}, {"tddate": null, "tmdate": 1482184373919, "tcdate": 1482184373919, "number": 3, "id": "Sk0nICB4l", "invitation": "ICLR.cc/2017/conference/-/paper463/official/review", "forum": "BkIqod5ll", "replyto": "BkIqod5ll", "signatures": ["ICLR.cc/2017/conference/paper463/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper463/AnonReviewer3"], "content": {"title": "Modifies the way neighbors are computed for Graph-convolutional networks, but doesn't show that this modification is an improvement..", "rating": "3: Clear rejection", "review": "Previous literature uses data-derived adjacency matrix A to obtain neighbors to use as foundation of graph convolution. They propose extending the set of neighbors by additionally including nodes reachable by i<=k steps in this graph. This introduces an extra tunable parameter k, so it needs some justification over the previous k=1 solution. In one experiment provided\u00a0(Merk), using k=1 worked better. They don't specify which k that used, just that it was big enough for their to be p=5 nodes obtained as neighbors. In the second experiment (MNIST), they used k=1 for their experiments, which is what previous work (Coats & Ng 2011) proposed as well. A compelling experiment would compare to k=1 and show that using k>1 gives improvement strong enough to justify an extra hyper-parameter."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512577133, "id": "ICLR.cc/2017/conference/-/paper463/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper463/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper463/AnonReviewer2", "ICLR.cc/2017/conference/paper463/AnonReviewer1", "ICLR.cc/2017/conference/paper463/AnonReviewer3"], "reply": {"forum": "BkIqod5ll", "replyto": "BkIqod5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper463/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper463/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512577133}}}, {"tddate": null, "tmdate": 1481916998819, "tcdate": 1481916998819, "number": 1, "id": "Hky8MaWVx", "invitation": "ICLR.cc/2017/conference/-/paper463/official/review", "forum": "BkIqod5ll", "replyto": "BkIqod5ll", "signatures": ["ICLR.cc/2017/conference/paper463/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper463/AnonReviewer2"], "content": {"title": "Important problem, but lacks clarity and I'm not sure what the contribution is.", "rating": "3: Clear rejection", "review": "This work proposes a convolutional architecture for any graph-like input data (where the structure is example-dependent), or more generally, any data where the input dimensions that are related by a similarity matrix. If instead each input example is associated with a transition matrix, then a random walk algorithm is used generate a similarity matrix.\n\nDeveloping convolutional or recurrent architectures for graph-like data is an important problem because we would like to develop neural networks that can handle inputs such as molecule structures or social networks. However, I don't think this work contributes anything significant to the work that has already been done in this area. \n\nThe two main proposals I see in this paper are:\n1) For data associated with a transition matrix, this paper proposes that the transition matrix be converted to a similarity matrix. This seems obvious.\n2) For data associated with a similarity matrix, the k nearest neighbors of each node are computed and supply the context information for that node. This also seems obvious.\n\nPerhaps I have misunderstood the contribution, but the presentation also lacks clarity, and I cannot recommend this paper for publication. \n\nSpecific Comments:\n1) On page 4: \"An interesting attribute of this convolution, as compared to other convolutions on graphs is that, it preserves locality while still being applicable over different graphs with different structures.\"  This is false; the other proposed architectures can be applied to inputs with different structures (e.g. Duvenaud et. al., Lusci et. al. for NN architectures on molecules specifically). ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512577133, "id": "ICLR.cc/2017/conference/-/paper463/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper463/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper463/AnonReviewer2", "ICLR.cc/2017/conference/paper463/AnonReviewer1", "ICLR.cc/2017/conference/paper463/AnonReviewer3"], "reply": {"forum": "BkIqod5ll", "replyto": "BkIqod5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper463/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper463/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512577133}}}, {"tddate": null, "tmdate": 1481366461861, "tcdate": 1481366461855, "number": 2, "id": "rJ8asUtmx", "invitation": "ICLR.cc/2017/conference/-/paper463/public/comment", "forum": "BkIqod5ll", "replyto": "rk-cs8yXl", "signatures": ["~Yotam_Hechtlinger1"], "readers": ["everyone"], "writers": ["~Yotam_Hechtlinger1"], "content": {"title": "Respond to pre-review questions", "comment": "1&2. Please see the revision we've just submitted. This is a very important insight. Thank you for catching it. Following your remark, we noticed ties has been broken arbitrarily. Rather than making it consistent, we think that the fact that the model actually get 1.1 error rate that way makes the example more compelling. \n\n3. We agree that the spatial structure of the graph matters a lot. We actually think it what matters the most in this method. Notice this are two independent questions. \n\nConstructing an effective graph structure to a dataset is a graph learning problem heavily researched in some applications. This paper steps in once you have the graph structure. It provides a tool incorporating the information provided/learned from the graph structure into a standard supervised learning framework. \n\nYou can apply the graph convolution on any graph structure. But unlike the regular convolution, the structure is no longer trivial, and as you mention, it is very important. In that sense, you can think of our usage of the naive correlation matrix as a lower bound on the potential of the method.\n\nWe apologize for the delayed respond, some of us have been presenting at the NIPS workshops.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287566469, "id": "ICLR.cc/2017/conference/-/paper463/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkIqod5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper463/reviewers", "ICLR.cc/2017/conference/paper463/areachairs"], "cdate": 1485287566469}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481365033060, "tcdate": 1478294414065, "number": 463, "id": "BkIqod5ll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkIqod5ll", "signatures": ["~Purvasha_Chakravarti1"], "readers": ["everyone"], "content": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480730250435, "tcdate": 1480711049110, "number": 1, "id": "rk-cs8yXl", "invitation": "ICLR.cc/2017/conference/-/paper463/pre-review/question", "forum": "BkIqod5ll", "replyto": "BkIqod5ll", "signatures": ["ICLR.cc/2017/conference/paper463/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper463/AnonReviewer1"], "content": {"title": "Questions", "question": "1. p. 8, 4.2, How does the proposed method compare to regular CNNs with the same number of parameters?\n\n2. p. 8, 4.2, How one would ensure consistent ordering of pixels for all the spatial locations? For some pixels, the expected number of visits is the same and it's essential that the ties are broken consistently, otherwise, for some nodes, the top pixel would go before the bottom one, while for the others the order would be swapped.\n\n3. For some applications/datasets, the spatial structure of the graph matters a lot (e.g. relative locations of pixels w.r.t. the central pixel). How would one approach incorporating that information into the computation of the graph convolution?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959267771, "id": "ICLR.cc/2017/conference/-/paper463/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper463/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper463/AnonReviewer1"], "reply": {"forum": "BkIqod5ll", "replyto": "BkIqod5ll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper463/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper463/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959267771}}}, {"tddate": null, "tmdate": 1478554216703, "tcdate": 1478549556738, "number": 1, "id": "rkpNlD0xx", "invitation": "ICLR.cc/2017/conference/-/paper463/public/comment", "forum": "BkIqod5ll", "replyto": "BkIqod5ll", "signatures": ["~Tara_N_Sainath1"], "readers": ["everyone"], "writers": ["~Tara_N_Sainath1"], "content": {"title": "ICLR Paper Format", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format stating in the header \"submitted\" instead of \"published\" for your submission to be considered. Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Convolutional Neural Networks Generalization Utilizing the Data Graph Structure", "abstract": "Convolutional Neural Networks have proved to be very efficient in image and audio processing. Their success is mostly attributed to the convolutions which utilize the geometric properties of a low - dimensional grid structure. This paper suggests a generalization of CNNs to graph-structured data with varying graph structure, that can be applied to standard regression or classification problems by learning the graph structure of the data. We propose a novel convolution framework approach on graphs which utilizes a random walk to select relevant nodes. The convolution shares weights on all features, providing the desired parameter efficiency.  Furthermore, the additional computations in the training process are only executed once in the pre-processing step.  We empirically demonstrate the performance of the proposed CNN on MNIST data set, and challenge the state-of-the-art on Merck molecular activity data set.", "pdf": "/pdf/e5a6aa65a57e5d06fbe9c6e4ac38eacc64e4052f.pdf", "TL;DR": "A generalization of CNNs to standard regression and classification problems by using random walk on the data graph structure.", "paperhash": "hechtlinger|convolutional_neural_networks_generalization_utilizing_the_data_graph_structure", "conflicts": ["cmu.edu"], "keywords": ["Supervised Learning", "Deep learning"], "authors": ["Yotam Hechtlinger", "Purvasha Chakravarti", "Jining Qin"], "authorids": ["yhechtli@andrew.cmu.edu", "pchakrav@andrew.cmu.edu", "jiningq@andrew.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287566469, "id": "ICLR.cc/2017/conference/-/paper463/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkIqod5ll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper463/reviewers", "ICLR.cc/2017/conference/paper463/areachairs"], "cdate": 1485287566469}}}], "count": 11}