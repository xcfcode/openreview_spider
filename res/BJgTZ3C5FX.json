{"notes": [{"id": "yFrmYiBr1gF", "original": null, "number": 1, "cdate": 1589290850983, "ddate": null, "tcdate": 1589290850983, "tmdate": 1589291074283, "tddate": null, "forum": "BJgTZ3C5FX", "replyto": "BklBblb52Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1213/Public_Comment", "content": {"comment": "> Indeed, it requires an exponential number of samples to even differentiate between two batches of the same Gaussian [4].\nAre you referring to Lemma 1?", "title": "Exponential Number of Samples to Differentiate Batches of same Gaussian. "}, "signatures": ["~Alexander_Mathiasen2"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Alexander_Mathiasen2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative model based on minimizing exact empirical Wasserstein distance", "abstract": "Generative Adversarial Networks (GANs) are a very powerful framework for generative modeling. However, they are often hard to train, and learning of GANs often becomes unstable. Wasserstein GAN (WGAN) is a promising framework to deal with the instability problem as it has a good convergence property. One drawback of the WGAN is that it evaluates the Wasserstein distance in the dual domain, which requires some approximation, so that it may fail to optimize the true Wasserstein distance. In this paper, we propose evaluating the exact empirical optimal transport cost efficiently in the primal domain and performing gradient descent with respect to its derivative to train the generator network. Experiments on the MNIST dataset show that our method is significantly stable to converge, and achieves the lowest Wasserstein distance among the WGAN variants at the cost of some sharpness of generated images. Experiments on the 8-Gaussian toy dataset show that better gradients for the generator are obtained in our method. In addition, the proposed method enables more flexible generative modeling than WGAN.", "keywords": ["Generative modeling", "Generative Adversarial Networks (GANs)", "Wasserstein GAN", "Optimal transport"], "authorids": ["iohara@sys.i.kyoto-u.ac.jp", "takahito.ogawa@datagrid.co.jp", "tt@i.kyoto-u.ac.jp"], "authors": ["Akihiro Iohara", "Takahito Ogawa", "Toshiyuki Tanaka"], "TL;DR": "We have proposed a flexible generative model that learns stably by directly minimizing exact empirical Wasserstein distance.", "pdf": "/pdf/5fe5e5faaf322cf8b055489acbe0b29c6b833fb4.pdf", "paperhash": "iohara|generative_model_based_on_minimizing_exact_empirical_wasserstein_distance", "_bibtex": "@misc{\niohara2019generative,\ntitle={Generative model based on minimizing exact empirical Wasserstein distance},\nauthor={Akihiro Iohara and Takahito Ogawa and Toshiyuki Tanaka},\nyear={2019},\nurl={https://openreview.net/forum?id=BJgTZ3C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1213/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311651887, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJgTZ3C5FX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1213/Authors", "ICLR.cc/2019/Conference/Paper1213/Reviewers", "ICLR.cc/2019/Conference/Paper1213/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1213/Authors", "ICLR.cc/2019/Conference/Paper1213/Reviewers", "ICLR.cc/2019/Conference/Paper1213/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311651887}}}, {"id": "BJgTZ3C5FX", "original": "BkgYsLU5t7", "number": 1213, "cdate": 1538087940590, "ddate": null, "tcdate": 1538087940590, "tmdate": 1545355419341, "tddate": null, "forum": "BJgTZ3C5FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Generative model based on minimizing exact empirical Wasserstein distance", "abstract": "Generative Adversarial Networks (GANs) are a very powerful framework for generative modeling. However, they are often hard to train, and learning of GANs often becomes unstable. Wasserstein GAN (WGAN) is a promising framework to deal with the instability problem as it has a good convergence property. One drawback of the WGAN is that it evaluates the Wasserstein distance in the dual domain, which requires some approximation, so that it may fail to optimize the true Wasserstein distance. In this paper, we propose evaluating the exact empirical optimal transport cost efficiently in the primal domain and performing gradient descent with respect to its derivative to train the generator network. Experiments on the MNIST dataset show that our method is significantly stable to converge, and achieves the lowest Wasserstein distance among the WGAN variants at the cost of some sharpness of generated images. Experiments on the 8-Gaussian toy dataset show that better gradients for the generator are obtained in our method. In addition, the proposed method enables more flexible generative modeling than WGAN.", "keywords": ["Generative modeling", "Generative Adversarial Networks (GANs)", "Wasserstein GAN", "Optimal transport"], "authorids": ["iohara@sys.i.kyoto-u.ac.jp", "takahito.ogawa@datagrid.co.jp", "tt@i.kyoto-u.ac.jp"], "authors": ["Akihiro Iohara", "Takahito Ogawa", "Toshiyuki Tanaka"], "TL;DR": "We have proposed a flexible generative model that learns stably by directly minimizing exact empirical Wasserstein distance.", "pdf": "/pdf/5fe5e5faaf322cf8b055489acbe0b29c6b833fb4.pdf", "paperhash": "iohara|generative_model_based_on_minimizing_exact_empirical_wasserstein_distance", "_bibtex": "@misc{\niohara2019generative,\ntitle={Generative model based on minimizing exact empirical Wasserstein distance},\nauthor={Akihiro Iohara and Takahito Ogawa and Toshiyuki Tanaka},\nyear={2019},\nurl={https://openreview.net/forum?id=BJgTZ3C5FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Syg3jNhEJE", "original": null, "number": 1, "cdate": 1543976099949, "ddate": null, "tcdate": 1543976099949, "tmdate": 1545354495812, "tddate": null, "forum": "BJgTZ3C5FX", "replyto": "BJgTZ3C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1213/Meta_Review", "content": {"metareview": "This method proposes a primal approach to minimizing Wasserstein distance for generative models. It estimates WD by computing the exact WD between empirical distributions.\n\nAs the reviewers point out, the primal approach has been studied by other papers (which this submission doesn't cite, even in the revision), and suffers from a well-known problem of high variance. The authors have not responded to key criticisms of the reviewers. I don't think this work is ready for publication in ICLR.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "lack of novelty, variance in high dimensions"}, "signatures": ["ICLR.cc/2019/Conference/Paper1213/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1213/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative model based on minimizing exact empirical Wasserstein distance", "abstract": "Generative Adversarial Networks (GANs) are a very powerful framework for generative modeling. However, they are often hard to train, and learning of GANs often becomes unstable. Wasserstein GAN (WGAN) is a promising framework to deal with the instability problem as it has a good convergence property. One drawback of the WGAN is that it evaluates the Wasserstein distance in the dual domain, which requires some approximation, so that it may fail to optimize the true Wasserstein distance. In this paper, we propose evaluating the exact empirical optimal transport cost efficiently in the primal domain and performing gradient descent with respect to its derivative to train the generator network. Experiments on the MNIST dataset show that our method is significantly stable to converge, and achieves the lowest Wasserstein distance among the WGAN variants at the cost of some sharpness of generated images. Experiments on the 8-Gaussian toy dataset show that better gradients for the generator are obtained in our method. In addition, the proposed method enables more flexible generative modeling than WGAN.", "keywords": ["Generative modeling", "Generative Adversarial Networks (GANs)", "Wasserstein GAN", "Optimal transport"], "authorids": ["iohara@sys.i.kyoto-u.ac.jp", "takahito.ogawa@datagrid.co.jp", "tt@i.kyoto-u.ac.jp"], "authors": ["Akihiro Iohara", "Takahito Ogawa", "Toshiyuki Tanaka"], "TL;DR": "We have proposed a flexible generative model that learns stably by directly minimizing exact empirical Wasserstein distance.", "pdf": "/pdf/5fe5e5faaf322cf8b055489acbe0b29c6b833fb4.pdf", "paperhash": "iohara|generative_model_based_on_minimizing_exact_empirical_wasserstein_distance", "_bibtex": "@misc{\niohara2019generative,\ntitle={Generative model based on minimizing exact empirical Wasserstein distance},\nauthor={Akihiro Iohara and Takahito Ogawa and Toshiyuki Tanaka},\nyear={2019},\nurl={https://openreview.net/forum?id=BJgTZ3C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1213/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352922835, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJgTZ3C5FX", "replyto": "BJgTZ3C5FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1213/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1213/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1213/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352922835}}}, {"id": "rygLK5fqnX", "original": null, "number": 3, "cdate": 1541184126038, "ddate": null, "tcdate": 1541184126038, "tmdate": 1541533327122, "tddate": null, "forum": "BJgTZ3C5FX", "replyto": "BJgTZ3C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1213/Official_Review", "content": {"title": "promising results and idea", "review": "The paper proposed to use the exact empirical Wasserstein distance to supervise the training of generative model. To this end, the authors formulated the optimal transport cost as a linear programming problem. The quantitative results-- empirical Wasserstein distance show the superiority of the proposed methods.\n \nMy concerns come from both theoretical and experimental aspects:\nThe linear-programming problem Eq.(4)-Eq.(7) has been studied in existing literature.\nThe contribution is about combining this existing method to supervise a standard neural network parametrized generator, so I am not quite sure if this contribution is sufficient for the ICLR submission.\nIn such a case, further experimental or theoretical study about the convergence of Algorithm 1 seems important to me.\n \nAs to the experiments, firstly, EWD seems to be a little bit biased since EWD is literally used to supervise the training of the proposed method.\nOther quantitative metric studies can help justifying the improvement.\nAlso, given that the paper brings the WGAN family into comparison, the large scale image dataset should be included since WGAN have already demonstrated their success.\n \nLast things, missing parentheses in step 8 of Algorithm 1 and overlength of url in references.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1213/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative model based on minimizing exact empirical Wasserstein distance", "abstract": "Generative Adversarial Networks (GANs) are a very powerful framework for generative modeling. However, they are often hard to train, and learning of GANs often becomes unstable. Wasserstein GAN (WGAN) is a promising framework to deal with the instability problem as it has a good convergence property. One drawback of the WGAN is that it evaluates the Wasserstein distance in the dual domain, which requires some approximation, so that it may fail to optimize the true Wasserstein distance. In this paper, we propose evaluating the exact empirical optimal transport cost efficiently in the primal domain and performing gradient descent with respect to its derivative to train the generator network. Experiments on the MNIST dataset show that our method is significantly stable to converge, and achieves the lowest Wasserstein distance among the WGAN variants at the cost of some sharpness of generated images. Experiments on the 8-Gaussian toy dataset show that better gradients for the generator are obtained in our method. In addition, the proposed method enables more flexible generative modeling than WGAN.", "keywords": ["Generative modeling", "Generative Adversarial Networks (GANs)", "Wasserstein GAN", "Optimal transport"], "authorids": ["iohara@sys.i.kyoto-u.ac.jp", "takahito.ogawa@datagrid.co.jp", "tt@i.kyoto-u.ac.jp"], "authors": ["Akihiro Iohara", "Takahito Ogawa", "Toshiyuki Tanaka"], "TL;DR": "We have proposed a flexible generative model that learns stably by directly minimizing exact empirical Wasserstein distance.", "pdf": "/pdf/5fe5e5faaf322cf8b055489acbe0b29c6b833fb4.pdf", "paperhash": "iohara|generative_model_based_on_minimizing_exact_empirical_wasserstein_distance", "_bibtex": "@misc{\niohara2019generative,\ntitle={Generative model based on minimizing exact empirical Wasserstein distance},\nauthor={Akihiro Iohara and Takahito Ogawa and Toshiyuki Tanaka},\nyear={2019},\nurl={https://openreview.net/forum?id=BJgTZ3C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1213/Official_Review", "cdate": 1542234279685, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJgTZ3C5FX", "replyto": "BJgTZ3C5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1213/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335897563, "tmdate": 1552335897563, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1213/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BklBblb52Q", "original": null, "number": 2, "cdate": 1541177341427, "ddate": null, "tcdate": 1541177341427, "tmdate": 1541533326913, "tddate": null, "forum": "BJgTZ3C5FX", "replyto": "BJgTZ3C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1213/Official_Review", "content": {"title": "Review for \"Generative model based on minimizing exact empirical Wasserstein distance\".", "review": "The authors propose to estimate and minimize the empirical Wasserstein distance between batches of samples of real and fake data, then calculate a (sub) gradient of it with respect to the generator's parameters and use it to train generative models.\n\nThis is an approach that has been tried[1,2] (even with the addition of entropy regularization) and studied [1-5] extensively. It doesn't scale, and for extremely well understood reasons[2,3]. The bias of the empirical Wasserstein estimate requires an exponential number of samples as the number of dimensions increases to reach a certain amount of error [2-6]. Indeed, it requires an exponential number of samples to even differentiate between two batches of the same Gaussian[4]. On top of these arguments, the results do not suggest any new finding or that these theoretical limitations would not be relevant in practice. If the authors have results and design choices making this method work in a high dimensional problem such as LSUN, I will revise my review.\n\n[1]: https://arxiv.org/abs/1706.00292\n[2]: https://arxiv.org/abs/1708.02511\n[3]: https://arxiv.org/abs/1712.07822\n[4]: https://arxiv.org/abs/1703.00573\n[5]: http://www.gatsby.ucl.ac.uk/~gretton/papers/SriFukGreSchetal12.pdf\n[6]: https://www.sciencedirect.com/science/article/pii/0377042794900337", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1213/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative model based on minimizing exact empirical Wasserstein distance", "abstract": "Generative Adversarial Networks (GANs) are a very powerful framework for generative modeling. However, they are often hard to train, and learning of GANs often becomes unstable. Wasserstein GAN (WGAN) is a promising framework to deal with the instability problem as it has a good convergence property. One drawback of the WGAN is that it evaluates the Wasserstein distance in the dual domain, which requires some approximation, so that it may fail to optimize the true Wasserstein distance. In this paper, we propose evaluating the exact empirical optimal transport cost efficiently in the primal domain and performing gradient descent with respect to its derivative to train the generator network. Experiments on the MNIST dataset show that our method is significantly stable to converge, and achieves the lowest Wasserstein distance among the WGAN variants at the cost of some sharpness of generated images. Experiments on the 8-Gaussian toy dataset show that better gradients for the generator are obtained in our method. In addition, the proposed method enables more flexible generative modeling than WGAN.", "keywords": ["Generative modeling", "Generative Adversarial Networks (GANs)", "Wasserstein GAN", "Optimal transport"], "authorids": ["iohara@sys.i.kyoto-u.ac.jp", "takahito.ogawa@datagrid.co.jp", "tt@i.kyoto-u.ac.jp"], "authors": ["Akihiro Iohara", "Takahito Ogawa", "Toshiyuki Tanaka"], "TL;DR": "We have proposed a flexible generative model that learns stably by directly minimizing exact empirical Wasserstein distance.", "pdf": "/pdf/5fe5e5faaf322cf8b055489acbe0b29c6b833fb4.pdf", "paperhash": "iohara|generative_model_based_on_minimizing_exact_empirical_wasserstein_distance", "_bibtex": "@misc{\niohara2019generative,\ntitle={Generative model based on minimizing exact empirical Wasserstein distance},\nauthor={Akihiro Iohara and Takahito Ogawa and Toshiyuki Tanaka},\nyear={2019},\nurl={https://openreview.net/forum?id=BJgTZ3C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1213/Official_Review", "cdate": 1542234279685, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJgTZ3C5FX", "replyto": "BJgTZ3C5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1213/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335897563, "tmdate": 1552335897563, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1213/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylIWjW827", "original": null, "number": 1, "cdate": 1540918014414, "ddate": null, "tcdate": 1540918014414, "tmdate": 1541533326683, "tddate": null, "forum": "BJgTZ3C5FX", "replyto": "BJgTZ3C5FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1213/Official_Review", "content": {"title": "Title claim seems wrong", "review": "The paper \u2018Generative model based on minimizing exact empirical Wasserstein distance' proposes\na variant of Wasserstein GAN based on a primal version of the Wasserstein loss rather than the relying\non the classical Kantorovich-Rubinstein duality as first proposed by Arjovsky in the GAN context.\nComparisons with other variants of Wasserstein GAN is proposed on MNIST.\n\nI see little novelty in the paper. The derivation of the primal version of the problem is already \ngiven in  \nCuturi, M., & Doucet, A. (2014, January). Fast computation of Wasserstein barycenters. In ICML (pp. 685-693).\n\nUsing optimal transport computed on batches rather the on the whole dataset is already used in (among\nothers)\n Genevay, A., Peyr\u00e9, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. AISTATS\n Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., & Courty, N. (2018). DeepJDOT: Deep Joint distribution optimal transport for unsupervised domain adaptation. ECCV  \n\nAlso, the claim that the exact empirical Wasserstein distance is optimized is not true. The gradients, evaluated on \nbatches, are biased. Unfortunately, the Wasserstein distance does not enjoy similar U-statistics as MMD. It is very \nwell described in the paper (Section 3): \nhttps://openreview.net/pdf?id=S1m6h21Cb\n\nComputing the gradients of Wasserstein on batches might be seen a kind of regularization, but it remains to be\nproved and discussed.\n\nFinally, the experimental validation appears insufficient to me (as only MNIST or toy datasets are considered).\n\n\nTypos:\n Eq (1) and (2): when taken over the set of all Lipschitz-1 functions, the max should be a sup ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1213/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative model based on minimizing exact empirical Wasserstein distance", "abstract": "Generative Adversarial Networks (GANs) are a very powerful framework for generative modeling. However, they are often hard to train, and learning of GANs often becomes unstable. Wasserstein GAN (WGAN) is a promising framework to deal with the instability problem as it has a good convergence property. One drawback of the WGAN is that it evaluates the Wasserstein distance in the dual domain, which requires some approximation, so that it may fail to optimize the true Wasserstein distance. In this paper, we propose evaluating the exact empirical optimal transport cost efficiently in the primal domain and performing gradient descent with respect to its derivative to train the generator network. Experiments on the MNIST dataset show that our method is significantly stable to converge, and achieves the lowest Wasserstein distance among the WGAN variants at the cost of some sharpness of generated images. Experiments on the 8-Gaussian toy dataset show that better gradients for the generator are obtained in our method. In addition, the proposed method enables more flexible generative modeling than WGAN.", "keywords": ["Generative modeling", "Generative Adversarial Networks (GANs)", "Wasserstein GAN", "Optimal transport"], "authorids": ["iohara@sys.i.kyoto-u.ac.jp", "takahito.ogawa@datagrid.co.jp", "tt@i.kyoto-u.ac.jp"], "authors": ["Akihiro Iohara", "Takahito Ogawa", "Toshiyuki Tanaka"], "TL;DR": "We have proposed a flexible generative model that learns stably by directly minimizing exact empirical Wasserstein distance.", "pdf": "/pdf/5fe5e5faaf322cf8b055489acbe0b29c6b833fb4.pdf", "paperhash": "iohara|generative_model_based_on_minimizing_exact_empirical_wasserstein_distance", "_bibtex": "@misc{\niohara2019generative,\ntitle={Generative model based on minimizing exact empirical Wasserstein distance},\nauthor={Akihiro Iohara and Takahito Ogawa and Toshiyuki Tanaka},\nyear={2019},\nurl={https://openreview.net/forum?id=BJgTZ3C5FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1213/Official_Review", "cdate": 1542234279685, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJgTZ3C5FX", "replyto": "BJgTZ3C5FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1213/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335897563, "tmdate": 1552335897563, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1213/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}