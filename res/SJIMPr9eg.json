{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396464080, "tcdate": 1486396464080, "number": 1, "id": "BydV2fL_x", "invitation": "ICLR.cc/2017/conference/-/paper256/acceptance", "forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396464554, "id": "ICLR.cc/2017/conference/-/paper256/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396464554}}}, {"tddate": null, "tmdate": 1482129050199, "tcdate": 1482129050199, "number": 3, "id": "ByfsAgHVg", "invitation": "ICLR.cc/2017/conference/-/paper256/official/review", "forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "signatures": ["ICLR.cc/2017/conference/paper256/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper256/AnonReviewer2"], "content": {"title": "The contribution is incremental with no impressive comparison results", "rating": "3: Clear rejection", "review": "This paper proposes a boosting based ensemble procedure for residual networks by adopting the Deep Incremental Boosting method that was used for CNN's(Mosca & Magoulas, 2016a). At each step t, a new block of layers are added to the network at a position p_t and the weights of all layers are copied to the current network to speed up training.\n\nThe method is not sufficiently novel since the steps of Deep Incremental Boosting are slightly adopted. Instead of adding a layer to the end of the network, this version adds a block of layers to a position p_t (starts at a selected position p_0) and merges layer accordingly hence slightly adopts DIB. \n\nThe empirical analysis does not use any data-augmentation. It is not clear whether the improvements (if there is) of the ensemble disappear after data-augmentation.  Also, one of the main baselines, DIB has no-skip connections therefore this can negatively affect the fair comparison. The authors argue that they did not involve state of art Res Nets since their analysis focuses on the ensemble approach, however any potential improvement of the ensemble can be compensated with an inherent feature of Res Net variant. The boosting procedure can be computationally restrictive in case of ImageNet training and Res Net variants may perform much better in that case too. Therefore the baselines should include the state of art Res Nets and Dense Convolutional networks hence current results are preliminary.\n\nIn addition, it is not clear how sensitive the boosting to the selection of injection point.\n\nThis paper adopts DIB to Res Nets and provides some empirical analysis however the contribution is not sufficiently novel and the empirical results are not satisfactory for demonstrating that the method is significant.\n\nPros\n-provides some preliminary results for boosting of Res Nets\nCons\n-not sufficiently novel: an incremental approach \n-empirical analysis is not satisfactory", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512646218, "id": "ICLR.cc/2017/conference/-/paper256/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper256/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper256/AnonReviewer1", "ICLR.cc/2017/conference/paper256/AnonReviewer3", "ICLR.cc/2017/conference/paper256/AnonReviewer2"], "reply": {"forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper256/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper256/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512646218}}}, {"tddate": null, "tmdate": 1482125474323, "tcdate": 1482125474323, "number": 2, "id": "rkqolxB4e", "invitation": "ICLR.cc/2017/conference/-/paper256/official/review", "forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "signatures": ["ICLR.cc/2017/conference/paper256/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper256/AnonReviewer3"], "content": {"title": "Lack of comparison", "rating": "4: Ok but not good enough - rejection", "review": "The authors mention that they are not aiming to have SOTA results.\nHowever, that an ensemble of resnets has lower performance than some of single network results, indicates that further experimentation preferably on larger datasets is necessary.\nThe literature review could at least mention some existing works such as wide resnets https://arxiv.org/abs/1605.07146 or the ones that use knowledge distillation for ensemble of networks for comparison on cifar.\nWhile the manuscript is well-written and the idea is novel, it needs to be extended with experiments.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512646218, "id": "ICLR.cc/2017/conference/-/paper256/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper256/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper256/AnonReviewer1", "ICLR.cc/2017/conference/paper256/AnonReviewer3", "ICLR.cc/2017/conference/paper256/AnonReviewer2"], "reply": {"forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper256/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper256/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512646218}}}, {"tddate": null, "tmdate": 1481895057188, "tcdate": 1481895057188, "number": 1, "id": "SJY52vWVg", "invitation": "ICLR.cc/2017/conference/-/paper256/official/review", "forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "signatures": ["ICLR.cc/2017/conference/paper256/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper256/AnonReviewer1"], "content": {"title": "Interesting ideas, unconvincing execution, lack of comparisons to the literature", "rating": "3: Clear rejection", "review": "The paper under consideration proposes a set of procedures for incrementally expanding a residual network by adding layers via a boosting criterion.\n\nThe main barrier to publication is the weak empirical validation. The tasks considered are quite small scale in 2016 (and MNIST with a convolutional net is basically an uninteresting test by this point). The paper doesn't compare to the literature, and CIFAR-10 results fail to improve upon rather simple, single-network published baselines (Springenberg et al, 2015 for example, obtains 92% without data augmentation) and I'm pretty sure there's a simple ResNet result somewhere that outshines these too. The CIFAR100 results are a little bit interesting as they are better than I'm used to seeing (I haven't done a recent literature crawl), and this is unsurprising -- you'd expect ensembles to do well when there's a dearth of labeled training data, and here there are only a few hundred per label. But then it's typical on both CIFAR10 and CIFAR100 to use simple data augmentation schemes which aren't employed here, and these and other forms of regularization are a simpler alternative to a complicated iterative augmentation scheme like this.\n\nIt'd be easier to sell this method either as an option for scarce labeled datasets where data augmentation is non-trivial (but then for most image-related applications, random crops and reflections are easy and valid), but that would necessitate different benchmarks, and comparison against simpler methods like said data augmentation, dropout (especially, due to the ensemble interpretation), and so on.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512646218, "id": "ICLR.cc/2017/conference/-/paper256/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper256/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper256/AnonReviewer1", "ICLR.cc/2017/conference/paper256/AnonReviewer3", "ICLR.cc/2017/conference/paper256/AnonReviewer2"], "reply": {"forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper256/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper256/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512646218}}}, {"tddate": null, "tmdate": 1481134797319, "tcdate": 1481134797314, "number": 2, "id": "ByH0G0SQe", "invitation": "ICLR.cc/2017/conference/-/paper256/public/comment", "forum": "SJIMPr9eg", "replyto": "r1qvFQCzl", "signatures": ["~Alan_Mosca1"], "readers": ["everyone"], "writers": ["~Alan_Mosca1"], "content": {"title": "revision added", "comment": "We have uploaded a new revision of the paper which we believe addresses the comments.\n\nWe have not found \"white-box\" to be used anywhere in the literature, so we added a short paragraph to explain what we mean in section 1.\nWe added a clarification on how the residual block is initialised (and all other layers) in section 4.\nWe could not verify that our experimental results were outside of a Bernoulli confidence interval, but we have made the observation that, because we are using the same \"lock-stepped\" initialised weights for both experiments, this is not the same as a random sample.\nWe have added a clarification in Table 3 as to where the residual blocks are added in our experiments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287663046, "id": "ICLR.cc/2017/conference/-/paper256/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJIMPr9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper256/reviewers", "ICLR.cc/2017/conference/paper256/areachairs"], "cdate": 1485287663046}}}, {"tddate": null, "tmdate": 1481134433688, "tcdate": 1481134433680, "number": 1, "id": "BkqPZ0HQe", "invitation": "ICLR.cc/2017/conference/-/paper256/public/comment", "forum": "SJIMPr9eg", "replyto": "SJoA5gW7l", "signatures": ["~Alan_Mosca1"], "readers": ["everyone"], "writers": ["~Alan_Mosca1"], "content": {"title": "revision added", "comment": "We have uploaded a new revision of the paper which addresses most of the comments. Details of network architecture at round 1 are given in Table 3, while details of the additional convolutional block (which in BRN is a residual block) are given in Table 4. Details of the training are given in section 4.\n\nWe did not compare to state-of-the-art ResNet variants, because our goal was to show the ensemble algorithm. We have added a comment about this in section 4.\n\nComparisons for training time are given in Table 2.\n\nWe did not have results on ImageNet at the time of writing, however, we are currently running the experiments and will add the results as a new revision as soon as they have finished running."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287663046, "id": "ICLR.cc/2017/conference/-/paper256/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJIMPr9eg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper256/reviewers", "ICLR.cc/2017/conference/paper256/areachairs"], "cdate": 1485287663046}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481133776657, "tcdate": 1478280974354, "number": 256, "id": "SJIMPr9eg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJIMPr9eg", "signatures": ["~Alan_Mosca1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480817362702, "tcdate": 1480817362696, "number": 2, "id": "SJoA5gW7l", "invitation": "ICLR.cc/2017/conference/-/paper256/official/comment", "forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "signatures": ["ICLR.cc/2017/conference/paper256/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper256/AnonReviewer2"], "content": {"title": "comparisons and setup", "comment": "- Can you give the details of the experiment setup e.g. parameters to be tuned, algorithm to train at each step of boosting etc? Also can you give the details of networks architecture and references? \n- Can you elaborate on comparison to state of resNet variants, dense convolutional network?\n- Can you give also comparison on training time?\n- Do you have any result on Imagenet?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287662906, "id": "ICLR.cc/2017/conference/-/paper256/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJIMPr9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper256/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper256/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper256/reviewers", "ICLR.cc/2017/conference/paper256/areachairs"], "cdate": 1485287662906}}}, {"tddate": null, "tmdate": 1480632674543, "tcdate": 1480632674538, "number": 1, "id": "r1qvFQCzl", "invitation": "ICLR.cc/2017/conference/-/paper256/pre-review/question", "forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "signatures": ["ICLR.cc/2017/conference/paper256/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper256/AnonReviewer1"], "content": {"title": "Terminology and comparisons", "question": "- Can you clarify the definition of, and perhaps provide a citation for, the use of the phrase \"white box\"? \n- Do I understand correctly that residual blocks are always added sequentially (as in, they are placed after the most recently added block)?\n- How is the added residual block initialized?\n- In Figure 4, which you say illustrates BRN's consistent superiority to DIB, how many test set examples do these differences amount to? What are the Bernoulli confidence intervals on these estimates of the test accuracy given n = 10,000?\n- Table 3 is quite confusing. Where are the residual blocks added in these architectures?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Boosted Residual Networks", "abstract": "In this paper we present a new ensemble method, called Boosted Residual Networks,\nwhich builds an ensemble of Residual Networks by growing the member\nnetwork at each round of boosting. The proposed approach combines recent developements\nin Residual Networks - a method for creating very deep networks by\nincluding a shortcut layer between different groups of layers - with the Deep Incremental\nBoosting, which has been proposed as a methodology to train fast ensembles\nof networks of increasing depth through the use of boosting. We demonstrate\nthat the synergy of Residual Networks and Deep Incremental Boosting has better\npotential than simply boosting a Residual Network of fixed structure or using the\nequivalent Deep Incremental Boosting without the shortcut layers.", "pdf": "/pdf/2615de05c9edd0e70aa0950f3f1979e73958033a.pdf", "paperhash": "mosca|boosted_residual_networks", "conflicts": ["bbk.ac.uk"], "keywords": [], "authors": ["Alan Mosca", "George D. Magoulas"], "authorids": ["a.mosca@dcs.bbk.ac.uk", "gmagoulas@dcs.bbk.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959376860, "id": "ICLR.cc/2017/conference/-/paper256/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper256/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper256/AnonReviewer1"], "reply": {"forum": "SJIMPr9eg", "replyto": "SJIMPr9eg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper256/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper256/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959376860}}}], "count": 9}