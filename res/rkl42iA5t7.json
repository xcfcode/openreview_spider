{"notes": [{"id": "H1gNVJS1HV", "original": null, "number": 11, "cdate": 1549909804280, "ddate": null, "tcdate": 1549909804280, "tmdate": 1549909804280, "tddate": null, "forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "content": {"title": "Closing comments", "comment": "Thanks to the reviewers and the AC for their feedback. This process of review has strengthened and clarified our paper significantly.  Closing comments about the final review topics can be found below:\n\n1. \"Results on large-scale tasks such as Imagenet\" was addressed in the rebuttal, as acknowledge by the AC.\n\n2.  \"Compression after the fact may not be as good as training with a modified loss function that does compression jointly\u201d.  Without discussing here the merits and the drawbacks of changing the loss, in our comparisons, we showed that we obtain better results than other state of the art techniques, two of which do modify the loss function. There may be in the future techniques which effectively use modification of the loss, however, to the best of our knowledge, this statement is currently not supported by evidence.\n\n3.  \"Insufficient comparisons on ResNet architectures which make comparisons against previous works harder\u201d. The simplicity and improvements of our proposed technique are evident in the experiments we have presented. Experiments show that our technique achieves better results on a variety of architectures (a simple CNN, VGG-16 and ResNet) and datasets (CIFAR10, CIFAR100 and ImageNet).  ResNet on ImageNet (we used VGG-16 instead) is the only missing combination. As we expand our presented results, we look forward to covering all experiments which reviewers feel are critical.  As it is always possible to describe a subjectively useful or missing experiment, we hope the community will continue to revise evaluations criteria to emphasize progress rather than coverage."}, "signatures": ["ICLR.cc/2019/Conference/Paper700/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619414, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper700/Authors|ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619414}}}, {"id": "rkl42iA5t7", "original": "H1g_KrhcKm", "number": 700, "cdate": 1538087851700, "ddate": null, "tcdate": 1538087851700, "tmdate": 1545355439731, "tddate": null, "forum": "rkl42iA5t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1etpz_ex4", "original": null, "number": 1, "cdate": 1544745665241, "ddate": null, "tcdate": 1544745665241, "tmdate": 1545354477596, "tddate": null, "forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Meta_Review", "content": {"metareview": "The authors propose a technique for compressing neural networks by examining the correlations between filter responses, by removing filters which are highly correlated. This differentiates the authors\u2019 work from many other works which compress the weights independent of the task/domain.\n\nStrengths:\nClearly written paper\nPFA-KL does not require additional hyperparameter tuning (apart from those implicit in choosing \\psi)\nExperiments demonstrate that the number of filters determined by the algorithm scale with complexity of the task\n\nWeaknesses:\nResults on large-scale tasks such as Imagenet (subsequently added by the authors during the rebuttal period)\nCompression after the fact may not be as good as training with a modified loss function that does compression jointly\nInsufficient comparisons on ResNet architectures which make comparisons against previous works harder\n\nOverall, the reviewers were in agreement that this work (particularly, the revised version) was close to the acceptance threshold. In the ACs view, the authors addressed many of the concerns raised by the reviewers in the revisions. However, after much deliberation, the AC decided that the weaknesses 2, and 3 above were significant, and that these should be addressed in a subsequent submission.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting approach to compression based on analyzing filter activations."}, "signatures": ["ICLR.cc/2019/Conference/Paper700/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper700/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353120504, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353120504}}}, {"id": "BJlCzy7q37", "original": null, "number": 2, "cdate": 1541185302295, "ddate": null, "tcdate": 1541185302295, "tmdate": 1544222754167, "tddate": null, "forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Review", "content": {"title": "A decent pruning strategy.", "review": "The paper proposes to prune convolutional networks by analyzing the observed correlation between the filters of a same layer as expressed by the eigenvalue spectrum of their covariance matrix. The authors propose two strategies to decide of a compression level, one based on an eigenvalue threshold, the other one based on a heuristic based on the KL divergence between the observed eigenvalue distribution and the uniform one. This is a bit bizarre but does not require searching a parameter. Once one has decided the number of filters to keep, one can either retrain the network from scratch, or iteratively remove the most correlated filters, which, unsurprisingly, work better.\n\nThe authors perform credible experiments on CIFAR-10 and CIFAR-100 that show the results one would expect. They should probably have run ImageNet experiments because many earlier papers on this topic use it as a benchmark and because the ImageNet size often reveals different behaviors.\n\nIn conclusion, this is a very decent paper, but not a very exciting one.\n\n-------------\n\nAfter reading the authors' response and their additional experiments, I still see this work as a very decent paper, but not a very exciting one. This is why I rank this paper somewhat above the acceptance threshold. I could be proven wrong if this approach becomes the method of choice to prune networks, but I would need to see a lot more comparisons to be convinced. \n\n \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper700/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Review", "cdate": 1542234399768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783648, "tmdate": 1552335783648, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xvnmK414", "original": null, "number": 9, "cdate": 1543963567220, "ddate": null, "tcdate": 1543963567220, "tmdate": 1543963567220, "tddate": null, "forum": "rkl42iA5t7", "replyto": "SylHM_0xJV", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "content": {"title": "ResNet-34 vs ResNet-56", "comment": "Dear reviewer,\n\nAfter exploring the experimental settings of the different state of the art techniques used in our comparison, we see ResNet-34 is not typically used with CIFAR, so unfortunately adding ResNet-34 experiments will not provide additional comparisons.\n\nAlmost all papers present results on CIFAR solely with VGG-16. Only us and Li et al. additionally report results with ResNet (and we both use ResNet-56). \n\nPapers that use ResNet-34 do so mainly for ImageNet and often include VGG-16 (which we have for comparison). \n\nIn order to augment Table 1 (as you suggest), we will include results for ResNet-34 on ImageNet.\n\nThank you once more."}, "signatures": ["ICLR.cc/2019/Conference/Paper700/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619414, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper700/Authors|ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619414}}}, {"id": "ryl3i9nH37", "original": null, "number": 1, "cdate": 1540897443822, "ddate": null, "tcdate": 1540897443822, "tmdate": 1543901231967, "tddate": null, "forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Review", "content": {"title": "Interesting hyper-parameter free compression, but missing experiments and clarification/corrections needed", "review": "This paper introduces an approach to compressing a trained neural network by looking at the correlation of the filter responses in each layer. Two strategies are proposed: one based on trying to preserve the energy of the original activations and one based on looking at the KL divergence between the normalized eigenvalues of the activation covariance matrix and the uniform distribution.\n\nStrengths:\n- The KL-divergence-based method is novel and has the advantage of not requiring to define any hyper-parameter.\n- The results show the good behavior of the approach.\n\nWeaknesses:\n\nMethod:\n- One thing that bothers me is the spatial max pooling of the activations of convolutional layers. This means that is two filters have high responses on different regions of the input image, they will be treated as correlated. I do not understand the intuition behind this.\n- In Section 2, the authors mention that other methods have also proposed to take the activation into account for pruning, but that they aim to minimize the reconstruction error of these activations. In fact, this is also what PFA-En does; for a given dimension, PCA gives the representation that minimizes the reconstruction error. Therefore, the connection between this method and previous works is stronger than claimed by the authors.\n- While it is good that the KL-divergence-based method does not rely on any hyper-parameter, the function \\psi used in Eq. 3 seems quite ad hoc. As such, there has also been some manual tuning of the method.\n\nExperiments:\n- In Table 1, there seems to be a confusion regarding how the results of FGA are reported. First, in (Peng et al., 2018), the %FLOPS is reported the other way around, i.e., the higher the better, whereas here the lower the better. Similarly, in (Peng et al., 2018), a negative \\Delta in accuracy means an improved performance (as stated in the caption of their Table 2, where the numbers reported here were taken). As such, the numbers reported here, and directly taken from this work, are misinterpreted. \n- Furthermore, Peng et al., 2018 report much better compression results, with %FLOP compression going up to 88.58%. Why are these results not reported here? (To avoid any misunderstanding, I would like to mention that I am NOT an author of (Peng et al., 2018)).\n- Many of the entries in Table 1 are empty due to the baselines not reporting results on these datasets or with the same network. This makes an actual comparison more difficult.\n- Many compression methods report results on ImageNet. This would make this paper more convincing.\n- While I appreciate the domain adaptation experiments, it would be nice to see a comparison with Masana et al., 2017, which also considers the problem of domain adaptation with network compression and, as mentioned in Section 2, also makes use of the activations to achieve compression.\n\nRelated work:\n- It is not entirely clear to me why tensor factorization methods are considered being so different from the proposed approach. In essence, they also perform structured network pruning.\n- The authors argue that performing compression after having trained the model is beneficial. This is in contrast with what was shown by Alvarez & Salzmann, NIPS 2017, where incorporating a low-rank prior during training led to higher compression rates.\n- The authors list (Dai et al., 2018) as one of the methods that aim to minimize the reconstruction error of the activations. Dai et al., 2018 rely on the mutual information between the activations in different layers to perform compression. It is not entirely clear to me how this relates to reconstruction error.\n\nSummary:\nI do appreciate the idea of aiming for a hyper-parameter-free compression method. However, I feel that there are too many points to be corrected or clarified and too many missing experiments for this paper to be accepted to ICLR.\n\nAfter Response:\nI appreciate the authors' response, which clarified several of my concerns. I would rate this paper as borderline. My main concern now is that the current comparison with existing method still seems too incomplete, especially with ResNet architectures, to really draw conclusions. I would therefore encourage the authors to revise their paper and re-submit it to an upcoming venue.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper700/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Review", "cdate": 1542234399768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783648, "tmdate": 1552335783648, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xCdNmQJE", "original": null, "number": 7, "cdate": 1543873653672, "ddate": null, "tcdate": 1543873653672, "tmdate": 1543873653672, "tddate": null, "forum": "rkl42iA5t7", "replyto": "SylHM_0xJV", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "content": {"title": "Further clarifications", "comment": "Thank you for your further feedback.\n\n> Can you provide the full reference to Jordao et al., 2018?\n\nThis is the full title and link: \u201cPruning Deep Neural Networks using Partial Least Squares\u201d (https://arxiv.org/pdf/1810.07610v1.pdf). We will add this reference to our paper too.\n\n> Peng et al., 2018 report results in terms of FLOP compression. For completeness of the comparison, it would be interesting to also have these numbers for the detailed results you provided above\n\nThe information about FLOPs is already available in Table 1. In summary: Peng's algorithm (FGA) achieves a better FLOPs compression than our proposal (PFA), while PFA achieves a better footprint reduction and accuracy improvement  than FGA. \n\nSpecifically, the FLOPs of the compressed model with respect to the original architecture are 43% for FGA  (i.e. 57% reduction) and 52.7% for PFA (i.e., 47.3% reduction). On the other hand, the footprint of the compressed model with respect to the original architecture is 74.4% for FGA (i.e. 25.6% reduction) and  69.3% for PFA (i.e., 30.7% reduction). Finally the Top-1 and Top-5 accuracy improvement with respect to the original model are 0.82% and 0.94% respectively for FGA, and 2.39% and 1.41% respectively for PFA. \n\n> While I agree, several papers report results using a ResNet-34. Your choice of a ResNet-56, for which there are almost no other results, seems strange.\n\nThank you for pointing out this aspect. We have provided results on ResNet-56 because we followed the experimental settings of Li et al. for CIFAR-10. In retrospect we agree that ResNet-34 would have been a more popular choice. We do not expect the conclusions about PFA to change with ResNet-34. We believe we have provided enough quantitative and qualitative information for a reader to form her or his own judgment  about how PFA compares with the state of art. We are, however, happy to do those experiments and report the new results in the paper if this helps to remove any doubts.\n\nThank you again for your review, we appreciate the time you invested in it."}, "signatures": ["ICLR.cc/2019/Conference/Paper700/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619414, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper700/Authors|ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619414}}}, {"id": "SylHM_0xJV", "original": null, "number": 6, "cdate": 1543723021028, "ddate": null, "tcdate": 1543723021028, "tmdate": 1543723021028, "tddate": null, "forum": "rkl42iA5t7", "replyto": "rJxU4AU76X", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "content": {"title": "A few more comments", "comment": "Thank you for the clarification and for the new results. I nonetheless have a few more comments/questions:\n\n- Can you provide the full reference to Jordao et al., 2018? It does not seem to be cited in the paper.\n\n- Peng et al., 2018 report results in terms of FLOP compression. For completeness of the comparison, it would be interesting to also have these numbers for the detailed results you provided above.\n\n- Regarding Table 1, you mentioned that there is no agreed benchmark. While I agree, several papers report results using a ResNet-34. Your choice of a ResNet-56, for which there are almost no other results, seems strange.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper700/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper700/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619414, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper700/Authors|ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619414}}}, {"id": "rJxU4AU76X", "original": null, "number": 3, "cdate": 1541791278450, "ddate": null, "tcdate": 1541791278450, "tmdate": 1542322340745, "tddate": null, "forum": "rkl42iA5t7", "replyto": "ryl3i9nH37", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "content": {"title": "Answers to comments on weaknesses", "comment": "Thank you for your comments. We appreciate your review.\n\n> ... spatial max pooling... I do not understand the intuition behind this.\n\nPooling is a relaxation to ease the next step in the process. Jordao et al. 2018 compares different forms of pooling for compression: global max pooling (as in PFA), avg pooling and a spatial preserving max pooling. They observe that global max pooling performs the best.\n\nThe intuition is that if two filters are correlated they might be redundant for the end task, even if they learn different features. For example, in order to decide if an image contains a face there is no need to detect nose, mouth,  eyes, etc... one (or more) of these features might be sufficient. That said, we agree that exploring alternatives to max pooling is a potential for future research.\n\n> ... other methods have also proposed to take the activation into account for pruning, ... but they aim to minimize the reconstruction error.... In fact, this is also what PFA-En does;\n\nPFA-En uses the spectral energy of the filters' responses only to decide how many filters should be preserved. Our filter selection does not account for the reconstruction error.\n\n> While it is good that the KL-divergence-based method does not rely on any hyper-parameter, the function \\psi used in Eq. 3 seems quite ad hoc. As such, there has also been some manual tuning of the method.\n\n\\psi could be tuned to a given task, though we do not. The proposed \\psi is the function that empirically worked the best in an initial evaluation, and has not been tuned in any of our experiments.\n\n> In Table 1, there seems to be a confusion regarding how the results of FGA are reported. ...\n\nThank you for spotting this mistake.\n\n> Peng et al., 2018 report much better compression results, with %FLOP compression going up to 88.58%. Why are these results not reported here?\n\nTable 1 is meant to provide a numerical comparison for similar compression rates. Fig. 2b provides all compression rates for FGA (Peng et al.). However, we made the same mistake in reporting the numbers for FGA. Here is the correct comparison:\n     Footprint; Accuracy change\nFGA  11.31%;   -1.93%\nPFA   9.64%;    -2.37%\n\nFGA  13.20%;    -0.57%\nPFA  14.37%%;  -0.27%\n\nFGA  18.54%;    -0.02%\nPFA  19.27%;    +0.50%\n\nFGA  39.67%;    +0.39%\nPFA  43.11%;    +1.70%\n\n> Many of the entries in Table 1 are empty ... This makes an actual comparison more difficult.\n\nWe agree. Sadly there is no agreed benchmark. We hope, that the amount of comparison provided is enough for the reader to form an opinion. Such opinion should also be influenced by other factors (beyond numbers): easy of implementation, practicality, ease of parameters tuning, etc...\n\n> Many compression methods report results on ImageNet. This would make this paper more convincing.\n\nWe have just completed the experiments on ImageNet. Here is a summary:\n                                                        Footprint;  Top1 change; Top5 change\nPFA-KL from scratch                   69.30%\t    -1.89%             -0.97%\nPFA-KL with filter selection        69.30%\t    +2.39%            +1.41%\n\n> While I appreciate the domain adaptation experiments, it would be nice to see a comparison with Masana et al., 2017.\n\nOur results on domain adaptation are meant to support our claim that with PFA different complexities in the tasks lead to different compressions.\n\n> It is not entirely clear to me why tensor factorization methods are considered being so different from the proposed approach.\n\nIn the paper, by \u201ctensor factorization\u201d we refer to those algorithms that split weight tensor into a sequence of smaller tensors. By \u201cstructured pruning\u201d we refer to those algorithms (like PFA) that remove full filters from the current layer, without attempting to replace or approximate them. This terminology follows other discussions, such as Peng et al. 2018, Liu et al. 2017, and Li et al. 2017.\n\n> The authors argue that performing compression after having trained the model is beneficial. This is in contrast with what was shown by Alvarez&Salzmann, NIPS 2017.\n\nAlvarez&Salzmann (A&S) claim that is beneficial to modify the original loss in order to induce some properties in the full model in order to ease compression. This is not in contrast with our findings. The difference between A&S and PFA is that PFA does not need to modify the loss. From here on, the workflow is the same: we both compress after training and fine-tune the compressed model.\n\n> I do appreciate the idea of aiming for a hyper-parameter-free compression method. However, I feel that there are too many points to be corrected or clarified and too many missing experiments for this paper to be accepted to ICLR.\n\nThank you for your time and consideration; we hope you will find our answers and new experiments satisfactory. We believe our work is stronger after addressing your concerns. Please let us know if you have further questions and consider updating your rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper700/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619414, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper700/Authors|ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619414}}}, {"id": "HJxNw2I7Tm", "original": null, "number": 2, "cdate": 1541790811532, "ddate": null, "tcdate": 1541790811532, "tmdate": 1542322325131, "tddate": null, "forum": "rkl42iA5t7", "replyto": "BJlCzy7q37", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "content": {"title": "Preview of the results on ImageNet", "comment": "Thank you for your positive comments and for your feedback and time. Please find a preview of the results on ImageNet below.\n\n> They should probably have run ImageNet experiments because many earlier papers on this topic use it as a benchmark and because the ImageNet size often reveals different behaviors. \n\nWe agree with the reviewer. We have just completed the experiments on ImageNet. Here is a summary of the results.\n\n                                                        Footprint;  Top1 change; Top5 change\nPFA-KL from scratch                   69.30%\t    -1.89%             -0.97%\nPFA-KL with filter selection        69.30%\t    +2.39%            +1.41%\n\nWe will report these results. In addition to the experiments on ImageNet, are there other enhancements that would elevate the paper?\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper700/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619414, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper700/Authors|ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619414}}}, {"id": "Skeeao8ma7", "original": null, "number": 1, "cdate": 1541790648514, "ddate": null, "tcdate": 1541790648514, "tmdate": 1542322313106, "tddate": null, "forum": "rkl42iA5t7", "replyto": "HJlRitO53Q", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "content": {"title": "Evidence of our claim, and preview of the results on ImageNet (including complexity analysis and actual times)", "comment": "We appreciate your review and would like to thank you for your feedback and time. Please find our answers below.\n\n> Therefore, in theory (nothing shown), different task would provide different outputs while similar works would compress in the same manner.\n\nThe experiments presented in section 4.5 and Figure 3 provide evidence that with PFA different complexities in the tasks lead to different architectures; similar tasks lead to similar architectures. We will clarify the connection between the claim and Section 4.5. In those experiments, we show how the architecture produced by PFA (starting from the same trained model) differs depending on the complexity of the task. For example, VGG-16 trained on CIFAR-100 and then compressed using 10 labels (R1 line in Figure 3.b) has 150 filters in the last layer (and overall it has 52% of the original filters), whereas a simpler task with 2 labels (S4 line in Figure 3.b) has only 45 filters in the last layer (and overall it has 36% of the original filters). The compression obtained in the two cases is clearly different and reflects the complexity of the tasks with respect to what the trained model has already learnt. On the other hand, the two tasks with 10 labels (R1 and R2 lines) lead to similar compression.\n\n> I still miss results in larger systems including imagenet. How all this approach actually scales with the complexity of the network and the task?\n\nWe have just completed the experiments on ImageNet. Here is a summary. In terms of complexity, we have a dedicated section in Appendix C. However, we will consider move the main conclusions to the main paper. In summary, the complexity of the PFA algorithm per layer is O(mn^2 + n^3), where n is the number of filters, and m is the number of samples. The task does not affect the complexity because the labels are not used in PFA. For the experiment on ImageNet (m=1.2M, ILSVRC2012) PFA took the longest on the last two layers (n = 4096) which were computed in roughly 120 seconds. Here is the full table of times when computing PFA sequentially (non-parallel CPU implementation):\n\nblock 0 (64 filters)\nconv0: 1.19s\nconv1: 1.28s\n\nblock 1 (128 filters)\nconv0: 2.74s\nconv1: 2.83s\n\nblock 2 (256 filters)\nconv0: 4.26s\nconv1: 4.83s\nconv2: 4.78s\n\nblock 3 (512 filters)\nconv0: 8.92s\nconv1: 9.43s\nconv2: 9.92s\n\nblock 4 (512 filters)\nconv0: 9.18s\nconv1: 9.22s\nconv2: 9.22s\n\nfully connected block (4096 filters)\nfc1: 142.17s\nfc2: 112.74s\n\nAs mentioned in Appendix C, PFA has to run once at the end of the training step and as shown above the time consumed by PFA is a negligible compared to the whole training time. In exchange for this marginal extra-time PFA provides the long-term benefit of a smaller footprint and faster inference, which in the lifetime of a deployed network will quickly surpass the time initially required by PFA. In addition, when working in a setting where the network is periodically re-trained with new incoming data, having a smaller network will add to the saved time.\n\nIn terms of performance, these are the results of PFA-KL on ImageNet\n                                                        Footprint;  Top1 change; Top5 change\nPFA-KL from scratch                   69.30%\t    -1.89%             -0.97%\nPFA-KL with filter selection        69.30%\t    +2.39%            +1.41%\n\n> There have been recent approaches incorporating low-rank approximations that would be interesting to couple with this approach. I am surprissed these are not even cited ('Compression aware training' at NIPS 2017 or Coordinating filters at ICCV2017 both with a similar approach (based on weights tho). Pairing with those approaches seems a strong way to improve your results.\n\nThank you for the references. We will add them to our state-of-the-art review. Both techniques are interesting and could further improve the compression rate of PFA. The approach of modifying the original loss in order to induce a specific property in the full model is smart but goes against the philosophy that we have adopted for PFA. We envision PFA to be easy to use: no-parameter strategy (PFA-KL), no need to modify the original loss function (which would require additional hyper-parameters tuning), and the ability to start from pre-trained models or to use known training hyper-parameters. Nevertheless we are eager to consider how these other techniques could be paired with PFA in future work and see what level of improvement can be achieved.\n\nWe hope we have answered all your concerns. Please let us know if you think there are more opportunities for improvement."}, "signatures": ["ICLR.cc/2019/Conference/Paper700/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619414, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper700/Authors|ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619414}}}, {"id": "BkgKD7doaX", "original": null, "number": 4, "cdate": 1542320993129, "ddate": null, "tcdate": 1542320993129, "tmdate": 1542320993129, "tddate": null, "forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "content": {"title": "New version of the manuscript available", "comment": "We have uploaded a new version of our paper to address reviewers' comments. Here are the highlights of the changes:\n\n- We have added the references mentioned by the reviewers.\n- We have clarified a few sentences to avoid misunderstandings.\n- We have included a section regarding the complexity analysis of PFA, as well as the actual time required by PFA to run on each layer of VGG-16 using the ImageNet dataset.\n- We have included compression results on the ImageNet dataset.\n- We have corrected the numbers in Table 1 regarding the FGA algorithm.\n\nThanks to the changes recommended by the reviewers, the claims in the paper are strengthened.  Independently of the architecture or dataset, PFA consistently provides better compression or accuracy than the state of the art.\n\nWe would like to thank the reviewers once more for their valuable feedback. We hope they will find the changes satisfactory or we will wait for new feedback."}, "signatures": ["ICLR.cc/2019/Conference/Paper700/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619414, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkl42iA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper700/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper700/Authors|ICLR.cc/2019/Conference/Paper700/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers", "ICLR.cc/2019/Conference/Paper700/Authors", "ICLR.cc/2019/Conference/Paper700/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619414}}}, {"id": "HJlRitO53Q", "original": null, "number": 3, "cdate": 1541208486491, "ddate": null, "tcdate": 1541208486491, "tmdate": 1541533762409, "tddate": null, "forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper700/Official_Review", "content": {"title": "Interesting approach, second time reviewing", "review": "This paper proposes a compression method based on spectral analysis. The basic idea is to analyse correlation between responses of difference layers and select those that are more relevant discarding the others. That, in principle (as mentioned in the paper) differs from other compression methods based on compressing the weights independently of the data being used. Therefore, in theory (nothing shown), different task would provide different outputs while similar works would compress in the same manner. \n\nThen, the paper proposes a greedy algorithm to select those filters to be kept rather than transforming the layer (as it has been usually done in the past [Jaderberg et al]). This is interesting (from a practical point of view) as would lead to direct benefits at inference time. \n\nThis is the second time i review this paper. I appreciate the improvements from the first submission adding some interesting results. \n\nI still miss results in larger systems including imagenet. How all this approach actually scales with the complexity of the network and the task?\n\nThere have been recent approaches incorporating low-rank approximations that would be interesting to couple with this approach. I am surprissed these are not even cited ('Compression aware training' at NIPS 2017 or Coordinating filters at ICCV2017 both with a similar approach (based on weights tho). Pairing with those approaches seems a strong way to improve your results. \n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper700/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK COMPRESSION USING CORRELATION ANALYSIS OF LAYER RESPONSES", "abstract": "Principal Filter Analysis (PFA) is an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprint. We propose two compression algorithms: the first allows a user to specify the proportion of the original spectral energy that should be preserved in each layer after compression, while the second is a heuristic that leads to a parameter-free approach that automatically selects the compression used at each layer. Both algorithms are evaluated against several architectures and datasets, and we show considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. In our tests we also demonstrate that networks compressed with PFA achieve an accuracy that is very close to the empirical upper bound for a given compression ratio. Finally, we show how PFA is an effective tool for simultaneous compression and domain adaptation.", "keywords": ["Artificial Intelligence", "Deep learning", "Machine learning", "Compression"], "authorids": ["xsuaucuadros@apple.com", "lzappella@apple.com", "napostoloff@apple.com"], "authors": ["Xavier Suau", "Luca Zappella", "Nicholas Apostoloff"], "TL;DR": "We propose an easy to implement, yet effective method for neural network compression. PFA exploits the intrinsic correlation between filter responses within network layers to recommend a smaller network footprints.", "pdf": "/pdf/86af237b32e7e38780eda929b8127ae2954eb0f0.pdf", "paperhash": "suau|network_compression_using_correlation_analysis_of_layer_responses", "_bibtex": "@misc{\nsuau2019network,\ntitle={{NETWORK} {COMPRESSION} {USING} {CORRELATION} {ANALYSIS} {OF} {LAYER} {RESPONSES}},\nauthor={Xavier Suau and Luca Zappella and Nicholas Apostoloff},\nyear={2019},\nurl={https://openreview.net/forum?id=rkl42iA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper700/Official_Review", "cdate": 1542234399768, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkl42iA5t7", "replyto": "rkl42iA5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper700/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335783648, "tmdate": 1552335783648, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper700/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 13}