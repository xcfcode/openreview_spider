{"notes": [{"id": "H1lBYCEFDB", "original": "ByxXnQ__wB", "number": 1247, "cdate": 1569439357459, "ddate": null, "tcdate": 1569439357459, "tmdate": 1577168243887, "tddate": null, "forum": "H1lBYCEFDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LB86Nt4JaL", "original": null, "number": 1, "cdate": 1576798718517, "ddate": null, "tcdate": 1576798718517, "tmdate": 1576800918047, "tddate": null, "forum": "H1lBYCEFDB", "replyto": "H1lBYCEFDB", "invitation": "ICLR.cc/2020/Conference/Paper1247/-/Decision", "content": {"decision": "Reject", "comment": "The authors analyze the natural gradient algorithm for training a neural net from a theoretical perspective and prove connections to the K-FAC algorithm. The paper is poorly written and contains no experimental evaluation or well established implications wrt practical significance of the results.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1lBYCEFDB", "replyto": "H1lBYCEFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716493, "tmdate": 1576800266649, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1247/-/Decision"}}}, {"id": "H1ehOq-ssB", "original": null, "number": 4, "cdate": 1573751411698, "ddate": null, "tcdate": 1573751411698, "tmdate": 1573751411698, "tddate": null, "forum": "H1lBYCEFDB", "replyto": "Byx-sXrqjB", "invitation": "ICLR.cc/2020/Conference/Paper1247/-/Official_Comment", "content": {"title": "Response to author feedback", "comment": "I stick with my assessment, which seems pretty consistent with the others, except that the first reviewer even pointed out several flaws (buried in the mass of maths, apparently).\n\nI maintain that this paper has very little chance of creating any downstream impact, because it just offers another way to look at K-FAC, it does not give improvements, or even lead the way to them. The maths for this way is so heavy I doubt it will lead to any real progress. AI/ML is not pure math, we need to focus on tangible improvements in practice, or explain why heuristics work.\n\nI'd encourage the authors to use their new viewpoint grounded in pure math to develop some useful extension or improvement of K-FAC and demonstrate that in serious experiments, and resubmit elsewhere.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1247/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1247/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lBYCEFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference/Paper1247/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1247/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1247/Reviewers", "ICLR.cc/2020/Conference/Paper1247/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1247/Authors|ICLR.cc/2020/Conference/Paper1247/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158946, "tmdate": 1576860549633, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference/Paper1247/Reviewers", "ICLR.cc/2020/Conference/Paper1247/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1247/-/Official_Comment"}}}, {"id": "Byx-sXrqjB", "original": null, "number": 3, "cdate": 1573700505339, "ddate": null, "tcdate": 1573700505339, "tmdate": 1573700505339, "tddate": null, "forum": "H1lBYCEFDB", "replyto": "BJlVUPfMKr", "invitation": "ICLR.cc/2020/Conference/Paper1247/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We would like to thank the reviewer for carefully reading our manuscript and their feedback. We address them below:\n\nFirst, please see our response to Reviewer 2 concerning the practical significance of our work. K-FAC is essentially the only scalable and low-overhead method available for applying second-order optimization to large neural networks (eg. ImageNet classifiers), so developing a systematic understanding of its intrinsic properties is important. Since neural networks and optimization are among core topics at ICLR and invariances are among the topics neural network optimization researchers are most interested in understanding; our work is certainly aligned with the scope of ICLR.\n\n\u201cHeavy math\u201d:\n\nWe agree that the math is indeed heavy for a machine learning audience but differential geometry and abstract linear + affine algebra are the appropriate tools for the job. As we explained in Section 2 of the paper, the reason why invariance properties of natural gradient are obtained automatically is due to the fact that it admits an intrinsic differential-geometric construction. The invariances of K-FAC, as an approximation to natural gradient, should be derived in exactly the same way; as opposed to manual matrix-algebra manipulations which offer little insight to the true nature of the algorithm. \n\nMoreover, we wrote the paper as self-contained as possible; we provided background sections in the Appendix covering all the necessary mathematical concepts and tools needed for this paper.\n\n\u201cStochastic estimates of K-FAC/Fisher matrix\u201d:\n\nOur current theory in this paper is based on taking the expectation over inputs on the output Fisher matrix (actually our setup covered pullback metrics which is much more general). We agree that it would be interesting to investigate the stochastic version; however, we believe this to be outside the scope of our paper. The recent paper [1] demonstrates that taking empirical estimates of the Fisher matrix is not only a very different mathematical object than the Fisher matrix itself; but it can also behave quite differently even on simple optimization problems over toy models.\n\n[1] Kunstner, Frederik, Lukas Balles, and Philipp Hennig. \"Limitations of the Empirical Fisher Approximation.\" arXiv preprint arXiv:1905.12558 (2019).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1247/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lBYCEFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference/Paper1247/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1247/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1247/Reviewers", "ICLR.cc/2020/Conference/Paper1247/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1247/Authors|ICLR.cc/2020/Conference/Paper1247/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158946, "tmdate": 1576860549633, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference/Paper1247/Reviewers", "ICLR.cc/2020/Conference/Paper1247/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1247/-/Official_Comment"}}}, {"id": "r1eFpGr5oS", "original": null, "number": 2, "cdate": 1573700289057, "ddate": null, "tcdate": 1573700289057, "tmdate": 1573700289057, "tddate": null, "forum": "H1lBYCEFDB", "replyto": "Skx1SGmx5r", "invitation": "ICLR.cc/2020/Conference/Paper1247/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We would like to thank the reviewer for their comments and feedback. We address them below:\n\n\u201cWhy is invariance important?\u201d\n\nApplying transformations to a neural network; for example, replacing logistic activation functions with tanh, whitening the inputs or activations and batch normalization, has lead to significant speed-ups in optimization performance. The de-facto optimization method, Stochastic Gradient Descent (SGD), is not invariant to any of these transformations. It makes sense for us to use an optimization algorithm which is invariant to such transformations. In this paper, we provided a complete analysis of the invariance properties of K-FAC for a variety of network architectures; demonstrating that it is fully invariant (not just first-order) to all affine reparameterizations, which covers many of the common tricks and transformations used when training neural networks.\n\n\u201cSignificance + practical impact?\u201d\n\nOur current work is meant to be purely a theoretical study of the K-FAC algorithm and its invariance properties. The ultimate objective of adopting the intrinsic approach (framing everything in coordinate-free mathematical objects) is that it allows us to easily reason about invariance properties of the algorithm. Our framework allows a unified and cohesive way of studying K-FAC for different architectures and metrics (not limited to just the Fisher matrix/metric) all at once. All of the invariance proofs can be encapsulated into basic theorems and so we would not be stuck with heavy matrix algebra manipulations every time we wanted to derive invariance properties. Indeed, prior to our work, proving K-FAC invariance properties for MLPs and ConvNets in [1] and [2] respectively involved pages of tedious calculations (some of which were completely redundant with each other). \n\nFrom the practical standpoint, K-FAC is one of the few scalable approximations to natural gradient, and this approximation has been applied in many contexts besides deep learning optimization, such as Bayesian neural nets, continual learning, and network pruning. Analyzing the invariances carefully, as we have done in this paper, can help inform all of these use cases.\n\n[1] Martens, James, and Roger Grosse. \"Optimizing neural networks with kronecker-factored approximate curvature.\"\n[2] Grosse, Roger B., and James Martens. \"A Kronecker-factored approximate Fisher matrix for convolution layers.\"\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1247/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lBYCEFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference/Paper1247/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1247/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1247/Reviewers", "ICLR.cc/2020/Conference/Paper1247/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1247/Authors|ICLR.cc/2020/Conference/Paper1247/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158946, "tmdate": 1576860549633, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference/Paper1247/Reviewers", "ICLR.cc/2020/Conference/Paper1247/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1247/-/Official_Comment"}}}, {"id": "H1lLrfrqjS", "original": null, "number": 1, "cdate": 1573700158432, "ddate": null, "tcdate": 1573700158432, "tmdate": 1573700158432, "tddate": null, "forum": "H1lBYCEFDB", "replyto": "B1l2T3Ue5B", "invitation": "ICLR.cc/2020/Conference/Paper1247/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We would like to thank the reviewer for carefully reading our paper and their comments. We address them below:\n\nConcerning the following mathematical mistakes:\n\n- Equation 2.5 and update equation at the top of page 4: We have corrected this in the revised version. However, we want to stress that the update rule in Theorem 3.2, the one central to our study, is correct: subtraction is well-defined on an affine space and on an affine space, all tangent spaces at different points are canonically isomorphic and so the update makes sense. This theorem is what enables us to show that our constructed K-FAC metric is exactly invariant (not just first-order) to all affine reparameterizations of the network.\n\n- The abstract distribution P_{\\upsilon|\\psi}(\\omega); we think of (\\psi,\\upsilon) as an abstract input-target pair where \\psi and \\upsilon are on input and output spaces which are themselves smooth manifolds. We have added this explanation in the revised version.\n\n- Evaluating w at a on page 7: A, Z are affine space and W is the affine space whose elements comprise of affine maps between A and Z. The \u201ca\u201d here refers to an element of A so the evaluation map W \u2192 Z takes an element \u201cw\u201d in W; itself an affine mapping from A to Z, evaluates it at the element \u201ca\u201d, to give an element in Z. We have added this explanation also in the revised version.\n\nWe would like to mention that we spent lots of time and effort on how to present this work to a machine learning conference venue within the page limit; for example, choosing good notations which machine learning researchers are familiar with (as opposed to ones more conventional in mathematics) as well as devoting entire sections in the Appendix to explain all the necessary mathematical tools and concepts to ensure that the paper is as self-contained as possible. As mentioned at the end of the introduction, we realize that the paper frequently goes back and forth between coordinate-independent and dependent objects. Hence, to aid the reader, we provided a complete table summarizing the interplay between the two scenarios in Appendix A. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1247/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1lBYCEFDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference/Paper1247/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1247/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1247/Reviewers", "ICLR.cc/2020/Conference/Paper1247/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1247/Authors|ICLR.cc/2020/Conference/Paper1247/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504158946, "tmdate": 1576860549633, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1247/Authors", "ICLR.cc/2020/Conference/Paper1247/Reviewers", "ICLR.cc/2020/Conference/Paper1247/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1247/-/Official_Comment"}}}, {"id": "BJlVUPfMKr", "original": null, "number": 1, "cdate": 1571067724181, "ddate": null, "tcdate": 1571067724181, "tmdate": 1572972493712, "tddate": null, "forum": "H1lBYCEFDB", "replyto": "H1lBYCEFDB", "invitation": "ICLR.cc/2020/Conference/Paper1247/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper is concerned with tractable (approximate) forms of natural gradient updates for neural networks, in particular with the recent K-FAC approximation, which applies a set of approximation (layer-wise independence, Kronecker structure for affine maps) in order to obtain a Hessian that can be computed and inverted efficiently. K-FAC has been introduced for MLPs, and has previously been generalized to convolutional and certain recurrent NNs.\n\nThe stated goal of this paper is to provide a mathematical re-formulation of K-FAC in terms of Riemannian metrics. While K-FAC has been developed as approximation to the exact natural gradient update, they come up with a different Riemannian metric, definition of space, etc., such that in the end, K-FAC is the exact natural gradient for that. The authors here also obtain a more precise answer to invariance properties and, given some heavy maths, what they claim to be more elegant proofs of previously known properties of K-FAC.\n\nThe paper uses very heavy math, well \"over my head\" and likely most ICLR attendees. Along with me, they'll ask the obvious question of what this is good for. As far as I can see, there is nothing really new being proposed here in terms of practical consequences. The authors also do not make much effort to explain why their viewpoint is useful, say to obtain practically relevant insights in future work. So, as far as I am concerned, I do not see why this work should be of much relevance to ICLR, which is not an abstract maths conference.\n\nA final comment is that people have for a very long time tried to use second-order optimization for MLPs. The aspect that always was tricky there, is that SGD is *stochastic*, and the second-order info is hard to estimate from a mini-batch. The sets of approximations of K-FAC are pretty extreme, but they may just be needed to make things work in the end, because they may stabilize that \"stochastic inverse Fisher info matrix\" enough to not make the optimization process fail altogether. Now, all theoretical arguments, like \"invariance to this and that\", always ignore the crucial fact that you are talking about a stochastic estimate over a mini-batch, and your theory is always for E_x[...] \"being the truth\". It is not, it is just over a small mini-batch. I am not saying the additional theoretical insight from this work here (over previous K-FAC work), whatever it may be in the end, is not useful. I am just saying I'd be a lot more confident if the authors would specifically address the stochastic property."}, "signatures": ["ICLR.cc/2020/Conference/Paper1247/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1247/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lBYCEFDB", "replyto": "H1lBYCEFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576394180816, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1247/Reviewers"], "noninvitees": [], "tcdate": 1570237740162, "tmdate": 1576394180830, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1247/-/Official_Review"}}}, {"id": "Skx1SGmx5r", "original": null, "number": 2, "cdate": 1571988023105, "ddate": null, "tcdate": 1571988023105, "tmdate": 1572972493675, "tddate": null, "forum": "H1lBYCEFDB", "replyto": "H1lBYCEFDB", "invitation": "ICLR.cc/2020/Conference/Paper1247/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper analyzes the invariance properties of the K-FAC algorithm by reconstructing the algorithm in a coordinate-free way where the neural network is viewed as a series of affine mappings alternating with nonlinear activation functions. It converts the original metric into an approximate metric, whose coordinate representation matches the K-FAC approximation. So K-FAC can be viewed as the exact natural gradient under the new metric rather than an approximation under the Fisher metric.\n\nWhy is the invariance important? How does the proposed framework help us develop better algorithms? Without empirical studies, it is not easy\u00a0to see the significance of this work."}, "signatures": ["ICLR.cc/2020/Conference/Paper1247/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1247/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lBYCEFDB", "replyto": "H1lBYCEFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576394180816, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1247/Reviewers"], "noninvitees": [], "tcdate": 1570237740162, "tmdate": 1576394180830, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1247/-/Official_Review"}}}, {"id": "B1l2T3Ue5B", "original": null, "number": 3, "cdate": 1572003012146, "ddate": null, "tcdate": 1572003012146, "tmdate": 1572972493631, "tddate": null, "forum": "H1lBYCEFDB", "replyto": "H1lBYCEFDB", "invitation": "ICLR.cc/2020/Conference/Paper1247/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Natural gradient (NG) has been proven efficient in statistical learning, and one of its attractive properties is being invariant under smooth transformations of the parameter space. Computing NG is often difficult as one has to derive the Fisher matrix and its inverse. K-FAC offers an approximate method for approximating the NG with the risk of losing the invariant property.\n\nThis present paper offers a different approach: rather than approximating the exact NG directly as K-FAC does, it views the approximate K-FAC natural gradient as the exact \"natural gradient\" under the K-FAC metric. This is an interesting idea and the paper goes on to show that the new \"exact\" NG under the K-FAC metric (rather than the Fisher-Rao metric) is invariant under certain affine transformations.\n\nI find the paper extremely hard to follow. My main concern is that many concepts and math objects are at risk of being not mathematically rigorous or well defined. For example, equation (2.5) is not correct, this is not a genuine update based on exponential map. Similarly, the update equation in the first graph of page 4 doesn't make sense as \\mathcal{W} is an abstract manifold and the subtract here is not defined. The way paper is written has a high risk of causing confusion between coordinate-dependent and coordinate-free objects. On page 3, line -5, I have trouble understanding the abstract distribution P_{v|\\psi}(\\omega). On page 7, I don't understand what \"evaluating $w$ at $a$\" means, and even $a$ here isn't defined yet I think. In equation (2.3), $-w_k$ should be $w_k$ (i.e. no minus sign). There are many other typos and confusions that I opt not to  point out here.\n\nIn summary, the main issue of the paper in its current form is that its presentation isn't clear with many typos. I suggest the authors to re-write the paper carefully so that it is more accessible.\n\nI know that this paper focuses on theoretical properties of NG, but it would also be good if the authors offer some discussion on the use of their method in practice.\n\n  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1247/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1247/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kevin.kh.luk@gmail.com", "rgrosse@cs.toronto.edu"], "title": "A Coordinate-Free Construction of Scalable Natural Gradient", "authors": ["Kevin Luk", "Roger Grosse"], "pdf": "/pdf/fdf34b6c7974a5c2cf37092307eae848039e99d4.pdf", "TL;DR": "We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; exact affine invariances follows immediately.", "abstract": "Most neural networks are trained using first-order optimization methods, which are sensitive to the parameterization of the model. Natural gradient descent is invariant to smooth reparameterizations because it is defined in a coordinate-free way, but tractable approximations are typically defined in terms of coordinate systems, and hence may lose the invariance properties. We analyze the invariance properties of the Kronecker-Factored Approximate Curvature (K-FAC) algorithm by constructing the algorithm in a coordinate-free way. We explicitly construct a Riemannian metric under which the natural gradient matches the K-FAC update; invariance to affine transformations of the activations follows immediately. We extend our framework to analyze the invariance properties of K-FAC appied to convolutional networks and recurrent neural networks, as well as metrics other than the usual Fisher metric.", "keywords": ["Natural gradient", "second-order optimization", "K-FAC", "parameterization invariance", "deep learning"], "paperhash": "luk|a_coordinatefree_construction_of_scalable_natural_gradient", "original_pdf": "/attachment/e07407e2fb50d27c6b95a113e1ce65c3703a6428.pdf", "_bibtex": "@misc{\nluk2020a,\ntitle={A Coordinate-Free Construction of Scalable Natural Gradient},\nauthor={Kevin Luk and Roger Grosse},\nyear={2020},\nurl={https://openreview.net/forum?id=H1lBYCEFDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1lBYCEFDB", "replyto": "H1lBYCEFDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1247/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576394180816, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1247/Reviewers"], "noninvitees": [], "tcdate": 1570237740162, "tmdate": 1576394180830, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1247/-/Official_Review"}}}], "count": 9}